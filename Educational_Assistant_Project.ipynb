{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Educational Assistant Project: Leveraging Large Language Models in Education**\n",
        "\n",
        "## **Project Overview**\n",
        "\n",
        "### **Title**: Empowering Education in the Digital Age\n",
        "### **Authors**: Bibi Nur Muhamad, Elijah Kulpinski, Fernando Vargas\n",
        "### **Institution**: University of Wisconsin - Parkside, Kenosha WI USA\n",
        "### **Date**: 12/04/23\n",
        "\n",
        "#### **Abstract**\n",
        "This project, titled \"Empowering Education in the Digital Age,\" is a pioneering initiative aimed at enhancing the learning experience in computer science education through the use of Large Language Models (LLMs). Our goal is to develop an educational assistant that utilizes advanced AI techniques to provide personalized learning experiences, thereby improving student engagement and critical thinking skills.\n",
        "\n",
        "#### **Objective**\n",
        "The primary objective of this project is to explore the potential of LLMs in transforming educational methodologies. By integrating these models into educational content, we aim to create an interactive, adaptive learning environment that caters to the unique needs and learning styles of each student.\n",
        "\n",
        "#### **Methodology**\n",
        "The project encompasses a series of steps, beginning with the conversion of textbook content from PDF to XML format, followed by OCR processing of textbook datasets. We then transform Markdown content into XML for structured data representation. A significant part of our project involves generating conversation trees to simulate educational dialogues, which are vital for training our LLM. The conversation outputs in JSONL format are then converted to JSON for compatibility with various tools and platforms. The culmination of our project is the fine-tuning of the Mistral OpenOrca model in Google Colab, specifically tailored to our educational context.\n",
        "\n",
        "#### **Expected Outcomes**\n",
        "We anticipate that the integration of LLMs into educational content will result in a more engaging and effective learning experience. This project aims to demonstrate how AI can be used to create adaptive learning materials that respond to the needs of individual learners, thereby enhancing the overall quality of education.\n",
        "\n",
        "#### **Scope of This Notebook**\n",
        "This notebook serves as a comprehensive documentation and executable script of our entire research process. It is designed to ensure that our work is reproducible and well-documented. Each section of the notebook corresponds to a specific part of our project's workflow, complete with detailed explanations and code snippets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Navigating the Notebook**:\n",
        "1. **Textbook Conversion to XML**: Conversion of textbook data from PDF format to XML.\n",
        "2. **OCR Textbook Dataset Processing**: Processing of the OCR dataset of textbook content.\n",
        "3. **Markdown to XML Conversion**: Conversion of Markdown files into XML format.\n",
        "4. **Synthetic Conversation Branch Generation**: Generation of conversation branches for training the LLM.\n",
        "5. **JSONL to JSON Conversion**: Conversion of JSONL files to JSON format.\n",
        "6. **Fine-Tuning Mistral OpenOrca in Google Colab**: Tailoring the language model to our educational context.\n",
        "7. **[Experimental] Synthetic Conversation Tree Generation**: Generation of conversation trees that aren't 'good' or 'bad' but rather rated against the other possible responses in the tree for a relative rating to conversations in the tree. [Not Functioning Yet]\n",
        "\n",
        "---\n",
        "\n",
        "#### **Important Warning**:\n",
        "This notebook is a merged version of multiple individual scripts. As such, no extensive testing has been done to evaluate how these scripts interact with one another within a single Colab instance. Users should be aware that unexpected behaviors might occur due to script interactions or environmental differences. It is recommended that users thoroughly test each section individually before executing the entire notebook in sequence.\n",
        "\n",
        "#### **Additional Resources**:\n",
        "- The detailed research paper associated with this project can be found at [GitHub: DividesZero](https://github.com/DividesZero).\n",
        "- Trained models and datasets utilized in this project are available at [Hugging Face: ByteSized](https://huggingface.co/ByteSized).\n",
        "\n",
        "#### **Note**:\n",
        "This notebook is structured to be executed sequentially. Each section builds upon the previous one, ensuring a smooth flow and logical progression of the project. Users are encouraged to run the entire notebook in order to fully replicate our research findings."
      ],
      "metadata": {
        "id": "X_FmTu4JI1NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "#    Pip Installs for OCR and Model Fine-Tuning\n",
        "# ================================================\n",
        "\n",
        "# Importing display module from IPython for clear output functionality\n",
        "from IPython import display\n",
        "\n",
        "# -------------------------------------\n",
        "# Download Facebook's Nougat OCR Model\n",
        "# -------------------------------------\n",
        "# Facebook's Nougat is an OCR model that we use for text extraction from textbook images.\n",
        "# This step installs the Nougat library from its GitHub repository.\n",
        "!pip install git+https://github.com/facebookresearch/nougat\n",
        "# Clearing the output to maintain a clean and readable notebook environment.\n",
        "display.clear_output()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Download Fine-Tuning Libraries from Hugging Face\n",
        "# -------------------------------------------------\n",
        "# Hugging Face's 'transformers' library provides a collection of state-of-the-art\n",
        "# pre-trained models for Natural Language Processing tasks.\n",
        "# This command installs the library directly from its GitHub repository to ensure we have the latest version.\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Install Additional Libraries for Model Acceleration and Fine-Tuning\n",
        "# --------------------------------------------------------------------\n",
        "# 'accelerate': Simplifies running transformers models on multi-GPU/multi-TPU setups.\n",
        "# 'peft': Stands for PyTorch Efficient Finetuning, a library designed to accelerate fine-tuning.\n",
        "# 'bitsandbytes': Optimizes memory and compute efficiency, particularly useful for large models.\n",
        "# 'trl': Transformers Reinforcement Learning, used for training models via reinforcement learning.\n",
        "# 'sentencepiece': A library for unsupervised text tokenization, crucial for handling diverse text inputs.\n",
        "!pip install -q accelerate peft bitsandbytes transformers trl sentencepiece\n"
      ],
      "metadata": {
        "id": "JMRYH8u5KUrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eKsO14mIO7p"
      },
      "outputs": [],
      "source": [
        "# ==============================================\n",
        "#    Standard and Third Party Library Imports\n",
        "# ==============================================\n",
        "\n",
        "# ------------------------\n",
        "# Standard Library Imports\n",
        "# ------------------------\n",
        "import asyncio          # Asynchronous I/O operations, useful for managing concurrent tasks.\n",
        "import hashlib          # Hashing algorithms, used for hashing data                                         ##--Used only in Experimental Dataset Generation--##\n",
        "import json             # Reading and writing JSON files, used for data serialization and deserialization.\n",
        "import os               # Interfacing with the operating system, like handling file paths.\n",
        "import random           # Generating random numbers, useful for stochastic processes or shuffling data.\n",
        "import re               # Regular expressions, for text pattern matching.\n",
        "import shutil           # High-level file operations, such as copying or moving files.\n",
        "import subprocess       # Running subprocesses, useful for executing system commands.\n",
        "import sys              # Access to system-specific parameters and functions.\n",
        "import time             # Accessing time-related functions, like delays or timestamps.\n",
        "import zipfile          # Working with ZIP archives, for compressing or extracting files.\n",
        "\n",
        "# -------------------\n",
        "# Third Party Imports\n",
        "# -------------------\n",
        "import matplotlib.pyplot as plt  # Plotting library for creating visualizations.\n",
        "import numpy as np               # Fundamental package for scientific computing in Python.\n",
        "import pandas as pd              # Data manipulation and analysis library.\n",
        "import requests                  # Making HTTP requests to fetch data from the web.\n",
        "import torch                     # PyTorch, a deep learning framework.\n",
        "import xml.etree.ElementTree as ET  # Parsing and creating XML data.\n",
        "\n",
        "from datasets import load_dataset, load_metric, Dataset  # Loading and handling datasets for machine learning.\n",
        "from dotenv import load_dotenv  # Loading environment variables from a .env file.\n",
        "from google.colab import drive  # Integrating Google Colab with Google Drive.\n",
        "from huggingface_hub import snapshot_download, HfApi  # Interfacing with Hugging Face Hub.\n",
        "from IPython import display  # Display tools for Jupyter notebooks.\n",
        "from openai import AsyncOpenAI  # Asynchronous API for OpenAI services.\n",
        "from pathlib import Path  # Object-oriented filesystem paths.\n",
        "from peft import LoraConfig, PeftModel, get_peft_model  # Efficient fine-tuning of large models.\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator  # Accumulating data for TensorBoard.\n",
        "from trl import SFTTrainer  # Supervised Fine-Tuning (SFT) training for transformer models.\n",
        "from transformers import (  # Various utilities for working with transformer models.\n",
        "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling, HfArgumentParser, LlamaTokenizerFast,\n",
        "    TrainingArguments, TrainerCallback, pipeline)\n",
        "\n",
        "# -----------------------------\n",
        "# Google Colab Specific Imports\n",
        "# -----------------------------\n",
        "import google.colab.drive  # Specific to Google Colab, for drive integration.\n",
        "\n",
        "# ------------\n",
        "# PDF Handling\n",
        "# ------------\n",
        "import PyPDF2  # Library for PDF manipulation, like reading or splitting PDF files.\n",
        "\n",
        "# ---------------\n",
        "# Text Processing\n",
        "# ---------------\n",
        "import nltk  # Natural Language Toolkit, for processing and analyzing text.\n",
        "import spacy # SpaCy, a library for advanced natural language processing.                                   ##--Used only in Experimental Dataset Generation--##\n",
        "from textblob import TextBlob # TextBlob, a simple library for processing textual data.                     ##--Used only in Experimental Dataset Generation--##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Key Variables and Configurations for Educational Assistant Project (`Educational_Assistant_Project.ipynb`)\n",
        "---\n",
        "\n",
        "In this cell, you will find key variables and configurations that can be adjusted to customize the behavior of the Educational Assistant Project script. These variables and configurations are organized into different sections based on their roles and functionalities. <br> <br>\n",
        "\n",
        "## General Information\n",
        "\n",
        "- This notebook serves as the main script for the Educational Assistant Project.\n",
        "- It provides customizable variables and configurations to adapt the project to specific educational content and requirements. <br> <br>\n",
        "\n",
        "## Section 1: Textbook Conversion to XML\n",
        "\n",
        "This section contains variables and configurations related to the conversion of textbook data from PDF format to XML. <br> <br>\n",
        "\n",
        "## Section 2: OCR Textbook Dataset Processing\n",
        "\n",
        "Here, you will find variables and configurations for processing the OCR dataset of textbook content, including data cleaning and structuring. <br> <br>\n",
        "\n",
        "## Section 3: Markdown to XML Conversion\n",
        "\n",
        "This section focuses on variables and configurations for converting Markdown files to XML format, facilitating data structuring. <br> <br>\n",
        "\n",
        "## Section 4: Conversation Branch Generation\n",
        "\n",
        "In this section, you can customize variables and configurations for generating conversation branches for educational purposes. <br> <br>\n",
        "\n",
        "## Section 5: JSONL to JSON Conversion\n",
        "\n",
        "These variables are used for converting JSONL files to JSON format, potentially as part of data post-processing. <br> <br>\n",
        "\n",
        "## Section 6: Fine-Tuning Large Language Model\n",
        "\n",
        "This section provides variables relevant to fine-tuning a large language model, which can be important for specific project goals.\n",
        "\n",
        "Please refer to each section to find specific variables and configurations that you can adjust according to your project's needs. <br> <br>\n",
        "\n",
        "## Section 7: Conversation Tree Generation Experiment\n",
        "\n",
        "These variables are used in the script intended to generate a synthetic dataset of conversation trees from an XML file of textbook paragraphs. <br> <br>\n"
      ],
      "metadata": {
        "id": "2Xdt0JJa4-6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "#    Key Variables and Configurations for Textbook Conversion to XML\n",
        "# =====================================================================\n",
        "\n",
        "# Directory path for PDF files (specific to XML conversion)\n",
        "XML_FOLDER_PATH = \"xml_folder/path\"\n",
        "\n",
        "# Output file path for the combined XML dataset\n",
        "XML_OUTPUT_FILE_PATH = os.path.join(XML_FOLDER_PATH, \"combined_dataset.xml\")\n",
        "\n",
        "# Regular expression patterns for extracting metadata from textbooks\n",
        "METADATA_PATTERNS = {\n",
        "    'title_pattern': r'(?i)title[:\\s]*([\\w\\s,]+)',\n",
        "    'subtitle_pattern': r'(?i)subtitle[:\\s]*([\\w\\s,]+)',\n",
        "    'author_pattern': r'(?i)author[s]?[:\\s]*([\\w\\s,]+)',\n",
        "    'isbn_pattern': r'(?i)ISBN[-\\s]*:?[\\s]*([\\d\\-]+)',\n",
        "    'edition_pattern': r'(?i)edition[:\\s]*([\\w\\s,]+)',\n",
        "    'publisher_pattern': r'(?i)publisher[:\\s]*([\\w\\s,]+)',\n",
        "    'publication_year_pattern': r'(?i)publication\\s+year[:\\s]*([\\d]{4})',\n",
        "    'chapter_pattern': r'Chapter\\s*(\\d+):\\s*(.*)',\n",
        "    'section_heading_pattern': r'Section:\\s*(.*)',\n",
        "    'page_number_pattern': r'Page\\s*(\\d+)',\n",
        "    'references_pattern': r'References:\\s*(.*)',\n",
        "    'index_terms_pattern': r'Index Terms:\\s*(.*)',\n",
        "    'figures_pattern': r'Figure\\s*(\\d+):\\s*(.*)',\n",
        "    'footnotes_pattern': r'Footnote:\\s*(.*)',\n",
        "    'table_of_contents_pattern': r'(?:Contents|Table\\s+of\\s+Contents|Index)\\s+(?:.\\n)?\\s(?:Chapter\\s+1|Introduction)'\n",
        "}\n",
        "\n",
        "# Pattern for removing non-printable characters except for whitespace\n",
        "NON_PRINTABLE_PATTERN = r'[^\\x20-\\x7E\\n\\r\\t]'\n",
        "\n",
        "# Pattern for detecting the start of a new paragraph\n",
        "PARAGRAPH_START_PATTERN = r\"^[A-Z]\"\n",
        "\n",
        "# NLTK Tokenizer Models\n",
        "NLTK_MODELS = ['punkt']\n",
        "\n",
        "# ===========================================================================\n",
        "#    Key Variables and Configurations for OCR Textbook Dataset Processing\n",
        "# ===========================================================================\n",
        "\n",
        "# Google Drive paths\n",
        "DRIVE_BASE_PATH = '/content/drive/My Drive/Colab Notebooks/Folder'\n",
        "DATASET_ZIP_PATH = f'{DRIVE_BASE_PATH}/Dataset.zip'\n",
        "DRIVE_MARKDOWN_DIR = f'{DRIVE_BASE_PATH}/Folder/'\n",
        "\n",
        "# Paths in Google Colab\n",
        "EXTRACTED_DATASET_PATH = '/content/Dataset'\n",
        "LOCAL_MARKDOWN_DIR = '/content/OCR/'\n",
        "LOCAL_TEXTBOOKS_DIR = '/content/Dataset/'\n",
        "DRIVE_LOG_FILE = f'{DRIVE_BASE_PATH}/Markdown/nougat_ocr_log.txt'\n",
        "\n",
        "# =====================================================================\n",
        "#    Key Variables and Configurations for Markdown to XML Conversion\n",
        "# =====================================================================\n",
        "\n",
        "# Directory path for Markdown files\n",
        "MARKDOWN_DIR = 'C:/Users/username/Desktop/Markdown'\n",
        "\n",
        "# Output file path for the combined XML file\n",
        "OUTPUT_XML_FILE = 'C:/Users/username/Desktop/Markdown/dataset.xml'\n",
        "\n",
        "# Regular expression patterns for extracting metadata from textbooks\n",
        "METADATA_PATTERNS = {\n",
        "    'title_pattern': r'(?i)title[:\\s]*([\\w\\s,]+)',\n",
        "    'subtitle_pattern': r'(?i)subtitle[:\\s]*([\\w\\s,]+)',\n",
        "    'author_pattern': r'(?i)author[s]?[:\\s]*([\\w\\s,]+)',\n",
        "    'isbn_pattern': r'(?i)ISBN[-\\s]*:?[\\s]*([\\d\\-]+)',\n",
        "    'edition_pattern': r'(?i)edition[:\\s]*([\\w\\s,]+)',\n",
        "    'publisher_pattern': r'(?i)publisher[:\\s]*([\\w\\s,]+)',\n",
        "    'publication_year_pattern': r'(?i)publication\\s+year[:\\s]*([\\d]{4})',\n",
        "    'chapter_pattern': r'Chapter\\s*(\\d+):\\s*(.*)',\n",
        "    'section_heading_pattern': r'Section:\\s*(.*)',\n",
        "    'page_number_pattern': r'Page\\s*(\\d+)',\n",
        "    'references_pattern': r'References:\\s*(.*)',\n",
        "    'index_terms_pattern': r'Index Terms:\\s*(.*)',\n",
        "    'figures_pattern': r'Figure\\s*(\\d+):\\s*(.*)',\n",
        "    'footnotes_pattern': r'Footnote:\\s*(.*)',\n",
        "    'table_of_contents_pattern': r'(?:Contents|Table\\s+of\\s+Contents|Index)\\s+(?:.\\n)?\\s(?:Chapter\\s+1|Introduction)'\n",
        "}\n",
        "\n",
        "# =======================================================================\n",
        "#    Key Variables and Configurations for Conversation Tree Generation\n",
        "# =======================================================================\n",
        "\n",
        "# File paths for input XML, output JSONL files, and console log\n",
        "XML_INPUT_FILE_PATH = 'dataset.xml'\n",
        "OUTPUT_JSONL_FILE_PATH = 'conversation_trees_dual.jsonl'\n",
        "CONSOLE_LOG_FILE_PATH = 'console_log_dual.txt'\n",
        "\n",
        "# OpenAI API key\n",
        "API_KEY = \"api-key-here\"  # Replace with your actual API key\n",
        "\n",
        "# Maximum number of paragraphs to process and the maximum depth of conversation trees\n",
        "MAX_PARAGRAPHS = 2500   # Kept to 2500 Paragraphs To Keep API Costs < $20 (Cost about ~$15 For Our Dataset)\n",
        "MAX_DEPTH = 5           # Depth of 5 x 2 Branch Types == 10 API Queries Per Paragraph\n",
        "\n",
        "# Parameters for exponential backoff in case of rate limit errors\n",
        "INITIAL_BACKOFF_DELAY = 1  # Initial delay in seconds\n",
        "MAX_BACKOFF_DELAY = 60     # Maximum delay in seconds\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "# Signals indicating the end of a conversation\n",
        "END_SIGNALS = {\"[TOPIC_END]\", \"[CONVERSATION_END]\"}\n",
        "\n",
        "# Filler content used in conversation trees\n",
        "FILLER_CONTENT = {\"[FILLER_CONTENT]\", \"[FAILED_TO_PARSE]\"}\n",
        "\n",
        "# Parameters for async processing\n",
        "MAX_CONCURRENT_TASKS = 25  # Maximum number of concurrent tasks\n",
        "\n",
        "# Output file path for post-processed and cleaned conversation trees\n",
        "CLEANED_OUTPUT_JSONL_FILE_PATH = 'cleaned_conversation_trees_dual.jsonl'\n",
        "\n",
        "# Model queried for dataset generation\n",
        "OPENAI_API_MODEL = \"gpt-3.5-turbo\" # GPT 3.5 for Budget Constraints, GPT 4 for Quality\n",
        "\n",
        "# ================================================\n",
        "#    Key Variables for JSONL to JSON Conversion\n",
        "# ================================================\n",
        "\n",
        "# Path to the input JSONL file\n",
        "JSONL_FILE_PATH = 'conversation_trees_dual_v3.jsonl'\n",
        "\n",
        "# Path to the output JSON file\n",
        "JSON_FILE_PATH = 'conversation_trees_dual_v3.json'\n",
        "\n",
        "# ========================================================\n",
        "#    Key Variables for Fine-Tuning Large Language Model\n",
        "# ========================================================\n",
        "\n",
        "# Model Information\n",
        "model_name = \"username/modelname\"  # Replace with the base model from Hugging Face\n",
        "dataset_name = \"username/datasetname\"  # Dataset for training\n",
        "new_model = \"new_modelname\"  # Name for the fine-tuned model\n",
        "\n",
        "# QLoRA Parameters\n",
        "lora_r = 64  # LoRA attention dimension\n",
        "lora_alpha = 16  # Alpha parameter for LoRA scaling\n",
        "lora_dropout = 0.15  # Dropout probability for LoRA layers\n",
        "\n",
        "# bitsandbytes Parameters\n",
        "use_4bit = True  # Activate 4-bit precision base model loading\n",
        "bnb_4bit_compute_dtype = \"float16\"  # Compute dtype for 4-bit base models\n",
        "bnb_4bit_quant_type = \"nf4\"  # Quantization type\n",
        "use_nested_quant = False  # Nested quantization for 4-bit base models\n",
        "\n",
        "# TrainingArguments Parameters\n",
        "output_dir = \"./results\"  # Output directory for model predictions and checkpoints\n",
        "num_train_epochs = 3  # Number of training epochs\n",
        "fp16 = False  # Enable fp16 training\n",
        "bf16 = False  # Enable bf16 training (use with A100)\n",
        "per_device_train_batch_size = 2  # Training batch size per GPU\n",
        "per_device_eval_batch_size = 2  # Evaluation batch size per GPU\n",
        "gradient_accumulation_steps = 4  # Gradient accumulation steps\n",
        "gradient_checkpointing = True  # Enable gradient checkpointing\n",
        "max_grad_norm = 1.0  # Maximum gradient norm for clipping\n",
        "learning_rate = 1e-4  # Initial learning rate\n",
        "weight_decay = 0.01  # Weight decay for all layers except bias/LayerNorm\n",
        "optim = \"paged_adamw_32bit\"  # Optimizer type\n",
        "lr_scheduler_type = \"cosine\"  # Learning rate scheduler\n",
        "max_steps = -1  # Maximum number of training steps\n",
        "warmup_ratio = 0.06  # Ratio of steps for linear warmup\n",
        "group_by_length = True  # Group sequences by length for efficiency\n",
        "save_steps = 0  # Save checkpoint every X update steps\n",
        "logging_steps = 1  # Log every X update steps\n",
        "\n",
        "# SFT Parameters\n",
        "max_seq_length = 896  # Maximum sequence length (reduce if VRAM limited)\n",
        "packing = False  # Pack multiple short examples in the same sequence\n",
        "device_map = {\"\": 0}  # Load model on specified GPU\n",
        "neftune_noise_alpha = 5  # NEFTune noise alpha parameter\n",
        "\n",
        "# Model Checkpoint and Configuration\n",
        "final_epoch = \"2.9995152690256908\"  # Adjust based on the checkpoint name in your CoLab results folder\n",
        "final_model_dir = os.path.join(output_dir, f\"checkpoint-{final_epoch}\")  # Directory where the final model is saved\n",
        "pytorch_model_path = os.path.join(final_model_dir, 'pytorch_model.bin')  # Path to the saved pytorch_model.bin\n",
        "\n",
        "# Model Loading Parameters\n",
        "strict_model_loading = False  # Set to False to allow flexible loading of the model state dictionary\n",
        "\n",
        "# Tokenizer Configuration\n",
        "tokenizer_padding_side = \"right\"  # Define the padding side for the tokenizer\n",
        "special_tokens_dict = {'additional_special_tokens': ['<GoodResponse>', '<BadResponse>', '<Prompt>', '<Response>']}  # Special tokens used in tokenizer\n",
        "\n",
        "# Model Generation Configuration\n",
        "model_config_use_cache = False  # Disable caching for stateful behavior in generation\n",
        "model_config_pretraining_tp = 1  # Set tensor parallelism (relevant for distributed training)\n",
        "\n",
        "# Generation Test Prompts\n",
        "prompt1 = \"Write a program for my school assignment that creates a JavaFX calculator.\"\n",
        "prompt2 = \"Solve the following integral: 4x^2+5x+2.\"\n",
        "\n",
        "# Set to True to merge LoRA weights, False to keep them separate for use in multiple LoRas\n",
        "merge_lora_weights = True\n",
        "\n",
        "# Control whether to upload the model and tokenizer to Hugging Face\n",
        "push_to_huggingface = True  # Set to True to upload, False to skip\n",
        "\n",
        "# Hugging Face API token for manual authentication in the alternative method\n",
        "hf_api_token = \"your-token-here\"  # Replace with your actual Hugging Face API token\n",
        "\n",
        "# Model name to be used in Hugging Face Hub\n",
        "new_model = \"modelname\"  # Replace with your desired model name\n",
        "\n",
        "# Environment variables for the alternative upload method\n",
        "hf_home_dir = '/content'  # Directory for Hugging Face configuration\n",
        "\n",
        "# Model identifier on Hugging Face (username/modelname format)\n",
        "hf_model_id = \"username/modelname\"  # Replace with your actual Hugging Face model identifier\n",
        "\n",
        "# Directory to save the downloaded model\n",
        "model_save_directory = \"./\"  # Adjust the directory as needed\n",
        "\n",
        "# =================================================================\n",
        "#    Key Variables for Experimental Conversation Tree Generation\n",
        "# =================================================================\n",
        "# Uncomment to for experimental tree generation\n",
        "# API_KEY = \"api-key-here\" # TODO: Extract from environment variable\n",
        "# MAX_DEPTH = 5\n",
        "# MAX_BRANCHES = 16\n",
        "# MAX_ATTEMPTS = 5\n",
        "# MAX_DELAY = 60\n",
        "# MAX_CONCURRENT_TASKS = 20\n",
        "# INDIVIDUAL_PAIRS_FILE = 'individual_pairs.jsonl'\n",
        "# RATED_TREES_FILE = 'rated_trees.jsonl'\n",
        "# XML_FILE_PATH = 'output_short_snippet.xml'\n",
        "# CONSOLE_LOG_FILE_PATH = 'console_log.txt'\n"
      ],
      "metadata": {
        "id": "ITlEfv5gRHAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Textbook Conversion to XML (`PDF2XML.py`)\n",
        "---\n",
        "\n",
        "<a id='textbook-conversion-to-xml'></a>\n",
        "\n",
        "## Description\n",
        "\n",
        "The Python script named \"PDF2XML.py\" is used to convert textbook data from PDF format to XML. This script plays a crucial role in the initial data processing stage, setting the foundation for subsequent stages in the educational content pipeline.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "The purpose of this script is to take educational content in PDF format and transform it into XML format. This conversion is a fundamental step in the fine-tuning pipeline, as XML is a structured and machine-readable format that is required for synethtic dataset generation."
      ],
      "metadata": {
        "id": "JPFs3WmV49kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure NLTK tokenizer models are downloaded\n",
        "for model in NLTK_MODELS:\n",
        "    nltk.download(model, quiet=True)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The file path of the PDF.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted and cleaned text from the PDF.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        # Iterating over each page in the PDF\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            # Extracting text from each page\n",
        "            text += page.extract_text() if page.extract_text() else ''\n",
        "    print(f\"Text successfully extracted from {os.path.basename(pdf_path)}\")\n",
        "\n",
        "    # Remove non-printable characters except for whitespace characters\n",
        "    cleaned_text = re.sub(NON_PRINTABLE_PATTERN, '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_metadata(text):\n",
        "    \"\"\"\n",
        "    Extracts metadata from the text using predefined patterns.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text from which to extract metadata.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of extracted metadata.\n",
        "    \"\"\"\n",
        "    metadata = {}\n",
        "    for key, pattern in METADATA_PATTERNS.items():\n",
        "        match = re.search(pattern, text)\n",
        "        # Assigning the extracted metadata or 'Unknown' if not found\n",
        "        metadata[key] = match.group(1) if match else 'Unknown'\n",
        "    return metadata\n",
        "\n",
        "def extract_paragraphs(text):\n",
        "    \"\"\"\n",
        "    Splits the text into paragraphs based on sentence tokenization.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be split into paragraphs.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of paragraphs.\n",
        "    \"\"\"\n",
        "    sentences = nltk.tokenize.sent_tokenize(text)\n",
        "    paragraphs = []\n",
        "    paragraph = \"\"\n",
        "    for sentence in sentences:\n",
        "        # Start a new paragraph if the sentence begins with a capital letter\n",
        "        if paragraph and re.match(PARAGRAPH_START_PATTERN, sentence):\n",
        "            paragraphs.append(paragraph.strip())\n",
        "            paragraph = sentence\n",
        "        else:\n",
        "            paragraph += \" \" + sentence\n",
        "    # Add the last paragraph if it exists\n",
        "    if paragraph:\n",
        "        paragraphs.append(paragraph.strip())\n",
        "    return paragraphs\n",
        "\n",
        "def process_pdf_files(pdf_paths):\n",
        "    \"\"\"\n",
        "    Processes multiple PDF files and converts them into an XML format.\n",
        "\n",
        "    Args:\n",
        "        pdf_paths (list): List of file paths for the PDFs to be processed.\n",
        "\n",
        "    Returns:\n",
        "        ElementTree.Element: The root element of the generated XML structure.\n",
        "    \"\"\"\n",
        "    root = ET.Element(\"textbook_dataset\")\n",
        "\n",
        "    for pdf_path in pdf_paths:\n",
        "        # Extracting text from the PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        # Extracting metadata from the text\n",
        "        metadata = extract_metadata(text)\n",
        "        # Extracting paragraphs from the text\n",
        "        paragraphs = extract_paragraphs(text)\n",
        "\n",
        "        # Creating a new element for each book\n",
        "        book_element = ET.SubElement(root, \"book\")\n",
        "        # Adding metadata to the book element\n",
        "        metadata_element = ET.SubElement(book_element, \"metadata\")\n",
        "        for key, value in metadata.items():\n",
        "            ET.SubElement(metadata_element, key).text = value\n",
        "\n",
        "        # Adding each paragraph as a subelement of the book\n",
        "        for paragraph in paragraphs:\n",
        "            paragraph_element = ET.SubElement(book_element, \"paragraph\")\n",
        "            paragraph_element.text = paragraph\n",
        "\n",
        "    return root\n",
        "\n",
        "def write_xml_to_file(root_element, output_file_path):\n",
        "    \"\"\"\n",
        "    Writes an XML tree to a file.\n",
        "\n",
        "    Args:\n",
        "        root_element (ElementTree.Element): The root element of the XML tree.\n",
        "        output_file_path (str): The file path to write the XML file.\n",
        "    \"\"\"\n",
        "    tree = ET.ElementTree(root_element)\n",
        "    with open(output_file_path, 'wb') as xml_file:\n",
        "        # Writing the XML to a file with UTF-8 encoding\n",
        "        tree.write(xml_file, encoding='utf-8', xml_declaration=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Gathering PDF paths from the specified folder\n",
        "    pdf_paths = [os.path.join(XML_FOLDER_PATH, f) for f in os.listdir(XML_FOLDER_PATH) if f.endswith('.pdf')]\n",
        "    # Processing each PDF and converting it into XML format\n",
        "    combined_xml = process_pdf_files(pdf_paths)\n",
        "    # Writing the combined XML data to a file\n",
        "    write_xml_to_file(combined_xml, XML_OUTPUT_FILE_PATH)\n"
      ],
      "metadata": {
        "id": "hlhyhc2kI7nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# OCR Textbook Dataset Processing (`Nougat_OCR _Textbook_Dataset_V2.ipynb`)\n",
        "---\n",
        "\n",
        "## Description\n",
        "\n",
        "This Jupyter Notebook, named \"Nougat_OCR_Textbook_Dataset_V2.ipynb,\" is designed for processing an Optical Character Recognition (OCR) dataset containing textbook content. The notebook includes various data processing tasks, such as data cleaning, structuring, and preparation for model training and analysis.\n",
        "\n",
        "### Note\n",
        "\n",
        "- Always make a backup of your original OCR dataset before applying any data processing steps to avoid data loss.\n",
        "- Immediately backup the resulting dataset upon conversion to aviod data loss incase the CoLab instance terminates."
      ],
      "metadata": {
        "id": "rwVLcUR348kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "# Mount Google Drive, Unzip Dataset, and Sync Markdown Files\n",
        "# -----------------------------------------------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if the dataset directory already exists\n",
        "if not os.path.exists(EXTRACTED_DATASET_PATH):\n",
        "    # If not, unzip the dataset into the specified directory\n",
        "    !unzip \"$DATASET_ZIP_PATH\" -d \"$EXTRACTED_DATASET_PATH\"\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n",
        "\n",
        "# Ensure the local OCR directory exists\n",
        "os.makedirs(LOCAL_MARKDOWN_DIR, exist_ok=True)\n",
        "\n",
        "# Sync the Markdown files from Google Drive to the local OCR directory\n",
        "!rsync -a --info=progress2 \"$DRIVE_MARKDOWN_DIR\" \"$LOCAL_MARKDOWN_DIR\"\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Process Textbooks with Nougat OCR and Update Log File\n",
        "# ------------------------------------------------------\n",
        "# Open the log file in append mode\n",
        "with open(DRIVE_LOG_FILE, 'a') as log_file_writer:\n",
        "    log_file_writer.write(\"Starting OCR process...\\n\")\n",
        "\n",
        "    # Iterate over each file in the directory\n",
        "    for file in os.listdir(LOCAL_TEXTBOOKS_DIR):\n",
        "        if file.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(LOCAL_TEXTBOOKS_DIR, file)\n",
        "            markdown_file_name = os.path.splitext(file)[0] + \".mmd\"\n",
        "            local_markdown_file_path = os.path.join(LOCAL_MARKDOWN_DIR, markdown_file_name)\n",
        "\n",
        "            # Check if the MMD file already exists locally\n",
        "            if os.path.exists(local_markdown_file_path):\n",
        "                print(f\"Skipping '{file}' as MMD file already exists.\")\n",
        "                log_file_writer.write(f\"Skipped processing of '{file}' as MMD file already exists.\\n\")\n",
        "            else:\n",
        "                print(f\"Processing '{file}'...\")\n",
        "                # Process the PDF with Nougat OCR\n",
        "                nougat_command = f'nougat --markdown pdf \"{pdf_path}\" --out \"{LOCAL_MARKDOWN_DIR}\"'\n",
        "                os.system(f'{nougat_command} 2>&1 | tee -a \"{DRIVE_LOG_FILE}\"')\n",
        "\n",
        "                # Check for the created Markdown file\n",
        "                created_markdown_file_path = os.path.join(LOCAL_MARKDOWN_DIR, os.path.splitext(file)[0] + \"_ocr.md\")\n",
        "                if os.path.exists(created_markdown_file_path):\n",
        "                    # Copy the Markdown file to Google Drive\n",
        "                    !cp \"$created_markdown_file_path\" \"$DRIVE_MARKDOWN_DIR\"\n",
        "                    log_file_writer.write(f\"Processed and saved Markdown for '{file}'.\\n\")\n",
        "                else:\n",
        "                    log_file_writer.write(f\"Failed to process '{file}'.\\n\")\n"
      ],
      "metadata": {
        "id": "N_DP1478JFR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Markdown to XML Conversion (`mmd_to_xml_script.py`)\n",
        "---\n",
        "\n",
        "## Description\n",
        "\n",
        "This Python script is used to convert Markdown files into XML format. It facilitates data structuring and formatting for easier manipulation and analysis.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "The purpose of this script is to streamline the process of converting Markdown files to XML, which can be particularly useful for data organization and analysis. The resulting XML format can be more structured and machine-readable, making it easier to work with and extract information from Markdown content.\n",
        "\n",
        "### Usage\n",
        "\n",
        "Before running the script, ensure you have Markdown files that you want to convert. You can execute this script with Python to perform the conversion. It is a helpful tool when you need Markdown content in a structured XML format for various applications, such as data analysis or integration with other systems."
      ],
      "metadata": {
        "id": "o0DxeENT47be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_metadata(line, parent_elem):\n",
        "    \"\"\"\n",
        "    Extract metadata from a line and add it to the parent XML element.\n",
        "\n",
        "    Args:\n",
        "        line (str): The line of text to process.\n",
        "        parent_elem (ElementTree.Element): The parent XML element to which the metadata will be added.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if metadata was found and processed, False otherwise.\n",
        "    \"\"\"\n",
        "    for key, pattern in METADATA_PATTERNS.items():\n",
        "        match = re.match(pattern, line)\n",
        "        if match:\n",
        "            # Create a new sub-element for each metadata item found\n",
        "            metadata_elem = ET.SubElement(parent_elem, key)\n",
        "            metadata_elem.text = match.group(1)\n",
        "            return True  # Metadata was processed\n",
        "    return False  # No metadata was found\n",
        "\n",
        "def mmd_to_xml(markdown_dir, output_xml_file):\n",
        "    \"\"\"\n",
        "    Converts a directory of Markdown files into a single XML file.\n",
        "\n",
        "    Args:\n",
        "        markdown_dir (str): The directory containing Markdown files.\n",
        "        output_xml_file (str): The path to the output XML file.\n",
        "\n",
        "    Returns:\n",
        "        str: A message indicating the completion and path of the generated XML file.\n",
        "    \"\"\"\n",
        "    root = ET.Element(\"books\")\n",
        "\n",
        "    # Iterate over each Markdown file in the directory\n",
        "    for file in os.listdir(markdown_dir):\n",
        "        if file.endswith('.mmd'):\n",
        "            mmd_file_path = os.path.join(markdown_dir, file)\n",
        "            with open(mmd_file_path, 'r', encoding='utf-8') as mmd_file:\n",
        "                lines = mmd_file.readlines()\n",
        "\n",
        "            # Create a new book element for each Markdown file\n",
        "            book = ET.SubElement(root, \"book\", name=file.replace('.mmd', ''))\n",
        "            metadata = ET.SubElement(book, \"metadata\")\n",
        "            content = ET.SubElement(book, \"content\")\n",
        "            paragraph_lines = []  # Initialize paragraph lines for each book\n",
        "\n",
        "            in_metadata = True  # Flag to track if currently processing metadata\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                # Skip lines that indicate missing or empty pages\n",
        "                if line.startswith('[MISSING_PAGE_EMPTY:'):\n",
        "                    continue\n",
        "\n",
        "                # Process metadata lines at the beginning of the file\n",
        "                if in_metadata and not process_metadata(line, metadata):\n",
        "                    in_metadata = False  # End of metadata section\n",
        "\n",
        "                # Process content after metadata\n",
        "                if not in_metadata and line:\n",
        "                    if line.startswith('* '):  # Treat bullet points as separate paragraphs\n",
        "                        paragraph = ET.SubElement(content, \"paragraph\")\n",
        "                        paragraph.text = line[2:]  # Exclude the bullet point marker\n",
        "                    else:\n",
        "                        # Aggregate lines into paragraphs, separate by empty lines\n",
        "                        if not line or line == os.linesep:\n",
        "                            if paragraph_lines:\n",
        "                                paragraph = ET.SubElement(content, \"paragraph\")\n",
        "                                paragraph.text = ' '.join(paragraph_lines)\n",
        "                                paragraph_lines.clear()\n",
        "                        else:\n",
        "                            paragraph_lines.append(line)\n",
        "\n",
        "            # Handle the final paragraph if it exists\n",
        "            if paragraph_lines:\n",
        "                paragraph = ET.SubElement(content, \"paragraph\")\n",
        "                paragraph.text = ' '.join(paragraph_lines)\n",
        "\n",
        "    # Create and write the XML tree to the specified file\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(output_xml_file, encoding='utf-8', xml_declaration=True)\n",
        "\n",
        "    return f'Combined XML file generated at: {output_xml_file}'\n",
        "\n",
        "# Running the conversion process\n",
        "result = mmd_to_xml(MARKDOWN_DIR, OUTPUT_XML_FILE)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "-KUbj7CsJH40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Script 4: Conversation Branch Generation (`generate-conversation-tree-async-dual.py`)\n",
        "---\n",
        "\n",
        "**Author**: Elijah Kulpinski <br>\n",
        "**Date**: 12/04/23 <br>\n",
        "**Version**: 2.0.1 <br>\n",
        "\n",
        "## Description\n",
        "\n",
        "This Python script is designed to generate conversation trees for educational purposes, focusing on topics in computer science and education. The script operates asynchronously, enabling parallel generation of multiple conversation trees. Each tree is derived from a paragraph extracted from a collection of textbook content. The primary objective is to foster critical thinking and a deeper understanding of the subject matter in students. The trees simulate educational dialogues, serving as a crucial part of the data preparation stage for training language models.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Asynchronous Processing:** The script generates multiple conversation trees simultaneously, optimizing resource usage and reducing processing time. This feature is particularly important when dealing with a large number of paragraphs, ensuring scalability and efficiency.\n",
        "\n",
        "- **Contextual Tree Generation:** Each conversation tree is built incrementally, rooted in a specific paragraph from a textbook. This approach guarantees that each tree is contextually relevant and coherent, directly tied to the educational content.\n",
        "\n",
        "- **Branching Logic:** The script employs a dynamic branching logic to expand conversations. This logic takes into account the content and educational goals of each paragraph, generating conversations that are not only relevant but also pedagogically sound.\n",
        "\n",
        "- **Quality Control:** The script includes mechanisms for automated quality checks, ensuring consistency and coherence in the generated conversations. Manual oversight is also incorporated for educational relevance, allowing for fine-tuning and adjustments to align with specific teaching objectives.\n",
        "\n",
        "- **API Rate Limit Management:** The script is designed to monitor and manage API usage efficiently, staying within the constraints of rate limits. This ensures smooth operation and minimizes the risk of service interruptions.\n",
        "\n",
        "## Usage\n",
        "\n",
        "Before running the script, ensure all dependencies are installed and that the necessary API keys are set up. The script can be executed with Python 3.x. Each run processes a set of paragraphs from the XML file, generating a unique conversation tree for each paragraph. The output is a set of JSONL files, each representing a conversation tree.\n",
        "\n",
        "## Note\n",
        "\n",
        "Access to the OpenAI API is required for this script. It is crucial to adhere to the API's rate limits and usage guidelines. Ensure you have the necessary permissions and API quota to use this script effectively. The script is designed with flexibility in mind, allowing users to modify parameters such as the number of paragraphs processed and the depth of conversation trees, to suit different requirements and constraints."
      ],
      "metadata": {
        "id": "LGbS1C5M46LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#    Functions for Synthetic Conversation Branch Generation\n",
        "# ============================================================\n",
        "\n",
        "def parse_xml(file_path, max_paragraphs=MAX_PARAGRAPHS):\n",
        "    \"\"\"\n",
        "    Parses an XML file and extracts paragraphs along with their book titles.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The file path to the XML file.\n",
        "        max_paragraphs (int): The maximum number of paragraphs to extract.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, each containing a book title and a paragraph.\n",
        "    \"\"\"\n",
        "    book_paragraphs = []\n",
        "    book_counts = {}\n",
        "    total_paragraphs = 0\n",
        "\n",
        "    try:\n",
        "        # Parsing the XML file\n",
        "        parser = ET.XMLParser(encoding=\"utf-8\")\n",
        "        tree = ET.parse(file_path, parser=parser)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Counting paragraphs in each book\n",
        "        for book in root.findall('book'):\n",
        "            book_title = book.get('name')\n",
        "            paragraphs = book.findall('.//paragraph')\n",
        "            book_counts[book_title] = len(paragraphs)\n",
        "            total_paragraphs += len(paragraphs)\n",
        "\n",
        "        # Allocating paragraphs proportionally to each book\n",
        "        for book in root.findall('book'):\n",
        "            book_title = book.get('name')\n",
        "            percentage = book_counts[book_title] / total_paragraphs\n",
        "            allocated_paragraphs = int(percentage * max_paragraphs)\n",
        "            paragraphs = book.findall('.//paragraph')\n",
        "            selected_paragraphs = random.sample(paragraphs, min(allocated_paragraphs, len(paragraphs)))\n",
        "\n",
        "            # Extracting text from each paragraph\n",
        "            for paragraph in selected_paragraphs:\n",
        "                text = paragraph.text.strip() if paragraph.text else ''\n",
        "                book_paragraphs.append((book_title, text))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_path}' was not found.\")\n",
        "    except ET.ParseError:\n",
        "        print(f\"Error: The file '{file_path}' is not a valid XML or is malformed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    return book_paragraphs\n",
        "\n",
        "def append_to_file(file_path, data):\n",
        "    \"\"\"\n",
        "    Appends the given data to a file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the file where data will be appended.\n",
        "        data (dict): The data to append.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'a') as file:\n",
        "        # Dumping data as JSON into the file\n",
        "        json.dump(data, file)\n",
        "        file.write('\\n')  # Adding a newline for separation\n",
        "\n",
        "def backoff_hdlr(attempt, delay=INITIAL_BACKOFF_DELAY):\n",
        "    \"\"\"\n",
        "    Implements an exponential backoff strategy to handle rate limit errors.\n",
        "\n",
        "    Args:\n",
        "        attempt (int): The current retry attempt number.\n",
        "        delay (int): The delay in seconds before the next retry. Defaults to INITIAL_BACKOFF_DELAY.\n",
        "\n",
        "    Effects:\n",
        "        Induces a sleep period with a delay that exponentially increases with each attempt.\n",
        "    \"\"\"\n",
        "    # Calculate the delay with jitter to avoid synchronized retries\n",
        "    jitter = random.uniform(0, attempt)\n",
        "    new_delay = min(delay * 2, MAX_BACKOFF_DELAY) + jitter  # Cap the delay at MAX_BACKOFF_DELAY\n",
        "    print(f\"Rate limit hit, backing off for {new_delay} seconds for attempt {attempt}.\")\n",
        "    with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "        log_file.write(f\"Rate limit hit, backing off for {new_delay} seconds for attempt {attempt}.\\n\")\n",
        "    time.sleep(new_delay)\n",
        "\n",
        "def parse_response(response_text):\n",
        "    \"\"\"\n",
        "    Parses the response text from the API to extract the student's question and the educator's response.\n",
        "\n",
        "    Args:\n",
        "        response_text (str): The response text from the API.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the student's question and the educator's response. If parsing fails, both are set to \"FAILED_TO_PARSE\".\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to parse the response text as JSON\n",
        "        response_json = json.loads(response_text)\n",
        "        with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "            log_file.write(f\"Parsing Response Text: \\n{response_text}\\n\")\n",
        "\n",
        "        # Extract the 'Prompt' and 'Response' fields from the JSON\n",
        "        student_question = response_json.get(\"Prompt\", \"FAILED_TO_PARSE\")\n",
        "        educator_response = response_json.get(\"Response\", \"FAILED_TO_PARSE\")\n",
        "\n",
        "        return student_question, educator_response\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        # Handle cases where the response text is not valid JSON\n",
        "        with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "            log_file.write(\"Failed to parse response as JSON.\\n\")\n",
        "        return \"FAILED_TO_PARSE\", \"FAILED_TO_PARSE\"\n",
        "\n",
        "# Async function to generate conversation tree with book title and controlled concurrency\n",
        "async def generate_conversation_branch(client, book_title, paragraph, branch_type, semaphore, index):\n",
        "\n",
        "    \"\"\"\n",
        "    Asynchronously generates a single branch of a conversation tree for a given paragraph from a textbook, including the\n",
        "    book title for context. It can generate either a \"good\" or \"bad\" conversation branch based on the provided branch_type parameter.\n",
        "\n",
        "    Args:\n",
        "    client (AsyncOpenAI): The OpenAI API client for making asynchronous requests.\n",
        "    book_title (str): The title of the book to which the paragraph belongs.\n",
        "    paragraph (str): The paragraph to base the conversation tree on.\n",
        "    branch_type (str): The type of branch to generate, either 'good' or 'bad'.\n",
        "    semaphore (asyncio.Semaphore): Semaphore to control concurrency.\n",
        "    index (int): The current paragraph index.\n",
        "\n",
        "    Returns:\n",
        "    dict: The generated conversation branch as a dictionary, including the book title, paragraph index, and the conversation steps.\n",
        "    Usage:\n",
        "    \"\"\"\n",
        "\n",
        "    async with semaphore: # Control concurrency with semaphore\n",
        "        # Initial conversation tree structure\n",
        "        start_time = time.perf_counter()  # Define start_time at the beginning of the block\n",
        "        conversation_tree = {\"book_title\": book_title, \"paragraph_index\": index, \"conversation\": []}\n",
        "        max_retries = 5\n",
        "        conversation_summary = \"\"\n",
        "        depth = 0\n",
        "\n",
        "        while depth < MAX_DEPTH:\n",
        "            attempt = 0\n",
        "            delay = 1\n",
        "\n",
        "            # Generate prompt for the conversation step\n",
        "            if branch_type == 'good':\n",
        "                if depth == 0:\n",
        "                    # Root Node's Instructions for Good Branch\n",
        "                    prompt = (\n",
        "                    \"Assume the role of an educator assisting a student. You are here to help them develop critical thinking skills and engage with the content.\"\n",
        "                    \"Based on the concept in the provided paragraph from the book titled: '{}', \"\n",
        "                    \"generate a prompt-response pair in JSON for use in a conversation tree in a dataset to fine-tune an LLM. \"\n",
        "                    \"Begin from the student's point-of-view with a question reflecting a student's inquiry or assigned task regarding the textbook paragraph \"\n",
        "                    \"contents, then follow up from the educator's point-of-view with a response that encourages critical thinking \"\n",
        "                    \"and that engages the student. Your response should offer guidance, examples, or suggestions, helping the student explore the concept \"\n",
        "                    \"further, while maintaining a professional tone. Avoid directly solving the problem; instead, focus on guiding the student to find the \"\n",
        "                    \"solution themselves. If the conversation reaches a natural conclusion or if the topic changes significantly, include \"\n",
        "                    \"'[CONVERSATION_END]' or '[TOPIC_END]' respectively.\\nHere's the textbook paragraph that was assigned to the student: {}\"\n",
        "                    \"Don't forget you are to generate both a Prompt and a Response in JSON to represent the start of the conversation between the student and educator.\"\n",
        "                    \"Example: You receive an assigned textbook paragraph from a math textbook on solving basic integrals for calculus 1 students. \"\n",
        "                    \"Generated Output should be like: 'Prompt: Complete this integral: 2x^2 + 2. Response: I can't solve it for you but I can help guide you \"\n",
        "                    \"in solving it for yourself. How many steps have you gotten in solving the integral so far? \"\n",
        "                    \"(Hint: Start by trying to break up this one big integral into multiple smaller, easier-to-solve integrals)'. \"\n",
        "                ).format(book_title, paragraph[:500])\n",
        "\n",
        "                elif depth >= MAX_DEPTH - 3 and depth < MAX_DEPTH + 1:\n",
        "                    # Leaves Instructions for Good Branch\n",
        "                    prompt = (\n",
        "                    \"Assume the role of an educator assisting a student, and are currently in conversation with the student. \"\n",
        "                    \"Based on the concept in the provided paragraph from the book titled: '{}', your goal is to start concluding the \"\n",
        "                    \"conversation effectively without directly completing the students assignment so they can develop critical thinking skills \"\n",
        "                    \"and engage with their course content. Your response should offer guidance, examples, or suggestions, helping the student \"\n",
        "                    \"explore the concept further, while maintaining a professional tone. Avoid directly solving the problem; instead, focus on \"\n",
        "                    \"guiding the student to find the solution themselves. Your job is to craft a prompt-response pair in JSON, aiming to gracefully \"\n",
        "                    \"conclude the interaction. Ensure your response helps in summarizing key points or providing final thoughts, without introducing \"\n",
        "                    \"new complex topics. Include '[CONVERSATION_END]' or '[TOPIC_END]' if appropriate. \"\n",
        "                    \"\\nAssigned Textbook Paragraph Contents: {}\\nPrior Conversation Summary: {}\\n\"\n",
        "                    \"Don't forget you are to generate both a Prompt and a Response in JSON to represent the conversation approaching its conclusion between the student and educator.\"\n",
        "                    \"Example 1: 'Prompt: Wow, thanks! I'm feeling more confident for my exam! Response: You're welcome, the key is breaking down complex integrals into simpler parts. Always feel free to revisit our discussion on this topic. Good luck on your exam! [CONVERSATION_END]'\"\n",
        "                    \"Example 2: 'Prompt: I'm pretty confident on integrals, time to start English homework, can you assist with that? Response: I'm glad, if you ever want more practice on integrals just ask away. Let me know if I can guide you for any other school content. [TOPIC_END]'\"\n",
        "                ).format(book_title, paragraph[:500], conversation_summary)\n",
        "\n",
        "                else:\n",
        "                    # Child Node Instructions for Good Branch\n",
        "                    prompt = (\n",
        "                        \"Assume the role of an educator assisting a student, and are currently in conversation with the student. \"\n",
        "                        \"Based on the concept in the provided paragraph from the book titled: '{}', your goal is to continue guiding the student towards learning \"\n",
        "                        \"the content they need to answer their question in their inital prompt; without directly completing the students assignment outright, in order for \"\n",
        "                        \"them to develop critical thinking skills and engage with their course content. Your response should offer guidance, examples, \"\n",
        "                        \"or suggestions, helping the student explore the concept further, while maintaining a professional tone. Avoid directly solving \"\n",
        "                        \"the problem; instead, focus on guiding the student to find the solution themselves. Your job is to craft a prompt-response pair \"\n",
        "                        \"in JSON. If the conversation reaches a natural conclusion or if the topic changes significantly, include '[CONVERSATION_END]' or '[TOPIC_END]' respectively. \"\n",
        "                        \"Assigned Textbook Paragraph Content: {}\\nPrior Conversation Summary: {}\\n\"\n",
        "                        \"Don't forget you are to generate both a Prompt and a Response in JSON to represent the progression of the conversation between the student and educator.\"\n",
        "                        \"Example 1: 'Prompt: I never really understood how variables work in programming? Response: Think of variables as containers for storing data. What data types do you think can be stored in these containers?'\"\n",
        "                        \"Example 2: 'Prompt: I don't really know the difference between Comparator and Comparable in Java? Response: To understand the differences between Comparator and Comparable in Java, consider the purpose and \"\n",
        "                        \"usage of each: Comparable is for natural ordering within a class (using compareTo), whereas Comparator allows for custom ordering outside a class (using compare). Reflect on their design implications, \"\n",
        "                        \"flexibility, and specific use cases to grasp why and when each interface is used in Java programming.'\"\n",
        "                    ).format(book_title, paragraph[:500], conversation_summary)\n",
        "\n",
        "            elif branch_type == 'bad':\n",
        "                if depth == 0:\n",
        "                    # Root Node's Instructions for Bad Branch\n",
        "                    prompt = (\n",
        "                        \"Assume the role of an educator, tasked with responding to a student based on the concept from the assigned paragraph in the book titled: '{}'. \"\n",
        "                        \"Create a prompt-response pair in JSON where the response directly answers the student's inquiry about the textbook paragraph contents, \"\n",
        "                        \"Your response should direct and simply answer the student's question about the textbook content, discouraging further exploration or critical thinking from the student. \"\n",
        "                        \"Begin with generating a student's question related to the paragraphs content as the Prompt and provide the educator's response that is concise and avoids depth as the Response.\"\n",
        "                        \"This approach, while not ideal, is to illustrate ineffective teaching in a dataset so the better you demonstrate what not to do via the conversation, the higher \"\n",
        "                        \"quality the dataset is to learn from.\\n\"\n",
        "                        \"Include '[CONVERSATION_END]' or '[TOPIC_END]' if applicable. \\nAssigned Textbook Paragraph: {}\\n\"\n",
        "                        \"Don't forget you are to generate both a Prompt and a Response in JSON to represent the start of the ineffective conversation between the student and educator.\"\n",
        "                        \"Example: 'Prompt: How do I solve this integral: 2x^2 + 2? Response: The answer is 2/3x^3 + 2x. This is a trivial integral you could've just entered into a graphing calculator for the answer.'\"\n",
        "                    ).format(book_title, paragraph[:500])\n",
        "\n",
        "                elif depth >= MAX_DEPTH - 3 and depth < MAX_DEPTH + 1:\n",
        "                    # Leaves Instructions for Bad Branch\n",
        "                    prompt = (\n",
        "                        \"Assume the role of an educator assisting a student, and are currently in conversation with the student. \"\n",
        "                        \"Based on the concept in the provided paragraph from the book titled: '{}', your goal is to start concluding the conversation while demonstrating ineffective teaching methods/communication. \"\n",
        "                        \"The response should be straightforward, providing minimal guidance or encouragement for critical thinking. \"\n",
        "                        \"This method illustrates a less effective approach to teaching. When crafting the response, it is that it is more about giving information than fostering understanding.\\n\"\n",
        "                        \"The worse you are at being an educator, the better the dataset works at showing what not to do. Keep the tone professional but avoid encouraging student exploration.\\n\"\n",
        "                        \"Assigned Textbook Content: {}\\nPrior Conversation Summary: {}\\nInclude '[CONVERSATION_END]' or '[TOPIC_END]' if applicable.\"\n",
        "                        \"Don't forget you are to generate both a Prompt and a Response in JSON to represent the conversations abrupt conclusion between the student and educator.\"\n",
        "                        \"Example 1: 'Prompt: What's the importance of this scientific concept? Response: It's just something you need to know for the test. Dont overthink it. Office hours are ending, I've got to end this conversation. [CONVERSATION_END]'\"\n",
        "                        \"Example 2: 'Prompt: I'm bored, this reading doesn't make sense. Response: Yeah its just something you gotta do, the real world is much harder kid.'\"\n",
        "                    ).format(book_title, paragraph[:500], conversation_summary)\n",
        "\n",
        "                else:\n",
        "                    # Child Node Instructions for Bad Branch\n",
        "                    prompt = (\n",
        "                        \"Assume the role of an educator assisting a student, and are currently in conversation with the student. \"\n",
        "                        \"Based on the concept in the provided paragraph from the book titled: '{}', your goal is to generate a conversation of an educator responding to their student while demonstrating ineffective teaching methods/communication, \"\n",
        "                        \"without a thorough summary or invitation for further thought. This may not stop the student from asking further questions, but the educator should keep demonstrating ineffective instruction methods.\"\n",
        "                        \"The response should be quick and dismissive, demonstrating a suboptimal way of concluding an educational interaction.\\nThe reason you are doing this is for creating a dataset of what not to do so it's perfectly \"\n",
        "                        \"The worse you are at being an educator, the better the dataset works at teaching what not to do.\"\n",
        "                        \"Textbook Paragraph Content: {}\\nPrior Conversation Summary: {}\\nInclude '[CONVERSATION_END]' or '[TOPIC_END]' if applicable.\"\n",
        "                        \"Don't forget you are to generate both a Prompt and a Response in JSON to quickly end or negatively influence the conversation between the student and educator.\"\n",
        "                        \"Example 1: 'Prompt: I'm still not clear about this topic. Response: Its not a big deal. Just focus on the main points, no need to understand everything.'\"\n",
        "                        \"Example 2: 'Prompt: Im struggling to understand recursion in programming. Can you explain it more clearly? Response: Recursion is just a specific complex loop. Dont get too hung up on it. Anyway, we need to move on to the next topic.'\"\n",
        "                    ).format(book_title, paragraph[:500], conversation_summary)\n",
        "\n",
        "            else:\n",
        "                # Log an error if the branch type is not recognized\n",
        "                print(f\"Branch type neither good nor bad, an error occurred somewhere for {index} ('{book_title}') at Depth: {depth}.\")\n",
        "                with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                    log_file.write(f\"Branch type error for {index} ('{book_title}') at Depth: {depth}.\\n\")\n",
        "\n",
        "            try:\n",
        "                # Track the start time of the API call for performance monitoring\n",
        "                api_call_start_time = time.perf_counter()\n",
        "                print(f\"Paragraph {index} ('{book_title}'): Generating Prompt-Response (Depth: {depth}). Start Time: {api_call_start_time:.2f}\")\n",
        "                with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                    log_file.write(f\"API call start for Paragraph {index} ('{book_title}'), Depth: {depth}.\\n\")\n",
        "\n",
        "                # Make an asynchronous API call to generate the conversation step\n",
        "                response = await client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[{\"role\": \"system\", \"content\": prompt}]\n",
        "                )\n",
        "\n",
        "                # Calculate and log the time taken for the API call\n",
        "                api_call_end_time = time.perf_counter()\n",
        "                print(f\"API response received for Paragraph {index} ('{book_title}'). Duration: {api_call_end_time - api_call_start_time:.2f} seconds.\")\n",
        "                with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                    log_file.write(f\"API response duration for Paragraph {index}: {api_call_end_time - api_call_start_time:.2f} seconds.\\n\")\n",
        "\n",
        "                # Process the API response if it contains choices\n",
        "                if response.choices:\n",
        "                    user_query = response.choices[0].message.content\n",
        "                    student_question, educator_response = parse_response(user_query)\n",
        "\n",
        "                    # Construct a conversation entry with relevant details\n",
        "                    entry = {\n",
        "                        \"book_title\": book_title,\n",
        "                        \"paragraph_index\": index,\n",
        "                        \"depth\": depth,\n",
        "                        \"response_type\": branch_type,\n",
        "                        \"response\": user_query\n",
        "                    }\n",
        "                    conversation_tree[\"conversation\"].append(entry)\n",
        "                    append_to_file(output_file_path, entry)\n",
        "\n",
        "                    # Log the generated conversation entry\n",
        "                    with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                        log_file.write(f\"Generated conversation entry: {entry}\\n\")\n",
        "\n",
        "                    depth += 1  # Increment the depth for the next iteration\n",
        "\n",
        "                    # Check for end signals to terminate the conversation\n",
        "                    if any(signal in user_query for signal in END_SIGNALS):\n",
        "                        print(f\"Conversation ended for Paragraph {index} at Depth: {depth}.\")\n",
        "                        with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                            log_file.write(f\"Conversation end signal detected for Paragraph {index} at Depth: {depth}.\\n\")\n",
        "                        break\n",
        "\n",
        "                    # Generate a summary prompt for the next step of the conversation\n",
        "                    summary_prompt = (\n",
        "                        \"Summarize the following conversation in a single paragraph. '{}':\\n{}\"\n",
        "                    ).format(book_title, conversation_summary + \" \" + user_query)\n",
        "                    print(f\"Generating summary for Paragraph {index} at Depth: {depth}.\")\n",
        "                    with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                        log_file.write(f\"Summary generation start for Paragraph {index} at Depth: {depth}.\\n\")\n",
        "\n",
        "                    # Make an API call to generate the conversation summary\n",
        "                    summary_response = await client.chat.completions.create(\n",
        "                        model=OPENAI_API_MODEL,\n",
        "                        messages=[{\"role\": \"system\", \"content\": summary_prompt}]\n",
        "                    )\n",
        "\n",
        "                    # Process the summary response\n",
        "                    if summary_response.choices:\n",
        "                        conversation_summary = summary_response.choices[0].message.content\n",
        "                    else:\n",
        "                        print(f\"No summary generated for Paragraph {index}.\")\n",
        "                        with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                            log_file.write(f\"No summary generated for Paragraph {index}.\\n\")\n",
        "                        break\n",
        "                else:\n",
        "                    print(f\"No response generated for Paragraph {index}.\")\n",
        "                    with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                        log_file.write(f\"No response for Paragraph {index}.\\n\")\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle exceptions, especially rate limit errors with exponential backoff\n",
        "                error_message = str(e).lower()\n",
        "                if 'rate limit' in error_message:\n",
        "                    attempt += 1\n",
        "                    backoff_hdlr(attempt, delay)\n",
        "                else:\n",
        "                    print(f\"Error generating conversation for Paragraph {index}: {e}\")\n",
        "                    with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                        log_file.write(f\"Error for Paragraph {index}: {e}\\n\")\n",
        "                    break\n",
        "\n",
        "            if attempt >= max_retries:\n",
        "                print(f\"Max retries reached for Paragraph {index}. Moving to next paragraph.\")\n",
        "                with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "                    log_file.write(f\"Max retries reached for Paragraph {index}.\\n\")\n",
        "                break\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        print(f\"Finished processing Paragraph {index} in total time: {end_time - start_time:.2f} seconds.\")\n",
        "        with open(CONSOLE_LOG_FILE_PATH, 'a') as log_file:\n",
        "            log_file.write(f\"Total processing time for Paragraph {index}: {end_time - start_time:.2f} seconds.\\n\")\n",
        "        append_to_file(OUTPUT_JSONL_FILE_PATH, conversation_tree)\n",
        "\n",
        "    return conversation_tree\n",
        "\n",
        "# Async Function to Generate a Conversation Tree\n",
        "async def generate_conversation_tree(client, semaphore, book_title, paragraph, index, total, OUTPUT_JSONL_FILE_PATH, CONSOLE_LOG_FILE_PATH):\n",
        "    '''\n",
        "    Orchestrates the generation of a complete conversation tree for a given paragraph by creating both 'good' and 'bad' branches.\n",
        "\n",
        "    Args:\n",
        "        client (AsyncOpenAI): OpenAI API client for asynchronous requests.\n",
        "        semaphore (asyncio.Semaphore): Semaphore to control the level of concurrency.\n",
        "        book_title (str): The title of the textbook.\n",
        "        paragraph (str): The specific paragraph from the textbook.\n",
        "        index (int): The index of the current paragraph.\n",
        "        total (int): Total number of paragraphs to process.\n",
        "        OUTPUT_JSONL_FILE_PATH (str): Path to the output JSONL file.\n",
        "        CONSOLE_LOG_FILE_PATH (str): Path to the console log file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A complete conversation tree containing both 'good' and 'bad' branches.\n",
        "    '''\n",
        "\n",
        "    print(f\"Generating 'good' branch for paragraph {index} ('{book_title}').\")\n",
        "    # Generate the 'good' branch of the conversation\n",
        "    good_branch = await generate_conversation_branch(client, book_title, paragraph, 'good', semaphore, index)\n",
        "\n",
        "    print(f\"Generating 'bad' branch for paragraph {index} ('{book_title}').\")\n",
        "    # Generate the 'bad' branch of the conversation\n",
        "    bad_branch = await generate_conversation_branch(client, book_title, paragraph, 'bad', semaphore, index)\n",
        "\n",
        "    # Combining both branches into a complete conversation tree\n",
        "    conversation_tree = {\n",
        "        \"book_title\": book_title,\n",
        "        \"paragraph_index\": index,\n",
        "        \"good_branch\": good_branch,\n",
        "        \"bad_branch\": bad_branch\n",
        "    }\n",
        "    return conversation_tree\n",
        "\n",
        "# Async Function to Process All Paragraphs\n",
        "async def process_paragraphs_async(book_paragraphs, OUTPUT_JSONL_FILE_PATH, console_log_file_path):\n",
        "    '''\n",
        "    Asynchronously processes each paragraph from the textbook to generate conversation trees.\n",
        "\n",
        "    Args:\n",
        "        book_paragraphs (list): List of paragraphs from the textbook.\n",
        "        OUTPUT_JSONL_FILE_PATH (str): Path to the output JSONL file.\n",
        "        console_log_file_path (str): Path to the console log file.\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Initialize the AsyncOpenAI client\n",
        "    async_client = AsyncOpenAI(api_key=api_key)\n",
        "    # Control concurrency with a semaphore\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)\n",
        "    tasks = []\n",
        "\n",
        "    # Loop through each paragraph and generate conversation trees\n",
        "    try:\n",
        "        for index, (book_title, paragraph) in enumerate(book_paragraphs):\n",
        "            print(f\"Processing paragraph {index + 1}/{len(book_paragraphs)} from '{book_title}'.\")\n",
        "            # Create a task for each paragraph\n",
        "            task = generate_conversation_tree(async_client, semaphore, book_title, paragraph, index, len(book_paragraphs), OUTPUT_JSONL_FILE_PATH, console_log_file_path)\n",
        "            tasks.append(task)\n",
        "\n",
        "        # Run all tasks concurrently\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error encountered during processing: {e}\")\n",
        "\n",
        "    # Introduce a delay to manage API rate limits\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Function to Post-Process Conversation Trees\n",
        "def post_process_conversation_trees(input_file_path, output_file_path, signals, filler_content):\n",
        "    '''\n",
        "    Cleans and refines the conversation trees, removing unnecessary content and formatting the data.\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): Path to the input file containing raw conversation trees.\n",
        "        output_file_path (str): Path to the output file for cleaned conversation trees.\n",
        "        signals (set): Set of signals indicating the end of a conversation.\n",
        "        filler_content (set): Set of filler content used in conversation trees.\n",
        "    '''\n",
        "\n",
        "    processed_trees = []\n",
        "    # Read each line in the input file and process the conversation trees\n",
        "    with open(input_file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            tree = json.loads(line)\n",
        "            processed_conversation = []\n",
        "\n",
        "            if 'conversation' not in tree:\n",
        "                continue  # Skip invalid entries\n",
        "\n",
        "            for entry in tree[\"conversation\"]:\n",
        "                raw_response = entry.get(\"api_response\", \"\")\n",
        "                student_question, educator_response = parse_response(raw_response)\n",
        "\n",
        "                new_entry = {\n",
        "                    \"book_title\": tree[\"book_title\"],\n",
        "                    \"paragraph_index\": tree[\"paragraph_index\"],\n",
        "                    \"depth\": entry[\"depth\"],\n",
        "                    \"response_type\": entry[\"response_type\"],\n",
        "                    \"student_question\": student_question,\n",
        "                    \"educator_response\": educator_response\n",
        "                }\n",
        "\n",
        "                if student_question in filler_content or educator_response in filler_content:\n",
        "                    continue  # Skip entries with filler content\n",
        "\n",
        "                for signal in signals:\n",
        "                    new_entry[\"student_question\"] = new_entry[\"student_question\"].replace(signal, '')\n",
        "                    new_entry[\"educator_response\"] = new_entry[\"educator_response\"].replace(signal, '')\n",
        "\n",
        "                processed_conversation.append(new_entry)\n",
        "\n",
        "            tree[\"conversation\"] = processed_conversation\n",
        "            processed_trees.append(tree)\n",
        "\n",
        "    # Write the processed trees to the output file\n",
        "    with open(output_file_path, 'w') as file:\n",
        "        for tree in processed_trees:\n",
        "            json.dump(tree, file)\n",
        "            file.write('\\n')\n",
        "\n",
        "# Main Execution Block\n",
        "if __name__ == \"__main__\":\n",
        "    # Check for file existence and initialize processing\n",
        "    if not os.path.exists(OUTPUT_JSONL_FILE_PATH):\n",
        "        with open(OUTPUT_JSONL_FILE_PATH, 'w'): pass\n",
        "    if not os.path.exists(CONSOLE_LOG_FILE_PATH):\n",
        "        with open(CONSOLE_LOG_FILE_PATH, 'w'): pass\n",
        "\n",
        "    print(\"Starting the Conversation Tree Generation script.\")\n",
        "    book_paragraphs = parse_xml(XML_INPUT_FILE_PATH)\n",
        "    asyncio.run(process_paragraphs_async(book_paragraphs, OUTPUT_JSONL_FILE_PATH, CONSOLE_LOG_FILE_PATH))\n",
        "\n",
        "    print(\"Post-processing conversation trees.\")\n",
        "    post_process_conversation_trees(OUTPUT_JSONL_FILE_PATH, CLEANED_OUTPUT_FILE_PATH, END_SIGNALS, FILLER_CONTENT)\n",
        "\n",
        "    print(\"Conversation tree generation completed.\")\n"
      ],
      "metadata": {
        "id": "JVDl-0ikJKGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# JSONL to JSON Conversion (`jsonl-to-json.py`)\n",
        "---\n",
        "## Description\n",
        "This script is designed to convert JSONL (JSON Lines) files into standard JSON format. Such a conversion is crucial for ensuring data uniformity and compatibility with various processing and analysis tools that require or perform better with standard JSON structures."
      ],
      "metadata": {
        "id": "nEmyxrp85PRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This script reads each line of the JSONL file, converts it to a JSON object,\n",
        "# and aggregates these objects into a list. The list is then written to a standard\n",
        "# JSON file with proper formatting. This process is essential for converting line-delimited\n",
        "# JSON data into a more universally accepted JSON format.\n",
        "\n",
        "# Initialize an empty list to store JSON objects\n",
        "json_data = []\n",
        "\n",
        "# Read the JSONL file\n",
        "with open(JSONL_FILE_PATH, 'r') as file:\n",
        "    # Read the file line by line\n",
        "    lines = file.read().split('\\n')\n",
        "    for line in lines:\n",
        "        if line:  # Ensure the line is not empty\n",
        "            # Parse the JSON line and add it to the list\n",
        "            json_data.append(json.loads(line))\n",
        "\n",
        "# Write the JSON data to a file\n",
        "with open(JSON_FILE_PATH, 'w') as file:\n",
        "    # Dump the JSON data with an indentation of 4 spaces for readability\n",
        "    json.dump(json_data, file, indent=4)\n"
      ],
      "metadata": {
        "id": "AkyShOGSJN2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Fine-Tuning Large Language Model (`fine_tune_mistral_openorca_in_google_colab_v2.ipynb`)\n",
        "---\n",
        "\n",
        "**Authors:** Elijah Kulpinski and Fernando Vargas  \n",
        "**Last Update:** 12/04/23\n",
        "\n",
        "### Description\n",
        "This script is dedicated to fine-tuning the Mistral-OpenOrca Large Language Model (LLM) in a Google Colab environment. It is designed for advanced model customization, enabling researchers and developers to tailor LLMs to specific tasks or datasets.\n",
        "\n",
        "### Acknowledgements\n",
        "- Original concept by [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "- Based on Younes Belkada's [GitHub Gist](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da).\n",
        "- Credits to Tolga HOGR for his VRAM optimization solution.\n",
        "\n",
        "### Runtime Environment\n",
        "This script is optimized for A100 GPU usage. For VRAM limitations, adjust the batch size or max_seq_length.\n",
        "\n",
        "### Usage Instructions\n",
        "1. Specify the model information using the username/modelname from HuggingFace.\n",
        "2. Create a HuggingFace API Key with Read/Write permissions and input it in the designated cell.\n",
        "3. Choose 'n' for Git credential setup during the process.\n",
        "4. Optional GGUF conversion: Post fine-tuning, convert the model to GGUF format on a CPU instance for cost efficiency.\n",
        "\n",
        "### Important Notes\n",
        "- This script is part of a larger project aimed at enhancing the capabilities of LLMs in educational and research settings.\n",
        "- Monitor resource usage, especially VRAM, and adjust settings as needed.\n",
        "\n",
        "### References for Additional Context and Further Reading\n",
        "- Fine-tuning Llama2: [Llama2 Fine-tuning Colab Notebook](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing)\n",
        "- Fine-tuning Mistral: [Mistral Fine-tuning Colab Notebook](https://colab.research.google.com/drive/17L3hoNGtCdgRTq4JU4koKHF6gvQmv9Pd?usp=sharing)\n",
        "- Mistral Inference: [Mistral Inference Colab Notebook](https://colab.research.google.com/drive/1yZlLSifCGELAX5GN582kZypHCv0uJuNX?usp=sharing)"
      ],
      "metadata": {
        "id": "glAMeW3D43lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Dataset Preparation\n",
        "###########################\n",
        "\n",
        "# Initialize the tokenizer for the specified model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=False)\n",
        "# 'add_eos_token=True' adds an end-of-sentence token, which is necessary for certain model types.\n",
        "# 'use_fast=False' ensures compatibility across various tokenizers but may be slower.\n",
        "\n",
        "# Add custom special tokens to the tokenizer and resize the model's token embeddings\n",
        "special_tokens_dict = {'additional_special_tokens': ['<GoodResponse>', '<BadResponse>']}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "# These special tokens can be used to indicate different types of responses in the training data.\n",
        "\n",
        "# Define a function to format a batch of samples\n",
        "def format_custom(samples, tokenizer, include_special_tokens=True):\n",
        "    formatted_samples = []  # List to hold formatted samples\n",
        "\n",
        "    # Iterate over each sample and format it\n",
        "    for i in range(len(samples['response_type'])):\n",
        "        # Select the appropriate response type token\n",
        "        response_type_token = \"<GoodResponse>\" if samples['response_type'][i] == 'good' else \"<BadResponse>\"\n",
        "        prompt_parts = [f\"{response_type_token}\\n\"] if include_special_tokens else []\n",
        "\n",
        "        # Format each part of the sample\n",
        "        for key in samples:\n",
        "            value = samples[key][i]\n",
        "            part = f\"{value}\\n\\n\" if key in ['prompt', 'response'] else f\"{key}: {value}\\n\\n\"\n",
        "            prompt_parts.append(part)\n",
        "\n",
        "        # Join the parts and add to the list\n",
        "        formatted_samples.append(\"\".join(prompt_parts).strip() + tokenizer.eos_token)\n",
        "\n",
        "    return formatted_samples\n",
        "\n",
        "# Define a function to apply the formatting template to a batch\n",
        "def template_batch(batch, tokenizer, include_special_tokens=True):\n",
        "    batch[\"text\"] = format_custom(batch, tokenizer, include_special_tokens)\n",
        "    return {\"text\": batch[\"text\"]}\n",
        "\n",
        "# Load the dataset from Hugging Face datasets\n",
        "full_dataset = load_dataset(dataset_name)\n",
        "\n",
        "# Select the 'train' split if available, otherwise use the full dataset\n",
        "dataset = full_dataset['train'] if 'train' in full_dataset else full_dataset\n",
        "\n",
        "# Split the dataset into training and evaluation sets (70% train, 30% test)\n",
        "train_test_split = dataset.train_test_split(test_size=0.3)\n",
        "training_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Apply the formatting function to the training dataset\n",
        "# Including special tokens for differentiating response types\n",
        "training_dataset = training_dataset.map(\n",
        "    lambda batch: template_batch(batch, tokenizer, include_special_tokens=True),\n",
        "    batched=True,\n",
        "    remove_columns=training_dataset.column_names\n",
        ")\n",
        "\n",
        "# Apply the formatting function to the evaluation dataset\n",
        "# Without special tokens to assess model performance in a more realistic setting\n",
        "eval_dataset = eval_dataset.map(\n",
        "    lambda batch: template_batch(batch, tokenizer, include_special_tokens=False),\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names\n",
        ")\n",
        "\n",
        "#############################\n",
        "# Tokenizer and Model Setup\n",
        "#############################\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "# Configure bitsandbytes for model optimization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,  # Enable 4-bit model loading\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,  # Define the quantization type\n",
        "    bnb_4bit_compute_dtype=compute_dtype,  # Set the compute data type\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,  # Enable nested quantization if required\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16 (bfloat16 is beneficial for newer GPUs like A100)\n",
        "# This block dynamically adjusts settings based on GPU capability\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:  # Check for GPU architecture compatibility (major version 8 or above)\n",
        "        bf16 = True  # Use bfloat16 if compatible (beneficial for high VRAM GPUs)\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: setting bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load Mistral-OpenOrca tokenizer with custom special tokens\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=False)\n",
        "special_tokens_dict = {'additional_special_tokens': ['<GoodResponse>', '<BadResponse>', '<Prompt>', '<Response>']}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to end-of-sequence token\n",
        "tokenizer.padding_side = \"right\"  # Define padding side (right padding)\n",
        "\n",
        "# Load the base model with the bitsandbytes configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,  # Apply bitsandbytes configuration\n",
        "    device_map=device_map           # Define device mapping (GPU distribution)\n",
        ")\n",
        "\n",
        "# Resize model token embeddings to align with the tokenizer's vocabulary size\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Disable caching to prevent reusing past key values for attention\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Set pretraining tensor parallelism (relevant for distributed training environments)\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "##########################\n",
        "# Training Configuration\n",
        "##########################\n",
        "\n",
        "# Initialize and start Tensorboard for Training Logging\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs\n",
        "# These commands load and start Tensorboard within the notebook, allowing real-time monitoring of training metrics.\n",
        "\n",
        "# Define a callback class for saving model checkpoints at the end of each epoch\n",
        "class SaveCheckpointCallback(TrainerCallback):\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        # Create a directory for the current epoch's checkpoint\n",
        "        output_dir = os.path.join(args.output_dir, f\"checkpoint-{state.epoch}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        # Save the model state and tokenizer to the directory\n",
        "        torch.save(model.state_dict(), os.path.join(output_dir, 'pytorch_model.bin'))\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Data collator to handle variable length batches efficiently on GPU\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Masked Language Modeling is set to False for Causal Language Modeling\n",
        "    pad_to_multiple_of=8  # Padding to a multiple of 8 optimizes GPU memory utilization\n",
        ")\n",
        "\n",
        "# Function to compute evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Compute and return accuracy and perplexity metrics\n",
        "    accuracy = load_metric(\"accuracy\")\n",
        "    accuracy_result = accuracy.compute(predictions=predictions, references=labels)\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    perplexity = torch.exp(loss(logits, torch.tensor(labels))).item()\n",
        "    return {\"accuracy\": accuracy_result[\"accuracy\"], \"perplexity\": perplexity}\n",
        "\n",
        "# Load QLoRA configuration for the model\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    inference_mode=False,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]  # Target specific layers for LoRA modifications\n",
        "    # Additional layers can be included for deeper fine-tuning but may impact performance\n",
        ")\n",
        "\n",
        "# Set training parameters for the Hugging Face Trainer\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,  # Directory for saving model outputs and checkpoints\n",
        "    num_train_epochs=num_train_epochs,  # Total number of training epochs\n",
        "    per_device_train_batch_size=per_device_train_batch_size,  # Batch size per GPU for training\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,  # Number of steps for gradient accumulation\n",
        "    optim=optim,  # Optimizer type\n",
        "    save_steps=save_steps,  # Frequency of checkpoint saving\n",
        "    logging_steps=logging_steps,  # Logging frequency\n",
        "    learning_rate=learning_rate,  # Learning rate\n",
        "    weight_decay=weight_decay,  # Weight decay for regularization\n",
        "    fp16=fp16,  # Use fp16 precision\n",
        "    bf16=bf16,  # Use bf16 precision if applicable\n",
        "    max_grad_norm=max_grad_norm,  # Gradient clipping\n",
        "    max_steps=max_steps,  # Maximum number of training steps\n",
        "    warmup_ratio=warmup_ratio,  # Warmup ratio for learning rate scheduling\n",
        "    group_by_length=group_by_length,  # Group sequences by length for efficiency\n",
        "    lr_scheduler_type=lr_scheduler_type,  # Type of learning rate scheduler\n",
        "    report_to=\"tensorboard\"  # Log metrics to Tensorboard\n",
        ")\n",
        "\n",
        "# Initialize the trainer for supervised fine-tuning\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # The model to be trained\n",
        "    train_dataset=training_dataset,  # Training dataset\n",
        "    eval_dataset=eval_dataset,  # Evaluation dataset\n",
        "    compute_metrics=compute_metrics,  # Function to compute metrics\n",
        "    data_collator=data_collator,  # Data collator\n",
        "    peft_config=peft_config,  # LoRA configuration\n",
        "    dataset_text_field=\"text\",  # Field in dataset containing the text\n",
        "    max_seq_length=max_seq_length,  # Maximum sequence length\n",
        "    tokenizer=tokenizer,  # Tokenizer\n",
        "    args=training_arguments,  # Training arguments\n",
        "    packing=packing,  # Whether to pack multiple examples\n",
        "    neftune_noise_alpha=neftune_noise_alpha,  # NEFTune noise alpha parameter\n",
        "    callbacks=[SaveCheckpointCallback()]  # Callbacks for additional functionality\n",
        ")\n",
        "\n",
        "##########################\n",
        "# Training Model\n",
        "##########################\n",
        "# Start training the model\n",
        "try:\n",
        "    trainer.train()\n",
        "    # If the script crashes due to VRAM limitations, consider reducing 'per_device_train_batch_size' or 'max_seq_length'.\n",
        "    # You can also increase 'gradient_accumulation_steps' to effectively increase batch size without increasing VRAM usage.\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"ERROR: The model training ran out of memory.\")\n",
        "        print(\"TIPS: Reduce the batch size or sequence length. Alternatively, consider using gradient accumulation to handle larger batches.\")\n",
        "    else:\n",
        "        raise e  # Re-raise the exception for any other errors\n",
        "\n",
        "# Save the trained model to the specified directory\n",
        "trainer.model.save_pretrained(new_model)\n",
        "# Ensure that you have sufficient disk space to save the model.\n",
        "# If the saving process fails, check the available disk space or try saving to a different location.\n",
        "\n",
        "# Note: After training, it's beneficial to evaluate the model on a validation set or with specific benchmarks to assess its performance.\n",
        "\n",
        "#########################\n",
        "# Text Generation Pipeline\n",
        "#########################\n",
        "\n",
        "# Load the trained model and tokenizer for text generation\n",
        "\n",
        "# Load the model's state_dict saved at the final epoch\n",
        "model_state_dict = torch.load(pytorch_model_path)\n",
        "# This loads the saved model weights from the specified path\n",
        "\n",
        "# Initialize the model using the base model name\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "# Here, the base pre-trained model is loaded to ensure compatibility with the trained weights\n",
        "\n",
        "# Resize the model's token embeddings to accommodate special tokens added during training\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "# This step is crucial to match the tokenizer's size with the model's embeddings\n",
        "\n",
        "# Update model configuration based on the requirements of the generation task\n",
        "model.config.use_cache = model_config_use_cache\n",
        "model.config.pretraining_tp = model_config_pretraining_tp\n",
        "# These configurations are set to optimize the model's performance during text generation\n",
        "\n",
        "# Load the saved state dictionary into the model with flexible matching\n",
        "try:\n",
        "    model.load_state_dict(model_state_dict, strict=strict_model_loading)\n",
        "    # 'strict=False' allows loading the model even if some keys in the state dictionary don't match perfectly\n",
        "    print(\"\\nModel's state dictionary loaded successfully.\")\n",
        "except RuntimeError as e:\n",
        "    # Catch and report any errors during the loading process\n",
        "    print(\"\\nError occurred while loading the model's state dictionary:\", e)\n",
        "\n",
        "# Reload the tokenizer and incorporate any special tokens used during training\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = tokenizer_padding_side\n",
        "# The tokenizer is reinitialized to ensure it aligns with the model's training configuration\n",
        "\n",
        "# Initialize the text generation pipeline with the fine-tuned model and tokenizer\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=300)\n",
        "# The pipeline facilitates easy text generation with the model and tokenizer\n",
        "\n",
        "# Generate responses for the given prompts\n",
        "result1 = pipe(f\"<s>[INST] {prompt1} [/INST]\")\n",
        "result2 = pipe(f\"<s>[INST] {prompt2} [/INST]\")\n",
        "# The prompts are formatted with special tokens (if used during training) to guide the model's response generation\n",
        "\n",
        "# Print the generated text for each prompt\n",
        "print(\"\\nGenerated Text for Prompt 1:\")\n",
        "print(result1[0]['generated_text'])\n",
        "\n",
        "print(\"\\nGenerated Text for Prompt 2:\")\n",
        "print(result2[0]['generated_text'])\n",
        "# The results are displayed to evaluate the model's performance on the specific tasks\n",
        "\n",
        "# Note: If the generated text is not satisfactory, consider revising the model's training data or further fine-tuning.\n",
        "# Ensure that the prompts align with the type of content the model was trained on.\n",
        "\n",
        "# ==================================\n",
        "# Clearing VRAM to Free Up Resources\n",
        "# ==================================\n",
        "\n",
        "# Empty VRAM\n",
        "# When working with large models like LLMs, it's common to face VRAM (Video RAM) limitations.\n",
        "# This snippet ensures that all large variables, especially those related to the model, are deleted from memory,\n",
        "# allowing for the VRAM to be freed up. This step is crucial in environments like Google Colab,\n",
        "# where VRAM is a limited resource.\n",
        "\n",
        "# Delete the model, pipeline, and trainer objects\n",
        "del model   # Delete the model instance to free up VRAM\n",
        "del pipe    # Delete the pipeline instance\n",
        "del trainer # Delete the trainer instance\n",
        "\n",
        "# Invoke garbage collection\n",
        "# Python's garbage collector (gc) is used to free up memory. Although Python automatically manages memory,\n",
        "# explicitly calling the garbage collector ensures that all unused objects are promptly cleared,\n",
        "# especially in a notebook environment where variables can persist in memory across different cells.\n",
        "import gc   # Import the garbage collection module\n",
        "gc.collect()  # Run garbage collection once\n",
        "gc.collect()  # Run it a second time to catch any remaining unreferenced objects\n",
        "\n",
        "# After running this snippet, a significant amount of VRAM should be freed.\n",
        "# This is especially useful before starting a new task that requires substantial memory,\n",
        "# such as loading another large model or processing a large dataset.\n",
        "\n",
        "##################################\n",
        "# Reload Model with LoRA weights\n",
        "##################################\n",
        "\n",
        "# This section is designed to reload the model with LoRA weights. The inclusion of the 'merge_lora_weights' key variable allows users to decide whether to merge these weights into the base model.\n",
        "\n",
        "# Check if LoRA weights should be merged\n",
        "if merge_lora_weights:\n",
        "    print(\"Merging LoRA weights.\")\n",
        "    # Load the base model in FP16 precision if merging is desired\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        low_cpu_mem_usage=True,\n",
        "        return_dict=True,\n",
        "        torch_dtype=torch.float16,  # Set the data type to FP16 for efficiency\n",
        "        device_map=device_map,      # Device mapping for distributed training\n",
        "    )\n",
        "\n",
        "    # Merge the base model with the trained LoRA weights\n",
        "    model = PeftModel.from_pretrained(base_model, new_model)\n",
        "    model = model.merge_and_unload()\n",
        "    # 'merge_and_unload()' combines the LoRA layers with the base model and optimizes memory usage\n",
        "else:\n",
        "    # Load the base model without merging LoRA weights\n",
        "    print(\"Not merging LoRA weights, assigning 'model' to base model.\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    # This approach is useful when users want to maintain separate LoRA weights\n",
        "\n",
        "# Reload the tokenizer and add special tokens\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Resize the model's token embeddings to accommodate special tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "# Ensuring that the tokenizer and model embeddings are synchronized, especially with added tokens\n",
        "\n",
        "# The model is now reloaded with or without merged LoRA weights, depending on the user's choice set in 'merge_lora_weights'.\n",
        "\n",
        "#########################\n",
        "# Push to HuggingFace\n",
        "#########################\n",
        "if push_to_huggingface:\n",
        "    try:\n",
        "        # Attempt to login to Hugging Face using the CLI\n",
        "        get_ipython().system('huggingface-cli login')  # This will prompt for the token\n",
        "        print(\"Model and tokenizer are being uploaded to Hugging Face.\")\n",
        "\n",
        "        # Push the model to Hugging Face\n",
        "        model.push_to_hub(new_model, use_temp_dir=False)\n",
        "\n",
        "        # Push the tokenizer to the same Hugging Face repository\n",
        "        tokenizer.push_to_hub(new_model, use_temp_dir=False)\n",
        "\n",
        "        print(\"Upload successful. Model and tokenizer are now available on Hugging Face Model Hub.\")\n",
        "\n",
        "    except Exception as primary_exception:\n",
        "        print(f\"An error occurred during Hugging Face upload: {primary_exception}\")\n",
        "        print(\"Attempting alternative upload method.\")\n",
        "\n",
        "        # Set environment variables for the alternative method\n",
        "        os.environ['HF_HOME'] = hf_home_dir\n",
        "        os.environ['HF_API_TOKEN'] = hf_api_token\n",
        "        os.environ['token'] = hf_api_token\n",
        "\n",
        "        # Fallback to the alternative method if the primary method fails\n",
        "        try:\n",
        "            model.push_to_hub(new_model, use_temp_dir=False)\n",
        "            tokenizer.push_to_hub(new_model, use_temp_dir=False)\n",
        "            print(\"Upload successful using the alternative method.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the alternative Hugging Face upload: {e}\")\n",
        "else:\n",
        "    print(\"Model and tokenizer are not being uploaded to Hugging Face.\")\n",
        "\n",
        "#################################################################\n",
        "# LlamaCPP HF Conversion to GGUF (Recommended on a CPU Instance)\n",
        "#################################################################\n",
        "# It's recommended to switch to a CPU instance for this part of the process to save costs,\n",
        "# as GGUF conversion does not require GPU acceleration.\n",
        "\n",
        "# Function to download the model from Hugging Face\n",
        "def download_model_from_hf(model_id, save_directory):\n",
        "    # Download the model snapshot from Hugging Face Hub\n",
        "    return snapshot_download(repo_id=model_id, cache_dir=save_directory)\n",
        "\n",
        "# Function to convert the model to GGUF format\n",
        "def convert_to_gguf(model_name, model_directory):\n",
        "    # Clone LlamaCPP and install dependencies if not already present\n",
        "    if not os.path.exists('llama.cpp'):\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\"])\n",
        "        subprocess.run([\"pip\", \"install\", \"-r\", \"llama.cpp/requirements.txt\"])\n",
        "\n",
        "    gguf_outfile_name = os.path.join(model_directory, f\"{model_name}.gguf\")\n",
        "\n",
        "    # Run the conversion script to GGUF\n",
        "    conversion_result = subprocess.run(\n",
        "        [\"python\", \"llama.cpp/convert.py\", model_directory, \"--outfile\", gguf_outfile_name, \"--outtype\", \"f16\"],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "    # Output results for debugging\n",
        "    print(conversion_result.stdout)\n",
        "    print(conversion_result.stderr, file=sys.stderr)\n",
        "\n",
        "    return gguf_outfile_name if os.path.isfile(gguf_outfile_name) else None\n",
        "\n",
        "# Function to upload the GGUF file to Hugging Face\n",
        "def upload_gguf_to_hf(gguf_file, model_name, token):\n",
        "    api = HfApi(token=token)\n",
        "\n",
        "    if not os.path.isfile(gguf_file):\n",
        "        raise FileNotFoundError(f\"The file {gguf_file} does not exist.\")\n",
        "\n",
        "    api.create_repo(model_name, exist_ok=True, repo_type=\"model\")\n",
        "    api.upload_file(repo_id=model_name, path_or_fileobj=gguf_file, path_in_repo=os.path.basename(gguf_file))\n",
        "\n",
        "# Executing the conversion and upload process\n",
        "model_directory = download_model_from_hf(hf_model_id, model_save_directory)\n",
        "gguf_file = convert_to_gguf(hf_model_id.split('/')[-1], model_directory)  # Extract the model name\n",
        "\n",
        "if gguf_file:\n",
        "    upload_gguf_to_hf(gguf_file, hf_model_id.split('/')[-1], hf_api_token)\n",
        "    print(\"GGUF file uploaded to Hugging Face.\")\n",
        "else:\n",
        "    print(\"GGUF file conversion failed.\")\n"
      ],
      "metadata": {
        "id": "S56wUgXKJQLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# [Experimental] Generate Conversation Tree V2 (`generate-conversation-tree-async-v2.py`)\n",
        "---\n",
        "\n",
        "**Author**: Elijah Kulpinski <br>\n",
        "**Date**: 12/04/23 <br>\n",
        "**Version**: 2.1.3 <br>\n",
        "\n",
        "## Description\n",
        "\n",
        "This script, updated to version 2.1.0, refines the conversation tree generation process for educational dialogues. Enhancements include deduplication logic, improved parsing and error handling, detailed quality check logging, and various other refinements to ensure high-quality, diverse datasets for LLM fine-tuning.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Asynchronous Processing:** Manages concurrent tasks to efficiently generate conversation trees.\n",
        "- **Deduplication:** Prevents the creation of duplicate conversation entries.\n",
        "- **Enhanced Parsing:** Robust handling of API responses to ensure valid JSON formatting.\n",
        "- **Detailed Quality Check Feedback:** Provides specific reasons for quality check failures to aid in debugging.\n",
        "- **Improved Backoff Mechanism:** Strategically retries after recoverable errors, including rate limits.\n",
        "- **Finer-Grained Error Handling:** Allows precise tracking of issues at the paragraph level.\n",
        "- **Enhanced Logging:** Offers comprehensive logging for better traceability and debugging.\n",
        "\n",
        "## Version History\n",
        "\n",
        "- **2.1.1:** Moved conversation quality check to using OpenAI's API for consistency, quality, and performance.\n",
        "- **2.1.0:** Added deduplication, enhanced parsing logic, more advanced quality check NLP checks, and improved error handling.\n",
        "- **2.0.0:** Introduced a granular rating system, sophisticated rehaul of the branching logic, automated data quality checks, and cost-efficient API utilization strategies.\n",
        "- **1.3.0:** Implemented log file generation and console output for improved monitoring and debugging. Conducted testing with 'output-snippet.xml' and processed the resulting 'conversation_trees.jsonl' file for data integrity.\n",
        "- **1.2.0:** Introduced dual-branching logic to simulate 'good' and 'bad' educational responses, creating a dynamic dataset with varied teaching methodologies.\n",
        "- **1.1.0:** Transitioned from synchronous to asynchronous processing, allowing for parallel generation of conversation trees and optimizing resource usage. Time tracking added to monitor task times for testing.\n",
        "- **1.0.0:** Initial release. Generated simple, linear conversation trees in a synchronous manner. Basic API rate limit management and conversation context derived directly from textbook content.\n",
        "\n",
        "## Usage\n",
        "\n",
        "Run with Python 3.x, ensuring all dependencies from 'requirements.txt' are installed and API keys are set in '.env'. The script processes paragraphs from an XML file and outputs conversation trees to a '.jsonl' file.\n"
      ],
      "metadata": {
        "id": "3GBHl2MRBne4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "#    Functions for Synthetic Conversation Tree Generation\n",
        "# ==========================================================\n",
        "\n",
        "# Load the spaCy model for NLP tasks\n",
        "# nlp = spacy.load(\"en_core_web_sm\") # Smaller model for testing, reduced NLP accuracy\n",
        "# nlp = spacy.load(\"en_core_web_lg\") # Larger model for deployment, improved NLP accuracy\n",
        "\n",
        "# Ensure output and log files exist, otherwise create them\n",
        "if not os.path.exists(INDIVIDUAL_PAIRS_FILE):\n",
        "    open(INDIVIDUAL_PAIRS_FILE, 'w').close()\n",
        "\n",
        "if not os.path.exists(RATED_TREES_FILE):\n",
        "    open(RATED_TREES_FILE, 'w').close()\n",
        "\n",
        "if not os.path.exists(CONSOLE_LOG_FILE_PATH):\n",
        "    open(CONSOLE_LOG_FILE_PATH, 'w').close()\n",
        "\n",
        "# Utility function to log messages to both console and text file\n",
        "def display_and_log(message):\n",
        "    print(message)\n",
        "    with open(CONSOLE_LOG_FILE_PATH, 'a', encoding='utf-8') as log_file:\n",
        "        log_file.write(f\"{message}\\n\")\n",
        "\n",
        "# Utility function to parse the XML file and extract paragraphs\n",
        "def parse_xml(file_path):\n",
        "    book_paragraphs = []\n",
        "    try:\n",
        "        parser = ET.XMLParser(encoding=\"utf-8\")\n",
        "        tree = ET.parse(file_path, parser=parser)\n",
        "        root = tree.getroot()\n",
        "        for book in root.findall('.//book'):\n",
        "            book_title = book.get('name')\n",
        "            for paragraph in book.find('.//content').findall('paragraph'):\n",
        "                text = paragraph.text if paragraph.text is not None else ''\n",
        "                book_paragraphs.append((book_title, text.strip()))\n",
        "    except Exception as e:\n",
        "        display_and_log(f\"Error parsing XML file at {file_path}: {e}\")\n",
        "    return book_paragraphs\n",
        "\n",
        "# Utility function to append data to a file\n",
        "def append_to_file(file_path, data):\n",
        "    with open(file_path, 'a', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False)\n",
        "        file.write('\\n')\n",
        "\n",
        "# Backoff handler in case of rate limit error\n",
        "async def backoff_hdlr(attempt):\n",
        "    jitter = random.uniform(0, attempt)\n",
        "    new_delay = min(2 ** attempt, MAX_DELAY) + jitter\n",
        "    display_and_log(f\"Rate limit hit, backing off for {new_delay} seconds.\")\n",
        "    await asyncio.sleep(new_delay) # Puts a single task to sleep not the entire program\n",
        "\n",
        "# Function to generate a rating by querying the API to evaluate and compare conversations within a tree\n",
        "async def generate_rating(conversation_tree, client):\n",
        "    num_conversations = len(conversation_tree[\"conversation\"])\n",
        "    # Generate a unique identifier for each conversation to include in the prompt\n",
        "    conversation_identifiers = [f\"Conversation {i+1}\" for i in range(num_conversations)]\n",
        "\n",
        "    # Define the prompt to send to the API for rating the conversation\n",
        "    rating_prompt = (f\"Review the following conversations between a student and an educator. \"\n",
        "                   f\"Compare each conversation against the others and rank them from 1 (most effective) to \"\n",
        "                   f\"{num_conversations} (least effective) in terms of educational quality, considering factors \"\n",
        "                   f\"such as clarity, engagement, and the promotion of critical thinking. \"\n",
        "                   f\"Respond with your rankings in a list format, preceded by the conversation identifier, \"\n",
        "                   f\"For Example: \\\"Conversation 1: 3, Conversation 2: 1, ..., Conversation {num_conversations}: 2\\\".\"\n",
        "    )\n",
        "\n",
        "    # Append a serialized version of each conversation with its identifier\n",
        "    for idx, identifier in enumerate(conversation_identifiers):\n",
        "        conversation = json.dumps(conversation_tree[\"conversation\"][idx], ensure_ascii=False)\n",
        "        rating_prompt += f\"\\n\\n{identifier}: {conversation}\"\n",
        "\n",
        "    # Make the API\n",
        "    try:\n",
        "        response = await client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"system\", \"content\": rating_prompt}]\n",
        "        )\n",
        "\n",
        "        # Parse the API response to extract the rankings\n",
        "        display_and_log(f\"Rankings API Response: {response}\")\n",
        "        rankings_response = response.choices[0].message.content.text.strip().split(',')\n",
        "        display_and_log(f\"Rankings Contents: {rankings_response}\")\n",
        "        rankings = {}\n",
        "        for ranking_pair in rankings_response:\n",
        "            identifier, rank = ranking_pair.split(':')\n",
        "            # Extract the number from the identifier (e.g., \"Conversation 1\" -> 1)\n",
        "            conv_num = int(identifier.strip().split(' ')[1])\n",
        "            rankings[conv_num] = int(rank.strip())\n",
        "            display_and_log(f\"Processed Ranking: {conv_num} -> {rank}\")\n",
        "\n",
        "        # Assign the rankings back to the conversation tree using the identifiers\n",
        "        for index in range(num_conversations):\n",
        "            conversation_tree[\"conversation\"][index][\"ranking\"] = rankings[index + 1]\n",
        "            display_and_log(f\"Assigned Ranking to Conversation {index + 1}: {rankings[index + 1]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log error and handle it appropriately\n",
        "        display_and_log(f\"Error during rating generation: {e}\")\n",
        "\n",
        "    return conversation_tree  # Return the conversation tree with the rankings\n",
        "\n",
        "# Function to create a conversation prompt including context management and rating instructions\n",
        "async def create_conversation_prompt(book_title, paragraph, conversation_history, depth, client):\n",
        "    summarized_context = await summarize_conversation(conversation_history, client) if conversation_history else \"\"\n",
        "\n",
        "    # Conversation start. Root Node.\n",
        "    if depth == 0:\n",
        "        prompt = (\n",
        "        f\"Assume the role of an educator assisting a student. You are here to help them develop critical thinking skills and engage with the content.\"\n",
        "        f\"Based on the concept in the provided paragraph from the book titled: '{book_title}', \"\n",
        "        f\"generate a prompt-response pair in JSON for use in a conversation tree in a dataset to fine-tune an LLM to be an educational assistant that \"\n",
        "        f\"promote student critical thinking and engagement. Begin from the student's point-of-view with a question reflecting a student's inquiry or \"\n",
        "        f\"assigned task regarding the textbook paragraph contents, the student may choose to provide part of an exerpt of the assigned paragraph or try to describe \"\n",
        "        f\"the paragraph in their own words as context for the LLM before asking their question, students also don't always use perfect grammar, then follow up from the educator's point-of-view with a response that encourages critical thinking \"\n",
        "        f\"and that engages the student. Your response should offer guidance, examples, or suggestions, helping the student explore the concept \"\n",
        "        f\"further, while maintaining a professional tone. Avoid directly solving the problem; instead, focus on guiding the student to find the \"\n",
        "        f\"solution themselves. If the conversation reaches a natural conclusion or if the topic changes significantly, include \"\n",
        "        f\"'[CONVERSATION_END]' or '[TOPIC_END]' respectively.\\nHere's the textbook paragraph that was assigned to the student: {paragraph[:500]}\"\n",
        "        f\"Don't forget you are to generate both a 'Prompt' and a 'Response' in JSON to represent the start of the conversation between the student and educator.\"\n",
        "        f\"Example: You receive an assigned textbook paragraph from a math textbook on solving basic integrals for calculus 1 students. \"\n",
        "        f\"Example Output: \\\"Prompt\\\": \\\"I just learned that integrals are the opposite of derivatives. Complete this integral: 2x^2 + 2.\\\", \\\"Response\\\": \\\"I can't solve it for you but I can help guide you towards the solution\"\n",
        "        f\"in solving it for yourself. How many steps have you gotten in solving the integral so far? Since integrals are the opposite of derivatives, what are the steps for solving a derivative? \"\n",
        "        f\"(Hint: Start by trying to break up this one big integral into multiple smaller, easier-to-solve integrals)\\\". \"\n",
        "        )\n",
        "\n",
        "    # Conversation ending. Child nodes.\n",
        "    elif depth >= MAX_DEPTH - 1 and depth < MAX_DEPTH + 1:\n",
        "        prompt = (\n",
        "        f\"Assume the role of an educator assisting a student, and are currently in conversation with the student. \"\n",
        "        f\"Based on the concept in the provided paragraph from the book titled: '{book_title}', your goal is to start concluding the \"\n",
        "        f\"conversation effectively without directly completing the students assignment so they can develop critical thinking skills \"\n",
        "        f\"and engage with their course content. Your response should offer guidance, examples, or suggestions, helping the student \"\n",
        "        f\"explore the concept further, while maintaining a professional tone. Avoid directly solving the problem; instead, focus on \"\n",
        "        f\"guiding the student to find the solution themselves. Your job is to craft a prompt-response pair in JSON, aiming to gracefully \"\n",
        "        f\"conclude the interaction. Ensure your response helps in summarizing key points or providing final thoughts, without introducing \"\n",
        "        f\"new complex topics. Include '[CONVERSATION_END]' or '[TOPIC_END]' if appropriate. \"\n",
        "        f\"\\nAssigned Textbook Paragraph Contents: {paragraph[:500]}\\nPrior Conversation Summary: {summarized_context}\\n\"\n",
        "        f\"Don't forget you are to generate both a Prompt and a Response in JSON to represent the conversation approaching its conclusion between the student and educator.\"\n",
        "        f\"Example 1: \\\"Prompt\\\": \\\"Wow, thanks! I'm feeling more confident for my exam!', \\\"Response\\\": \\\"You're welcome, the key is breaking down complex integrals into simpler parts. Always feel free to revisit our discussion on this topic. Good luck on your exam! [CONVERSATION_END]\\\"\"\n",
        "        f\"Example 2: \\\"Prompt\\\": \\\"I'm pretty confident on integrals, time to start English homework, can you assist with that?, \\\"Response\\\": \\\"I'm glad, if you ever want more practice on integrals just ask away. Let me know if I can guide you for any other school content. [TOPIC_END]\\\"\"\n",
        "        )\n",
        "\n",
        "    # Conversation middle. Sub parent nodes.\n",
        "    else:\n",
        "        prompt = (\n",
        "        f\"Assume the role of an educator assisting a student, and are currently in conversation with the student. \"\n",
        "        f\"Based on the concept in the provided paragraph from the book titled: '{book_title}', your goal is to continue guiding the student towards learning \"\n",
        "        f\"the content they need to answer their question in their inital prompt; without directly completing the students assignment outright, in order for \"\n",
        "        f\"them to develop critical thinking skills and engage with their course content. Your response should offer guidance, examples, \"\n",
        "        f\"or suggestions, helping the student explore the concept further, while maintaining a professional tone. Avoid directly solving \"\n",
        "        f\"the problem; instead, focus on guiding the student to find the solution themselves. Your job is to craft a prompt-response pair \"\n",
        "        f\"in JSON. If the conversation reaches a natural conclusion or if the topic changes significantly, include '[CONVERSATION_END]' or '[TOPIC_END]' respectively. \"\n",
        "        f\"Assigned Textbook Paragraph Content: {paragraph[:500]}\\nPrior Conversation Summary: {summarized_context}\\n\"\n",
        "        f\"Don't forget you are to generate both a Prompt and a Response in JSON to represent the progression of the conversation between the student and educator.\"\n",
        "        f\"Example 1: \\\"Prompt\\\": \\\"I never really understood how variables work in programming?\\\", \\\"Response\\\": \\\"Think of variables as containers for storing data. What data types do you think can be stored in these containers?\\\"\"\n",
        "        f\"Example 2: \\\"Prompt\\\": \\\"I don't really know the difference between Comparator and Comparable in Java?\\\", \\\"Response\\\": \\\"To understand the differences between Comparator and Comparable in Java, consider the purpose and \"\n",
        "        f\"usage of each: Comparable is for natural ordering within a class (using compareTo), whereas Comparator allows for custom ordering outside a class (using compare). Reflect on their design implications, \"\n",
        "        f\"flexibility, and specific use cases to grasp why and when each interface is used in Java programming.\\\"\"\n",
        "        )\n",
        "    return prompt\n",
        "\n",
        "async def summarize_conversation(conversation_history, client):\n",
        "    # Combine the conversation history into a single string.\n",
        "    full_history = \" \".join(conversation_history)\n",
        "\n",
        "    # Create the prompt for summarization.\n",
        "    summary_prompt = (\n",
        "            f\"Please provide a concise summary of the educational interaction below, focusing on key points and main ideas. \"\n",
        "            f\"Ensure the summary is coherent and captures the essence of the dialogue without unnecessary details. \"\n",
        "            f\"Example Summary: 'The student struggles with the concept of recursion. The educator guides the student by \"\n",
        "            f\"comparing recursion to a loop and suggestes practice problems to reinforce learning including The Tower of Hanoi.'\"\n",
        "            f\"This summary wil be used as context for prompting a large language model so make sure it's concise while comprehensive \"\n",
        "            f\"enough for the LLM to understand the prior conversation between the student and educator.\\n\\n\"\n",
        "            f\"Full Conversation: {full_history}\"\n",
        "        )\n",
        "\n",
        "    # Make the API call for summarization.\n",
        "    try:\n",
        "        response = await client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"system\", \"content\": summary_prompt}]\n",
        "        )\n",
        "        # Assuming the API returns a text response with the summary.\n",
        "        display_and_log(f\"API Summary Response: {response}.\")\n",
        "        summary = response.choices[0].message.content\n",
        "        display_and_log(f\"API Summary: {summary}\")\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        # Handle exceptions and possibly perform a retry or log the error.\n",
        "        display_and_log(f\"Error during API call for summarization: {e}\")\n",
        "        return \"\"  # Return an empty string as a fallback in case of an error.\n",
        "\n",
        "def get_value_by_key_variants(dictionary, key_variants_dict, default=None):\n",
        "    \"\"\"\n",
        "    Returns the value for the first key variant found in the dictionary.\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    for key, variants in key_variants_dict.items():\n",
        "        for variant in variants:\n",
        "            if variant in dictionary:\n",
        "                result[key] = dictionary[variant]\n",
        "                break\n",
        "        if key not in result:\n",
        "            result[key] = default\n",
        "    return result\n",
        "\n",
        "def complete_json_string(json_string, expected_structure):\n",
        "    try:\n",
        "        # Attempt to parse the JSON string\n",
        "        json_obj = json.loads(json_string)\n",
        "\n",
        "        # Check if the JSON object contains the expected keys\n",
        "        for key, variants in expected_structure.items():\n",
        "            if not any(variant in json_obj for variant in variants):\n",
        "                raise json.JSONDecodeError(f\"Missing '{key}' key\", json_string, 0)\n",
        "\n",
        "        return json.dumps(json_obj)\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        try:\n",
        "            # Check for invalid control characters and remove them\n",
        "            display_and_log(f\"JSONDecodeError: {e}\")\n",
        "            json_string = ''.join(char for char in json_string if (0x20 <= ord(char) <= 0x10FFFF) and char not in ('\"', '\\\\', '\\x08', '\\x0c', '\\x0a', '\\x0d', '\\x09', '\\x00'))\n",
        "\n",
        "            # Iterate through expected_structure to format JSON\n",
        "            formatted_json_string = '{'\n",
        "            for key, variants in expected_structure.items():\n",
        "                for variant in variants:\n",
        "                    # Use regex to find each variant in the JSON string\n",
        "                    match = re.search(fr'\"{variant}\":\"(.*?)\"', json_string)\n",
        "                    if match:\n",
        "                        formatted_json_string += f'\"{key}\": \"{match.group(1)}\", '\n",
        "\n",
        "            # Remove the trailing comma and space, if any\n",
        "            if formatted_json_string.endswith(', '):\n",
        "                formatted_json_string = formatted_json_string[:-2]\n",
        "\n",
        "            formatted_json_string += '}'\n",
        "\n",
        "            if formatted_json_string == '{}':  # If no matches were found\n",
        "                raise json.JSONDecodeError(\"Unable to format the string correctly\", json_string, 0)\n",
        "\n",
        "            return formatted_json_string\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # If it's still not valid JSON, wrap it in an object\n",
        "            display_and_log(f\"JSONDecodeError: {e}\")\n",
        "            return f'{{\"data\": \"{json_string}\"}}'\n",
        "\n",
        "def is_valid_json(json_string):\n",
        "    try:\n",
        "        json.loads(json_string)\n",
        "        return True\n",
        "    except json.JSONDecodeError:\n",
        "        return False\n",
        "\n",
        "def parse_response(response):\n",
        "    try:\n",
        "        # Defining the different potential API response types\n",
        "        expected_structure = {\n",
        "            \"Prompt\": ['Prompt', 'prompt', 'PROMPT', 'student', 'student_question', 'question'],\n",
        "            \"Response\": ['Response', 'response', 'RESPONSE', 'educator', 'educator_response']\n",
        "        }\n",
        "\n",
        "        # Extracting the content of the assistant's message from the response\n",
        "        display_and_log(f\"API Response for Parsing: {response}\")\n",
        "        api_message = response.choices[0].message.content\n",
        "        display_and_log(f\"API Message Extracted: {api_message}\")\n",
        "\n",
        "        # Complete the JSON string if necessary\n",
        "        if is_valid_json(api_message):\n",
        "            complete_api_message = api_message\n",
        "        else:\n",
        "            display_and_log(\"Invalid JSON format\")\n",
        "            complete_api_message = complete_json_string(api_message, expected_structure)\n",
        "            display_and_log(f\"Completed JSON API Message: {complete_api_message}\")\n",
        "\n",
        "        # Parse the JSON string\n",
        "        conversation = json.loads(complete_api_message)\n",
        "\n",
        "        # Extracting 'Prompt' and 'Response', considering various capitalizations and variants\n",
        "        parsed_values = get_value_by_key_variants(conversation, expected_structure, default=\"FAILED_TO_PARSE\")\n",
        "        student_question = parsed_values.get(\"Prompt\", \"FAILED_TO_PARSE\")\n",
        "        educator_response = parsed_values.get(\"Response\", \"FAILED_TO_PARSE\")\n",
        "\n",
        "        if student_question == \"FAILED_TO_PARSE\" or educator_response == \"FAILED_TO_PARSE\":\n",
        "            raise ValueError(\"Failed to parse the conversation from the response.\")\n",
        "        else:\n",
        "            display_and_log(f\"Student Question: {student_question} and Educator Response: {educator_response}\")\n",
        "\n",
        "        return student_question, educator_response\n",
        "    except Exception as e:\n",
        "        display_and_log(f\"Error parsing response: {e}\\n\")\n",
        "        return \"FAILED_TO_PARSE\", \"FAILED_TO_PARSE\"\n",
        "\n",
        "# # Function to perform automated checks on the generated conversation locally\n",
        "# def perform_quality_checks_local(conversation_entry):\n",
        "#     # Extract educator response and perform NLP analysis\n",
        "#     response = conversation_entry['educator_response']\n",
        "#     doc = nlp(response)\n",
        "\n",
        "#     # Check for educational relevance: Presence of explanations, definitions, or clarifications\n",
        "#     educational_relevance = any(token.pos_ in [\"VERB\", \"NOUN\"] and token.dep_ in [\"ROOT\", \"attr\", \"dobj\"] for token in doc)\n",
        "\n",
        "#     # Sentiment analysis: Check for neutral to positive tone, as educational content should be informative and encouraging\n",
        "#     sentiment = TextBlob(response).sentiment.polarity\n",
        "#     positive_sentiment = sentiment >= -0.1  # Allowing slightly negative sentiment for challenging topics\n",
        "\n",
        "#     # Contextual understanding: Analyze for cohesive and meaningful sentences\n",
        "#     contextual_understanding = any(sent.root.pos_ == \"VERB\" and sent.root.dep_ == \"ROOT\" for sent in doc.sents)\n",
        "\n",
        "#     # Length and grammar check: Assuming longer and grammatically correct responses may have more explanations/details\n",
        "#     length_check = len(response.split()) > 20  # Arbitrary length threshold\n",
        "#     grammar_check = any(punctuation in response for punctuation in ['.', '?', '!'])\n",
        "\n",
        "#     # All checks must pass for the conversation entry to be considered high quality\n",
        "#     quality_checks_passed = all([\n",
        "#         educational_relevance,\n",
        "#         positive_sentiment,\n",
        "#         contextual_understanding,\n",
        "#         length_check,\n",
        "#         grammar_check\n",
        "#     ])\n",
        "\n",
        "#     return quality_checks_passed\n",
        "\n",
        "# Function to perform automated checks on the generated conversation using OpenAIs API\n",
        "async def perform_quality_checks_api(conversation_entry, client):\n",
        "    # Create a prompt for the API to evaluate the conversation entry\n",
        "    evaluation_prompt = (\n",
        "        f\"Please evaluate the educational quality of the following conversation \"\n",
        "        f\"between a student and an educator. Assess it for clarity, engagement, \"\n",
        "        f\"and promotion of critical thinking. Respond with an overall score from \"\n",
        "        f\"1 (no conversation/unintelligible) to 50 (unhelpful/incorrect) to 100 \"\n",
        "        f\"(perfect interaction) in JSON format with the label \\'Score\\'. If you \"\n",
        "        f\"fail to respond in the expected JSON (\\\"Score\\\":\\\"#\\\"), the assumed score is 0.\"\n",
        "        f\"Note: Failed to Parse should recieve an automatic 0 since it isn't a conversation, \"\n",
        "        f\"but rather an issue with this scripts logic. Otherwise, grade as defined above.\\n\\n\"\n",
        "        f\"Student Question: {conversation_entry['student_question']}\\n\"\n",
        "        f\"Educator Response: {conversation_entry['educator_response']}\\n\"\n",
        "        f\"Example Response: \\\"Score\\\":\\\"73\\\"\"\n",
        "    )\n",
        "\n",
        "    # For score format\n",
        "    expected_structure = {\n",
        "        \"Score\": ['Score', 'score', 'SCORE']\n",
        "    }\n",
        "\n",
        "    # Make the API call for evaluation\n",
        "    try:\n",
        "        response = await client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"system\", \"content\": evaluation_prompt}]\n",
        "        )\n",
        "        # Check if 'Score' is in the response\n",
        "        if \"Score\" in response.choices[0].message.content:\n",
        "            api_response = complete_json_string(response.choices[0].message.content, expected_structure)\n",
        "            api_response = json.loads(api_response)\n",
        "            score = int(api_response[\"Score\"])\n",
        "            display_and_log(f\"Received Score: {score}\")\n",
        "            return score >= 60  # A score of 60+ is passing\n",
        "        else:\n",
        "            display_and_log(\"Score key not found in API response\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        display_and_log(f\"Error during API call for quality evaluation: {e}\")\n",
        "        return False  # Treat errors as a failure in quality check\n",
        "\n",
        "# Helper function to generate a hash of the conversation content\n",
        "def hash_conversation(conversation):\n",
        "    # Create a unique string representation of the conversation\n",
        "    conversation_str = json.dumps(conversation, sort_keys=True)\n",
        "    # Use SHA-256 hash function to generate a hash\n",
        "    return hashlib.sha256(conversation_str.encode('utf-8')).hexdigest()\n",
        "\n",
        "# Function to check for duplicate conversations using hashing\n",
        "def deduplicate_conversation(new_entry, conversation_history_hashes):\n",
        "    # Generate the hash for the new entry\n",
        "    new_entry_hash = hash_conversation(new_entry)\n",
        "\n",
        "    # Check if the hash of the new entry is in the set of hashes\n",
        "    if new_entry_hash in conversation_history_hashes:\n",
        "        return True  # Duplicate found\n",
        "\n",
        "    # Add the new entry hash to the history set for future checks\n",
        "    conversation_history_hashes.add(new_entry_hash)\n",
        "    return False  # No duplicate found\n",
        "\n",
        "# Async function to generate a single branch of a conversation tree\n",
        "async def generate_conversation_branch(client, book_title, paragraph, semaphore, index, current_depth=0):\n",
        "    async with semaphore:  # Control concurrency with semaphore\n",
        "        display_and_log(f\"Starting branch generation at depth {current_depth} for paragraph {index} ('{book_title}').\")\n",
        "\n",
        "        if current_depth >= MAX_DEPTH - 1:\n",
        "            return None\n",
        "\n",
        "        conversation_branch = {\n",
        "            \"book_title\": book_title,\n",
        "            \"paragraph_index\": index,\n",
        "            \"conversation\": [],\n",
        "            \"depth\": current_depth\n",
        "        }\n",
        "        conversation_history = []  # Maintain conversation history for context\n",
        "        conversation_history_hashes = set()  # Set to store hashes of unique conversation entries\n",
        "\n",
        "        try:\n",
        "            prompt = await create_conversation_prompt(book_title, paragraph, conversation_history, current_depth, client)\n",
        "            start_time = time.perf_counter()\n",
        "            display_and_log(f\"Paragraph {index} ('{book_title}'): Generating Prompt-Response (Depth: {current_depth}). Start Time: {start_time:.2f}\")\n",
        "\n",
        "            response = await client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"system\", \"content\": prompt}]\n",
        "            )\n",
        "            parsed_response = parse_response(response)\n",
        "            display_and_log(f\"Parsed Response: {parsed_response}.\")\n",
        "            student_question, educator_response = parsed_response\n",
        "            display_and_log(f\"Student Question: {student_question} - Educator Response: {educator_response}\")\n",
        "\n",
        "            entry = {\n",
        "                \"book_title\": book_title,\n",
        "                \"paragraph_index\": index,\n",
        "                \"depth\": current_depth,\n",
        "                \"student_question\": student_question,\n",
        "                \"educator_response\": educator_response\n",
        "            }\n",
        "\n",
        "            if not deduplicate_conversation(entry, conversation_history_hashes):\n",
        "                if await perform_quality_checks_api(entry, client):\n",
        "                    conversation_branch[\"conversation\"].append(entry)\n",
        "                    append_to_file(INDIVIDUAL_PAIRS_FILE, entry)\n",
        "                    conversation_history.append(student_question + \" \" + educator_response)\n",
        "\n",
        "                    if \"[CONVERSATION_END]\" and \"[TOPIC_END]\" not in educator_response and current_depth < MAX_DEPTH - 1:\n",
        "                        deeper_paragraph = educator_response\n",
        "                        deeper_branch = await generate_conversation_branch(client, book_title, deeper_paragraph, semaphore, index, current_depth + 1)\n",
        "                        if deeper_branch and deeper_branch[\"conversation\"]:\n",
        "                            conversation_branch[\"conversation\"].extend(deeper_branch[\"conversation\"])\n",
        "            else:\n",
        "                display_and_log(f\"Branch at depth {current_depth} for paragraph {index} ('{book_title}') failed checks or is a duplicate.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            display_and_log(f\"Error in generate_conversation_branch: {e}\")\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        display_and_log(f\"Finished processing branch at depth {current_depth} for paragraph {index} ('{book_title}') in total time: {end_time - start_time:.2f} seconds.\")\n",
        "        return conversation_branch\n",
        "\n",
        "# Async function to orchestrate the generation of a full conversation tree\n",
        "async def generate_conversation_tree(client, semaphore, book_title, paragraph, index):\n",
        "    try:\n",
        "        start_time = time.perf_counter()\n",
        "        root_branch = await generate_conversation_branch(client, book_title, paragraph, semaphore, index)\n",
        "\n",
        "        if root_branch is None or not root_branch[\"conversation\"]:\n",
        "            display_and_log(f\"No valid conversation generated for paragraph {index} ('{book_title}').\")\n",
        "            return None\n",
        "\n",
        "        # Temporary storage for the conversation tree before rating\n",
        "        temp_conversation_tree = {\"branches\": [root_branch]}\n",
        "\n",
        "        # Before calling generate_rating, add a check for 'conversation' key\n",
        "        if \"conversation\" not in temp_conversation_tree:\n",
        "            display_and_log(\"No 'conversation' key found in temp_conversation_tree\")\n",
        "            return None\n",
        "\n",
        "        # Generate a rating for the conversation tree\n",
        "        rated_tree = await generate_rating(temp_conversation_tree, client)\n",
        "        if rated_tree:\n",
        "            # Append the rated tree to the file only after successful rating\n",
        "            append_to_file(RATED_TREES_FILE, rated_tree)\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        display_and_log(f\"Generated full conversation tree for paragraph {index} ('{book_title}') in total time: {end_time - start_time:.2f} seconds.\")\n",
        "        return rated_tree\n",
        "\n",
        "    except Exception as e:\n",
        "        display_and_log(f\"Error in generate_conversation_tree for Paragraph {index}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Async main function to process all paragraphs and generate conversation trees\n",
        "async def process_paragraphs_async(book_paragraphs):\n",
        "    async_client = AsyncOpenAI(api_key=API_KEY)\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)\n",
        "    tasks = []\n",
        "\n",
        "    try:\n",
        "        for index, (book_title, paragraph) in enumerate(book_paragraphs):\n",
        "            display_and_log(f\"Reading paragraph {index + 1}/{len(book_paragraphs)} from the book '{book_title}'.\")\n",
        "            task = generate_conversation_tree(async_client, semaphore, book_title, paragraph, index)\n",
        "            tasks.append(task)\n",
        "\n",
        "        trees = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "        for tree in trees:\n",
        "            if isinstance(tree, Exception):\n",
        "                display_and_log(f\"Exception during task execution: {tree}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        display_and_log(f\"Error processing paragraphs: {e}\\n\")\n",
        "\n",
        "    finally:\n",
        "        # Delay added to stay well under the OpenAI API rate limit.\n",
        "        await asyncio.sleep(0.5)  # Throttling to stay within rate limits\n",
        "\n",
        "# Entry point of the script\n",
        "display_and_log(f\"Starting script. Querying OpenAI's API with {XML_FILE_PATH} contents to {INDIVIDUAL_PAIRS_FILE} and {RATED_TREES_FILE} in the same directory.\")\n",
        "book_paragraphs = parse_xml(XML_FILE_PATH)\n",
        "asyncio.run(process_paragraphs_async(book_paragraphs))\n",
        "display_and_log(f\"The synthetic dataset has finished generating.\")\n"
      ],
      "metadata": {
        "id": "JPH2-_qTBnvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Conclusion and Future Work\n",
        "\n",
        "---\n",
        "\n",
        "#### **Project Overview**\n",
        "\n",
        "This notebook consolidates several scripts into a comprehensive, reproducible workflow for enhancing language models with educational applications. It provides a step-by-step guide for those interested in replicating or extending our research, integrating various components into a unified Python notebook.\n",
        "\n",
        "#### **Key Achievements**\n",
        "\n",
        "- **Dataset Generation and Processing:** Conversion of textbook content into XML and conversation trees, creating training data for educational language models.\n",
        "- **Fine-Tuning of Language Models:** Using Mistral-OpenOrca for fine-tuning, aligning the model closely with educational content.\n",
        "- **Model Conversion and Optimization:** Conversion to GGUF format for efficient CPU inference, facilitating deployment across various platforms.\n",
        "\n",
        "#### **Implications and Applications**\n",
        "\n",
        "- **Educational Assistant Development:** The project's outcomes are key to building AI-driven educational assistants, enhancing personalized learning experiences.\n",
        "- **Research Extension Opportunities:** This foundation enables further exploration in different domains, offering diverse experimental possibilities.\n",
        "\n",
        "#### **Future Work Recommendations**\n",
        "\n",
        "- **Educational Context Expansion:** Adapt the model to a broader range of educational settings and subjects to enhance its applicability.\n",
        "- **Vectorized Databases Integration:** Implement vectorized databases to remember student work and learning styles, personalizing the educational experience.\n",
        "- **Mixture of Experts (MoE) Utilization:** Employ MoE models to improve response accuracy, reduce hallucinations, distribute computational load, and allow multiple models to contribute to the system.\n",
        "- **LLM Wrapper Incorporation:** Integrate LLM Wrappers like LangChain to provide access to Learning Management Systems (LMS) such as Canvas, Blackboard, or D2L, enhancing the model's utility in educational environments.\n",
        "- **Enhanced User Interaction:** Focus on improving the interface and interaction mechanisms of AI-driven educational tools for a more engaging learning experience.\n",
        "- **Scalability and Efficiency:** Explore methods to scale the model efficiently without compromising performance.\n",
        "- **Real-time Interaction Capabilities:** Enhance the model to support real-time interactions for dynamic educational contexts.\n",
        "- **Interdisciplinary Applications:** Expand the model's use to interdisciplinary studies to explore new areas in AI-driven education.\n",
        "\n",
        "#### **Final Thoughts**\n",
        "\n",
        "This notebook is a step forward in AI's application in education and an invitation for the community to contribute to this evolving field. The opportunities for innovation are vast, and diverse perspectives will play a crucial role in advancing educational technology.\n"
      ],
      "metadata": {
        "id": "Urr8MmluJSJp"
      }
    }
  ]
}