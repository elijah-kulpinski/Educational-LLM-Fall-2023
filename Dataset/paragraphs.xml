<?xml version="1.0" encoding="UTF-8"?>
<books>
	<book name="Back to the University's Future - The Second Coming of Humboldt">
		<content>
			<paragraph>
				<![CDATA[Abstract The Introduction identifies the innovative features of Wilhelm von
            Humboldt’s ‘modernist’ reinvention of the university as a concept and an institution
            in the early nineteenth century, especially with an eye to how these features might
            be highlighted and extended today and into the future. The core Humboldtian prin-
            ciple is the unity of research and teaching in the person of the ‘academic’, under-
            stood as a free inquirer who serves as an exemplar for students to emulate in their
            own way as part of their personal development (Bildung). Unlike prior and subse-
            quent conceptions of the university, the Humboldtian one was not about ‘credential-
            ing’ people as ‘experts’, but about giving a focus and direction to their humanity.
            Six themes, which recur through this book, capture the Humboldtian spirit: futur-
            ism, historical consciousness, judgement, translation, knowledge as a public good,
            and professionalism.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Universities epitomize ‘legacy institutions’ that continue to exist mainly because
            they have already existed. However, it is not clear that their existence is required for
            their major functions to be performed. These major functions are teaching and
            research. That they should be performed by the same set of people working in the
            same place – indeed, arguably as the same activities – was the idea of Wilhelm von
            Humboldt, an early nineteenth liberal philosopher, linguist and Prussian Minister of
            Education. By merging the two functions he reinvented the university as a dynamic
            engine of nation-building, and more generally the flagship vehicle of ‘Enlightenment’.
            Today we underestimate the significance of Humboldt’s achievement because it
            occurred when the university had been widely regarded by the avant garde thinkers
            of his youth – the French philosophes – as a lumbering medieval throwback. This
            book plans to update and defend Humboldt’s vision for the twenty-first century,
            fully realizing that not only has the character of teaching and research changed but
            also that the nation-state no longer holds the same promise that it did in
            Humboldt’s day.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If I told you that the Humboldtian university nowadays lives ‘virtually’, you
            might think that I mean that it has migrated online. But that is too charitable. The Humboldtian university has barely got a grip – let alone imposed its logic – on the
            internet’s endless frontiers. On the contrary, I mean ‘virtually’ in its pre-cyber,
            deflated sense: Humboldt’s spirit continues to haunt the self-justifications given by
            both academics and their administrators for their activities, without justifying the
            activities themselves. It is almost as if latter-day Humboldtians, ranging from
            Juergen Habermas to the average university administrator, continue to sing the same
            lyrics, regardless of the melody – let alone the instruments – being played. Jean-
            Francois Lyotard (1983) made great sport of this fact in his 1979 report on the state
            of higher education, published as The Postmodern Condition. He highlighted the
            false consciousness of these latter-day Humboldtians. They talk as if the modern
            university has been the natural home of free inquiry, even though the most important
            innovations of the modern period – from industrial chemistry in the nineteenth
            century to more recent advances in information technology and the biomedical
            sciences – have required interdisciplinary if not transdisciplinary knowledge typi-
            cally developed outside of traditional academic settings. In each case, the university
            was initially resistant, with many of the original researchers either lured away from
            academia altogether or supported there only with the funding of private foundations]]>
			</paragraph>
			<paragraph>
				<![CDATA[The university itself became a serious player in this process only by dedicating
            degree programs to emerging forms of knowledge (e.g., molecular biology, com-
            puter science). When it has failed to do so, argued Lyotard, it has resembled its
            medieval forebear in being primarily in the business of reproducing epistemic
            authority – a ‘diploma mill’ or ‘paper belt’, as it is put in justifiably disparaging
            terms today (Gibson, 2022; Noble, 2001). Admittedly, Lyotard’s critique of the
            Humboldtian myth may have reflected France’s own non-Humboldtian modern
            higher education history, whereby research and teaching remain divided into two
            sorts of institutions. But it also reflected the more general global weakening of the
            state as a driver of mass social change. Thus, Lyotard adumbrated ideas that are now
            widely taken-for-granted about the affinity between postmodernism and neoliberal-
            ism, whereby the epistemic agency of transnational capitalism shapes the overall
            direction of knowledge production.]]>
			</paragraph>
			<paragraph>
				<![CDATA[So, how does the university make its way in the future, especially when the
            world’s knowledge is increasingly not deposited in dedicated academic buildings
            and other public spaces (i.e., libraries) but distributed across the internet, to which
            everyone has great but variable access? My overall strategy is simple: The
            Humboldtian university should plant its original seed in this new, partly virtual soil.
            The original Humboldtian academics curated and contextualized different claims to
            knowledge by a public exercise of judgement about what, how and why various
            things should be placed in the curriculum. It was in the spirit of setting an example
            in the sense of ‘paradigm’. What academics do for the students, the students will
            need to do for themselves once they leave the university — and face the internet as
            their primary source of knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Humboldt’s own immediate context was Prussia, especially in its potential role
            as unifier of the German peoples. Prussia was an aspiring player on a European
            scene that was dominated by two countervailing forces: On the one hand, there were
            realms still under the sway of the Roman Catholic Church’s version of natural law theory, which served to legitimize inherited forms of epistemic and political author-
            ity; on the other, there was the increasingly alluring presence of English liberalism,
            with its loosening of traditional legal strictures to enable the free mobility of capital,
            labour and goods, all increasingly underwritten by forms of non-academic knowl-
            edge that we now associate with the ‘Industrial Revolution’. In this continent-wide
            struggle, the universities – not least Oxford and Cambridge – were still very much
            aligned with the Church. Indeed, they were primarily about reproducing the sort of
            knowledge that maintained the social order. Against this backdrop, the French
            Enlightenment philosophers called for the state to establish research institutions that
            were not constrained by the need to perpetuate that status quo. On the contrary,
            these new institutions might provide the basis for a new form of elite ‘technical’
            training suited for the emerging scientific-industrial order – what became Les
            Grandes Écoles.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, the modern French segregation of research from teaching concedes
            defeat in the struggle to democratize knowledge. Thus, there is none of Humboldt’s
            Romantic presentation of educators as reporters from the frontlines of inquiry;
            instead, the medieval repositories of received wisdom have morphed into purveyors
            of technical training at the highest level. Behind this difference in visions rested a
            difference in the relationship between academia and the state. Napoleon ultimately
            saw academia as a stabilizing force, in which research would feed into policy mak-
            ing and education would be designed, Plato-style, for the level of decision-making
            that people in society would need to take, given their place within it. The results are
            well known, and they have often flown under the banner of ‘Positivism’ (Fuller,
            2006a: Chaps. 14 and 15). Most French political leaders have been graduates of one
            of the Grandes Écoles, the French were the pioneers of intelligence testing for pur-
            poses of streaming students through the education system, and Pierre Bourdieu
            (1988) became a leading critical sociologist in the late twentieth century by imagin-
            ing that, for better or worse, the whole world might be run in this distinctly French
            way. And as universities have come to be increasingly seen as training grounds and
            credentials mills, while research has come to depend more explicitly on external
            funding, a case can be made that even institutions formally dedicated to the Humboldtian spirit have devolved into something more Napoleonic in practice –
            that is, ‘management schools’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Humboldt took the basic Enlightenment idea of education as Bildung, a process
            of self-development, and projected it onto what he envisaged as the emerging
            nation-state of Germany. Nowadays, we would say that Humboldt wanted to turn
            universities into incubators of human capital. In either case, it would require a level
            of harmonization of the activities of the state and academia, but without either party
            seeing the maintenance of the existing social order as the goal. Students would be
            educated in a future-facing way, based on the research of their teachers. They would
            be expected to learn things that their parents – even if they attended the same uni-
            versities – did not learn, since the state would be presenting society as always en
            marche (apologies to Emmanuel Macron). ‘Academic freedom’ became such a ‘hot
            button’ issue in the Humboldtian context because these newly empowered teacher-
            researchers were prone to undermine the political conditions under which such
            licensed vanguardism was maintained. This was the concern that Max Weber (1958)
            expressed in his legendary introductory speech to graduate students, ‘Science as a
            Vocation’: How can you not bite the hand that feeds you, as your appetite for knowl-
            edge grows?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Philosophy was central to the Humboldtian university just as theology was to its
            medieval predecessor. But it was a certain kind of philosophy, which we nowadays
            call ‘German idealism’. All the signature figures of this movement – Fichte,
            Schelling and Hegel – were central to the early days of the University of Berlin, the
            epicentre of the Humboldtian revolution in higher education. Their philosophies
            have been described as ‘systematic’ and ‘encyclopaedic’ in style. They certainly
            aimed to match the comprehensiveness of, say, Thomas Aquinas and some of the
            academic followers of Leibniz who dominated German universities before
            Humboldt – that is, those who taught the man whose pushback from his own educa-
            tion provided philosophical inspiration for both Humboldt and the idealists:
            Immanuel Kant. Philosophy in the Humboldtian lecture theatre offered more a pro-
            spectus for future exploration than a map of already conquered domains. It was a
            course of study more for romantic heroes than rationalistic experts (Fuller, 2020a:
            Chap. 9).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Over the past two centuries, Humboldt’s romance of endless inquiry has been
            institutionally domesticated. In this book, that lost future is represented by what I
            call deviant interdisciplinarity. But the loss of that future happened gradually. It
            perhaps began in nineteenth century German academia, when philosophy and his-
            tory started to self-segregate in a manner analogous to grammar and rhetoric in the
            medieval Trivium. ‘Intellectual history’ remains the orphaned child of this rupture,
            evidence of the modern academy’s intolerance of Hegel’s view that history is phi-
            losophy teaching by examples. In the twentieth century, there were flickering
            attempts to recover the original idealist vision of philosophy through a proactive
            approach to librarianship that still travels under the name of ‘social epistemology’
            (Zandonade, 2004). Nowadays the spirit survives outside of academia with the cen-
            trality increasingly accorded to curation in the digital infosphere (Bhaskar, 2016).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Indeed, it was the spirit in which ‘epistemology’ was introduced into English by
            the Scottish metaphysician James Ferrier (1854) in the mid-nineteenth century. His
            basic point was that anything that we deem ‘unknown’ is knowable; otherwise, it
            would never have become part of knowledge-talk (Fuller, 2007a: 32). This core
            intuition of German idealism, which testifies to the power of naming, may be under-
            stood as the complement of the famous Wittgenstein aphorism, ‘Whereof one can-
            not speak, thereof one must be silent’. Its ‘apophatic’ theological basis – which
            cannot be explored here – is the extent to which the Abrahamic deity can be named,
            described or otherwise captured in language. In its secular Humboldtian guise, the
            path is clear: The academic imperative is to translate the ‘unknowable’ into the
            ‘knowable but unknown’ and ultimately the ‘known’, all in aid of a secular salvation
            narrative (aka ‘progress’). ‘Faustian’ would not be an inappropriate epithet for this
            attitude.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In practical terms, students entering the Humboldtian university would be intro-
            duced to the state of play regarding all knowledge, in terms of which they would
            then need to position themselves as they chart their own course through life. Unlike
            the medieval curriculum, whose Trivium and Quadrivium were largely about train-
            ing the mind, voice, eye, ear, etc. of the whole person (i.e., ‘skills-based’, in today’s
            lingo), Humboldt’s modern curriculum presumes that students will have already
            received such training at Gymnasium. The university’s task then would be to open
            the imagination, a capacity that all this prior training is meant to have prepared,
            resulting in more specific pursuits and the more advanced training (aka ‘methods’)
            associated with particular fields of inquiry. The success of this approach in its origi-
            nal incarnation can be measured by the role that German idealism played as the
            seedbed for disciplinary specialization in the nineteenth century, albeit often pro-
            ducing outcomes that stood in opposition to the teachings of the original idealist
            philosophers (Schnädelbach, 1984). Karl Marx vis-à-vis Hegel is an extreme but
            hardly the only case in point. In retrospect, the relative neglect of German idealism
            today may reflect its having provided the intellectual husk or scaffolding (or ‘lad-
            der’ à la Wittgenstein) for everything that we recognize as organized conceptual and
            empirical inquiry in the academy: Once it has done its job, it is discarded. To repur-
            pose a turn of phrase that Marx had adapted from Fichte, the Humboldtian revolu-
            tion has unwittingly resulted in the withering away of philosophy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Of course, Humboldt, Fichte and Marx agreed on the withering away of the state
            in its most excessive form, namely, as an administrative apparatus that arrogates to
            itself the powers of the authorizing sovereign. (Political scientists nowadays call
            this the ‘principal-agent’ problem.) Humboldt and Fichte thought that the university
            might provide the model for this ultimate political ‘withering’, which helps to
            explain why philosophers were made rectors: They inspired rather than directed.
            Yes, universities are institutions – among the very oldest in the West – but their
            primary purpose is the perpetuation of free inquiry, something above and beyond
            sheer corporate survival. Academic administration is solely in service of that end,
            not the ‘end in itself’ that it sometimes seems to have become over the years.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Humboldt’s model was eighteenth-century Deism, according to which humans
            come to inhabit the home that God had created for them, thereby rendering God redundant, resulting in what Voltaire satirized as deus absconditus. It opened the
            door to ideas of building ‘Heaven on Earth’, in which humans come to complete
            material creation (Becker, 1932). It became the calling card of ‘progress’ in the
            nineteenth and twentieth centuries, as Europe’s historically Christian culture
            increasingly criticized clerical authority for its grounding in parental authority. It
            also helps to explain the appeal to the Enlightenment by the 1960s’ student revolu-
            tionaries, who saw it as a collective outworking of the Oedipus Complex: Kill the
            father! In this respect, the Humboldtian university has stood opposed to the tradition
            of Oxbridge colleges operating in loco parentis. Indeed, a strict adherence to Kant’s
            classic definition of ‘Enlightenment’ as a movement would delegitimize paternal-
            ism altogether, extending well beyond the Catholic papacy and more broadly
            Christian appeals to the ‘divine right of kings’. One would no longer simply grow
            into the role of one’s ancestors but rather would use their legacy as capital, a protean
            basis for investment into projects potentially unrelated to their source. Indeed, capi-
            talism’s spread of exchange relations to an unprecedented, if not unlimited, extent
            across society has normalized this way of thinking about all forms of inheritance.]]>
			</paragraph>
			<paragraph>
				<![CDATA[From this standpoint, it is patronizing to use ‘applied’ to refer to knowledge cre-
            ated outside of academic settings yet somehow ending up bearing an academic sig-
            nature. That is to reverse the causal order of knowledge production – indeed, in a
            way that motivated Nietzsche’s deconstruction of Western morals (Culler, 1982). Of
            course, everything had to come from somewhere, which means that anyone could be
            the source of anything. But that’s just the logic of probabilities speaking. It follows
            that whoever does it first is simply a matter of chance; hence, the logical positivist
            contempt – shared by Karl Popper – for the very idea of a ‘logic of discovery’. The
            Humboldtian university aims to oppose the entrenchment of that chance discovery
            moment in all its guises, be it the ‘cult of genius’ or intellectual property rights.
            Implied here is the principle that for knowledge to be universally valid, it must be
            universally available.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pace Miranda Fricker (2007), I have called this principle epistemic justice
            (Fuller, 2007b: 24–29). Its roots go back to the mid-seventeenth century Port Royal
            Logic, a widely used textbook in what we now call ‘critical thinking’ for two centu-
            ries, which was inspired by Descartes. It featured a contrast between ‘demonstra-
            tion’ and ‘persuasion’ in terms of the range of audiences – more vs. less – that are
            likely to be brought round to the speaker’s conclusion (Arnauld & Nicole, 1996).
            Clearly, the rhetorical power of Galileo’s experiments was in the back of the authors’
            minds. The US philosopher of science Thomas Nickles (1980) updated this sensibil-
            ity for our own times when he defined a ‘justified’ scientific knowledge claim in
            terms of its ‘discoverability’, suggesting that the claim’s validity is intrinsically tied
            to the availability of multiple paths to its realization. This is not so different from
            Frege’s account of how the identity of Venus was established by coordinating data
            about the ‘Morning Star’ and the ‘Evening Star’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Even today the modern university pays at least lip service to Humboldt’s original
            vision, if only to mimic the watered-down version that remains society’s default
            understanding of what a university is. Most people presume that academics engage
            in teaching and research, and that the two are somehow related to each other in ways from which both students and the wider public benefit. Thus, people are often sur-
            prised – and increasingly scandalized – when they learn that some academics in fact
            do not teach but rather spend most of their time working for private contractors. At
            the same time, from the start of its modern Humboldtian reinvention, universities
            have been subject to competing pulls that have threatened its coherence and raison
            d’être. After all, the modern era has also witnessed the rise of both vocational train-
            ing centers and dedicated research institutes as mutually exclusive, client-driven
            bodies. Nevertheless, the Humboldtian university’s claim to uniqueness has rested
            on its capacity to ‘manufacture knowledge as public good’, a phrase that I have long
            used to capture a simple but profound idea: Knowledge enlightens only if the advan-
            tage enjoyed by its original creators is removed through mass dissemination.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The core Humboldtian intuition championed in this book is epitomized in the
            slogan: Research is elite, but teaching is democratic. Humboldt converted the pros-
            elytizing mission of the university’s monastic past (the source of ‘discipline’ in
            ‘discipleship’) into the more generically ‘universalist’ aspirations that arose from
            the secular Enlightenment, whereby research and teaching represent, respectively,
            the ‘private’ and ‘public’ faces of reason, in Kant’s (1999) sense. (Here one needs
            to think of the ‘privacy’ of reason as lying in the training and credentials that first
            licenses one to assert knowledge claims, which is then ‘publicized’, once others
            lacking that background can nevertheless understand and potentially accept the
            claims.) To be sure, his strategy appealed to several principles that do not sit easily
            with each other, resulting in the modern university being at once the home of both
            free expression and the redistribution of privilege.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is telling that Noam Chomsky (1971) made Humboldt the linchpin of his
            famous 1970 lecture, ‘Language and Freedom’, where he explicitly tied his linguis-
            tic theory to his left-libertarian brand of politics. His aim was to defend ‘academic
            freedom’ in the broadest sense, whereby the academic exemplifies in the classroom
            the sort of freedom that might be emulated in society at large. Like Max Weber
            (1958) fifty years earlier, Chomsky appreciated that the Humboldtian academic
            self-presents as a paradigm of personal conduct, which means that his or her future-
            facing orientation is manifested in both content and manner. Specifically, the aca-
            demic challenges students in a language sufficiently close to their own that they feel
            capable (at least in principle) to respond as equals to the professor. In Chomsky’s
            own case, his great learning has been normally hidden behind an approachable
            style, in both academic and public settings.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the second half of the twentieth century, people came to expect that a univer-
            sity degree could secure lucrative employment, which has turned out to be a poi-
            soned chalice from which universities have nevertheless gladly drunk (Fuller,
            2016a: Chap. 1). On a geopolitical level, it reflected global capitalism’s erosion of
            the project of nation-building, which had been the original frame of the modern
            reinvention of the university. Universities have tried to adapt, albeit with decidedly
            mixed results. The overall effect has been to blur ‘university’ as a brand of knowl-
            edge organization. In that sense, my aim is to remind readers of what made the
            ‘university’ such a distinctive yet exportable brand for more than two centuries.
            Here one shouldn’t underestimate the role that both the idea and the institution of the ‘university’ played in Germany’s rise from a disorganized central European
            backwater to the topflight of global knowledge and industry by the end of the nine-
            teenth century. Nowadays nation-building isn’t as central to the narrative of human-
            ity as it was in the nineteenth and twentieth centuries. Moreover, universities have
            become entangled in matters that have extended and/or compromised the
            Humboldtian mission. Thus, this book does not promise that every institution that
            currently bears the name ‘university’ would survive my Humboldt-inspired sense of
            renewal. But then as critics of the university rightly say, there is no special reason
            for universities to exist indefinitely. We therefore need to reaffirm the distinctive-
            ness of the brand.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In these pages, I propose that we go ‘meta’ on Humboldt, a move that suits our
            ‘post-truth’ times. The Humboldtian mission should not depend exclusively on the
            state. It should find other vehicles. Indeed, one of the great hidden strengths of the
            US ecology of higher education is that it does not constitute a ‘system’ at all. Rather,
            it is a patchwork of institutions, whose leaders have been based on private founda-
            tions, often dating from before the founding of the nation itself. The origins of the
            Ivy League are not so different from Oxbridge’s, including their clerical and pater-
            nalistic trappings. Nevertheless, by the early twentieth century – and especially after
            the First World War raised America’s global ascendancy to full self-consciousness –
            these institutions quickly adopted and scaled up the Humboldtian model. It turned
            the US into the world’s cynosure for higher education, a status that a century later it
            still holds – and is likely to hold into the foreseeable future, even if the US suffers
            more general geopolitical decline. In this respect at least, the US continues to follow
            in the UK’s footsteps.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My argument for relaunching the Humboldtian vision of the university –
            ‘Humboldt 2.0, if you will’ – is predicated on a certain understanding of Humboldt’s
            ideal of the ‘unity of teaching and research’ as embodied in the person of the aca-
            demic, one that I have been pursuing for the past quarter-century (Fuller, 2000a). It
            involves regarding the classroom as the crucible for forging this vaunted unity as
            researchers engage in what I have dubbed the ‘creative destruction of social capital’
            through the process of teaching. If by ‘research’ we mean the production of new
            knowledge (even if it is about what we already know), then it is by definition an elite
            achievement. Someone is always recognized as having done something first. To be
            sure, some have followed philosopher Charles Sanders Peirce and sociologist
            Robert Merton in suggesting that the scientific spirit is animated by the desire for
            such recognition. But even if that is a fact of human psychology, if knowledge is to
            be understood as a public good, let alone an agent of democracy, then these initially
            esoteric achievements need to be made more generally available. This means that
            teaching is the necessary complement to research.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thus, the Humboldtian aims to ensure that priority of discovery does not lay the
            groundwork for entitlement, a version of rentiership that effectively forces future
            researchers to traverse the same path – typically requiring the same credentials – of
            their predecessors to make further progress. Such belief in ‘path dependency’, as
            economists say, is routinely taken for granted by commentators both inside and
            outside of academia who declare that our world’s increasing complexity means that more people need to spend more time to make sense of much less of what there is to
            know. Thus, we are told that the only way forward is ‘trust in experts’. It is against
            this modern superstition that our ‘post-truth condition’ justifiably rails. And on this
            matter, the Humboldtian is on the side of the ‘post-truthers’, especially as we live in
            the most educated time in history. Deference to cognitive authorities that sufficed
            even fifty years ago no longer carry the same intuitive force.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The path from Humboldt to post-truth had been quietly paved by philosophers in
            the nineteenth and twentieth centuries, as the aura surrounding ‘discovery’ in sci-
            ence was eclipsed by the need for any such discovery to be ‘justified’. Invariably,
            this meant arriving at a teachable process – typically associated with ‘logic’ in some
            sense – that would have enabled anyone to draw the same conclusion as the original
            ‘genius’ discoverer did. Indeed, in the 1980s the artificial intelligence pioneer
            Herbert Simon developed a family of computer programmes called ‘BACON’ that
            were designed to reduce the discovery process to a mechanical routine (Langley
            et al., 1987). While this project may seem to be a far cry from the Humboldtian mis-
            sion, it shares an interest in the demystification of cognitive authority. For some,
            though not all, the step too far that Simon and his colleagues took was to suggest
            that scientists in the future might derive the ideas they pursue from just such a
            machine. But would this be so different from being inspired by reading a book?]]>
			</paragraph>
			<paragraph>
				<![CDATA[In any case, for the Humboldtian, education is the primary instrument of democ-
            ratization. It works by making it easier for the next generation to make its own what
            previous generations had struggled to acquire – and even then, with only a few in
            full possession of it. One should not underestimate the extent to which most cultures
            saddle each generation with the burden of ‘honouring’ their ancestors, which often
            leaves them little time to do much else. In contrast, Humboldt’s future-facing,
            research-led orientation to education capitalized on the increasing physical pres-
            ence of books, journals and other written material – not to mention increased liter-
            acy in the population. The presence of libraries on university campuses symbolized
            this tendency. It turned teaching into an art of compression, whereby students no
            longer filled their minds with long passages of past authors learned by rote; rather,
            their teachers provided them with streamlined versions suited to the world yet to
            unfold in which they would spend the remaining and largest portion of their life.
            This explains the rise of mathematical formulas, first in the natural and later the
            social sciences, as well as the preoccupation with prosody and logical form in more
            humanistic fields. Students would be taught these as paradigms for organizing their
            thoughts into various forms of expression as they go forward; whatever else they
            might need to complete their thoughts would be supplied by the university library.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the early twentieth century, anthropologist Alfred Kroeber (1917) appeared to
            have the Humboldtian university in mind when he described humanity’s emerging
            ‘superorganic’ nature. Kroeber, a first-generation American of German parents, is
            now perhaps mainly known as the father of the feminist science-fiction writer,
            Ursula K(roeber) Le Guin. Writing at a time when John Dewey’s influence was
            ascendant among American educators, Kroeber argued that Homo sapiens is set
            apart from other animal species by intergenerational efficiency savings, which
            enable us to exert greater mastery over an ever-expanding part of our environment, not least that indefinite temporal horizon called ‘the future’. Kroeber’s intellectual
            infrastructure today extends to the technological noösphere whose public face is the
            internet, nowadays sometimes metaphysically glossed as the ‘Technium’ (Kelly,
            2010). Most interestingly, it has facilitated a move against complexity in the growth
            of knowledge, which I have called ‘disintermediation’ (Fuller, 2000a: 114). While
            it may be tempting to dismiss this movement as ‘vulgarisation’, especially when
            summaries replace original texts or dead metaphors supplant living arguments, it is
            better to think of it as a stage in the ongoing economization of thought, whereby the
            maintenance of any distinction must be justified by the material difference it makes.
            For those of the generation before Dewey and Kroeber, such as Peirce and Ernst
            Mach, this was how science was generally seen to have transformed biological evo-
            lution to social progress, thereby raising humans above their animal origins. For
            them the epitome of such ‘economy’ was the mathematical formula, which com-
            pressed a potential infinity of observations into a short string of symbols that could
            be manipulated in a system operating with a finite number of rules.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the same time, it’s easy to forget the ancient roots of this way of conceptual-
            izing our relationship to knowledge, which are crucial for understanding educa-
            tion’s dual modern role in individual emancipation and social progress. The Greek
            rhetoricians originally spoke of topoi, the projection of memory onto physical loca-
            tions for purposes of recall in a speech. The speaker would thus come to associate
            delivering a speech with navigating a space. The Sophists economized on this pro-
            cess by introducing the Athenians to systematic writing, or grammar, which enabled
            speakers to put their memories – and thoughts more generally – on paper, a more
            portable space that would be repeatedly consulted and elaborated. In Plato’s
            Phaedrus, Socrates famously railed against this practice, which he believed might
            falsify thought altogether. His argument masked Plato’s deeper concerns that mass
            literacy would make it possible for anyone to script their own course of action, a
            suspicious art linked to the theatre, which routinely blurs the line between what is
            real and fake – at least in Plato’s mind (Fuller, 2020a: Conclusion). As it turns out,
            Plato was worrying two millennia ahead of his times, as these private writing prac-
            tices triggered the process that eventuated in Kroeber’s ‘superorganic’ world popu-
            lated by numerous external memory stores and ‘informatized’ virtual spaces.
            Indeed, a measure of the extent to which the traditional distinction between the
            intellectual resources proper to the individual and to the individual’s environment
            has changed is that cognitive scientists are increasingly happy to take us to possess
            ‘extended minds’ if we have not turned into ‘cyborgs’ altogether (Clark, 2003).]]>
			</paragraph>
			<paragraph>
				<![CDATA[All of this makes for challenging times for education generally – and for the
            university as a specific educational institution. Elon Musk’s wearable brain-
            computer interface ‘Neuralink’, though itself still more promise than product, looms
            as a spectre on the horizon. It raises the following question that runs as a red thread
            in the pages that follow: What is the added value of university life – for both aca-
            demics and students – at a time when not only is the research agenda dictated by
            extramural funding but also employers are turning to alternative means of training,
            qualification and selection? The answer proposed here involves replaying the uni-
            versity’s history in a new key that focuses on the institution’s centrality to the translation of humanity’s collective knowledge to reduce the gap between its pro-
            ducers and its users. Yet, social media has already achieved the material side of this
            ambition. So, what’s academia’s distinctive contribution? The Humboldtian answer
            lies in the cultivation of what Kant called judgement, an attitude of mind that sits
            between the religious (Jesuit) capacity for discernment and the secular (aesthetic)
            capacity for connoisseurship. This attitude cuts across all fields of inquiry in aid of
            organizing what has been known into what will need to be known, by separating
            what is worth and not worth knowing. Judgement in this sense is exercised by both
            students and their teachers as they repurpose the past in the present with an eye to
            the future. In today’s terms, it’s all about curation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As a guide to the argument in these pages, the distinct spirit of the ‘Humboldtian’
            university is encapsulated in the following six themes:
            1. Futurism as the Soul of the Humboldtian University, 2. Historical Consciousness as the Source of 
            Humboldtian Futurism, 3. Judgement as the Key Humboldtian Attitude, 4. Translation as the Key Humboldtian 
            Activity, 5. Knowledge as a Public Good as the Humboldtian Ideal, 6. Academic Professionalism in the 
            Humboldtian Vision.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Futurism as the Soul of the Humboldtian University: The original genius of
            Humboldt’s vision lay in its focus on the ‘future’ as the unoccupied terrain in
            which the aspirational German peoples might take the lead by incorporating the
            best and deleting the worst elements of, on the one hand, traditional modes of
            authority (represented by King + Church) and, on the other, the modern radical-
            ism of French republicanism and English liberalism. To a certain extent, this
            tension had been already brewing within the university itself, as Immanuel Kant
            (1996) had made clear in his 1798 essay, ‘The Conflict of the Faculties’, where
            he proposed that ‘philosophy’ (understood as a modern discipline) should adju-
            dicate between the conflicting claims to authority advanced by the medieval dis-
            ciplines of law, theology and medicine. This background explains the ‘dialectical’
            and ‘synthetic’ character of the German idealist philosophers who were the
            original standard bearers of the Humboldtian vision at the University of Berlin.
            Research and teaching were ‘unified’ in that the teacher did research to resolve
            such cross-disciplinary conflicts in his own mind and then presented a rational-
            ized version of that journey to the students, which was offered in the spirit of a
            model that they might adapt as they make their own personal journeys through
            life. Put another way, students were given a prospector’s guide to the open vistas
            of knowledge available for them to pursue. It amounted to a ‘deviant interdisci-
            plinarity’ approach to knowledge, whereby existing academic disciplines are
            understood to be no more than way stations for thought in humanity’s collective
            inquiries.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Historical Consciousness as the Source of Humboldtian Futurism: The
            Humboldtian approach to history sees the past as a repository of potential futures
            that have yet to be fully realized but may still come to pass under the right cir-
            cumstances. It marked a sea change from the medieval and early modern univer-
            sities stress on the continuity of knowledge and practice over time, punctuated
            by occasional eccentric (‘miraculous’) incidents, followed by a restoration of the
            ‘natural order’. In the modern era, ‘tradition’ was the word that came to be used
            to capture this pre-Humboldtian sensibility, which was projected onto cultures
            worldwide that had yet undergone ‘Enlightenment’. Put bluntly, the non-West
            came to be the site of what the West was trying to leave behind from its own his-tory (Said, 1978). In its place, two different but equally ‘modern’ approaches to
            history arose: one steadfast in staying the course set by the original Enlightenment
            by not reverting to tradition, and the other more open to the past but as a basis for
            an alternative future, ‘Another Enlightenment’, so to speak. The success of the
            Industrial Revolution and the failure of the French Revolution fueled these alter-
            native visions, respectively. The former has been seen as more ‘pragmatic’ and
            broadly aligned to liberalism, the latter more ‘utopian’ and broadly aligned to
            socialism. The Humboldtian university has catered to both sensibilities. Indeed,
            as my coinage of ‘utopian knowledge capitalism’ in this book suggests, they are
            not so distinct. Indeed, the capitalist slogan ‘creative destruction’ and its social-
            ist counterpart ‘permanent revolution’ are two sides of the same coin (Fuller,
            2021a): We can neither revert to the presumed past nor assume that the future
            will be an extended version of the present. Past and future are co-produced in the
            present. Understood this way, education is the student’s version of what their
            teachers routinely undergo in the research process – both in the name of Bildung,
            aka life as inquiry. Moreover, it counteracts the blindness that comes from think-
            ing that we live in the most advanced period in history, which makes it hard to
            imagine how people in the past managed to bring us to where we are now. Yet,
            such an imagination may come in handy if the world as we know it were sud-
            denly to end.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Judgement as the Key Humboldtian Attitude: The primary quality of the
            Humboldtian academic is judgement. It lies behind notions as exalted as aca-
            demic freedom and as mundane as marking students’ work. It is rooted in the
            legal power to dispose of a case by deciding its normative standing (I.e., acquit-
            ted/convicted, pass/fail, etc.). By the late eighteenth century, it had become a
            generalized feature of perception, following the invention of ‘aesthetics’ as a
            distinct science. This extended notion of judgement figured prominently in
            Kant’s later work, which inspired the young Humboldt. It involves relating the
            composition of a work to the larger concept that informs it. In this regard, both
            teacher and student are ‘free’ to compose matter into form, each allowing space
            for the other, but in the end, to varying degrees of success. The modern idea of
            the ‘critic’ in the arts (extending to the ‘public intellectual’ as critic of society)
            derives from this sensibility. ‘Criticism’ in this sense aims to liberate form from
            matter by saying how the created work facilitates or impedes the realization of
            the creator’s idea. This implies an approach to education that joins the perform-
            ing arts and the laboratory sciences in common cause. Both frame the pedagogi-
            cal problem-space as ‘demonstrations’ rather than ‘examinations’, whereby
            students’ knowledge is tested by personally participating in an activity that
            allows many ways to succeed and fail. Critical judgement of these demonstra-
            tions involves treating them more in the spirit of completing an ambiguous
            Gestalt figure than matching a fixed template. In this respect, Humboldtian peda-
            gogy goes against the spirit of Thomas Kuhn’s (1970) famous account of ‘nor-
            mal science’ as puzzle solving. Students are expected to exercise judgement to
            the same extent as their teachers. But this then introduces an element of chance
            in the evaluation process.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Translation as the Key Humboldtian Activity: A striking way to think about
            the difference between the medieval and the modern university is that the former
            presumed that the universe of knowledge was distributed, which in turn justified
            deference to expertise, whereas the latter presumed that the educated person
            could know whatever needs to be known ‘in translation’, which would then min-
            imize his or her reliance on experts. This marks the difference between the uni-
            versity operating as a training ground for administrators and for autonomous
            individuals. That Humboldt was a linguist is not an accidental feature of his
            vision. The central problem of translation – as translators themselves often put
            it – is to convey the ‘source’ to the ‘target’, i.e., to render in a second language
            what has been already rendered in the first (Bassnett, 1980). This is the task of
            teaching, where the ‘source’ language is the research world, and the ‘target’
            language is the student world. The difference is that teaching is, so to speak,
            embodied translation, in that the teacher performs the translation so that the stu-
            dents might at first understand the imparted knowledge and then later use it for
            their own purposes. This helps to explain the original significance of the lecture
            in the modern university as a performance that went beyond an opportunity for
            students to copy the texts on which they will be examined. Among the many
            parallels between teaching and translation is the disparity between the contexts
            in which translated knowledge is produced and consumed – what philosophers
            of science call, respectively, the ‘context of discovery’ and the ‘context of justi-
            fication’. It is through teaching that research is ‘justified’ in this sense – and, in
            the Kantian sense, the privacy of reason is rendered public. The contemporary
            phrase ‘translational research’, which is usually limited to the application of
            biomedical research, also aptly captures Humboldtian pedagogy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowledge as a Public Good as the Humboldtian Ideal: Let us now shift
            focus from the character of the knowledge producer to that of the knowledge
            produced – namely, the university as the site where knowledge is manufactured
            as a public good. Key to understanding this phrase is what I have dubbed anti-
            rentiership. It goes back to the Humboldtian principle that knowledge empowers
            by being made as widely available as possible, which in turn drives the impera-
            tive to translate research into teaching. However, this general sensibility – core
            to the university’s modern democratizing mission – goes against the increasing
            prominence of intellectual property claims, which nowadays extend beyond
            industrial patents into teaching materials. But I also mean to include long-
            standing academic practices, including the credit-oriented academic citation sys-
            tem, the toll-like character of the peer review process, the institutional incentives
            for academic work to self-organize around ‘strong attractors’ that then become
            ‘paradigms’ (in Kuhn’s sense) and, last but not least, the moral panic surround-
            ing plagiarism, understood as a crime committed routinely by both academics
            and students. As all these policies shore up academic authority in the wider soci-
            ety, they also prevent academic knowledge from becoming a public good.
            Overcoming this impasse in the Humboldtian spirit requires a decoupling of
            academia’s processes of internal reproduction and of external dissemination. It
            would entail curbing the ‘mission creep’ of peer review, which has ended up becoming a dispersed regulatory mechanism for the entire knowledge system,
            even outside of academia. The Humboldtian would restrict peer review to deter-
            mining whether what is claimed in, say, an article is testable on its own terms,
            regardless of its relationship to the dominant paradigm’s research agenda. For its
            part, plagiarism would be reduced to judgements about whether a new piece of
            work ‘adds value’, notwithstanding the amount or character of the appropriation
            of the work of others that it involves.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Academic Professionalism in the Humboldtian Vision: Max Weber character-
            ized the Humboldtian sense of professionalism as ‘vocation’, a self-subordination
            to the voice of another, which amounts to accepting a role in a drama not of one’s
            own creation and which continues after one has left the stage. The original model
            was monastic discipline, a lifestyle inspired by God. Academic disciplines
            descend from this idea. However, discipline was always regarded as a means to
            an end, not an end in itself. Whereas discipline rendered the monks as instru-
            ments of God, in the modern era, this idea was secularized as the search for
            Truth, wherever it may lead, whatever the consequences. The etymology of
            ‘truth’ in words meaning faith, trust and loyalty is not trivial. In practice, it meant
            pushing the discipline’s epistemic constraints to their limits, which in turn under-
            wrote an ethic of ‘testability’, the most popular and self-conscious version of
            which was Karl Popper’s ‘falsifiability’ principle. In this respect, science nor-
            malizes heresy. However, in religion, this tradition has been countered by
            ‘priests’, whose job is to maintain the churches that serve the needs of those who
            have not received such a direct call from God and, more generally, to stabilize
            the institutional framework common to the monks and the pastorate. In the mod-
            ern era, the administrative class has served this role for universities, catering to
            the needs of both academics and students. But for Humboldt, this role was quite
            specific: namely, simply to enable the freedom of both academics and students
            to pursue the truth. Put more provocatively, academic administration should be
            in the business of publicizing not policing what transpires on campus. However,
            this vision of university administration has been hard to realize for various rea-
            sons, not least because academics and students can become too materially
            invested in university governance, which in turn diverts them from the search for
            Truth. For this reason, Humboldt thought that the state (given its overarching
            power and relative detachment) should be the ultimate underwriter of university
            administration. However, the morphing and long-term decline of the state have
            thrown this feature of Humboldt’s vision into serious doubt and needs to be
            rethought.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Abstract This chapter introduces two types of interdisciplinarity relevant to the
            organization of inquiry: normal and deviant. They differ over their attitudes to the
            history of science, underlying which are contrasting notions of the ‘efficient’ pur-
            suit of knowledge. The normal/deviant distinction was already inscribed in the uni-
            versity’s medieval origins in terms of the difference between Doctors and Masters,
            the legacy of which remains in the postgraduate/undergraduate degree distinction.
            The prospects for deviant interdisciplinarity in the history of the university were
            greatest from the early sixteenth to the early nineteenth century – the period that we
            now call ‘early modern’. Towards the end of that period, due to Kant and the genera-
            tion of idealists who followed him, philosophy was briefly the privileged site for
            deviant interdisciplinarity. After Hegel’s death, philosophy fell into decline and nor-
            mal interdisciplinarity began to take hold, resulting in today’s fixation on ‘exper-
            tise’. Prospects for a post-philosophical, deviant interdisciplinary vision are
            explored, largely through the lens of what after Jean-Baptiste Lamarck is now called
            ‘biology’. The chapter concludes with an Epilogue that considers contemporary
            efforts to engage philosophy in interdisciplinary work, in which William James fig-
            ures as a polestar.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Wilhelm von Humboldt made philosophy the modern university’s foundational
            discipline. And in many respects, it has remained that way, notwithstanding its
            increasing professionalization and specialization in the twentieth century.
            Philosophy may relate to interdisciplinarity in two distinct ways: On the one hand,
            philosophy may play an auxiliary role in the process of interdisciplinarity through a
            conceptual analysis of knowledge based on the special disciplines, which are
            presumed to be the main epistemic players. I characterise this version as normal
            because it captures the more common pattern of the relationship, which in turn reflects an acceptance of the division of organized inquiry into disciplines. On the
            other hand, philosophy may be itself the site to produce interdisciplinary knowledge,
            understood as a kind of second-order understanding of reality that transcends the
            sort of knowledge that the disciplines provide when left to their own devices. This
            is my own position, which I dub deviant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My old teacher Nicholas Rescher used to describe his research as
            ‘interdisciplinary’, which puzzled those who saw him as a solitary producer of
            books of systematic philosophy. He quickly explained that his own mind was the
            site of the interdisciplinary work, which left the audience amused by the apparent
            irony of the remark. In fact, Rescher was playing it straight, but this moment of
            unintended humour reveals two rather incommensurable views about the animus
            behind interdisciplinary work, each of which is tied a conception of philosophy’s
            role in the organization of knowledge. Normal interdisciplinary work occurs in
            research teams or, if it does occur in a single mind, one’s mental space is arranged
            as a factory with a clear division of labour amongst the disciplines, each of which
            contributes a discrete task to an overarching epistemic enterprise. But Rescher was
            talking about interdisciplinary work as a blending process whose distinctly
            ‘interdisciplinary’ character is more clearly present in the style of work than the
            finished product. Rescher’s view represents what I call the deviant interdisciplinarian
            perspective, which has been both persistent and persistently abnormal in Western
            intellectual history, where nowadays it is integral to the post-truth condition (Fuller,
            2018a: Chap. 4). It is also the form of interdisciplinarity that I champion.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The difference between normal and deviant interdisciplinarity turns on
            contrasting attitudes to the history of the rise of disciplines in organized inquiry.
            Both the normal and the deviant versions accept that the need for interdisciplinarity
            arises from the gradual specialisation of inquiry, which is typically portrayed as a
            division of labour from some primordial discipline – call it ‘theology’ or
            ‘philosophy’– that asks the most general and fundamental questions about the
            nature of reality. But this common image is then interpreted in two radically different
            ways, which are fairly called ‘normal’ and ‘deviant’ in two distinctive senses.
            Normal interdisciplinarity is not only the more widely subscribed view in our time
            (i.e., empirically ‘normal’) but it also portrays interdisciplinary inquiry as a natural
            outgrowth of disciplined inquiry (i.e., normatively ‘normal’). Correspondingly,
            deviant interdisciplinarity is not only the less subscribed view but also sees
            interdisciplinary inquiry as, in key respects, reversing epistemically undesirable
            tendencies inherent in disciplined inquiry.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The normal interdisciplinarian gives cognitive specialisation a positive, even a
            naturalistic spin, say, by associating it with arboreal exfoliation, as in Kuhn (1970),
            in which the specialisation attending a paradigm shift is explicitly associated with
            the ‘tree of life’ image used by Darwinists after Ernst Haeckel to capture biological
            speciation as a whole. However, this can be misleading, since Darwin’s own use
            implied that the fruits produced by the branches are eventually consumed (i.e., ren-
            dered extinct). Not surprisingly, then, it has been more common to envisage spe-
            cialisation in terms of the functional differentiation of the developing individual
            organism, which may better fit with what Kuhn had in mind (Fuller, 2007a: Chap.
            2). In any case, normal interdisciplinary inquiry is easily likened to the gathering of
            ripe fruit from the various branches to produce an intellectually satisfying dish. The
            value of the dish is entirely dependent on the quality of fruits from which it is pre-
            pared. This suggests that interdisciplinary inquiry is subordinate to – if not parasitic
            on – discipline-based inquiry]]>
			</paragraph>
			<paragraph>
				<![CDATA[In contrast, the deviant interdisciplinarian treats the ‘division of labour’ identified
            by his normal counterpart as a dispersion of effort, such that an overall sense of the
            ends of inquiry is lost – specifically how the various disciplines contribute to the full
            realization of the human being. The recovery of this loss is then the deviant
            interdisciplinarian’s task (Fuller & Collier, 2004: Chap. 2). The task is biblically
            rooted in the Tower of Babel, a ‘second Fall’ consisting in humanity’s self-arrogation
            of its divine entitlement for its own particular purposes (symbolised by the prolif-
            eration of tongues) without sufficiently attending to the deity’s overarching design.
            Such was the theological pretext for the modern idea that science should aim to
            fathom this unifying intelligence, what Newton, following Renaissance scholars,
            originally called prisca sapientia (‘pristine wisdom’) but which after Einstein has
            been popularised as the search for a ‘Grand Unified Theory of Everything’ (Harrison,
            2007). We shall return to this difference in attitude towards specialisation below, as
            it bears directly on the image of the philosopher in interdisciplinary inquiry.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One of the most striking differences between normal and deviant
            interdisciplinarity – already present in my Rescher story – is the locus and character
            of interdisciplinary work. Normal interdisciplinarity is designed for teamwork, as
            each disciplinary expertise is presumed to make a well-defined contribution to the
            final project, whereas deviant interdisciplinarity assumes that the differences in
            disciplinary expertise themselves pose an obstacle to the completion of the project.
            Of course, at one level, the normal interdisciplinarian could hardly disagree –
            namely, about the prima facie difficulties in translating and then integrating forms
            of knowledge that are tied so intimately to distinct technical jargons and skills. But
            for her these difficulties are surmountable because they correspond to well-defined
            domains, such that each expert knows (at least in principle) when to defer to a more
            adequately informed colleague (Kitcher, 1993: Chap. 8; Lamont, 2009).]]>
			</paragraph>
			<paragraph>
				<![CDATA[In contrast, the deviant interdisciplinarian takes this culture of deference to
            reflect something more sinister, epistemic rent-seeking, which in this book is
            referred to as academic rentiership (Fuller, 2002: Chap. 1; cf. Tullock, 1966). Here
            the distinctive skills associated with disciplinary expertise are portrayed as con-
            spiracies against the public interest: They give the misleading impression that the
            most reliable, if not only, means of arriving at valid conclusions in a given domain
            is by trusting someone who has undergone a specific course of training, regardless
            of its empirical track record on relevant past cases or the availability of less costly
            means likely to reach the same ends. Despite the best efforts by modern philoso-
            phers of science to warn against committing the genetic fallacy by distinguishing
            the contexts of discovery and justification, such a deep trust in expertise would seem
            to reproduce the very problem for which that distinction was meant to solve, since
            whatever epistemic virtue is displayed ‘expertise’, it pertains to how knowledge was
            acquired, rather than how is used. At least, this would be the start of the deviant interdisciplinarian’s critique of expertise, which will be pursued in more detail in
            the next section.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Normal interdisciplinarians are unlikely to be fazed by what I have said so far.
            Citing Kuhn (1970), they would whereby ‘normal science’ is most clearly marked
            by the default inclination to solve today’s problems by seeing them as versions of
            past problems, courtesy of their textbook representation. Normal science’s sense of
            ‘path dependency’, as economists say, characterises each discipline’s sense of meth-
            odological rigour, a capacity to abstract the essential elements of a problem and
            resolve them in a principled fashion. From this standpoint, the deviant interdiscipli-
            narian may seem ‘eclectic’ and ‘arbitrary’, very much as upstart entrepreneurs look
            to managers in established firms, where the former wish to ‘creatively destroy’ and
            the latter to ‘monopolize’ markets (Schumpeter Schumpeter, 1950). Keeping with
            the business analogy, normal and deviant interdisciplinarians differ in their under-
            standing of ‘efficiency’. On the one hand, normal interdisciplinarians appeal to effi-
            ciency in terms of the path-dependent nature of disciplinary specialisation – the fact
            that specialisation only increases over time and through sub-divisions in already
            existing specialities. This is a sense of ‘efficiency’ dictated by the environment –
            that is, the most economic means of dealing with real complexity discovered in the
            world. On the other hand, deviant interdisciplinarians appeal to environmental pres-
            sures that encourage the creative combination of previously distinct expertises into
            an all-purpose technique. This is a sense of ‘efficiency’ dictated by the inquirer –
            that is, the most economic means of dealing with the need to retain a unified sense
            of purpose in the face of centrifugal forces in the environment. Both glosses on
            ‘efficiency’ can lay claim to the title of ‘more evolved’, the one in a divergent
            Darwinian and the other in a convergent Lamarckian sense (cf. Arthur, 2009; Fuller,
            2015: Chap. 6).]]>
			</paragraph>
			<paragraph>
				<![CDATA[While expertise certainly possesses a socially situated character, the ‘situation’ is
            largely defined by ‘expert’, whose expertise is grounded not in the situation itself
            but in the expert’s professional accreditation. This typically involves specialised
            training, but not necessarily prior experience with the situation. In this respect, the
            etymological root of ‘expert’ in ‘experienced’ sends an equivocal signal. The ‘prior
            experience’ may consist of no more than the expert’s education and/or direct
            acquaintance. Thus, the knowledge on which experts draw is alternatively cast as
            based on ‘templates’ or ‘precedents’, allowing for both a rationalist and an empiri-
            cist basis for the epistemology of expertise, respectively. Moreover, this ambiguity
            strengthens the expert’s hand in justifying his or her own practice. Even if the expert
            has never directly encountered the current situation, s/he may claim that it belongs
            to a kind of situation that others in the expert’s field have previously encountered. In this way, the unfamiliar is rendered familiar to the expert in a way that secures the
            confidence of the client. Moreover, it is no mere social constructivist nicety.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Anyone who contracts the services of an expert effectively licenses the expert to
            exercise discretion over the matter of concern to the client, which remains vaguely
            defined yet no less urgent prior to the expert’s intervention. Someone seeking the
            services of a physician for an ailment is a paradigm case (Fuller, 1988: Chap. 12).
            The client expects the expert to define the situation in a way that addresses the cli-
            ent’s concern. But it entails approaching the situation in ways that differ from the
            client’s default modes of epistemic access. This shift in perspective aims to convert
            the situation of concern into a ‘soluble problem’. The ‘success’ of the expert-client
            transaction is judged primarily in terms of the client’s acceptance that the expert has
            made a ‘good faith’ attempt to solve the client’s problem. Whether the client’s prob-
            lem is actually solved – or the client simply comes to understand the nature of his or
            her situation better – is a secondary concern. Indeed, if the client wishes to contest
            the expert’s handling of the client’s problem, then other experts of the same kind
            need to be engaged in the ensuing litigation to determine the occurrence of any
            ‘malpractice’. Malpractice is not something that clients can judge for themselves
            without additional expert input. However, if the expert is found guilty of malprac-
            tice, then the expert may be formally expelled from the professional peer group. In
            that case, the knowledge possessed by this defrocked expert would no longer count
            as expertise, even though the content of the defrocked expert’s knowledge would
            not have changed.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This account of expertise does not fit comfortably within the competing
            stereotypes of knowledge in modern epistemology: knowing how and knowing that,
            the former associated with practices and the latter with propositions. The difference
            lies in the ontology of knowledge and hence the mode of access properly called
            ‘epistemic’ with regard to the relevant objects. Expertise involves a rather different
            approach to epistemology – a different ontology of knowledge, if you will – one that
            is perhaps most familiar from the religious sphere. Here we need to recall that when
            ‘expert’ started to be used regularly in legal proceedings (i.e., ‘expert witnesses’) in
            Third Republic France, the non-expert was called a ‘lay’ person, a word whose
            implications philosophers appear to be curiously oblivious to, even though it is still
            regularly used in public discussions of expertise. The original suggestion was that
            experts constituted a secular clergy, perhaps even a replacement for the Roman
            Catholic version in terms of their authority. To be sure, this characterisation has not
            always been made to the advantage of the experts over the clergy, but it is worth
            dwelling on why the comparison has stuck over the years to understand the distinc-
            tive epistemology of expertise.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perhaps the most intuitively straightforward way to get to the heart of the matter
            is to compare a priest and, say, a physician or psychiatrist, focusing on their epis-
            temically relevant similarities. Much of what the secular experts say and do could
            be – and have been done – by priests in the past and, in some countries, even to this
            day. Moreover, in terms of ‘knowing how’ and ‘knowing that’, there is no reason to
            think that the reliability of one group has been better than the other, with regard to
            the efficacy or truth of what they have done or said. We simply lack a consistent track record of what might be called ‘client satisfaction’, which takes seriously the
            client’s own judgements of their transactions with experts. After all, alongside the
            many homoeopathy patients who end up accepting their death from cancer are the
            many chemotherapy patients who similarly accept their fate. Both groups may be
            satisfied with the choices they made, even if third party social epistemologists are
            not. Moreover, it is not clear whether there is a specifically ‘epistemological’ prob-
            lem here – or a more straightforward ‘cultural’ problem about how people should
            conduct their lives.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In any case, we live in a world in which a wide variety of drugs and treatments
            can be administered only by qualified medical practitioners, even though others not
            so qualified may display the same level of competence in ‘knowing how’ and
            ‘knowing that’ with regard to such matters. Here I mean people who act just as the
            experts would in the relevant conditions, but they lack expert authorization. In a
            sense, it is the complement of the ‘placebo effect’, whereby people claim to be
            cured with ‘fake drugs’ because someone they regard as an expert has prescribed
            them. In contrast, I mean non-experts who prescribe what the relevant experts would
            but are not trusted (as much) because they are not licensed as experts. To be sure,
            sometimes these people successfully masquerade as experts, at least until they over-
            play their hand by making a serious practical error or seeking to leverage their
            pseudo-expertise. This ‘overplaying’ amounts to the pseudo-expert provoking an
            investigation into credentials that s/he would otherwise have been presumed to
            possess.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At this point, recall our earlier observation that even when an expert makes errors
            that are so serious that the client files a claim of malpractice, the expert’s peer com-
            munity plays a significant role in determining the liability of the charged expert. In
            effect, the possession of expert credentials may serve to shield the practitioner from
            forms of punishment to which the pseudo-expert is automatically liable, which may
            then be amplified by any deception involved. Yet, the cognitive error and the mate-
            rial harm may be the same. What accounts for the difference – and should there be
            a difference? Here social epistemologists have been inclined to appeal to something
            called ‘trust’, which functions mainly as a euphemism for a kind of risk-taking,
            whereby a portion of one’s own sphere of judgement is forfeited to someone else
            who is presumed to be more capable of acting on that person’s behalf. ‘Forfeited’ is
            used deliberately to convey the fact that the person taking the risk recognizes their
            own incapacity to address a matter of concern to them. Political scientists and econ-
            omists, closely following the conceptual framework of the law, characterize the
            client-expert relationship as one of ‘principal-agent’, which captures well the volun-
            tary subordination of the will that is involved in clients’ ‘trust’ of experts
            (Ross, 1973).]]>
			</paragraph>
			<paragraph>
				<![CDATA[In religious times, people trusted priests because of their faith in ‘God’, however
            defined. Nowadays people trust medical practitioners because of their faith in
            ‘Science’, however defined. In both cases, ‘truth’ is the philosophical term of art
            used to cover the object of the faith shared by the principal and the agent, with the
            proviso that the relationship between the principal’s trusted agent and that larger
            object of faith is bound to be imperfect, yet epistemically superior to the principal’s own relationship to the object, especially with regard to the matter of immediate
            concern to the principal. The epistemically distinctive feature of expertise, then, is
            the distributed nature of the process of knowing, whereby the principal knows
            through the agent, in both main senses of ‘knowing’ in modern epistemology: the
            ‘knowing that’ something is true and ‘knowing how’ to apply the truth to reach a
            desirable practical outcome.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The original context for ‘principal-agent’ theory is relevant for understanding
            what exactly is ‘distributed’ here. Clearly agency is distributed, which may invite
            thoughts of an ‘extended mind’. But materially speaking, risk is distributed, such
            that the principal aims to minimize the cost of personal misjudgement by placing a
            bet on the agent’s chance of making a better judgement on the principal’s behalf. In
            effect, what social epistemologists substantively mean by ‘trust in experts’ is a ver-
            sion of ‘risk pooling’ in the insurance and financial trades. Both Descartes and
            Pascal, in their different ways, were arguing against engaging in such activities: One
            should bear the risks for oneself, with Descartes being somewhat more bullish than
            Pascal about the likely outcome. Hence, while both were avowed Christians, they
            were widely seen in their day as anti-clerical: They would rather place their faith
            (or, in Descartes’ case, ‘thinking’) in God directly than in a priest who then exer-
            cises that faith on their behalf.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this sense, the expert is in a double-sided relationship of ‘representation’: on
            the one hand, to the client for whom the expert is a trusted surrogate and, on the
            other hand, the object of knowledge to which the expert must remain loyal. I do not
            wish to argue conclusively here whether ‘representation’ is used univocally or
            equivocally in these two contexts. Suffice it to say, social constructivists (me
            included) follow in Hobbes’ footsteps in treating the two uses univocally, which in
            turn reflects the long historic drive toward increasing literalism in language, starting
            with Augustine and the Franciscan scholastics (Duns Scotus, Ockham, etc.), run-
            ning through the Protestants, Bacon, Descartes, Pascal and their secular progeny,
            not least Rousseau. In practice, contrary to its contemporary connotations, this ‘lit-
            eralism’ has fostered the view that one’s language should reflect what one thinks
            because speaking is an act of self-authorization: You should not say it unless you
            mean it. Here words like ‘belief’ and ‘representation’ function as euphemisms for
            this more potent idea.]]>
			</paragraph>
			<paragraph>
				<![CDATA[All the above philosophers and theologians are haunted by the creativity of God’s
            Word (logos), which ‘literally’ applies to humans as having been created imago dei
            (in Augustine-speak), notwithstanding the Fall recounted in Genesis. The modern
            preoccupation with the use of language to authorize control in both the legal and
            scientific spheres is its secular descendant, ‘positivism’ being its most self-conscious
            expression (Turner, 2010: Chap. 3; cf. Passmore, 1961). The arc of Anglo-German
            thought from Mill and Russell to Wittgenstein and Kripke made this concern central
            to what became ‘analytic philosophy’ in the twentieth century. It also helps to make
            sense of the persistent controversies surrounding expertise as a form of knowledge.
            The expert’s discretion to define the client’s situation for purposes of ministering to
            it amounts to a power to name, understood as a pretext for actions of a certain sort
            that the expert advises or takes on behalf of the client in an otherwise indeterminate situation. More to the point, the expert purports to resolve this indeterminacy by
            replacing what the client has identified through a ‘proper name’ (i.e., something
            unique) with a ‘definite description’ (i.e., a specific complex of repeatable proper-
            ties). The question then is whether that definite description – or even some succes-
            sion of such descriptions – is likely to be exhaustive of the situation originally
            identified by the client. Practically speaking, the answer is bound to vary from case
            to case. Theoretically speaking, resistance to expert advice may be seen as akin to
            the refusal to reduce proper names to definite descriptions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the philosophy of science, this sensibility is reflected in historically informed
            doubts that the truth about some ‘rigidly designated’ (in Kripke-speak) part of real-
            ity is likely to be reached by successively improved versions of the descriptors used
            in current authoritative accounts of that reality – Hilary Putnam’s ‘pessimistic meta-
            induction’ (Putnam, 1978). Philosophers routinely call this position ‘realism’, but it
            is more Platonic than Aristotelian in inspiration: It presumes a conception of reality
            to which we may gain access but not necessarily by our currently established means,
            which include those whom we now deem ‘expert’. Truth be told, it is also the
            implied metaphysics of ‘the end justifies the means’, which licenses even substan-
            tial deviations from established expertise if one – and perhaps more importantly,
            one’s ‘society’, however defined – is willing to absorb the risks involved.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The very idea that expertise should be seen as something above and possibly
            beyond competence arose in the 1980s and was associated with the drive to auto-
            mate complex decision-making in so-called ‘expert systems’, which remain a staple
            in the ‘knowledge management’ field in business schools (Fuller, 2002: Chap. 3). It
            was a response to the long-proven failure of human experts – starting with Paul
            Meehl’s research in the 1950s – to perform as well as mechanical counterparts in
            diagnosing various medical and psychiatric disorders, at least as judged by profes-
            sional handbook standards. In epistemological terms, it had been demonstrated that
            human expert judgement is ‘unreliable’ in real world settings (Faust, 1984). This led
            ethnographers to interview human experts to understand how they would process
            cases as they made their decisions under a variety of hypothetical conditions.
            Implied in this strategy was that the experts’ basic approach was sound but that it
            ran into difficulties when they reached their ‘natural limits’. ‘Natural limits’ should
            be understood to mean limits to both one’s professional training and processing
            capacities, including hot and cold cognitive biases. The intended result of this
            research was an ‘expert system’, which consisted of a user-friendly computer inter-
            face informed by a decision-tree-styled algorithm, the design of which was based on
            the expert interviews. It inspired many cognitive scientists and philosophers of sci-
            ence, me included, to countenance that a sufficiently advanced form of artificial
            intelligence may be the true reference class of the various qualitative and quantita-
            tive accounts of ‘rationality’ that philosophers have historically proposed
            (Fuller, 1993).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The backlash against this entire strategy, philosophically inspired by Hubert
            Dreyfus and still informing Harry Collins’ approach to expertise, was to argue that
            any such ‘expert system’ would always be insufficient to replace the human expert
            (Collins, 1990; Dreyfus & Dreyfus, 1986). The backlashers claimed that human performance in the relevant domain minus the above ‘natural limits’ would always
            be better than the ‘debugged’ human represented in the computer algorithm pro-
            gramming the expert system. Their stance reflected a larger background concern –
            namely, that an advanced form of artificial intelligence might significantly supersede
            human performance in the sorts of complex cognitive tasks traditionally seen as the
            exclusive preserve of humans. Unsurprisingly perhaps, it resulted in a sharper dis-
            tinction being drawn between ‘competence’ and ‘expertise’, almost as a proxy for
            ‘machine’ and ‘human’. The intended contrast was between relatively routine
            domain-specific judgements, which a well-programmed machine might deliver, and
            more ‘creative’ judgements that may suspend some of the problem-solving con-
            straints governing the domain, but without completely undermining the domain’s
            epistemic framework.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nevertheless, this did not deter artificial intelligence enthusiasts following the
            lead of Herbert Simon (myself included), who believed that the competent-expert
            distinction could be deployed to capture the difference between, say, on the one
            hand, Kuhn-style ‘normal scientists’ in classical physics who were incapable of
            thinking outside of their paradigm to solve the long-standing problems facing
            Newtonian mechanics, and on the other hand, Einstein, Heisenberg and the other
            early twentieth century revolutionaries who managed to radically transform physics
            without destroying it altogether (Langley et al., 1987; Nickles, 1980). Subsequent
            research drew the implied distinction in less world-historic terms, but the intuition
            guiding it is clear enough. It is one thing to acknowledge that Einstein and his com-
            rades dealt with outstanding physics problems in a different frame of mind from
            their more classical colleagues, and another to say that the success of their approach
            amounted to their ‘knowing something more’ than their classical colleagues, in
            some univocal sense of ‘know’. It would be reasonable to grant the former but deny
            the latter. Indeed, a classical physicist such as Henri Poincaré was probably more
            competent than Einstein by the academic standards of the day, yet that did not pre-
            vent Einstein from proving more expert in accounting for relative motion. These are
            good prima facie grounds for concluding that competence and expertise bear some
            sort of orthogonal relationship to each other. Let me briefly try to tease out the
            nature of this ‘orthogonality’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What unites this world-historic case of ‘expertise’ with more ordinary cases
            involving, say, doctor-patient is an acceptance that the decision-making context is
            open. In other words, the ‘normal’ ways of making sense of the situation are sus-
            pended – to an extent that remains to be specified – so that certain ‘abnormal’ ways
            of dealing with it are licensed. Due to the normalization of expertise in contempo-
            rary society, it is easy to forget the alien – perhaps even ‘incommensurable’ – char-
            acter of how a doctor typically approaches a patient. Nevertheless, patients license
            that alien treatment because they have come not to trust their own judgement on
            matters of health, notwithstanding their personal nature. Similarly, what Kuhn
            (1970) called ‘revolutionary science’ is made possible once normal scientists take
            an estranged stance toward their ‘normal science’ practices because of their failure
            to solve long-standing puzzles of central concern to them. Kuhn calls this shift in
            orientation a ‘crisis’ – and it reflects a recognition of the limits of what heretofore had passed for ‘competence’. To be sure, the intuitiveness of Einstein’s superior
            expertise depends on a retrospective evaluation of the different problem-solving
            approaches taken by him and Poincaré – that is, after their uptake by the relevant
            peer community in physics. By that standard, Einstein succeeded where Poincaré
            had failed. I shall return to this point shortly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What the above suggests is that expertise, rather than being an incremental or
            even a step-change advance on competence, operates in a radically different cogni-
            tive space in which context is king. Competence is about knowledge in a closed
            system, and expertise in an open system. Imagine Einstein travelling back just a bit
            in time, to, say, 1850, around when his parents were born. Would his approach to
            problem-solving been seen as expert or crazy by his physics colleagues? The answer
            would depend on the likelihood that the corresponding peer community would con-
            solidate around Einstein’s treatment of light as a constant in the understanding of
            motion. Alternatively, fast forward into the future that Simon and other AI enthusi-
            asts have envisaged – one in which an Einstein-like computer could not only adjust
            the parameters of the variables in its programme but change variables into con-
            stants, and vice versa, resulting in a substantially different programme. Would such
            a ‘superintelligent’ machine capable of projecting paradigm shifts whole cloth be
            regarded as a salutary revolutionary agent or a threat to the entire scientific enter-
            prise, if not the human condition generally? Much will depend on both first-order
            views about the state of science and second-order views about the conduct of sci-
            ence at the time such a machine becomes available.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The temporal character of expertise evaluations points to the inappropriateness
            of ‘reliability’ as a standard for judging experts. As a methodological concept asso-
            ciated with what is sometimes called ‘internal validity’, reliability is about the regu-
            larity with which the same conditions bring about the same effects. The term
            ‘mechanism’ is often used both literally and metaphorically to describe something
            that encompasses the ‘reliable’ relationship. However, if the conditions are not fully
            specified, then it is not possible to establish that relationship. Yet that is precisely the
            sort of situation in which a client would engage an expert – and part of that engage-
            ment would involve granting a license to the expert to complete the specification of
            the conditions, which in turn will circumscribe the interpretation and treatment that
            constitute the response. Insofar as a specific level of competence is required for
            expertise, its evaluation occurs far from the typical context of use. I refer here to the
            process by which experts acquire professional credentials, which may involve pass-
            ing specific academically and practically oriented examinations. In addition, mat-
            ters of competence may be central to the adjudication of a malpractice suit against
            an expert – but again, with the expert peer community playing a crucial determining
            role. However, if the expert receives no formal complaints in the aftermath of an
            engagement with a client, then the expert is presumed to have been competent,
            however s/he acted.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In sum, what passes for competence in expertise is really the background support
            of the expert peer community, who through its own mechanisms, independent of
            any context of use, have come to invest their trust in the expert in question. Their
            willingness to vouch for the expert in what are typically open-ended conditions of 25
            practice can make all the difference in determining the appropriateness of an expert’s
            actions. The expert community is effectively the corporate underwriter of expertise.
            Its power is regularly revealed these days by Pope Francis I, who has allowed high-
            ranking members of the clergy to be tried for various alleged forms malpractice in
            civil courts after having withdrawn the immunity of sacred office. This allows the
            court to judge whether the acts in question would have been appropriate, had they
            been committed by someone unqualified in the spiritual expertise associated with
            holding sacred office. Comparable secular examples might involve enquiries into
            the treatment of (human or animal) research subjects in laboratory experiments
            without presuming the privilege of the scientific vocation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[That ‘competence’ turns out to mean little more than the corporate underwriting
            of the expert community is obscured by a confused conception of ‘reliability’ in
            epistemology, especially its ‘naturalistic’ forms. The confusion comes from trying
            to capture at once two senses of ‘reliable’ in ordinary language: on the one hand, the
            methodologically relevant idea of regular occurrence, and on the other, the morally
            relevant idea of trustworthiness that the previous discussion highlighted. As we
            have seen, these are quite different ideas that normally inhabit different cognitive
            spaces. But it would require another paper to examine how this confused conception
            of reliability has wreaked havoc in the recent epistemology literature. Nearly three
            decades ago, I characterised this confusion as phlogistemology, in homage to that
            hallowed pseudo-substance, phlogiston (Fuller, 1996).]]>
			</paragraph>
			<paragraph>
				<![CDATA[25
            practice can make all the difference in determining the appropriateness of an expert’s
            actions. The expert community is effectively the corporate underwriter of expertise.
            Its power is regularly revealed these days by Pope Francis I, who has allowed high-
            ranking members of the clergy to be tried for various alleged forms malpractice in
            civil courts after having withdrawn the immunity of sacred office. This allows the
            court to judge whether the acts in question would have been appropriate, had they
            been committed by someone unqualified in the spiritual expertise associated with
            holding sacred office. Comparable secular examples might involve enquiries into
            the treatment of (human or animal) research subjects in laboratory experiments
            without presuming the privilege of the scientific vocation.
            That ‘competence’ turns out to mean little more than the corporate underwriting
            of the expert community is obscured by a confused conception of ‘reliability’ in
            epistemology, especially its ‘naturalistic’ forms. The confusion comes from trying
            to capture at once two senses of ‘reliable’ in ordinary language: on the one hand, the
            methodologically relevant idea of regular occurrence, and on the other, the morally
            relevant idea of trustworthiness that the previous discussion highlighted. As we
            have seen, these are quite different ideas that normally inhabit different cognitive
            spaces. But it would require another paper to examine how this confused conception
            of reliability has wreaked havoc in the recent epistemology literature. Nearly three
            decades ago, I characterised this confusion as phlogistemology, in homage to that
            hallowed pseudo-substance, phlogiston (Fuller, 1996).
            2.3 Normal and Deviant Interdisciplinarity as Alternative
            Ways of Organizing the University
            If normal interdisciplinarity aims to exfoliate the complexity of reality and its
            deviant counterpart aims to recover some lost unity of knowledge, then how is the
            historic relationship between philosophy and science cast in the two cases? In
            normal interdisciplinarity the philosopher recedes from the first-order field of
            epistemic play in order to referee jurisdictional disputes between disciplines, which
            typically turn on the need for conceptual clarification and logical analysis of
            evidence. She acts as an honest broker between their competing epistemic claims
            but without imposing an epistemic regime of her own. This is close to the self-
            avowed ‘underlabourer’ role first self-ascribed by John Locke (vis-à-vis the ‘master
            builder’ Newton) and advocated by many of today’s analytic philosophers of science
            (Fuller, 2000b: Chap. 6; Fuller, 2006a: Chap. 3). In contrast, deviant interdisciplinarity
            would have the philosopher use her own understanding of the goal of disciplined
            inquiry – roughly, an epistemic super-universalism that aims to have all people
            know all things – as implying standards against which to judge the adequacy of any
            given disciplinary configuration.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This is how philosophy has laid claim to being the foundational discipline of the
            university (or ‘queen of the sciences’, when directly challenging theology) since Wilhelm von Humboldt re-launched the university as an institution dedicated to the
            resolution of what Kant (1996) memorably called ‘the conflict of the faculties’,
            more about which below (Fuller, 2007b: 208–13; Fuller, 2009: Chap. 1). From this
            standpoint, both German idealism and logical positivism – though not normally
            seen as philosophical bedfellows – turn out to be exemplars of deviant interdiscipli-
            narity. Indeed, each provided the paradigmatic gloss for the nineteenth and twenti-
            eth centuries, respectively, on what it means for ‘all people to know all things’. It is
            different from the sense of ‘unity of knowledge’ originally promoted by Auguste
            Comte under the guise of ‘positivism’ and his English contemporary William
            Whewell as ‘consilience’. These would lie between the normal and deviant poles of
            interdisciplinarity, as both granted a strong integrationist role to a philosophically
            inspired discipline (sociology for Comte, natural theology for Whewell) but without
            presuming that everyone should or can have access to such a unified understanding
            of reality (Fuller, 2007a: Chap. 2; Fuller, 2010b: Chap. 3).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Given that the university is the only institution explicitly dedicated to the
            indefinite pursuit of knowledge, it should come as no surprise that the normal-
            deviant divide in interdisciplinary horizons was already present in its original
            medieval formation. The division was marked in two degrees of equal status that
            matriculants could receive: Master and Doctor. The former is the prototype for the
            deviant interdisciplinarian, the latter for the normal one. My old teacher Rescher
            followed the way of the Masters in seeing himself as the self-sufficient site of
            interdisciplinary integration, whereas today’s networks of distributed expertise
            follow the way of the Doctors, for whom interdisciplinarity always implies
            knowledge beyond the competence of any given individual. It is worth observing –
            though it cannot be followed up here – that the policy of treating the ways of the
            Master and the Doctor as sequential in a course of study from ‘undergraduate’ to
            ‘postgraduate’ training in today’s universities sends profoundly mixed messages
            about the ends of higher education, since ideally each of the two ways calls into
            question, if not outright obviate, the need for the other. Put starkly: If one could
            know for oneself all that is worth knowing, then there would be no need to defer to
            others; but if there really is too much of value for any one person to know, then the
            very idea of training self-sufficient individuals is futile. Thus, the Masters would be
            rid of the Doctors, and vice versa, respectively.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In terms of the mendicant orders that staffed the original universities, Franciscans
            were prominent amongst the Masters, Dominicans amongst the Doctors (Fuller,
            2011: Chap. 2). Included in the ranks of the former were John Duns Scotus and
            William of Ockham, in the latter Albertus Magnus and Thomas Aquinas. Perhaps
            not surprisingly, the Franciscans inspired heretics, while the Dominicans trained
            their inquisitors in the period leading up to the Reformation (Sullivan, 2011). In the
            medieval universities, a Master’s degree empowered one to read, write, speak, lis-
            ten, observe, calculate and measure – the so-called liberal arts. These skills equipped
            one with the self-mastery needed to deal with others as equals. This is the basis of
            ‘humanistic’ education, which by the nineteenth century had morphed into a stan-
            dard of democratic citizenship. Students were expected to integrate the liberal arts
            with their personal experience into a synthetic whole that expressed their unique public persona. In today’s terms, we would say that the Masters located the value of
            discipline-based knowledge not in its inherent pursuit but in the ‘transferable skills’
            it provided for living one’s life. In this regard, the Master literally embodied the life
            of the mind in the world. For Humboldt, this amounted to training people of be
            exemplars of Enlightenment, something that Max Weber’s 1918 lecture ‘Science as
            a Vocation’ tried to capture for the uncertainty and ambivalence that faced Europe. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[In contrast, the Doctors were trained to administer over specific aspects of
            reality – the body (medicine), the soul (theology) or the land (law) – that were
            understood in markedly geographic terms, remnants of which linger in the idea of
            ‘domains of knowledge’ separated by ‘disciplinary boundaries’. Doctoral training
            required a demonstration of one’s competence in the management and productive
            use of what academics still refer to as ‘field’. The academic degree effectively
            granted a license to work on that metaphorical plot of land with an expected yield
            on investment for one’s professional colleagues, the field’s shareholders. Anyone
            who wished to apply or simply pass through that field of knowledge had to acquire
            their own license or pay a toll (i.e., at least make formal reference) to a relevant
            expert. In that respect, the Doctors interpreted the idea of ‘interdisciplinary
            exchange’ economistically, such that one specialist traded the fruits of his original
            labours with those of another so that both specialists were mutually enriched in the
            process, with the overall effect of facilitating the societal governance. Abstracting a
            bit from this characterisation of the distinction between the Masters and Doctors,
            we can see the pedagogical basis for, respectively, the coherence and correspon-
            dence theories of truth, the former focused on the self as the source of epistemic
            unity and the latter on an ‘external reality’ conceptualised as a mappable terrain (cf.
            Fuller, 2009: 62–68).]]>
			</paragraph>
			<paragraph>
				<![CDATA[For its first five centuries, the university was mostly dominated by the Doctors,
            whose conservative bias began to receive uniformly unfavourable publicity during
            the Protestant Reformation. As we shall see below, what might be called the ‘long
            early modern period’ – say, the three centuries from the Protestant Reformation
            (early sixteenth century) to the Age of Romanticism (early nineteenth century) –
            was a time when the precedence of the Doctors over the Masters were increasingly
            challenged, though the Doctors would eventually regain the upper hand in the nine-
            teenth century, as universities came to be organized along disciplinary lines (Merz,
            1965). In retrospect, the figures of René Descartes and Francis Bacon stand out for
            their innovative ways of re-asserting the Masters’ prerogative against the scholastic
            Doctors. I shall now take each in turn.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Descartes’ declaration ‘cogito ergo sum’, was understood, at least by younger
            contemporaries such as Nicolas Malebranche, as re-asserting humanity’s epistemic
            overlap with God’s point of view in line with the biblical claim of our having been
            created in imago dei. What philosophers after Kant still call ‘a priori knowledge’
            has carried forward this interpretation – namely, that regardless of the actual history
            of science, every human is endowed with the wherewithal to reason for themselves
            from first principles about any point in space and time, which implies the capacity
            to conclude that knowledge could have developed more efficiently. (Logical positiv-
            ism eventually got the most rhetorical mileage from this possibility.) To be sure, as godlike but not full-fledged deities, we may reason falsely, but that does not take
            away from our godlike ambition and perhaps even its ultimate success, as long as
            we are not complacent about our epistemic foundations. This image of Descartes
            was expressly promoted by the French Enlightenment figures Condorcet and Turgot,
            who were attracted to the idea that the intellectual revolutionary need not recover
            the meaning of ancient Scriptures because the capacity for receiving and responding
            to God’s word is ‘always already’ constitutive of humanity’s birth right (Cohen,
            1985: Chap. 9).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Enlightenment followed up Francis Bacon’s particular brand of anti-
            scholasticism by trying to make good on his avowed desire to re-plant ‘the tree of
            knowledge’ by requiring that disciplines reflect the logical outgrowth of specific
            mental faculties – as opposed to knowledge being simply allowed to flourish as
            esoteric, path-dependent traditions of scholarship (Darnton, 1984: Chap. 5).
            (Nowadays it might be cast in terms of a curriculum that capitalized on the brain’s
            spontaneous operations [Fuller, 2012a: Epilogue].) Such traditions only served to
            create sectarian differences that, in the case of England, eventuated in civil war in
            the generation after Bacon’s. As the Masters would have it, Bacon returned the
            quest for knowledge to the refinement of the entire mind to realize its full potential –
            that is, well beyond the narrow matter of learning which specific expert is entitled
            to deference when one lacks the relevant expertise. To Bacon’s Enlightenment fol-
            lowers, this shift in the ground of epistemic legitimation provided a scientific basis
            for supposing that any individual, simply by virtue of having been born with a fully
            functioning mind, could learn all that is worth knowing. This radical interpretation
            of Bacon, which probably came closer to his original intent than what became the
            Royal Society of London, eventuated in the 1789 revolutionary signature of ‘Liberty,
            Equality, Fraternity’. Against such Baconianism stood the clerics who controlled
            the universities but operated with what les philosophes regarded as confused or
            obscure conceptions of how the human mind worked, which in turn reflected a
            superstitious attitude toward how certain ideas have come to dominate the human
            condition (i.e., what has been is mistaken for what ought to be). The concrete sym-
            bol of this revisionist sentiment was L’Encyclopédie, a multi-volume work that was
            designed to be read not in service of specialised research but in the leisure of bour-
            geois salons, which would generate conversations closer to what symbolic interac-
            tionists call ‘perspective-taking’ than the mutually beneficial exchanges of
            information favoured by the Doctors.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For Descartes, Bacon and their Enlightenment followers, the ‘scientific method’
            functioned as a self-imposed discipline of lifelong learning, a generalisation and
            extension of the ‘arts of memory’ that checked the superstitious associations we are
            prone to make between the contingent and necessary features of knowledge, as epit-
            omised in our deference to clerical experts for reasons having more to do with the
            relatively exclusive means by which they have acquired their knowledge (i.e., the
            contingency of their professional training) than any proof that they possess the req-
            uisite competence to address a given problem. In this superstitious frame of mind,
            to give it a theological spin, we too quickly read signs of the eternal in the temporal.
            Thus, the clergy, God’s self-appointed representatives, end up being trusted as if they were the deity himself. Galileo’s reckless precursor, Giordano Bruno had been
            martyred in 1600 for insisting that everyone could realize their divine potential by
            comprehending their current beliefs as deductions from nature’s first principles,
            presumably as laid down by a creative deity indifferent to time, for whom the ‘order
            of knowing’ coincides with the ‘order of being’. Put bluntly, we might re-enact in
            our own minds how God unfolded Creation, and the coherence of that thought pro-
            cess – we would now say the successful execution of that simulation – would secure
            its validity (Yates, 1966). What eventually became the ‘scientific method’ – and
            later the Enlightenment ‘aude sapere’ and still later, and more prosaically, ‘Think
            for yourself!’ – effectively routinized Bruno’s dangerous charisma.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While Francis Bacon envisaged that a state-funded ‘House of Solomon’ would
            regularly perform experiments in the name of the scientific method, that would have
            been only the specialist wing of a mental regime that would ultimately set the stan-
            dard of right thinking in everyday life, as JS Mill’s canons of inductive proof have
            been sometimes treated (Lynch, 2001). In this respect, Bruno was the first ‘demo-
            cratic intellect’ who openly disputed authoritative opinion from a standpoint that he
            took to be available to everyone capable of mastering the arts of memory. His heroic
            status simply reflects the fact that there was hardly anyone quite like him in his day.
            Galileo at once learned from but kept his distance from Bruno, whom he had beaten
            for the chair in mathematics at the University of Padua. The closest Protestant
            equivalent to Bruno’s style of epistemic self-assertion was Michael Servetus, whom
            John Calvin had burned in Geneva about fifty years earlier. By the twentieth cen-
            tury, epistemologists had come to recast the distinction between the contingent (aka
            ‘psychological’) and necessary (aka ‘logical’) dimensions of inquiry in terms of the
            contexts of ‘discovery’ and ‘justification’, the latter functioning as the secular resi-
            due of the divine standpoint sought by Bruno, but without the administrative appa-
            ratus envisaged by Bacon (cf. Laudan, 1981: Chap. 11). While in the UK it is
            common to cite William Whewell as the key nineteenth century figure in this subli-
            mation of science’s divine aspirations, pivotal in Germany was Hermann Lotze, the
            physician-philosopher often credited with having canonised both the fact-value and
            the truth-validity distinctions that served to revolutionise methodology in the early
            twentieth century (Schnädelbach, 1984: 107; Heidelberger, 2004: Chap. 8).]]>
			</paragraph>
			<paragraph>
				<![CDATA[As the Scientific Revolution wore on, it became more widely accepted that a
            properly organized mind might know, if not everything, at least the limits of one’s
            knowledge, both empirically and conceptually. In this context, Kant’s ‘transcenden-
            tal’ method may be understood as an attempt to define philosophy as the second-
            order discipline dedicated to the pursuit of deviant interdisciplinarity – that is, the
            discipline tasked with providing unity and purpose to the other disciplines, which
            left to their own devices would pursue their own parochial knowledge interests to
            the exclusion of the others, and thereby fail to realize their synergistic potential.
            Kant (Kant, 1996) made this point most concretely in his late essay, The Conflict of
            the Faculties. This polemical essay fully modernised the Master’s standpoint as a
            call for university reform, one that inspired Wilhelm von Humboldt, in his capacity
            as Prussian Minister of Education, to re-dedicate the university as the environment
            for students to learn to think for themselves. In the meanwhile, the image of the autonomous thinker had received a Romantic makeover – less the heretical Bruno
            and more the polymathic Johann Wolfgang von Goethe – that is, someone who
            integrated several fields within himself to great effect, leaving an indelible trace on
            politics, science and art, but without turning himself into a martyr in the process.
            Indeed, the Humboldtian university was designed to generate just such individuals
            on a regular basis, each a unique integrator (a ‘genius’) of all that is worth knowing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Without denying the extent to which Humboldt’s vision was realized in nineteenth
            century Germany, philosophy itself fell into pronounced decline after Hegel’s death
            in 1830, as the discipline was seen as having slid into theology’s old dogmatic role
            of pre-emptively restricting the development of empirical knowledge in the name of
            ‘anticipating’ it (Collins, 1998: Chap. 12). Nevertheless, arguably the two most
            innovative sciences of the nineteenth century were attempts to operationalize the
            post-Kantian idealist take on the natural world, Naturphilosophie. These sciences,
            psychophysics and thermodynamics, were concerned with the conversion of mate-
            rial differences of degree to those of kind (i.e., ‘quantity into quality’ in Hegel-
            speak), or as Ernst Cassirer (1923) memorably put it, the conversion of ‘substance’
            into ‘function’. In this context, idealism’s Holy Grail, the ‘unity of subject and
            object’ was made experimentally tractable in terms of control over the physical
            conditions (the object pole) needed to alter sensory experience (the subject pole).
            Activities that in the eighteenth century would have been seen as tracking the ‘life
            force’ were thus increasingly subject to mathematical formulation as exchanges in
            ‘energy’, even as the latter term stood equivocally – and, after Einstein, unsuccess-
            fully – for both a phenomenological and a physical aspect of nature (Rabinbach,
            1990). The spirit of deviant interdisciplinarity travels most clearly after Hegel
            through this ‘energeticist’ tradition, which over the past two centuries has aimed for
            a ‘re-enchanted’ sense of science, which is to say, an epistemic state that is account-
            able to empirical reality yet in terms meaningful to the development of the human
            condition (Harrington, 1999: Chap. 1). Two of the most notable works in this vein
            from the mid-twentieth century were Koehler (1938) and Bertalanffy (1950).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The prospect of epistemic unity through translation principles was advanced
            from the 1850s to the 1950s – at first quite literally between forms of energy but
            increasingly between discourses subsumed under a common semantics (or ‘meta-
            language’). Whereas the neutral monist Hermann von Helmholtz focused on the
            human body itself as the ultimate transducer of caloric to psychic energy, the logical
            positivist Otto Neurath aspired to a kind of pan-disciplinary Esperanto through
            which the knowledge content of any specialised discourse could be communicated
            to any ordinary person (cf. Holton, 1993; Mendelsohn, 1974). In both cases epis-
            temic unity was presented as potentially resolving metaphysically inspired political differences, in effect updating Bacon’s juridical understanding of science alluded to
            earlier – indeed, now often extended to the cause of international diplomacy, a func-
            tion in which philosophers since Leibniz had dreamed of science serving (Berkowitz,
            2005; Schroeder-Gudehus, 1989). A key transitional figure between these two
            translation-based paths to unity is the last great defender of the universal energy
            principle, the chemist Wilhelm Ostwald, whose final (1921) edition of Annalen der
            Naturphilosophie included Wittgenstein’s Tractatus Logico-Philosophicus.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, these philosophical efforts to unify the sciences by finding a point
            neutral to their operation succeeded more at keeping increasingly disparate forms of
            inquiry under the umbrella term ‘science’ for purposes of public legitimation than
            in steering the conduct of inquiry. For the latter purpose, we may turn to two more
            interventionist ways of pursuing deviant interdisciplinarity, each turning on a dis-
            tinct image of the natural philosopher. She may be seen as either alchemist (i.e.,
            someone who could make the most with the least through skilful recombination of
            elements) or architect (i.e., someone who could design the conceptual blueprint for
            others to fill in empirically). In the balance hangs whether the quest for unified
            knowledge is interpreted as aiming for reduction or integration. Let us consider
            briefly the rather different ways in which ‘unity’ is conceptualised in the two cases.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Reduction refers to the relations among the objects of knowledge, whose ideal
            state involves minimising their possible interactions to achieve maximum effects.
            Thus, one is simultaneously trying to understand both the fundamental principles of
            matter – what the medieval alchemists called minima natura – and the prospects of
            their outworking. In contrast, integration is a process unique to the individuals
            whose idealist-style education equips them with a conceptual cartography of per-
            missible and preferred interdisciplinary traffic. What they make of this may lead to
            further social conflict but it will be among people who can be presumed to be
            knowledgeable of all that might be realized. At this point, politics takes over from
            science as the clash of worldviews. The specific differences in the disciplinary
            structure of the national university systems of Europe forged over the nineteenth
            century were largely institutionalised negotiated settlements of this sort (Merz, 1965).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Behind these two versions of deviant interdisciplinarity are alternative images of
            the deviant’s claims to ‘godlike genius’. Whereas integrationists aim to ‘imitate’
            God in the strict sense of retaining an ontological distance from the deity as they
            contest the best way to interpret the divine plan (i.e., their mental maps aspire to be
            a copy of the conceptual structure of the divine plan), reductionists would lay claim
            to occupy the divine standpoint for themselves, on the basis of which they would
            reconstruct (a possibly improved) nature from scratch, a ‘second creation’, to recall
            the phrase used equally for the twentieth century revolutions in nuclear physics and
            biotechnology (Crease & Mann, 1986; Wilmut et al., 2000).]]>
			</paragraph>
			<paragraph>
				<![CDATA[In terms of general approaches towards inquiry, reductionists are recognisable as
            practitioners of the Naturwissenschaften. integrationists of the Geisteswissen-
            schaften. We normally distinguish the two types of sciences in terms of their extreme
            cases – say, atomic physics and historical criticism. However, in what follows I
            focus on biology as the borderland discipline, or ‘liminal field’: Would biology be
            the ultimate synthetic molecular science, as revealed by the Naturwissenschaften or the foundational ‘science for life’ underpinning the Geisteswissenschaften? As the
            astute practitioner/historian of molecular biology Michel Morange (1998) has
            observed, this dual aspect of biology’s identity remains in the relatively independent
            research trajectories of, on the one hand, the mechanical sense of ‘life’ fostered by
            biotechnology and, on the other, the more holistic sense retained by modern evolu-
            tionary theory. The former is reductionist, the latter integrationist.]]>
			</paragraph>
			<paragraph>
				<![CDATA[On the one hand, the reductionist proposes to assist scientists to arrive at the
            foundational principles of nature that are responsible for everything, both as they
            are and as they could be. As suggested above, this project was associated with
            alchemy, which always existed in the interstices of what we would now call chem-
            istry and political economy (e.g., the conversion of lead to gold), which in turn
            explained the threat it posed to both secular and sacred authorities. But after Erwin
            Schrödinger’s (1955) famous 1943 Dublin lectures, ‘What Is Life?’, the alchemical
            ambition was decisively transferred to the interface of chemistry and biology, moti-
            vating physical scientists to fathom the structural-functional character of genes, the
            field that the Rockefeller Foundation itself had christened a decade earlier as
            ‘molecular biology’, which eventuated a decade after Dublin in the discovery of
            DNA’s double helix structure (Morange, 1998: Chaps. 7 and 8; Fuller, 2021b).]]>
			</paragraph>
			<paragraph>
				<![CDATA[From the DNA epiphany came three interdisciplinary styles of reductionism that
            persist to this day: (1) synthetic biology, closest in spirit to Schrödinger, which tests
            various molecular combinations for their genetic consequences, a project academi-
            cally first championed by Walter Gilbert (1991) and popularised by Craig Venter;
            (2) strategic chemical interventions to regulate gene expression, following the lead
            of François Jacob and Jacques Monod and now a mainstay of the pharmaceutical
            industry; (3) most ambitiously, the promoted by Eric Drexler (1986) under the name
            of ‘nanotechnology’, which would realize the alchemical dream of creating new
            beings from the fundamental re-organization of matter. This reductionist project has
            always had a spiritual side, harking back to what the historian of science Alastair
            Crombie (1994) called ‘maker’s knowledge’, the idea that motivated the hypothesis-
            testing style of scientific reasoning: namely, that as creatures ‘in the image and like-
            ness of God’, we are capable of divining the grammar of life so as someday to
            become capable of reverse engineering the process, at which time we might be able
            to improve, complete or even divert the course of creation – depending on the brand
            of heretical theology to which one subscribed.]]>
			</paragraph>
			<paragraph>
				<![CDATA[On the other hand, the integrationist projects the prospects for knowledge by
            imaginatively transcending the empirical limitations of the special disciplines. From
            this perspective, the emergence of qualitative or subjective difference may be seen
            as marking an increase in matter’s self-consciousness, again operationalised as
            increased powers of discrimination, culminating in the reflective philosopher-
            scientist as (in his/her person) the ultimate register of differences. Thus, Gustav
            Fechner (1801–1887), the most intellectually adventurous student of Schelling, the
            greatest of the original Naturphilosophen, spent his career trying to wed a mathe-
            matical psychophysics with a metaphysical panpsychism, as if Leibniz had under-
            gone a Romantic makeover (Heidelberger, 2004). Fechner had a vitalist view of the
            world according to which Newton’s inertial bodies and Goethe’s self-legislating 33
            individuals appear as polar states of matter-in-motion: The former constitute poten-
            tial yet to be realized at the level of both knowing and being (i.e., ‘dumb matter’),
            the latter potential fully realized at both levels (i.e., ‘enlightened genius’). Put
            bluntly, what Newton knew was radically distinct from his being, whereas what
            Goethe knew was integral to his being. This view attracted practitioners of the
            Geisteswissenschaften as the culmination of the entire scientific enterprise. Its end
            product would be not only a ‘science of life’ but also a ‘science for life’ (Veit-
            Brause, 2001). In their own distinctive ways, Ernst Mach, Charles Sanders Peirce
            and William James tried to follow Fechner’s lead. The same spirit also drove their
            contemporary, Friedrich Nietzsche, who was a student in Leipzig when Fechner
            held the chair in philosophy (Heidelberger, 2004: Chap. 7). But looming in the
            background of all these integrationist efforts was the spectre of the original evolu-
            tionist Jean-Baptiste Lamarck, whose reception has been all too typical of that of
            deviant interdisciplinarians.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Deviant interdisciplinarians can be hard to identify and trace in intellectual history
            because in the fullness of time much of their radical challenge comes to be accom-
            modated and/or distorted by mainstream disciplinary knowledge. From the stand-
            point of normal science historiography, the deviant interdisciplinarian can be made
            to look like someone who simply landed on the wrong side of such a wide range of
            debates that his or her seriousness and sanity may come into question. Such has
            been the fate of Jean-Baptiste Lamarck (1744–1829), the first curator of the inverte-
            brates section of the French Natural History Museum, who is normally credited
            with the first explicit theory of the evolution of life forms. The theory was proposed
            in the first decade of the nineteenth century as the cornerstone of a new discipline
            that he called ‘zoological philosophy’ or ‘biology’. The latter term stuck, though
            Lamarck’s original scope was much broader than that of today’s biological science.
            For Lamarck, biology aspired to what we would now call a ‘grand unified theory of
            everything’, where everything is presumed to have been endowed by the deity with
            a primitive life force that develops on its own accord (Packard, 1901: Chap. 19).]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the century prior to the Enlightenment, say, in figures such as Hobbes and
            Spinoza, the preferred term of art for this life force was conatus, which suggested
            that God’s will worked its way through otherwise unruly matter, issuing in some
            divinely sanctioned cosmic order (Fuller, 2012b). Deist tendencies in the
            Enlightenment detached God from any direct involvement in the process, resulting
            in a generic concept of besoin (‘need’), the term favoured by the French physio-
            cratic school of political economy in the generation before Lamarck’s to describe
            what two traders had to satisfy mutually in order to constitute a ‘just exchange’
            (Menudo, 2010). Lamarck added a further conceptual distinction: besoin vs. désir, or ‘need’ vs. ‘want’. The physiocrats, for whom the goal of political economy was
            a sustainable ecology, treated the two words interchangeably, whereas Lamarck
            clearly meant désir as a specification of besoin in terms of the particular sensory
            apparatus through which an organism finds that certain objects and environments
            enable it to flourish and develop (Bowler, 2005). While I may have no choice over
            the sorts of needs that are necessary for my survival (besoin), exactly how I satisfy
            those needs is an open question answers to which are provided by an account of my
            wants (désir).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lamarck’s deviant interdisciplinarian status is complicated by uncertainty over
            exactly how he would have had the disciplines integrated in aid of a master science
            of biology. Clearly, for Lamarck, what begins as the expression of divine will ends
            up as organic functionality, and in that respect, theology eventually yields to biol-
            ogy, corresponding to the increased specification and complexification of the life
            force over the course of natural history. But this process, while recognisably evolu-
            tionary, did not suggest any overarching image to its path. Nowadays it is common
            to represent Lamarck’s intentions in terms of a ‘convergent’ (as opposed to Darwin’s
            own quite clearly ‘divergent’) sense of the overall shape of natural history. That
            image is helped along by Lamarck’s view that the continued existence of relatively
            simple organisms marks them not as atavisms but as the latest spontaneously gener-
            ated life forms, all destined to ascend what Gillispie (1958) dubbed the ‘escalator of
            being’, whereby life re-incorporates or re-organizes itself as it learns to exert greater
            control over its environment. At the same time, Lamarck regarded freestanding
            chemical substances of the sort that his compatriot Antoine Lavoisier had begun to
            call ‘elements’ in a manner much closer to that of the last great phlogiston theorist,
            Joseph Priestley – namely, as the fossilised residues of the spent life force. In short,
            Lamarck inhabited an intellectual universe that was almost inside out from our own.
            Nevertheless, to his more ambitious later admirers, typically heretical Christians
            like Pierre Teilhard de Chardin (1955), Lamarck’s theory implied nothing less than
            the history of God’s self-realization in the world through a suitably ‘escalated’ (or
            ‘enhanced’, as we would now say) version of Homo sapiens.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lamarck’s project did not prevail, perhaps unsurprisingly, for a host of reasons
            ranging from charges of empirical disconfirmation and conceptual confusion to out-
            right theological anathema. In his own day, Lamarck’s conception of life went
            against the relatively static notions of health and illness associated with biology’s
            closest disciplinary rival, medicine. He did not begin his inquiries with a specimen
            of an existing healthy organism but with some imagined (divine?) origin from which
            the organism was hypothesized to have evolved. (The word ‘organism’ is ambigu-
            ous between individual and species because what might be ordinarily called a ‘spe-
            cies’ was for Lamarck an extended phase in the generic life process.) Indeed,
            Lamarck famously asked his students to imagine what might have been the original
            complex of needs that came to be realized in the succession of forms taken by an
            organism in a given environment (Gillispie, 1958). In that case, norms of health are
            to be found not in an organism’s statistically normal behaviours but in the vector
            revealed by examining the long-term history of its behaviours in a common environ-
            ment. Although no less than the great positivist Auguste Comte was impressed by these insights during his medical studies at Montpélier, and arguably based the
            modus operandi of his own historical epistemology on them, Lamarck’s reception
            had fallen foul of priests and positivists alike by the second half of the nineteenth
            century (Burkhardt, 1970). After all, Lamarck was effectively claiming that organ-
            isms contain powers that come to the fore only on a ‘need-to-respond’ basis that are
            then inherited by their offspring, making it then very difficult – if not impossible –
            draw a sharp distinction between normal and abnormal behaviour, given that today’s
            abnormality may anchor a new norm for future generations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Moreover, once Darwin’s theory of evolution by natural selection came on the
            scene in the mid-nineteenth century, Lamarck’s overarching vision was distilled
            into a point of empirical disagreement with Darwin over the nature of hereditary
            transmission. Lamarck was presented as the champion of what is nowadays known
            as the ‘inheritance of acquired traits’, whereby changes to an organism’s body can
            be passed to the organism’s offspring. This thesis is normally seen as having been
            discredited experimentally by the work of August Weismann, who in 1889 showed
            that cutting off the tails of twenty generations of mice had no effect on the length of
            the tails of subsequent generations. On this basis, the famed ‘Weismann barrier’,
            i.e., the impassable wall separating the somatic and the germ cell line became a
            central dogma of Darwinian evolution – albeit several decades before the genetic
            nature of the germ cell line was properly understood (Meloni, 2016). This last point
            is significant because greater genetic knowledge has actually provided potential
            opportunities for re-asserting the by-now stereotyped Lamarckian doctrine of the
            inheritance of acquired traits. Nevertheless, the Weismann barrier is still routinely
            brandished in high school biology textbooks as the silver crucifix designed to ward
            off any remaining Lamarckian vampires.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Yet, Lamarck may be vindicated in the end. The Weismann barrier appears to
            apply only for the period of evolutionary history that began with the emergence of
            organisms with a hard cell membrane (i.e., eukaryotes) and has begun to end with
            the cracking of the genetic code (Dyson, 2007). Between these two moments, the
            dominant means of transmitting genetic information has been ‘vertical’, that is,
            through lines of descent. But both before and after that period in evolutionary his-
            tory, more Lamarck-friendly ‘horizontal gene transfer’ may be dominant – facili-
            tated in the ancient case by porous cell membranes (something regularly evidenced
            in the spread of viruses), in the contemporary case by targeted biotechnological
            interventions. But in both cases, a change to the organism in its lifetime leaves a
            clear trace in the offspring, which may be itself enhanced or reversed in subsequent
            horizontal transfers of genetic information. Interestingly, this hypothesis – epito-
            mised in the slogan ‘Evolution itself had to evolve’ – was inspired by a microbiolo-
            gist, Carl Woese, whose origins in biophysics and repeated run-ins with Darwinian
            taxonomists marks his own career as that of a deviant interdisciplinarian.
            Nevertheless, Woese eventually succeeded in establishing the existence of a form of
            life more primitive than any previously acknowledged, now gathered in the king-
            dom of Archaea, whose character is reminiscent of Lamarck’s take on spontaneous
            generation (Sapp, 2008). Viewed in the broadest theoretical terms, the Darwinian
            definition of evolution as ‘common descent with modification’ may come to be seen as limited as Newtonian mechanics is in physics today. In other words, Darwin’s
            and Newton’s theories cover their respective target realities at the meso-level but not
            at the extremes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Various other Neo-Lamarckian revivals were attempted in the twentieth century –
            and continue to be mounted today. In the mid-twentieth century, some geneticists –
            not least a principal craftsman of the Neo-Darwinian synthesis (and Teilhard de
            Chardin enthusiast), Theodosius Dobzhansky – saw irradiated genes as a potential
            evolutionary accelerator that could produce Lamarckian effects by Darwinian
            means. However, the pioneer of radiation genetics, Hermann Muller, warned against
            this wishful thinking in the impending Nuclear Age as more likely to result in mal-
            adaptive offspring (Ruse, 1999: 109–110). A more promising offspring of the same
            period is ‘epigenetics’, a term coined by the animal geneticist Conrad Waddington
            to capture the fact that at least some genes are not inherited in their final form but
            are shaped in gestation and even early childhood (e.g., by diet or stress levels) in
            ways that can then be transmitted to offspring. Waddington saw the potential in the
            concept for explaining and remedying the tendency for class distinctions to biologi-
            cal ones over successive generations of, say, poor nutrition (Dickens, 2000: Chap.
            6). Of course, epigenetics in this strict sense is not classically Lamarckian in that the
            organism’s intentionality is not involved. However, Neo-Freudian theorists quickly
            seized upon the idea in a spirit that may have been truer to Lamarck’s (e.g.,
            Erikson, 1968).]]>
			</paragraph>
			<paragraph>
				<![CDATA[While many lessons may be drawn from the treatment that both Lamarck and
            Lamarckism have suffered at the hands of history, let me conclude by highlighting
            issues that are of more general relevance to the pursuit of deviant interdisciplinarity.
            First, in the spirit of the deviant enterprise, Lamarck made a point of calling his
            epistemic practice ‘zoological philosophy’, implying that he was aiming for some-
            thing much more metaphysically ambitious and normatively freighted than, say,
            Darwin, who presented himself as a natural historian not a natural philosopher.
            (Keep in mind that our sense of ‘scientist’ was hardly available to either of them.)
            One telling criticism of Lamarck, typically attributed to the early US developmental
            psychologist James Mark Baldwin and popularised by the anthropologist Gregory
            Bateson (1979), bears on the deviant interdisciplinary project as such – namely, a
            tendency to conflate first- and second-order perspectives on reality (Richards, 1987:
            Chap. 10). In particular, Lamarck presumed that if a shift in the distribution of traits
            in a population over several generations brought about adaptive improvement, then
            it was something that the individuals involved had been trying to achieve. Put that
            way, Lamarck appears to have fallen victim to the fallacy of division, as he seemed
            to presume that something present in the whole was also present in the parts.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, if the role of historical order in epistemic progress is taken more
            seriously, Lamarck may have effectively attributed an emergent second-order
            awareness to members of a population once they learned the consequences of what
            might well have been originally fortuitous adjustments. Thus, the insiders come to
            incorporate the standpoint of the outsider to accelerate their progress, as they render
            intentional what had been previously unintentional. (To speak in purely logical
            terms, the ‘Lamarckian trick’ may be characterised as rendering the ‘extensional’ in ‘intensional’ terms– that is, what is presented as a property common to the aggre-
            gate turns out to have been a property of each of the constitutive individuals.)
            Nowadays, after Bertalanffy (1950), this is called the ‘systems-level’ perspective,
            arguably the secular residue of divine creativity. At least that is one way to gloss the
            idea raised by Bertalanffy and others who have gone down the deviant interdiscipli-
            narity route in the recent past, namely, that humans, as the only animals without a
            natural habitat, are compelled to turn everywhere into its home. And the first step
            involves learning to adopt the ‘view from nowhere’, whereby every seemingly iso-
            lated event is understood as a means to a larger end.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I have presented a rather polarised fate for philosophy in a multi-disciplinary world:
            either an underlabourer for the various disciplines that have developed from its root
            (the ‘normal’ way) or an overlord who treats such variation as itself a problem for
            which philosophy provides the disciplinary solution (the ‘deviant’ way). The insti-
            tutional history of knowledge production over the past two centuries has tended
            against the latter position, which I nonetheless champion. In that time, pretenders to
            the role of philosophical overlord have been attracted to a rather general sense of
            ‘biology’ from which the discipline bearing that name has increasingly distanced
            itself, a fact most pointedly illustrated by the chequered reputation of Jean-Baptiste
            Lamarck.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I have omitted the prospect that philosophy might inform the other disciplines as
            an equal operating on a level playing field. It might even ‘go viral’ in the spirit of
            horizontal gene transfer, which is suited to the flattening of epistemic hierarchies
            brought about the internet in our post-truth times (Fuller, 2018a, 2020a). One might
            think of this as adding a missing ethical ingredient (Tuana, 2013) or providing a
            facilitating service (O’Rourke & Crowley, 2013). In both cases the point would be
            that philosophy enables disciplines to become truly ‘interdisciplinary’, in the sense
            of collaborating on a common epistemic project that would be otherwise unachiev-
            able in their individual capacities. However, this idea of philosophy as a discipline
            on par with other disciplines has been always justified more on institutional than
            intellectual grounds, the unsatisfactory consequences of which are routinely enacted
            in the so-called ‘analytic-continental’ divide in Anglophone philosophy, which may
            be seen as reproducing within the discipline philosophy an orthogonal version of the
            disciplinary distinctions that exist at large. In this respect, a maxim typically attrib-
            uted to Quine rings true: People study philosophy out of interest in either the history
            of its doctrines or the logic of its arguments – the former veering to hermeneutics,
            the latter mathematics. Yet somehow the ‘Department of Philosophy’ needs to
            accommodate both interests. In that context, which presumes the institutional stand-
            ing of philosophy as a discipline, the ‘interdisciplinary turn’ may be seen at the same time as an outward-looking turning away from frustrating internecine disputes
            within the discipline of philosophy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, there is a more positive way out. It involves taking a page from the
            Bildungsroman of William James, now normally seen as the first ‘professional’ US
            philosopher – which is to say, someone ensconced in an academic chair, and not a
            popular lay preacher à la Ralph Waldo Emerson or a rogue scientist à la Charles
            Sanders Peirce. Like many nineteenth-century secular middle-class people seeking
            a career that provided for both spiritual fulfilment and material security, James orig-
            inally studied medicine. But out of dissatisfaction with the curriculum, he travelled
            to Germany where he witnessed the interesting cross-disciplinary hybrids emerging
            in the wake of the institutional meltdown of idealist philosophy – most importantly
            ‘psychology’. This left James with a metaphysical worldview, called ‘neutral
            monism’, which enabled him to see the various academic disciplines as simply
            alternative ways of organizing experience, which through routinisation solidified
            into habits of thought. However, these habits could easily generate neuroses that
            would render the trained disciplinarians socially dysfunctional outside of peer-
            oriented contexts. Into this context the ‘philosopher’ would step to break or prevent
            such habits of thought.]]>
			</paragraph>
			<paragraph>
				<![CDATA[James took this idea quite literally, leading him to oppose the construction of a
            dedicated building for philosophy on the Harvard campus, what became Emerson
            Hall (Bordogna, 2008). James believed that the establishment such a site would
            inhibit philosophers in their nomadic (not peripatetic!) function, which indeed it did
            over the years. But interestingly, James did not think of philosophy’s role as merely
            an auxiliary ‘research support service’. Rather, his understanding of how specific
            disciplinary patterns of thoughts should be broken was informed by an overarching
            sense of the self-fulfilled person as a ‘rugged individualist’, a phrase popularised by
            Theodore Roosevelt, which led James to see philosophy as an especially verbally
            aggressive form of therapy. In this respect, James never really rejected the deviant
            interdisciplinary way of the medieval Masters or the German idealists but tried to
            adapt it to the early twentieth century American context. Perhaps he failed but that
            should not stop us from trying to do something comparable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Abstract The chapter begins with a discussion of academic freedom as ‘positive
            liberty’, which is a form of mutual recognition whereby other people provide the
            conditions for one’s own freedom. Academic freedom – and the corresponding fac-
            ulty of judgement – is a specific institutionalized version of positive liberty. In the
            original Humboldtian context, it served as an exemplar for overall social emancipa-
            tion. However, the exercise of academic freedom in the nineteenth and twentieth
            centuries was a mixed bag, resulting in much dynamism and innovation but also
            controversy and social disorder. The role of ‘bureaucracy’ (a term popularized by
            Max Weber) is subsequently discussed as a potentially facilitative second order
            form of judgement, though it has arguably contributed to a version of academic
            expertise that threatens to undermine the comprehensive and person-defining sense
            of ‘judgement’ promoted by Humboldt. This topic is explored especially in relation
            to developments in Germany and the United States, in both philosophy and psychol-
            ogy. The chapter ends with a genealogy of ‘judgement’ as the site of values-based
            reasoning, starting with the Stoic logical formalization of reasoning in the Athenian
            courts and extending to religious and secular settings in the modern period.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Academic freedom refers to the complementary rights and obligations that bind
            teachers and students together as ‘free inquirers’ in the broadest sense. In the classic
            Humboldtian sense, which Max Weber refashioned for the twentieth century, stu-
            dents would be free to inquire into the life they might lead, while their teachers
            would be free to inquire into the life (or ‘vocation’) that they have chosen for them-
            selves (Fuller, 2009: Chap. 1). In both cases, academic freedom has been histori-
            cally associated with a distinctive sense of judgement as a mental faculty. The fates
            of ‘academic freedom’ and ‘judgement’ as concepts rise and fall together. The roots
            of this sense of ‘judgement’ can be found in the judicial practices of ancient Athens
            and modern Europe, Stoic philosophy and heretical Christendom, as well as the aesthetic origins of German idealism and formal logic’s emergence from normative
            psychology. The institutional preconditions for academic freedom were laid down
            in the late Roman legal classification of universities with guilds, churches, monastic
            orders and city-states as indefinitely self-reproducing, self-governing social entities
            dedicated to activities whose value transcends the interests of their current practitio-
            ners (Fuller, 2000a: Chap. 4). Even in the thirteenth century the idea was controver-
            sial because members of such corporate bodies would be largely immune from laws
            to which they would have been normally subject as individual family members or
            royal subjects.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The level of controversy was raised further when academic freedom was made
            the cornerstone of the university in the modern nation-state by Wilhelm von
            Humboldt in 1810. As I shall elaborate shortly, this move enabled theology in the
            German states to be pursued as Wissenschaft – systematic knowledge for its own
            sake – without specifically pastoral concerns. The distinction is routinely enshrined
            today in the difference between a largely secular religious studies department in the
            arts or social sciences faculty and a free-standing divinity school that provides pro-
            fessional training for clerics of particular faiths. However, in its day, the distinction
            quickly became a source of divisiveness and ultimately civil revolt, the legacy of
            which is arguably felt in the ongoing ‘culture wars’. A flavour of the discontent may
            be gleaned by Karl Marx’s early preoccupation with Ludwig Feuerbach, a theolo-
            gian who migrated to the natural sciences to produce the cornerstone work in the
            anthropology of religious belief and ritual, The Essence of Christianity (1841). This
            general academic tendency toward what was often called naturalism (with a nod to
            Spinoza and Schelling) and materialism (with a nod to the French Enlightenment
            followers of Epicurus) fuelled the wave of ‘liberal’ revolutions in 1848 that set
            down a clear marker for the separation of Church and State as a general political
            principle – that is, beyond the separation that had already occurred in the German
            theology faculties.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is worth recalling the original context for Humboldt’s work, which both John
            Stuart Mill and Karl Popper later cited as formative in their own thinking. Nearly
            two decades before founding the University of Berlin, while still in his twenties,
            Humboldt published a defence of liberalism, The Limits of State Action in 1792
            (Humboldt, 1969). It was a Kant-inflected response to Friedrich Wilhelm II, who
            four years earlier had made Lutheranism the state church of Prussia, shortly after
            ascending to the throne on the death of his uncle Friedrich II (aka ‘Frederick the
            Great’), whose famed patronage of the Enlightenment included a defence of reli-
            gious diversity as a strategic nation-building function. More generally, Friedrich
            Wilhelm’s establishment of a state church contravened the separation of church and
            state that many Protestants – especially the ‘dissenters’ – had argued was essential
            to the sort of post-Catholic Christendom that had turned the Enlightenment into a
            full-blown European cultural movement in the eighteenth century. In this respect,
            such ‘universalising’ ideas as treaty-based international law, cosmopolitanism and
            even the ‘republic of letters’ were attempts to find secular replacements for various
            Catholic natural law-based doctrines.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Against this backdrop, Friedrich Wilhelm’s actions suggested to Humboldt and
            other thinkers inspired by Kant’s ‘What Is Enlightenment?’ that the state was trying
            to replace the role of the church in expressing the will of the people – instead of
            devolving that role to a generalised freedom of individual conscience, as Spinoza,
            John Locke and John Toland had originally argued (Israel, 2001). In retrospect, we
            might say that Humboldt and his allies were concerned that a state-church merger
            would eventually convert religious authoritarianism into political totalitarianism.
            That Friedrich Wilhelm’s son subsequently consolidated all the Protestants under
            state control, rendering himself head of a United Church, confirmed this ideological
            direction of travel. Arguably Hitler’s ascendancy in the face of the established cler-
            gy’s relative quiescence realized even more strongly the fears that Humboldtians
            had been expressing a century earlier.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nevertheless, the seeds of a countermovement to Friedrich Wilhelm’s centralis-
            ing tendency had been already sown in Humboldt’s day – namely, the division of the
            theology faculty into scientific (aka ‘critical-historical’) and pastoral duties (Collins,
            1998: Chap. 12). This distinction would provide an institutional microcosm for Max
            Weber’s (2004) landmark thinking about the radical separateness of science and
            politics. The distinction was originally drawn by Friedrich Schleiermacher, the
            founder of modern hermeneutics, who happened to be one of the main supporters of
            the consolidation of the Christian churches under the Prussian monarchy. Yet, at the
            same time he insisted that theology performed two functions that were best per-
            formed separately. The pastoral mission is about kindling the religious impulse,
            which Schleiermacher saw in terms of our personal sense of finitude generating a
            desire to remerge with an infinite being from which we came and have since then
            been separated (via Original Sin, in the Christian context). In contrast, theology’s
            scientific mission is no different from the scientific mission of any other discipline,
            which in this case is to study the conditions under which Christian message has
            been discovered and developed, with an eye to distinguishing what is essential and
            inessential to the message’s continued promotion. While Schleiermacher himself
            believed that the two branches of theology worked symbiotically, in practice the
            scientific side – very much inspired by Enlightenment rationalism – produced an
            increasingly stripped down (‘demythologised’) version of Christianity that called
            into question the institutional legitimacy the ‘church’ and even the divinity of Jesus
            himself.]]>
			</paragraph>
			<paragraph>
				<![CDATA[An unintended consequence of Schleiermacher’s division was that the two sides
            of theology worked against rather than with each other, at least from the standpoint
            of the Prussian monarchy. To cut a long story short, this dissonance reverberated
            throughout post-Napoleonic Europe, where Christian churches were increasingly
            forced into specific political arrangements with various secular rulers. It culminated
            in the failed 1848 liberal revolutions, out of the ashes of which emerged The
            Communist Manifesto, the path to which Marx and Engels had charted two years
            earlier in the posthumously published The German Ideology. The English transla-
            tion of this work in the 1960s is normally credited with having launched the ‘human-
            ist’ Marx, the prequel to the better-known economic determinist version that had
            dominated twentieth century Marxist interpretation. The work itself is a series of critical reflections on the leading scientific theologians of the day, the so-called
            ‘Young Hegelians’, who were accused of sowing the seeds of political dissent with-
            out considering how to channel that dissent into something that might result in a
            productive radical transformation of society. As Marx and Engels saw it, these ‘lib-
            eral theologians’, the most philosophically consequential of whom was Ludwig
            Feuerbach, either lacked the courage of their convictions or simply erred in suppos-
            ing that ‘free’ people would simply spontaneously self-organize into a new
            social order.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lest we forget, Humboldt’s original liberal vision predates all the above.
            Moreover, Humboldt was envisaging a more ambitious sense of the ‘devolution’ of
            authority than simply the freedom of conscience that the two subsequent genera-
            tions of Prussian monarchs institutionally suppressed. The Humboldtian vision is
            epitomized in the resonant phrase that Marx and Engels adapted from the idealist
            philosopher Fichte, the person whom Humboldt appointed as first Rector of the
            University of Berlin: the withering away of the state (Kriegel, 1995: Chap. 8). The
            sort of state that Humboldt and Fichte envisaged as withering away was the pater-
            nalistic state, including the ‘benevolent despotism’ of the likes of Frederick the
            Great that had enabled the Enlightenment’s brand of liberalism to flourish in the first
            place. To be sure, as the metaphor of withering suggests, the process would take
            some time but in the long term it would even overtake ‘parliamentary democracy’,
            understood as a system where the will of the people is not directly expressed but
            rather is mediated by elected representatives who largely decide matters by cutting
            deals amongst themselves, which may benefit their own interests than those of their
            constituents. Instead, the people would be collectively sovereign in the sense of an
            absolute monarch, Rousseau-style, whose decisions would then be implemented by
            a dedicated civil service.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Humboldt held that those who govern best, govern least, in the sense that the
            governors allow the governed to act in accordance with their will, which includes
            allowing them to bear the consequences of those actions. It was against this back-
            drop that John Stuart Mill popularised the word ‘responsible’ in the mid-nineteenth
            century to refer to one’s recognition and acceptance of how they are being held to
            account (McKeon, 1957). There are many ways to envisage a society organized on
            such a principle. For their part, Humboldt and Fichte appealed to Kant’s ‘Kingdom
            of Ends’, a society whose members treat each other not as means to their own ends
            but as ends in themselves. The key to this idea is that in order to treat others as ends
            in themselves, one must enable them to pursue their ends, responsibility for which
            they can then fully take. This means in practice that as one pursues one’s own ends,
            one must become a means to another’s ends. The result looks not so different from
            the sort of ‘commercial’ society envisaged by Adam Smith (McCloskey, 2006). If
            that idea seems counterintuitive, it is only because we underestimate the educative
            role of markets, something that was stressed by Smith’s French counterpart in the
            promotion of commercial society, the Marquis de Condorcet (Rothschild, 2001).
            For Condorcet, the prospect of multiple producers and consumers flourishing in a
            market with low or no entry costs forces producers to be more sensitive to consumer
            needs and consumers to be more discriminating about producers’ claims about their products (Fuller, 2006c). In effect, I learn more about who I am by being forced to
            discriminate between options. A market provides the range of opportunities neces-
            sary for that learning experience, based on which everyone can take a more effective
            sense of self-ownership precisely by engaging in mutual recognition.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Such a society would be arguably liberalism on steroids, especially when com-
            pared with the view of John Locke, who defined the just society in terms of the
            maximum jointly realizable level of freedom of its members. Whereas Locke had in
            mind the mutual non-interference of society’s members, Humboldt and Fichte
            thought in terms of the mutual facilitation of members. In Locke’s society, you have
            a right to be left alone; in Humboldt and Fichte’s society you have a duty to be rec-
            ognized. For Humboldt and Fichte, the state would be more proactive than simply
            the permanent protector of individual liberty, the view that Locke had inherited
            from Hobbes and adapted for his own purposes. Indeed, for these late German
            Enlightenment thinkers, the state would be the outright enabler of individual liberty,
            whose long-term success would be judged by its unobtrusiveness; hence the state’s
            ‘withering away’. The idea is that after a while, an appropriately educated (‘enlight-
            ened’) people would not need to be told what is in their best interest, let alone be
            forced to do something that they do not want to do. They would achieve the Kantian
            ideal of freely doing what they ought to do. The state would then be in the business
            of periodically offering a range of choices to the citizenry based on which they can
            act in consort by delivering a collective vote. At first, the contrast implied here looks
            like Isaiah Berlin’s (1958) famous distinction between ‘negative’ and ‘positive’ lib-
            erty. For Berlin, positive liberty is largely about people coming to realize their
            objective potential, in the sense of the old Stoic aphorism, ‘Freedom is the recogni-
            tion of necessity’. But based on the political experience of the first half of the twen-
            tieth century, Berlin saw positive liberty as capable of legitimizing authoritarian or
            even totalitarian regimes of the sort associated with ‘Big Brother’ in George
            Orwell’s 1984.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Notwithstanding these more authoritarian implications of positive liberty, espe-
            cially in twentieth century politics, it is striking how little the actual discourse of
            German idealism bears out this interpretation. Truer to its spirit of positive liberty is
            what the original neoliberals dubbed ‘liberal interventionism’, which positioned the
            state – just as the early twentieth century US Progressives had – as the ‘trust- busters’
            who unblock the flow of capital and labour to ensure their free association in the
            market, which the German idealists held to be the crucible of what they called ‘civil
            society’ (Jackson, 2009). In academia, the labour-capital nexus pertains specifically
            to knowledge. Indeed, this is how to think about Friedrich Althoff’s radical reforms
            to the German university system while Bismarck’s Minister of Higher Education,
            which produced the world’s most dynamic academic culture by the first decade of
            the twentieth century (Fuller, 2016a: 88–93).]]>
			</paragraph>
			<paragraph>
				<![CDATA[One lesson that Althoff learned from the unfolding of the Humboldtian univer-
            sity in the nineteenth century was that academics themselves are not necessarily the
            most reliable guardians of academic freedom. The temptation for professors to
            indoctrinate, if not outright anoint, the successors to their chairs, matched by a will-
            ingness of their students to succumb to such treatment in order to secure and enhance professional status, always threatened to create bottlenecks in the flow of knowl-
            edge. These bottlenecks, which continue to travel under such names as ‘disciplinary
            traditions’ and ‘schools of thought’, already struck Althoff as all too redolent of
            medievalism. Althoff’s pointed critique of ‘Humboldt in practice’ perhaps reflected
            his own failure to complete the doctorate and his subsequent refusal to accept a
            professorial chair. (A similar trajectory applies to the principal of a US venture capi-
            tal firm that nowadays invests in bright students who forgo university to become
            innovators [Gibson, 2022].) Althoff’s solution was to decide all professorial appoint-
            ments himself, in the spirit of a connoisseur, drawing on a largely confidential net-
            work of international advisors. The resulting decisions often went against local
            wishes, forcing ambitious academics to cash in whatever cultural capital they had
            amassed at their home institutions to play in Althoff’s national lottery
            (Backhaus, 1993).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Althoff’s younger contemporary Max Weber regarded the ‘Althoff System’ as
            simply a government attempt to abrogate the university’s historic right to self-
            determination – the naked exercise of power by a ‘bureaucracy’, a term Weber him-
            self popularized (Shils, 1974). But with the benefit of hindsight, it has become clear
            that Althoff’s policies resulted in an unprecedented cross-fertilization of ideas and
            approaches that enriched the emerging special sciences and provided a nationwide
            conception of intellectual competition that raised academic standards across institu-
            tions, which in turn set a standard that was emulated by emerging world powers, not
            least the United States (Fuller, 2016a: Chap. 3). Indeed, Althoff’s exercise of minis-
            terial power is best seen as an instance of the liberal interventionist state in action.
            Perhaps Weber, and later Isaiah Berlin, failed to make the connection between posi-
            tive liberty and liberal interventionism because they missed a crucial assumption of
            the original German idealists – namely, the fallibility of the agents who would be
            treated as ends in themselves: Even at their most calculating and instrumental,
            agents cannot anticipate the full consequences of their actions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this respect, to maximize freedom is by no means to eliminate error. Georg
            Lukacs (1948) drew attention to Hegel’s early reliance on Adam Smith’s ‘invisible
            hand’ as the source for his own (and Marx’s) ‘cunning of reason’. Informing this
            line of thinking – also shared by Kant, Humboldt and Fichte – was the idea that
            agents who engage in mutual facilitation, be it understood in terms of a ‘Kingdom
            of Ends’, a ‘civil society’ or simply a ‘market’, unwittingly generate consequences
            that go beyond the sum of their original ends. Indeed, the consequences may ulti-
            mately overwhelm those ends. Lukacs saw Marxism as positioned to be the long-
            term beneficiary of such an outcome in the case of capitalist civil society. However,
            a liberal interventionist state might equally benefit in the spirit of what the great late
            twentieth-century social science methodologist Donald Campbell (1988) called the
            ‘experimenting society’, namely, a state that treats society as a living laboratory,
            replete with feedback loops to allow for collective learning over time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the end, ‘positive liberty’ is best understood as Humboldt and Fichte having fol-
            lowed in Kant’s footsteps to reinvent classical republicanism in modern guise – but
            this time on a much larger scale and allowing for more individuality. Citizens in past
            republics normally represented themselves, but citizenship carried high entry costs,
            typically tied to disposable ‘wealth’ in a sense that approaches the modern sense of
            capital. Possessing a material stake in the future of your society – ‘skin in the game’,
            as the risk theorist Nassim Taleb (2018) would say – made you worth listening to in
            the first place, which then enabled the audience to discount anything seemingly self-
            serving in what the speaker says, while continuing to believe that they’re trying to
            speak on behalf of the collective. The great Enlightenment defender of republican-
            ism, Jean-Jacques Rousseau, believed on historical grounds that republics had to be
            small and populated by rough equals to maintain the value consensus needed for
            majority rule to be accepted as binding on everyone. Rousseau’s guiding intuition,
            which informed Kant’s formulation of the ‘Kingdom of Ends’, was that our antago-
            nists are sufficiently like us that we can imagine them as alternative – and occasion-
            ally better – versions of ourselves, which would then make it rational to abide by
            their collective judgement. In short, the cultivation of judgement requires learning
            how to both deliver and receive judgement.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Had Rousseau not died in 1778, he might have supported the American and
            French Revolutions as blows against tyranny in the name of self-governance, while
            disowning their constitutional aftermaths, mainly for pretending to do too much
            because the differences between people – in both socio-economic and spatial dis-
            tance – were too great to command a spontaneous unity of will. In contrast, Kant,
            Humboldt and Fichte managed to live through both the American and French
            Revolutions to completion, which they took to mark a step-change in history –
            namely, that republicanism could indeed be writ large in an increasingly liberal
            direction. Notwithstanding its disappointing results, the 1848 revolutions provided
            additional ballast to an idea behind the 1789 French Revolution, namely, that the
            nation-state replaces God as the object of faith in secular society, with ‘patriotism’
            supplanting the ‘paternalism’ of the Christian deity. This helps to explain the strate-
            gic longing that Germany developed toward the United States in the nineteenth
            century as the prospective vehicle for the rational unfolding of universal freedom,
            what Hegel dubbed the ‘world-historic spirit’. It is worth recalling some of the key
            moments of this relationship.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A convenient starting point for tracing this sentiment is the émigré political econ-
            omist Friedrich List’s application of Fichte and Schelling to understand the
            Jacksonian era, circa 1830 – which was also when Alexis Tocqueville visited
            America. The period marked the first time that the US was governed by people out-
            side of the founding generation – or their offspring, notably John Quincy Adams,
            whom Andrew Jackson defeated for the presidency in 1828. List contributed a
            vision of economic protectionism that would bring the states closer together in the ‘national interest’, understood at once as domestic and foreign policy. The vision
            was galvanized in the wake of the War of 1812, when the brief British occupation of
            Washington DC demonstrated that even after nearly two generations of formal inde-
            pendence, the US could not take its national sovereignty for granted. And while
            Jackson’s implementation of this vision expedited overall economic growth, it also
            exacerbated the nation’s internal social differences (I.e., the factory-based North v.
            the plantation-based South), which eventually led to Civil War. But that didn’t stop
            List’s vision being reimported to Germany as the Zollverein, the customs union that
            provided the basis for Bismarck’s unification of Germany in 1870.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The German-US connection deepened when Ernst Kapp migrated to Texas to
            support the Union in the Civil War. He was captivated by Ralph Waldo Emerson,
            who had given German idealism an American makeover as ‘Transcendentalism’, a
            philosophy of self-empowerment that was expressed in humanity’s harnessing of
            the powers of nature for its own ends. Indeed, over a century later, Marshall
            McLuhan simply replaced Emerson’s telegraph with television when he declared
            that ‘media’ (understood as extended sense organs) are ‘the extensions of man’.
            Thus, Kapp founded what we now call the ‘philosophy of technology’ (Mitcham,
            1994: Chap. 1).]]>
			</paragraph>
			<paragraph>
				<![CDATA[But of greater institutional import was the American adoption of the post-
            Humboldtian ‘graduate school’ model of education of the Bismarckian era, follow-
            ing considerable late nineteenth century scholarly exchange between the US and
            Germany. One especially astute observer of this general development was Woodrow
            Wilson, himself an early graduate of the ‘made for export’ German-style Johns
            Hopkins University – and to this day, the only US President to hold a PhD. The
            young academic Wilson, a founder of political science as a discipline in the US,
            argued for a Humboldt-style civil service to execute the decisions taken by the pres-
            idency, as informed by the deliberations of Congress, which would be reduced to a
            glorified talking shop rather than a proper legislative body. Like the UK Fabian
            Society at the time, Wilson regarded a professional civil service as a bulwark against
            the potentially capricious character of judgements taken in parliamentary democra-
            cies (Hilderbrand, 2013).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Of course, Wilson and the Fabians were not alone in their suspicions of parlia-
            ments. Notoriously, Carl Schmitt (1988), preferred to address the problem more
            directly by having the state embodied in a charismatic leader, who says and does
            what everyone has been already thinking and wanting. (Weber had also introduced
            ‘charisma’ into the sociological lexicon [Baehr, 2008].) To be sure, such a literal
            ‘dictator’ would forge exactly the link between positive liberty and authoritarianism
            that Isaiah Berlin feared. To be sure, Wilson’s Progressive predecessor Theodore
            Roosevelt had many of the charismatic qualities that aspiring dictators in the
            Schmittian mould – not least Donald Trump – have subsequently emulated. The
            point is made in Hawley (2008), written by a law student who subsequently became
            a Trump loyalist in the US Senate.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For his own part, Wilson took a more ‘behind the scenes’ approach, whereby the
            state accrues power by the civil service becoming a superordinate administrative
            class, a ‘bureaucracy’ in Weber’s dreaded sense. In Wilson’s day, it was symbolized by the introduction of a national income tax, which was intended for interstate infra-
            structure but was soon redeployed to finance US involvement in the First World
            War, even though the nation had not been directly attacked and enthusiasm for the
            war had to be manufactured by the likes of Walter Lippmann and Edward Bernays,
            who then drew very different lessons from this experience (Fuller, 2018a: Chap. 2;
            Fuller, 2020a: Chap. 5). To this day, Wilson is routinely vilified by American liber-
            tarians as the originator of ‘Big Government’, which has resulted in more than a
            century of American taxpayers upholding Wilson’s vision of the US as the under-
            writer of global security (Goldberg, 2007). Readers can judge for themselves
            whether this reveals Wilson to have been a hero or a villain.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Bureaucracy may be understood as positive liberty’s counterpart to the security
            function of the police and armed forces that proponents of negative liberty have
            claimed to be central to the state’s role in maintaining its core value of interpersonal
            non-interference. Positive liberty differs from negative liberty in extending the
            state’s mandate of protecting the citizenry from harms to one’s own sphere of free-
            dom that result from decisions taken by others to protecting them from such harms
            resulting from their own decisions. Thus, a litmus test of whether a nominally ‘lib-
            eral’ regime includes an element of positive liberty is whether its citizens can vol-
            untarily sell themselves into slavery, since that would mean freely restricting one’s
            own future sphere of freedom, perhaps indefinitely. Positive liberty would prohibit
            this from happening, whereas negative liberty might allow it. In the modern period,
            this difference in attitude has marked a fork in the road between liberalism in its
            ‘classical’ sense, which potentially allows for self-enslavement, and republicanism,
            which treats the maintenance of the conditions for freedom as seriously as the very
            exercise of freedom (cf. Nozick, 1974; Pettit, 1997). I have argued that the univer-
            sity constitutes a ‘republic’ in this sense, a point on which Weber and I could prob-
            ably agree (Fuller, 2000a: Chap. 1). In this context, the administrative class – Weber’s
            bureaucracy – may play a salutary role in the maintenance of academic freedom, be
            it personified by the university rector or the higher education minister. In that case,
            Weber’s objections to Althoff reflect Weber’s own confusion about the source and
            the function of Althoff’s decisions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The very idea of such an administrative class can be traced to the secularisation
            of angelic messengers in medieval Christendom (Agamben, 2011: Chap. 6). Once
            bureaucracy acquired a life of its own in late eighteenth-century Prussia, it was
            quickly exported around the world. Indeed, the German idealist sensibility was
            instrumental even in the formulation of the famed British civil service code, which
            effectively popularized the teachings of Hegel’s Oxford and Cambridge followers,
            starting with T.H. Green and later epitomized in F.H. Bradley’s Ethical Studies of
            1876, especially the essay, ‘My Station and Its Duties’ (O’Toole, 1990). Key to the
            code was the idea that the civil servant clarifies what Karl Popper (1957) called the
            ‘logic of the situation’, namely, the options available to an agent having to make a
            decision. This encounter – what in angelic discourse is called an ‘address’ – was
            meant to expand the agent’s sphere of freedom and responsibility from what it might
            otherwise be, were the agent left to their own devices. Weber saw the scientist’s
            relationship to the politician in such terms to ensure the integrity of both vocations. On the one hand, the scientist is free to lay out the possible consequences of the
            courses of action available to the politician without having to choose one; on the
            other hand, the politician is free to pursue whichever prospect he or she wishes but
            now under the burden of knowledge and hence responsibility for the outcomes. In
            the 1980s, a satirical version of this sensibility, whereby the ‘angelic’ civil servant
            is portrayed as the consummate ‘nudge’ artist, became the UK television hit series,
            ‘Yes, Minister!’]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, such bureaucratic nudging can result in its own ‘cunning of reason’
            effects. Consider the emergence of psychology as an academic discipline in
            Germany in the final quarter of the nineteenth century. In a nutshell, ambitious peo-
            ple trained in medicine, a highly competitive academic field, were encouraged to
            migrate to philosophy, a less competitive field, to secure professorships. Once
            ensconced, they then rendered traditional philosophical problems of metaphysics
            and epistemology tractable by the latest medical instruments and techniques, often
            under the guise of solving the ‘mind-body problem’ (Ben-David & Collins, 1966).
            Moreover, this trajectory was not unique to Germany. William James in the US had
            a somewhat similar personal history. However, an ironic long-term consequence of
            this development, which arose from the gradual separation of philosophy and psy-
            chology as academic departments in the twentieth century, was that judgement in its
            full-blown Kantian (and Humboldtian) sense came to lose salience as an object of
            systematic inquiry (Kusch, 1999).]]>
			</paragraph>
			<paragraph>
				<![CDATA[On the one hand, philosophy came to be concerned with the purely logical side
            of judgement, namely, the adoption of pro- and con- attitudes to propositions, which
            by the early twentieth century, under the mathematician Gottlob Frege’s influence,
            had evolved into the formal assignment of truth values, which in turn could be
            mechanically manipulated as algebraic formulae, resulting in ‘truth tables’. On the
            other hand, psychology initially focused more on the phenomenology of thinking,
            in which judgement came to be reduced to the process of directing one’s attention
            to a task set by the experimenter. But as experimental psychology became method-
            ologically more rigorous, doubts were raised as to the very existence of an experi-
            ence of thinking distinct from the contents of thought. Meanwhile what had dropped
            out from this split conception of judgement was the idea of an autonomous subject
            deciding to, say, assert a proposition or solve a problem. Both philosophers and
            psychologists, in their newly professionalized guises, seemed concerned only with
            objects of thought and sites of thinking – not with the integrity of the thinker as
            such, the judge who exercises judgement (Fuller, 2015: Chap. 3). In that respect,
            ‘judgement’ is arguably among relatively few terms that over the past century has
            lost, rather than acquired, technical meaning. Its historic associations with logic,
            law and art have been largely absent, especially in academic culture.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Yet, it would be false to say that interest in judgement as an object of inquiry
            completely disappeared. Rather, it went underground, especially once behaviorism
            became ascendant in psychology. Edward Lee Thorndike’s original interest in get-
            ting organisms to reach a prescribed goal with least effort effectively displaced
            judgement in the manner of ‘principal-agent theory’ in political science and eco-
            nomics, whereby the experimenter functions as ‘principal’ and the subject as ‘agent’ in the completion of some task (Ross, 1973). In this context, judgement happens
            behind the scenes of the experiment, as the experimenter selects the task that the
            subject then tries to execute as efficiently as it can, a process that may be improved
            over time through focused instruction. Thorndike’s research design, originally
            applied to cats, was extended to humans in Frederick Winslow Taylor’s influential
            approach to ‘scientific management’, which caught the eye of President Wilson
            (Hofstadter, 1955: Chap. 6). But in the end, Thorndike and Taylor were really train-
            ing their own judgement, teaching themselves how to get cats and humans, respec-
            tively, to achieve specific ends, of which Thorndike and Taylor were the ultimate
            judges. However, it remained unclear whether or how their subjects improved their
            own capacity for judgement. On the surface, it seemed that the subjects were just
            learning how to take orders.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, already by the First World War, Wolfgang Koehler’s (Koehler, 1925)
            pioneering work on primate problem-solving had begun to address this matter by
            situating the animal subject in an open problem space that could be resolved in vari-
            ous ways, some of which may surprise the experimenter. Koehler’s experimental
            design became the calling card of the ‘Gestalt’ movement in psychology, which
            re-entered behaviorism through Edward Chace Tolman and Egon Brunswik who
            collaborated at Berkeley in the 1930s–50s and were associated with the extended
            logical positivist community. In this context, Tolman (1948) coined the phrase ‘cog-
            nitive maps’, for the process whereby ‘rats and men’ code the physical space of, say,
            a maze as a conceptual space, akin to a decision tree, such that certain paths in the
            maze are associated with certain outcomes. Tolman and Brunswik’s students
            included not only Donald Campbell, mentioned earlier, but also Kenneth Hammond
            (1978), who perhaps did the most to relocate the study of judgement to the policy
            sciences. However, a signature feature of this research, which cuts across social and
            cognitive psychology and whose distinguished recent contributors have included
            Lee Ross, Richard Nisbett, Amos Tversky and Daniel Kahneman, is a fixation on
            errors and biases in judgement that persist even among expert reasoners. In this
            respect, the latter-day of study of judgement has continued to question the prospect
            for rational autonomy, which had been the basis on which Kant and Humboldt
            placed such great store by judgement as a mental faculty (Fuller, 1993: Coda).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Philosophy’s gradual loss of salience as the foundational discipline for all uni-
            versity subjects, starting after the death of Hegel and full realized in the aftermath
            of the First World War, provided an institutional backdrop for the decline of judge-
            ment (Schnädelbach, 1984). This movement also extended from Germany to
            America, but the outcome was somewhat different. Regarding himself as carrying
            on the work of his recently deceased friend Max Weber, Karl Jaspers famously
            argued that academic specialization meant that the sense of free inquiry promoted
            by ‘science as a vocation’ had to be pursued within – not beyond – disciplinary
            boundaries (Ringer, 1969: 105–7). To be sure, Weber’s methodological writings can
            be understood as implicitly staking out this claim as he defended the borders of
            sociological inquiry from the ‘higher order’ encroachments of Lebensphilosophie,
            dialectical materialism and social energeticism, which harked back to the pre- or
            meta-disciplinary conception of philosophy that had prevailed before Hegel’s death. Nowadays a vulgarized version of this Jasperized Weberian argument has become
            standard in defining the limits of academic freedom – namely, that one is free to
            speak publicly within one’s expertise, but not beyond it. However, when the argu-
            ment was first made in the 1920s, it served to destroy the soul of the classical
            German university, what Fritz Ringer has poignantly called ‘the decline of the
            German mandarins’. Those who refused to accept the Jasperian re-specification of
            the scope of academic freedom found themselves increasingly cast – sometimes
            willingly, sometimes not – as reactionaries who defended a nostalgic image of
            undergraduate liberal education against the alienated learning represented by the
            specialist doctors of the modern graduate school.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the case of the United States, a version of the Weber-Jaspers thesis was invoked
            by the American Association of University Professors in the early twentieth century
            to protect the tenure of social scientists whose theories and methods openly contra-
            dicted the political and religious beliefs of their academic employers (Fuller, 2009:
            Chap. 3). The results were mixed for the social scientists directly concerned but a
            long-term constructive consequence of this encounter was, starting in the 1930s, a
            rededication to general education at the undergraduate level as a counterbalance to
            the increasing significance of specialized postgraduate education. A bellwether fig-
            ure here was University of Chicago President Robert Maynard Hutchins, a major
            US public intellectual of the mid-twentieth century. A self-styled natural law theory
            ‘modernist’, Hutchins interpreted the theological and philosophical sources of aca-
            demic freedom syncretically – specifically, blending Aquinas and Locke – as a
            ‘natural right’. Though no political radical himself, Hutchins (1953) was instrumen-
            tal in keeping alive the broader ‘trans-disciplinary’ sense of academic freedom that
            originally animated Humboldt’s vision and continued to inform the self-
            understandings of both American teachers and students, arguably to this day. He
            called it ‘perennialism’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hutchins remained unafraid to pass judgement on the scholarship of his social
            science colleagues (which he found too specialized) and larger American society
            (which he found too conformist). Indeed, upon retirement from Chicago, Hutchins
            established the Center for the Study of Democratic Institutions, where he briefly
            made common cause with Students for a Democratic Society (SDS), whose 1962
            Port Huron Statement quickly became the manifesto for student movements across
            much of the world for the rest of the decade. SDS’s signature approach was to pass
            judgement on their elders over a range of issues, including the content and conduct
            of university education, the research activities of their teachers and, most impor-
            tantly, their generation’s own life prospects, given the looming threat of a third
            world war. Hutchins not only invited the student radicals to his Center in 1967, but
            he also commissioned ex-New Deal intellectual Rex Tugwell to propose a new US
            Constitution that would be at once more participatory and more accountable (Lacy,
            2013: Chaps. 5 and 6). Of course, Tugwell’s proposed constitutional convention
            never came to pass, but in the next chapter we shall see that the university as itself a
            ‘revolutionary constitution’ should continue to hold attraction.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Humboldt’s Kantian word for the exercise of judgement that is characterizes aca-
            demic freedom is Urteil. Philosophers are most familiar with Urteil as the guiding
            concept of Kant’s third critique, which is concerned with aesthetic and teleological
            judgement. As we shall see in the next chapter, Kant somewhat inverted the intuitive
            understanding of ‘aesthetic’ and ‘teleological’. We would normally think of their
            relationship as akin to that of a snapshot and an ongoing film. However, Kant
            thought more in terms of the generative capacity of the snapshot to inspire multiple
            films. In that respect, art is ‘purposive without being purposeful’. More to the point,
            art is inherently judgemental. Humboldt (1988) himself took seriously Kant’s claim
            that the mind’s spontaneous capacity for judgement provides the backdrop against
            which formal reasoning develops: Initially we think logically because we need to
            judge between alternative configurations in the manifold of experience that attract
            our attention. However, our power of discrimination is not free unless we can adopt
            a position from which to think about the alternatives that does not reflect our depen-
            dency on them: In other words, we can judge the alternatives in their own terms
            rather than based on how they impact on us. In ordinary German, Urteil is associ-
            ated with the issuance of a courtroom verdict, which serves as the foundation for
            passing a sentence.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Many modern legal intuitions remain grounded in this sense of judgement. Of
            special significance for the recent history of epistemology is the deep ‘facticity’ of
            the normative order, most closely associated with Hans Kelsen’s and Herbert Hart’s
            legal positivism, Scandinavian legal realism and arguably even Niklas Luhmann’s
            systems-theoretic sociology (Turner, 2010: Chap. 3). What all these perspectives
            share is the idea that a decision taken at a particular point serves to demarcate what
            subsequently is and is not permitted. In a specific courtroom setting, this judgement
            is delivered as a verdict, but in a larger setting it may serve as the constitutional
            basis for an entire legal system. Such judgements function as ‘axioms’, ‘rules’ or
            ‘precedents’ that must remain in place for all following actions (including later
            judgements) to enjoy legitimacy within the system. The three highlighted terms
            suggest somewhat different aspects of constraint. However, they do not obviate the
            deep facticity of the normative order promoted in modern legal theory – what US
            Founding Father John Adams called ‘an empire of laws, not men’. In twentieth-
            century epistemology, they were increasingly called ‘foundations’ in the sense that
            Richard Rorty (1979) eventually deconstructed. But we should never forget that
            early in that century, there had been a very active debate over the metaphysical sta-
            tus of such ‘foundations’ within logic, mathematics and physics – three fields that
            were drawing ever closer together (Collins, 1998: Chap 13). Some regarded these
            foundations as arbitrary conventions and others as reflective of some pre-existent
            reality. Nevertheless, both these extremes agreed that what really matters is the
            derivability of any further conclusions from the original judgement. It is what made
            those conclusions ‘principled’ in a sense that Kant could have appreciated, regardless of their ultimate source. Karl Popper and John Rawls were philosophers
            whose work in their somewhat different ways absorbed the full measure of this
            conception.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The grounding of Urteil in the law helps to explain its later theological, philo-
            sophical and scientific import. In ancient Athens, the judges were called hairetikoi,
            those delegated to decide the outcome of a case. The very need for such figures drew
            attention to a schism in the community, based on which plaintiffs and defendants
            were then licensed to mobilize witnesses and advocates. However, the hairetikoi
            were not professional lawyers like today’s judges but more akin to jurors – that is,
            citizens in good standing chosen for their neutrality to the case. They were trusted
            to set the standard of what is ‘fair’ and ‘reasonable’ in the case. In effect, the haire-
            tikoi had the power to ‘name the game’ in terms of which one of the adversaries
            might prove victorious (cf. Fuller, 2020a). The quest for this second-order sense of
            ‘judgement’ is still how trial lawyers address juries today, often taking the form, ‘If
            I were in your position, this is how I would understand the case and weigh the evi-
            dence brought before you…’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[After the fall of Athens, the Stoics sublimated this juridical style into a general
            theory that was inherited by modern epistemology. Their signature contribution was
            the idea of a criterion, or the standard by which judgement can resolve the ambigui-
            ties of a case into a determinate outcome, or verdict. Following the school’s founder
            Zeno, the Stoics presented this process as passing through three stages: prolepsis (a
            survey of the relevant possibilities), hypolepsis (a selection of one such possibility
            for further scrutiny) and catalepsis (the decision to accept that possibility as true).
            Zeno used the analogy of a hand: first, a palm with fingers apart; second, a palm
            with fingers together; third, fingers drawn together back into the palm as a fist.
            However, the stages are reversible. In particular, the second stage may revert to the
            first, whereby the possibility formerly held in the palm slips through the fingers.
            That is tantamount to the falsification of a hypothesis (Hankinson, 2003).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Stoic ‘sublimation’ of judgement that followed the fall of Athens involved
            slowing down its pace and the disentangling its parts. Arguably, the high value that
            the Athenians placed on rhetorical prowess led to a ‘rush to judgement’, which
            made the city-state at once so exciting and volatile, resulted in the downfall detailed
            in Thucydides’ Peloponnesian War. Here credit is due to Aristotle, who first divided
            the juridical style into apagoge and epagoge. More than a century ago, Charles
            Sanders Peirce captured the distinction as abduction and induction. The cognitive
            psychologist Daniel Kahneman (2011) has recently dubbed them ‘fast’ and ‘slow’
            thinking. Peirce’s elaboration of the distinction is perhaps the more pertinent –
            which was about how hypotheses are formed and tested, respectively. The original
            Athenian way of passing judgement tended to conflate the two in haste. In this
            context, apagoge was about quickly catching a criminal suspect and extracting a
            confession. The absence of alternative accounts (including suspects) of the harm
            amounted to conviction. As Aristotle observed, this is proof by showing that the
            opposite of what is presented as the case is highly unlikely, if not impossible. But
            have the alternatives really been given a chance to appear, or has the court simply
            capitulated to the rough justice of the lynch mob? The principle of Habeas corpus, which only comes into general force in the late seventeenth century, was partly
            designed to inhibit apagoge’s impulsiveness. In contrast, epagoge meant inference
            of a universal from a set of particulars. But it too might contribute to rough justice
            by encouraging judgements based on social stereotypes of the individual on trial.
            Even if there are alternative suspects, one individual may fit the stereotype of people
            who tend to cause such harms. In Kahneman’s terms, now part of popular discourse,
            this would constitute ‘confirmation bias’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Stoics importantly departed from Aristotle on the role of contingency in judge-
            ment. Aristotle is normally credited with having introduced the concept of contin-
            gency (endechomenon) while arguing that our relationship to the past and to the future
            is radically different: The past is knowable because it has already happened, whereas
            the future is unknowable because it has not yet happened. In response, the Stoics
            argued that what’s really at stake is not the unrealized nature of the future itself but the
            indeterminacy of the basis on which the future will be determined. In other words,
            logically speaking, what Aristotle treated as a first-order problem, the Stoics treated as
            a second-order problem. It was on this point of disagreement that the project today
            called ‘modal logic’ was launched (Fuller, 2018a: 139–140). The Stoics held that once
            the standard of judgement is decided for an unresolved case (aka ‘the future’), the
            outcome exists on a continuum whose poles are ‘impossibility’ and ‘necessity’: that is,
            it may fail to meet the standard and thereby excluded; or it may meet the standard and
            thereby included. The law calls the former ‘prohibition’ and the latter ‘obligation’. On
            this account, ‘contingency’ is whatever passes for the time being as ‘not impossible’
            yet also ‘not necessary’. This mode of reasoning clearly conforms to the logic of a
            verdict, which typically appears as a quasi-deduction from the legal principles and
            precedents that the judge has selected, combined with the evidence that judge has
            found most salient in disposing of the case. It follows that an ‘acquitted’ defendant is
            simply deemed innocent of the charges as presented in the trial, not necessarily exon-
            erated of the crime itself. After all, judicial decisions can in principle be overturned –
            and often are in the course of time. This feature of the law is institutionalized in ‘courts
            of appeal’, the ultimate of which is called a ‘supreme court’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Stoic approach to judgement was designed to rationalize a strenuous life in
            the public eye, where the reversal of fortunes is an ever present prospect. Thus, the
            leading Stoics have been politicians (Cicero, Seneca), emperors (Marcus Aurelius)
            and people whose great personal life transformations lead them to teach others
            (Epictetus). In more recent times, Stoicism has been associated with entrepreneurs.
            Unsurprisingly, it is the favorite philosophy of Silicon Valley (Holiday, 2014). The
            underlying moral psychology anticipates our post-truth condition. Contrary to con-
            ventional ‘truth-oriented’ epistemologies, in which ‘reason’ and ‘emotion’ are
            treated as discrete mental faculties that require the former to dominate the latter,
            ‘reason’ is simply the post facto name we give to an ordered soul, and ‘emotion’ to
            a disordered soul. They do not constitute the mind but are judgements about the
            mind. ‘Subjectivity’ is a crucial term in this context, but not for its connotations of
            ‘partiality’ or even ‘fallibility. Rather, what matters is the sense of ownership of
            judgements about oneself and the world. It was this sense that was behind the coin-
            age of ‘aesthetic’ in the Enlightenment to refer to a composite perception, or ‘worldview’, whereby coherence is brought to one’s identity by a drawing together
            of the often contradictory sensory inputs received from the world into a unique per-
            sonal perspective. This is what Kant called ‘autonomy’, again a nod to Stoicism.
            Thus, education should be designed to foster autonomy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Indeed, the very concept of the ‘aesthetic’, which carries both strong empirical
            and normative connotations, emerged as part of a general reassessment of the Stoics
            in the early modern period. The Stoics had been previously seen as closer to nihilists
            or atheists due to their refusal of Christian conversion even in the face St Paul’s
            early ministrations. However, once John Locke incorporated Stoic ideas to formu-
            late his influential account of personal identity as ‘self-ownership’, the tide began to
            turn (Golf-French, 2022). In particular, the idea of the soul as a ‘blank slate’ acquired
            new meaning. While the metaphor had always involved regarding experience as a
            kind of inscription, Locke interpreted it to imply that process was self-applied, as in
            the Marcus Aurelius’ Meditations, which were mainly written as post facto reflec-
            tions on deeds done (Hill & Nidumolu, 2021). The result was a conception of mind
            as an articulated response to the world, resulting in a judgement on whatever comes
            before it. Kant’s elaborate architectonic of the mind should be understood in light of
            this Lockean transformation of the Stoic legacy. Its closest analogue in recent times
            is not what passes for ‘blank slate’ in Steven Pinker’s (2002) notorious take-down
            of social science and utopian politics (which he conflates) from the standpoint of
            evolutionary psychology. Rather, and perhaps surprisingly, it is the theory of gen-
            erative syntax proposed by his teacher, Noam Chomsky, especially as Chomsky
            himself originally understood it – namely, as a recursively applied, sui generis organ.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here one needs to imagine, say, Marcus Aurelius literally writing his ‘self’ into
            existence by discovering its governing principles (aka ‘grammar’) through encoun-
            ters with the world that compel him to speak his mind on paper, even after having
            done so in deeds. This compulsion to produce a second-order reflection on first-
            order action captures the poiesis (‘productivity’) of language as creating another
            level of reality – a ‘metaverse’, if you will – in which the autonomous self comes
            into being (Chomsky, 1971; cf. Fuller, 2022). Put in terms closer to that of Humboldt,
            whose own contributions to linguistics Chomsky holds in high esteem, this process
            facilitates Bildung – that is, life as a ‘work in progress’, if not a ‘work of art’. More
            precisely, the quest for self-discovery is an endless struggle against the forces that
            threaten to pull apart one’s sense of identity. True to the etymology of ‘erring’ as
            ‘straying’, these threats are often portrayed in terms of one’s being steered off
            course. The protagonists of the great epic poems of the Classical world, the Iliad,
            Odyssey and Aeneid, underwent just this process at various points in their respective
            narratives. These heroes were placed in difficult situations where they had to make
            decisions that altered the course of both their own and others’ lives, sometimes for
            many generations – as in Aeneas’ founding of Rome.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Humboldtian university can be understood as a relatively sheltered environ-
            ment for simulating those sorts of ‘crossroads’ experiences, not only in the class-
            room where students encounter intellectually challenging lecturers but also in more
            self-organized settings, especially ‘student unions’. From that standpoint, seminars
            may be regarded as middle spaces. In a similar spirit, two centuries earlier, King James I’s personal lawyer, Francis Bacon, developed what we now call the ‘scien-
            tific method’, the centerpiece of which was a similar crossroads experience, only
            now applied to the natural world: that is, the ‘crucial experiment’, whereby a pro-
            posed explanation of some phenomenon is subject to stiff interrogation, with only
            two possible outcomes: be allowed to carry on or be forced to change. Even if the
            hypothesis survives the trial, it is simply ‘unfalsified’ but not necessarily ‘con-
            firmed’, just like the acquitted defendant. To his credit, Karl Popper picked up on
            this nuance of Bacon’s method in a way that his logical positivist colleagues had
            not, hence Popper’s preference for Kant over Hume as a philosopher of the empiri-
            cal world. For while Hume woke Kant from his dogmatic slumbers, the Critique of
            Pure Reason is dedicated to Bacon.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We shall return to Bacon in the next chapter, but the point here is that experience
            in its fully alive, self-conscious sense is tantamount to reflective judgement. It lives
            in the modern understanding of ‘perception’ as a kind of second-order experience –
            that is, experience judged. Thus, ‘prejudice’ amounts to perception before the pre-
            sentation of the object of experience. Here one might imagine a continuum extending
            between unconscious and reflective judgement, one pole of which is Kant’s ‘antici-
            pations of experience’, which provide the categorical structure of the mind. In the
            middle is what experimental psychology now calls ‘cognitive biases’ and Bacon
            originally called ‘idols of the mind’. The opposite pole is defined by Bacon’s crucial
            experiments and Popper’s falsifiable hypotheses. In this respect, universities are
            meant to provide a living laboratory for the cultivation of judgement, whereby prej-
            udices and biases are subjected to sustained critical reflection, the outcome of which
            is left for students to decide for themselves in their extramural pursuit of Bildung.]]>
			</paragraph>
			<paragraph>
				<![CDATA[From that standpoint, the sort of examinations that students routinely sit to deter-
            mine their degree class are little more than gatekeeping exercises to ensure univer-
            sity attendance, a bit like glorified primary and secondary school ‘report cards’ that
            were more concerned with such pseudo-moral qualities as ‘discipline’ and ‘deport-
            ment’ than with the enterprise of Bildung. This point has been increasingly over-
            looked in recent times as universities have become more explicitly ‘credentials
            mills’, whereby examination results are tied to employment opportunities, render-
            ing them somewhat closer in spirit to the Chinese civil service examinations to
            which Max Weber devoted so much attention in his efforts to fathom the ‘Oriental’
            sense of rationality. To be sure, the seeds of an ‘Occidental’ counterpart were sown
            with the introduction of ‘intelligence tests’ (later ‘aptitude tests’) in the early twen-
            tieth century as a means to track children through the educational system so they
            can realize what psychometricians of the day had defined as their ‘full potential’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When Urteil figures in logic, it is about classifying something as belonging to a
            category for purposes of drawing a conclusion about it. In metaphysics, to issue a
            judgement is to subsume a particular under a universal. For example, in the famous Aristotelian syllogism, the judgement ‘Socrates is mortal’ is based on Socrates
            being categorized as a human being. Socrates can be categorized in any number of
            ways, each accessing properties that he shares with other beings. But to judge
            Socrates as mortal, he must be classed as ‘human’, in the sense of Homo sapiens, a
            member of the animal kingdom, all whose members are mortal. The example high-
            lights the peculiar combination of chance, freedom and necessity that constitutes
            judgements. It recalls the Stoic disentangling of Athenian legal reasoning. First, one
            is thrown into an unresolved situation – namely, defining the ‘suspect’ Socrates for
            the purpose of further consideration. Second, it is clear that Socrates can be under-
            stood in many different ways. Third, it is equally clear that if Socrates is to be
            understood as mortal, it must be by virtue of his ‘humanity’ understood in the ani-
            mal sense of beings that undergo a certain life cycle. This way of linking logic and
            judgement was dominant until the dawn of the twentieth century. It was a kind of
            normative psychology that overlapped with fields that today’s logicians would treat
            as parts of rhetoric (aka ‘informal reasoning’) or the scientific method (aka ‘non-
            deductive inference’). Here it is worth recalling that John Stuart Mill and George
            Boole were mid-nineteenth century contemporaries who equally saw themselves as
            ‘logicians’ in this older, broader sense, albeit operating with somewhat different
            standards of validation. However, Gottlob Frege – and then more strikingly, Bertrand
            Russell and Alfred North Whitehead – repositioned Boole as the founder a new
            ‘symbolic logic’ that revealed the analytic power of subjecting reasoning to alge-
            braic notation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[An implication of this potted history is that the Humboldtian university was con-
            ceptualized in the older spirit of logic as a theory of judgement. It was probably the
            only legacy of German idealism that the original American pragmatists – Peirce,
            James and Dewey – never tried to officially disown in their quest to establish a
            world-historic sense of US philosophical originality. In all other respects, in relation
            to the idealists, the pragmatists suffered from what Harold Bloom (1973) called an
            ‘anxiety of influence’, which happens when an aspiring poet eclipses his or her
            predecessors by repeating them to more brilliant effect, so as to overshadow prede-
            cessors’ original contributions. A case in point is John Dewey’s magnum opus,
            Logic: The Theory of Inquiry, which is now read as a treatise on the metaphysical
            foundations of problem-solving, but might be usefully read alongside Robert
            Brandom’s endlessly heroic efforts to turn Hegel into a precursor of Wilfrid Sellars
            (Passmore, 1966: Chap. 7; cf. Brandom, 1994). A vestige of Dewey’s older way of
            thinking about logic remains in the concept of soundness, which in ordinary lan-
            guage is still regularly applied to judgements, as when we say someone ‘exercises
            sound judgement’. Formal logic tries to capture this by applying ‘soundness’ spe-
            cifically to arguments that are not only deductively valid but also contain true
            premises.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But what makes the premises of an argument true? In one sense, today’s logician
            would answer just as, say, Mill or Dewey would, namely, that the premise corre-
            sponds to the facts. But the meaning of ‘fact’ somewhat differs. Mill regarded a fact
            as an expression of one’s possible experience of the world – that is, of all the things
            one could have experienced, what one actually experienced is the fact. Mill’s sense of fact captures the spirit in which a lawyer approaches a witness during a trial.
            Dewey moves closer to the formal logician’s view by saying that a fact contributes
            to a solution to a problem, which provides a second-order frame of reference for
            determining the kind of facts one needs. This helps to explain why the original
            Gestalt psychology experiments on thinking – which were of Dewey’s vintage (i.e.,
            1920s) – had subjects approach the task in the spirit of problem-solving, sometimes
            simulating a task that past scientists had faced (Fuller, 2015: Chap. 3). Alfred Tarski
            (1943), who did the most to reconceptualize ‘truth’ in terms of the new symbolic
            logic, took ‘correspondence’ to the next level by treating the world of facts as a
            metalinguistic construction that gives meaning to first-order statements. The logical
            positivists quite fancied this move because it suggested that science might be the
            fact-bearing metalanguage in terms of which the meaning of ordinary propositions
            might be resolved, resulting in a conceptually unified understanding of all reality. In
            Tarski’s terms, ‘Socrates is human’ (the basis on which he is judged mortal) is true
            if and only if Socrates is indeed human (Passmore, 1966: Chap. 17).]]>
			</paragraph>
			<paragraph>
				<![CDATA[And what exactly is the nature of the mutual conditional at the heart of Tarski’s
            definition? After all, to someone still wedded to judgement-based logic, the defini-
            tion is too expansive; it fails to respect the judge’s discretion concerning the stand-
            point from which Socrates is to be understood for purposes of issuing a judgement.
            To accommodate the judge as free agent, the first ‘if’ condition needs to be dropped
            from the Tarski definition of correspondence. In other words, the bare fact that
            Socrates is human would not necessitate an assertion of his humanity, unless the
            judge had decided it was necessary to the context in which Socrates was under
            judgement. (And here ‘necessary’ should be understood as connoting a strong sense
            of relevance.) One suspects that this concern also lurked behind the preoccupation
            with ‘analyticity’ in late twentieth century Anglophone philosophy. Nearly all ana-
            lytic philosophers of the period felt the need to say something to say about it, usu-
            ally when talking about the role of ‘necessary truth’ in science.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In many respects, by endorsing Tarski, the logical positivists had reinvented the
            problem that Aristotle and the Stoics were trying to solve by disentangling the rea-
            soning behind making a judgement to avoid the ‘rush to judgement’ that ultimately
            doomed Athens. The point of distinguishing apagoge and epagoge was to show that
            any sense of ‘necessary truth’ that attached to the premises – as opposed to the con-
            clusion – of an argument was a cognitive illusion born of a failure of the counterfac-
            tual imagination compounded by confirmation bias. Nevertheless, Aristotle’s
            definition of ‘science’ kept open the prospect that there might be statements that are
            true for all cases in a given universe of discourse in a way that no other statements
            are. These would form the major premise of scientific syllogisms and be reasonably
            regarded as ‘necessary truths’. Following the Protestant Reformation, the Catholic
            concept of ‘natural law’ was reappropriated for this purpose by the emerging class
            of ‘natural philosophers’ who we now call ‘scientists’. Today’s default understand-
            ing of ‘laws of nature’ or ‘scientific laws’, courtesy of Isaac Newton, is its most
            notable legacy. Such ‘laws’ capture both empirical regularities (cf. epagoge) and
            counterfactual conditionals (cf. apagoge). Taken together, they constitute ‘neces-
            sary truths’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Indeed, Kant was so impressed by Newton’s achievement, yet at the same time
            struck by its limitations, that he invented a so-called ‘transcendental’ style of argu-
            mentation that granted Newton’s laws a special epistemological significance.
            Basically, the metaphysical presuppositions of the laws – absolute space, time and
            causation, all understood in Euclidean terms – defined the limits of human thought
            and hence our understanding of reality. We simply could not think our way out of
            them. All apparent attempts to do so – such as Leibniz’s rationalistic project of
            ‘theodicy’, which aimed to justify why the world had to be as it is – were fantasies
            that wildly extrapolated from what we can reasonably know to the mind of a God
            from whom we have been supposedly alienated since Adam’s Fall. To be sure,
            Kant’s own highly restricted view of human cognitive capacities delayed the accep-
            tance of nineteenth century innovations in mathematics that would have expedited
            what eventually became the early twentieth century revolutions in physics and logic
            (Fuller, 2019b).]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, Fichte, Schelling, Hegel – the so-called ‘German idealists’ who fol-
            lowed in the wake of Kant and led the Humboldtian university in its early years –
            repurposed the transcendental style to recover some of the intellectual ground that
            Kant conceded to our supposed ‘cognitive limitations’. In effect, the idealists privi-
            leged apagoge over epagoge. They took Kant to have mistakenly bought into
            Newton’s premise that the world can be exhaustively understood as ‘matter in
            motion’. In contrast, the general idealist strategy was to exfoliate the logical conse-
            quences of the conceptual presuppositions of the various sciences. The sciences
            themselves were understood not as different levels in the organization of matter, but
            as orthogonal paths to the realization of ‘mind’ (Geist) in the broad sense of ‘spirit’,
            ‘consciousness’ as well as ‘intellect’, the embodiment of which at any given time is
            what we call ‘humanity’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here it is worth reflecting on the sense in which the idealists privileged apagoge
            over epagoge. Many of the sciences – such as what became chemistry, biology and
            psychology – were not as empirically well-grounded as physics in the early nine-
            teenth century. In short, they lacked epagoge. Nevertheless, each their fundamental
            conceptions of the world possessed a certain prima facie plausibility that suggested
            a certain ‘logic of inquiry’, which in turn could be pursued as what we now call a
            ‘research program’. In short, they possessed apagoge. In this regard, Stephen
            Pepper’s (1942) ‘metaphysics as world hypotheses’ thesis captures very much the
            spirit of the idealist approach. Indeed, it is often overlooked that this style of pre-
            senting humanity’s epistemic horizons – which was integral to philosophy’s central-
            ity in the Humboldtian curriculum – did inspire the development of the special
            sciences throughout the nineteenth century, by the end of which self-styled ‘Neo-
            Kantian’ philosophers had begun to propose alternative schemes for reintegrating
            the sciences to comprise a unified image of humanity. Here Ernst Cassirer (1944,
            1950) deserves honorable mention as the most wide-ranging and sophisticated con-
            tributor to this side of the idealist project, which was already being eclipsed in his
            own day by the logical positivist project of unifying the sciences through a kind a
            ‘reductionism’ that in some respects revived Kant’s original fixation on Newton.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What does any of this have to do with today’s issues concerning academic free-
            dom? Notwithstanding its abstraction, the above discussion highlights the ways in
            which truth, validity, soundness and relevance are bound together with not only
            so-called facts-of-the-matter, over which we might exert relatively little control, but
            also how we organize and respond to them, over which we can exert much more
            control. Academic freedom relies on the concept of judgement to sharpen the differ-
            ence between these two perspectives, which in turn defines a domain in which aca-
            demics may legitimately exercise their freedom. Put in contemporary terms, the
            focus on judgement makes it possible to assert both one’s recognition of the brute
            facts and one’s openness to alternative theories by which they may be understood.
            The contrasting state-of-affairs would be one in which facts are seen as already
            bearing their own interpretation, such that to recognize them properly is ipso facto
            to draw the appropriate conclusion. One would expect this latter stance as the offi-
            cial epistemology of a society dominated by religious orthodoxy or political ideol-
            ogy. The contrast brings out the amount of free play effectively allowed under the
            rubric of ‘academic freedom’ and perhaps helps explain its controversial character
            both in and out of academia.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A train of thought that has migrated from the philosophy of science to the larger
            intellectual culture over the past sixty years has obscured the significance of what is
            at stake here. It turns on the idea, originating in Hanson (1958) but popularized in
            Kuhn (1970), of ‘theory-laden observation’. For many years it has been common to
            treat this phrase as an implicit license for relativism, reading it to mean that people
            will see only what their background beliefs allow them to see. No doubt this reading
            was influenced by Kuhn’s notorious incommensurability thesis, which implied that
            scientists are so wedded to the theories in which they were trained that a genera-
            tional change – that is, a cohort of scientists trained in the next theory – is necessary
            before a scientific revolution fully succeeds. In other words, scientists never change
            their minds simply based on the facts. But that is only the negative side of theory-
            laden observation. The positive side is that the facts do not speak for themselves but
            require a theory for their articulation. The license to produce such a theory is what
            academic freedom protects, as it requires passing judgement on what are the essen-
            tial and accidental features in a given domain.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here it is worth recalling the trivialization of the idea of ‘value judgement’ as it
            migrated from German to English academic discourse in the twentieth century.
            Nowadays value judgements are seen as aspects of one’s personal experience that
            are brought with mixed feelings to the research and teaching environment. They
            both enable and disable the observation of certain things. Value judgements are a
            sign of the researcher’s subjectivity and a potential contaminant to an objective
            appraisal of what is under investigation. Attitudes towards value judgements vary
            wildly in research methods textbooks: Positivists fear bias and distortion, while
            postmodernists celebrate the giving of ‘voice’. Without denying any of these points,
            the original German context placed the affirmation of value judgements in a more
            positive light – that is, as a defensible extension of academic freedom. I refer here
            to the widely misunderstood Werturteilstreit (‘value-judgement dispute’) that
            occurred in the first decade of the twentieth century, which resulted in the institutionalized separation of sociology from social policy in German academia
            (Proctor, 1991: Chap. 7).]]>
			</paragraph>
			<paragraph>
				<![CDATA[It was in the wake of the Werturteilstreit that ‘value-judgement’ and ‘value-
            freedom’ started to acquire international currency as constitutive of the academic
            ethos. In this context, Max Weber served as the point of reference for promoting
            these concepts, especially as his works were translated in English, courtesy of
            Edwards Shils and Talcott Parsons. However, lost in translation from Weber’s origi-
            nal context, yet still relevant today, is the question of who should supply the values
            that inform the theoretical standpoint from which academic research is conducted.
            Weber and his sociology colleagues wanted that power to remain in the hands of the
            academics themselves – not the government. In this respect, the Werturteilstreit may
            have been the first organized academic protest against what we now call ‘evidence-
            based policy’, whereby academics simply act as sophisticated data collectors and
            analysts for projects whose intellectual horizons had been set by their political pay-
            masters. This was certainly how Weber and sociology’s other early German found-
            ers sarcastically cast the German ‘social policy’ academics – namely, as purveyors
            of Kathedersozialismus (‘socialism of the chair’). To be fair, the social policy aca-
            demics tended to be worthy social democrats who wanted to ameliorate the condi-
            tions of industrial labor to prevent the proletarian revolution from happening in
            Germany, as Marx had predicted. Indeed, Bismarck’s grand preemptive strike on
            Marx’s prediction was a national insurance scheme that set in train the modern wel-
            fare state (Rueschemeyer & Van Rossem, 1996). Nevertheless, it was the principle
            of leaving value decisions about research focus in the hands of academics that
            Weber and his colleagues were meaning to uphold.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But this did not mean that Weber and his like-minded ‘sociologists’ were apoliti-
            cal. On the contrary, they were generally speaking liberals, which for them was less
            an ideology than a ‘meta-ideology’, in the sense of being a general attitude toward
            the relationship between value commitments and the pursuit of inquiry – namely,
            that it should never be dogmatic. Very much like Karl Popper, Weber took human
            fallibility and open-mindedness to be two sides of the same coin. But this had a
            specific sociological implication, namely, that the sphere of free inquiry – what I
            have called ‘the right to be wrong’ (Fuller, 2000a) – approximates a secular sanctu-
            ary (cf. Sassower, 2000). Thus, a remarkable feature of Weber’s many public
            engagements is that whether pro or con current government policy, his comments
            were stated so as not to jeopardize continued protection of academics to set their
            own theoretical horizons. At the same time, Weber conjoined that right with the duty
            to permit ‘value-free’ tests of those horizons – that is, by evidence collected from a
            standpoint independent of the theorist. In other words, ‘value-freedom’ refers spe-
            cifically to freedom from the values that inform the theory subject to validation.
            Another way to capture this sensibility is the distinction drawn by philosophers of
            science between the ‘context of discovery’ and the ‘context of justification’. The
            latter is ‘value-free’ in that the mark of a ‘scientific’ knowledge claim is the extent
            to which its validity can be established without one’s having undergone the same set
            of experiences and sharing the same value assumptions as the person who originally
            made the claim. This is what makes scientific validation different from, say, religious validation, which requires a sense of personal witnessing and existential
            commitment that psychologically inhibits the ‘hypothetical’ frame of mind in which
            one should entertain knowledge claims.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let us now return to the theological legacy of hairetikos, a member of an Athenian
            jury, which provides the root of the modern word heretic. Two substantial changes
            in Western intellectual culture enabled this transition. First, whereas Athenian jurors
            were picked randomly in response to a similarly random event that had created a
            serious division in society, ‘heretics’ in the medieval and modern senses self-select:
            They volunteer to stand judgement over their religion or society more generally –
            and to be themselves judged on those terms. Thus, even though ‘heretics’ in this
            sense typically regard themselves as revealing a long-standing tension, they are nor-
            mally seen as themselves the source of the tension. For this reason, heretics are
            often turned into scapegoats, who may be crucified, burned at the stake or, as in the
            case of Ibsen’s An Enemy of the People, subject to public humiliation and ostracism.
            Often the most effective strategy under these conditions is simply to walk away, as
            Martin Luther did after making his stand at the Diet of Worms in 1521. The second
            change was the presence of fixed written law that served as a basis to judge the
            heretic’s pronouncements. In the Middle Ages, this was provided by the Roman
            Empire’s Justinian Code, which was supported by the body of Christian theological
            commentary that we now call ‘natural law theory’. Since this corpus of writing was
            presumed to underwrite the taken for granted order of the world, heretical pro-
            nouncements easily triggered a sense of existential risk to the community to which
            they were addressed. A modern equivalent is the concept of ‘hard cases’ in the law,
            whereby a court hears a case whose complexity suggests that it might become a
            precedent for future cases. In this context, US Supreme Court Justice Oliver Wendell
            Holmes warned that hard cases make bad law because the striking character of such
            cases — typically due to idiosyncrasies of the time, place and parties involved – can
            easily lead the judge to rule in a way that undermines the spirit of the law. In effect,
            hard cases are modern heresies delivered in situ rather than in person. But of course,
            notwithstanding such admonitions, there may still be reasons to alter the status quo
            in the direction of the heretics and the hard cases (Dworkin, 1975).]]>
			</paragraph>
			<paragraph>
				<![CDATA[I earlier noted that the classical conception of academic freedom, with its focus
            on the exercise of independent judgement, was a product of the modern German
            university. I also observed that the first test of this conception came with the separa-
            tion of scientific (i.e. ‘critical-historical’) from pastoral theology. But another aspect
            of the religious origin of academic freedom is specifically denominational, reflect-
            ing its Protestant – not Catholic – roots. Consider that in the university’s original
            Catholic environment, the relation between teacher and student was at least as much pastoral as ‘instructional’ in the strict academic sense. The teacher was the surro-
            gate parent – ‘in loco parentis’ – of the student who lived away from home on uni-
            versity grounds. However, Humboldt reformulated the relationship between teachers
            and students as a system of complementary rights and duties, known as Lehrfreiheit
            (freedom to teach) and Lernfreiheit (freedom to learn). Teachers were obliged to
            open the minds of students to learning without indoctrinating them to a particular
            worldview (Metzger, 1955). The focus here was the very Protestant one of clearing
            a space – Heidegger’s Lichtung – so that students can freely decide what to believe –
            or at least what is important to learn (cf. Fuller, 2016c).]]>
			</paragraph>
			<paragraph>
				<![CDATA[An important waystation between the old Catholic-style pastoral view of the
            teacher-student relationship and the stark Protestant-style ‘decisionism’ of the mod-
            ern university was the Jesuit idea of cura personalis, in which the teacher serves as
            the student’s ‘companion’, trying to awaken a ‘life in Christ’ by appealing to the
            student’s own intellectual and psychic resources but making it clear that in the end
            it’s the student’s decision how they take the encounter. The hope is that the student
            would develop discernment. In our explicitly secular times, the pedagogical route of
            least resistance has been for the teacher to present the student with several options
            on a given matter, including their strengths and weaknesses but without drawing an
            authoritative conclusion. But as Max Weber (1958) would be the first to observe,
            this focus is always tricky to maintain in practice. Indeed, an academic especially
            influential with students in the post-Weber generation, Martin Heidegger, spoke of
            Gelassenheit (‘equanimity’), which was understood in the 1930s to license an indif-
            ference to conventionally opposing positions to search for a deeper, more personal
            truth – an attitude on which Nazism perversely fed.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The second feature of the classical conception of academic freedom that marked
            a shift from Catholicism to Protestantism is that the modern university professor
            claimed to find the spark of the divine from within, rather than the demigods and
            demagogues from without. The corresponding mode of institutional being was char-
            ismatic rather than collegial: German professors attracted students and created fief-
            doms that might evolve into disciplines, understood as secular ‘denominations’.
            This was opposed to, say, their Oxbridge counterparts, who were preoccupied with
            collective self-governance and displayed little interest in proselytism (Clark, 2006).
            Set against this institutional context, ‘judgement’ acquires an additional depth of
            meaning, which relates to what is ideally transmitted from teacher to student. The
            answer implied in the idea of Lernfreiheit is not mastery of a doctrine but the capac-
            ity to judge what is worth learning. It consists of adopting the attitude, though not
            necessarily the content, exemplified by the professor. Students who were incapable
            of ‘striking the pose’ of professorial authority for themselves remain in a state of
            ‘nonage’, that is, someone who waits for others to tell them what to think – as Kant
            famously characterized the pre-Enlightenment mind. In this respect, the sense
            ‘judgement’ promoted by academic freedom is tied to the promotion of subjectivity
            that first came into its own with early nineteenth century Romanticism. Schelling
            and Hegel set precedents for the Romantic expression of academic freedom in the
            natural and social sciences, respectively. Their most lasting legacy to this discussion
            is the idea that the point of the university is to unify knowledge in a way that renders it comprehensible to a single human mind, an aspiration of the curriculum that is
            underwritten by our having been created in the image and likeness of God.
            Humboldt’s Kant-inspired innovation here was to replace dogmatic theology with
            critical philosophy as the secular source of pedagogical unity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Both naturalistic and humanistic traces of this ‘unitarian’ impulse remain in con-
            temporary academic life and, perhaps predictably, test the limits of academic free-
            dom. On the humanistic side, there have been perennial and invariably polemical
            debates over ‘the canon’, that is, the finite number of ‘great books’ that anyone
            should master in order to be ‘liberally educated’. In the United States, where stu-
            dents generally are not required to select a major subject until half-way through
            their undergraduate education, debates over the canon have become totemic rituals,
            whereby academics bear witness to their calling and challenge others who appear to
            bear false witness. Elsewhere in the world, where students are expected to specialize
            much earlier in their academic careers, debates over the canon have turned into a
            staple of the public intellectual literature, in which academics contest the merits of
            various would-be classics with journalists and politicians who might have been
            once their students. On the naturalistic side, there is the equally controversial – and
            popular – preoccupation with ‘grand unified theories of everything’. Although this
            quest derives its impetus from physics, the same ambition has applied to the ‘Neo-
            Darwinian synthesis’ in biology since the 1940s (Fuller, 2007a: Chap. 2). Here the
            sense of unity consists of the ability of understanding the widest range of phenom-
            ena by appealing to the smallest set of general principles.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The striking point here – which makes the German idealists and the logical posi-
            tivists strangely natural bedfellows – is the significance of reduction in both the
            humanistic and naturalistic instances of unity. An interesting transitional figure
            between the idealists and the positivists was Emil DuBois-Reymond, a leading late
            nineteenth century physiologist and intellectual descendant of Schelling who popu-
            larised the idea of natural scientists as exemplars of academic freedom in his 1882
            inaugural address as Rector of the University of Berlin, Humboldt’s original base
            (Veit-Brause, 2002). Where an idealist like Hegel exhorted academics to embody
            the ‘world-historic spirit’ (Geistesweltgeschichte) by determining what from other
            times and places belong to the emerging self-consciousness of humanity, DuBois-
            Reymond spoke of their dedicated search for causal explanations as the driving
            force of history (Causalitätstrieb). In any case, it is unlikely that unity would have
            become such an esteemed epistemic value had not the individual human being qua
            free inquirer been its locus of concern. After all, the value of unity lies not in the
            sheer recognition of reality as inherently unified (because that is not possible – and
            here the post-Kantians defer to the deity) but in the active construction of a unified
            vision of reality as indicative of one’s autonomy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this context, the shibboleth ‘reductionism’ should be understood somewhat
            more charitably as referring to the autonomous person’s capacity to discriminate the
            wheat from the chaff in the phenomena he or she faces: What is causally relevant or
            explanatorily deep, as opposed to merely a striking accident or persuasive observa-
            tion? One takes responsibility for the answers given to such questions by articulat-
            ing a Weltbild (literally ‘world construction’) that attempts to make sense of them. Of course, it is to be expected that other autonomous individuals operating from
            different Weltbilder will contest these judgements – and in the context of these trials
            the significance of ‘value-freedom’ becomes clear. However, matters got compli-
            cated at the dawn of the twentieth century, when for various intellectual and politi-
            cal reasons physicists following Max Planck started to advocate that all members of
            the same academic discipline should adopt the same Weltbild, what Kuhn later
            popularized as ‘paradigm’ (Fuller, 2000b: Chap. 2). Thereafter, the distinction
            between being ‘original’ and being ‘crackpot’ started to acquire a salience within
            academia that it had previously lacked, as peer-based intellectual conformity came
            to be seen as a strength rather than a weakness.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Although Schelling and Hegel left stronger imprints in the development of the
            special sciences, the self-expressive ideal of academic freedom highlighted above
            was most explicitly embodied in their older contemporary, Johann Gottlieb von
            Fichte. In Fichte’s spoken and written work the various strands supporting this ideal
            were brought together – including the original philosophical defense of authorial
            copyright in publishing. This he presented as the legal recognition of intellectual
            freedom – a radical move in a country that lacked the generalized freedom of expres-
            sion underwritten in the recently enacted US Bill of Rights. In effect, Fichte argued
            that authors added something unique beyond the sheer physical labor of writing
            about a topic that deserved legal protection – namely, the distinctive stance they
            adopt towards the topic, as reflected in their unique mode of expression (Woodmansee,
            1984). Behind this conception lay the sense that self-assertion is by no means natu-
            ral. On the contrary, it is an endless struggle (Sturm und Drang) against conformity
            to nature, especially submission to animal appetites. Speaking one’s mind and find-
            ing one’s voice is hard work – and deserves to be rewarded accordingly – because
            one is always navigating between the Scylla of the herd mentality and the Charybdis
            of perverse contrarianism. Both are ‘instinctive responses’ that let others determine
            my own beliefs – in the latter case, I simply believe the opposite of whatever most
            others believe. Thus, they abandon the rational self-determination that is at the heart
            of academic freedom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Of course, as the histories of both Protestant Christianity and German academia
            demonstrated all too well, the price of encouraging such independence is fractious-
            ness, as students are effectively encouraged to outgrow their teachers, especially at
            the postgraduate level, where the stress is placed on ‘originality’ as the mark of
            research worthy of academic certification. Indeed, students’ own sense of charisma
            may extend to believing that they are better positioned to take forward a common
            body of knowledge than the professors from whom they learned it. The result is a
            proliferation of ‘schools’, academia’s version of churches that make it difficult to
            maintain the transgenerational, depersonalized sense of ‘discipline’ associated with
            monasticism. To be sure, academics continue to be reminded of this suppression of
            academic subjectivity in, say, the restricted use of personal names to refer to experi-
            mentally generated ‘effects’ – but not general principles of nature and, more gener-
            ally, the hermeneutically casual, if not irresponsible, stance that especially natural scientists are encouraged to adopt towards the ‘literature’ to which they contribute.
            The ease with which articles that have never been read, let alone read in context,
            attract huge citation counts speaks to an academic culture that is trying its hardest to
            erase any interest in the author when assessing knowledge claims (Fuller, 1997:
            Chap. 4).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Abstract This chapter looks at the many issues raised by Max Weber’s suggestion
            that the academic life is best understood as the secular equivalent of a religious call-
            ing. In his day, it was understood as an argument to keep scientific inquiry indepen-
            dent of political demands. But at a deeper level, it reflected the conflicted Protestant
            legacy of ‘calling’ (Beruf), which may be understood in terms of submission
            (Luther) or autonomy (Calvin). Weber clearly takes the Calvinist line, which is
            aligned with Kant’s conception of ‘purposiveness’, a sense of freedom that embraces
            the uncertainty (and risk) of where inquiry might lead. The aesthetic dimension of
            this idea, integral to Kant’s conception of judgement, is explored, especially against
            the role of ‘productivity’ in modern academia. Perhaps the most practical expres-
            sion of academia’s Calvinist calling is the expression of reasoned dissent in the face
            of a majority opinion, which is linked to Francis Bacon’s effort to make inquiry as
            game-like as possible, especially via the design of ‘crucial experiments’, a practice
            championed in the twentieth century by Karl Popper. The chapter ends by consider-
            ing the Humboldtian university as governed under what Bruce Ackerman dubs
            (with reference to the US Constitution) ‘revolutionary constitutionalism’ in a way
            that might satisfy Bacon’s original and still unrealized vision of organized inquiry.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The original 1917 context of Weber’s (1958) address to graduate students, ‘Science
            as a Vocation’, was notable in several respects, not least that it coincided with
            Weber’s appointment to one of the first German chairs in ‘sociology’. Weber him-
            self had been trained in law and migrated to economics, which followed the pattern
            of the establishment of economics as a discipline in the German-speaking world.
            (This also explains the distinctiveness of the so-called Austrian School of
            Economics.) But Weber migrated still further to ‘sociology’, a discipline that
            emerged in the German context in explicit opposition to ‘social policy’, a field to which many economists were already contributing as Kathedersozialisten (‘social-
            ists of the chair’), a polemical phrase for academics whose research aimed to pro-
            mote national advancement (Proctor, 1991: Part II). They functioned in a capacity
            that is nowadays associated with think-tanks (Rueschemeyer & Van Rossem, 1996).
            While generally aligned with the Social Democratic Party, their bottom line was
            ‘Germany First’. The ideological fluidity of this stance is perhaps most noteworthy
            in the case of Weber’s great lifetime rival, Werner Sombart, whose tenure as a
            ‘socialist of the chair’ eventually extended to an embrace of National Socialism.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The sense of ‘sociology’ to which Weber thought he was contributing – and
            which he developed as a more general thesis in ‘Science as a Vocation’ – was, logi-
            cally speaking, less contradictory than orthogonal to ‘social policy’. Weber wanted
            a social science that was free not only to challenge official interpretations of social
            phenomena but also to say things about them that did not use policy-relevant catego-
            ries – as when a social scientist crafts and tests an ‘ideal type’ that purports to cap-
            ture a wide range of social phenomena but which does not happen to conform to the
            conceptual framework of the state’s administrative apparatus. Indeed, Weber was
            envisaging a social science that conducts ‘original research’ that generates concep-
            tualisations and data that policymakers are not normally afforded. Of course, the
            state may contract academic sociologists to do policy-relevant research, but Weber’s
            point was that they should not go beyond presenting policymakers with options for
            possible courses of action based on the available data. In short, not only should
            academics retain their independence from the state but also the state should be com-
            pelled not to hide behind academics to justify whatever they decide to do. The
            autonomy of both science and politics needs to be upheld for both to operate as
            effectively as they can, which in turn explains the stress on ‘conviction’ that Weber
            places in his follow-up lecture on ‘Politics as a Vocation’ to a similar graduate stu-
            dent audience (Weber, 2004).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Weber’s emphasis on the mutual independence of science and politics rested on
            his distinct sense of ‘vocation’. The original audience to Weber’s lecture would have
            understood ‘vocation’ (Beruf) to mean one of two things, representing a significant
            parting of the ways in Protestant theology, which are usefully called ‘Lutheran’ and
            ‘Calvinist’. In terms of Weber’s specific discursive context, they correspond to the
            vocation of, respectively, the academic practising social policy and the one practis-
            ing sociology. Let us make the analogy explicit at the outset. The social policy
            academic professes the Lutheran vocation, insofar as s/he is willing to be fully
            incorporated into the state’s conceptual framework, effectively becoming a mouth-
            piece of the state. In contrast, the sociologist professes the Calvinist vocation, inso-
            far s/he engages in a mutually binding contract, each side of which promises to do
            the best they can, notwithstanding the consequences of their actions. Thus, the state
            allows the sociologist free rein in investigating a matter while it stays free to draw
            on the sociologist’s findings selectively for policy purposes. In this way, science and
            politics remain distinct vocations – and not the merged single vocation that the
            Lutheran approach suggests.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Lutheran calling occurs when God or his angelic messenger comes
            unexpectedly to say that you must radically re-orient your worldview, and your recognition of the source of that message leads you to follow dutifully. St Paul’s
            conversion on the road to Damascus is the paradigm case. In his former life, ‘Saul’
            was a leading Pharisee who was more concerned with maintaining the purity of the
            Jewish community in its state of Roman captivity than in fully realizing Judaism’s
            messianic promise. Thus, he persecuted both Jews who sold out to the Romans and
            those followed ‘prophets’ such as Jesus who disturbed the Jews’ limited autonomy.
            Similarly, Martin Luther as professor of moral theology had been more concerned
            with identifying the corruption of the Roman Catholic Church due to its secular
            entanglements than improving the lot of practicing Christians, either in this life or
            the next. Luther’s conversion, though frequently romanticized, amounted to his see-
            ing a way to turn secular authority to divine advantage without leaving the impres-
            sion that the king calls the shots. Rather, the king expresses the will of the people,
            the euphemism for which is ‘church’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, the Calvinist calling is closer to Weber’s intent. Whereas the Lutheran
            vocation involves subordinating oneself to a recognized authority to follow a deter-
            mined path, the Calvinist vocation is freely chosen yet its ultimate destination –
            while equally preordained by God — remains unknown to those who are receptive
            to the call. The Calvinist conversion experience lies in the realization that you are
            free to give yourself over to God, even as you understand that your fallen nature
            means that you are still likely to go astray. Moreover, whatever you do, you eventu-
            ally perish in this world and your posthumous fate is ‘always already’ known only
            to God. The mentality here is close to someone who willingly undertakes a risky
            venture to change one’s life, be it in the guise of a ‘leap of faith’ (e.g., the wager
            with his life that Blaise Pascal, a Calvin-leaning Catholic, claimed to have made) or
            an overseas investment (e.g., the persecuted English Puritans who paid to migrate to
            America to start a new life). The model is St Augustine’s account of his own
            Christian conversion in the Confessions, which takes the form of a gradual elimina-
            tion of alternative philosophies and religions to satisfy a deep spiritual yearning.
            When he comes by chance to a passage in Paul’s Letter to the Romans, the deal is
            sealed because he reads it as encapsulating his life and offering a future course.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Put brutally, the difference between the Lutheran and the Calvinist is that the
            former would have you submit your will to God, who is the vehicle to your salvation
            (assuming the sincerity of your conversion), while the latter would have you own a
            fate that God has ‘always already’ decided, effectively turning your life into a quest
            to figure out the rules of this ultimate game at which you may be either a winner or
            loser. The alternative senses of ‘humble’ implied in the Latin paulus, the source of
            the name ‘Paul’, capture the difference here. ‘Humble’ in the Lutheran sense sug-
            gests obedience to a known superior, whereas in the Calvinist sense it connotes
            taking a risk on an unknown superior. Such words as ‘submission’ and ‘trust’ are
            used in both contexts, since your fate is in someone else’s hands – but they are
            inflected differently. The difference is illustrated in the life of St Paul himself. His
            own moment of conversion fits the Lutheran sense of paulus, whereas the acts of
            conversion that he subsequently made possible to others (e.g., St Augustine) by his
            Epistles fit the Calvinist sense. Analogues from the secular world may prove help-
            ful. Consider the difference between servitude (Luther) and gambling (Calvin). In both cases, your fate is in someone else’s hands but only in the latter case do you
            continue to ‘own’ that fact, whereas in the former case you have forfeited ownership
            to the master. Thus, Luther held that faith is the key to salvation, whereas Calvin
            held that most – but not all — believers (as well as unbelievers) are damned. In
            many ways, this division in sensibility represents a schism that strikes at the heart of
            Christianity’s development after the death of Jesus, which in doctrinal history is
            captured by the adjectives ‘Petrine’ and ‘Pauline’, respectively, named for the two
            early disciples, who promoted the Christian message in rather different ways.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Following the Greek origin of his name, Jesus anointed ‘Peter’ as the rock on
            which the ‘Church’ was built, establishing an authoritative path to salvation through
            the proliferation of individual churches, each staffed with people authorized to
            spread what came to be known as ‘Christian doctrine’. In terms of ‘vocation’, the
            stress is placed on the sending of the call and remaining faithful to the original
            intention upon its receipt. The fact that Peter knew Jesus personally as a human
            being is significant. Indeed, the Roman Catholic Church has always portrayed papal
            succession as a virtual dynasty descending from Peter. Before Luther broke from
            the Church of Rome, he had been very much part of it, which added weight to his
            dissent. Not surprisingly perhaps, Lutheranism did not break with all the institu-
            tional structures and liturgical practices of the Catholic Church; rather it established
            independent versions in parallel to the Church of Rome. In light of that history, it is
            perhaps unsurprising that Frederick the Great’s successors were able to co-opt the
            Lutherans in much the same way as earlier secular European monarchs had co-
            opted the Catholics.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In contrast, Calvin began life not as a theologian but as a lawyer with Humanist
            sympathies. He focused more on the receiving of the call, which necessarily involves
            interpretation and hence the prospect of error, however faithful one wishes to be to
            what has been heard. For the Calvinist, the key feature of Paul’s conversion is that
            he encounters Jesus only as an apparition: He never meets the human Jesus. The
            uncertain nature of that initial encounter colours all that follows: Paul takes a chance
            by reorienting the rest of his life on the assumption that it was the Messiah who had
            addressed him on the road to Damascus. But he always remained free to change
            course, which made his conversion his alone. The strength of his faith was thus
            measured by its durability under various challenging conditions. Paul’s Epistles,
            which were addressed to a wide range of Mediterranean audiences, can be read as a
            set of test cases for the universality of Jesus’ message. The challenge facing Paul
            himself was twofold: Not only must he remain faithful to the Christian message, but
            he also had to persuade each audience in terms that were true to them. This task of
            varying the presentation while retaining the essence would be the hallmark of
            another Protestant lawyer, Francis Bacon, who had been raised as a Calvinist and
            supervised the translation of the first English (‘King James’) version of the Bible.
            Modern philosophy of science calls this ‘hypothesis testing’. And in both the reli-
            gious and the scientific context, the extent to which one needs to change course – if
            at all — after having undergone such a ‘trial of faith’ has remained a central practi-
            cal concern.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I have dwelt on these alternative theological interpretations of ‘vocation’ because
            Weber’s clear preference for Calvin over Luther carries significant epistemological
            and political implications about the source of ‘meaning’ in the academic life. To be
            sure, his student audience might have expected that preference, given Weber’s
            unique focus on Calvinism as the spiritual source of the capitalist economy, bureau-
            cratic governance and other signature ‘rationalizing’ projects across all sectors of
            modern society that presume the prospect of indefinite progress, subject to endless
            measurement and calculation. However, it would have been equally well known that
            Weber saw these developments as placing humanity in an ‘iron cage’ (Mitzman,
            1969: Chap. 3). This is due to the so-called means-ends reversal, which from the
            mid-nineteenth century onward has been frequently invoked to explain a variety of
            unintended system-level social phenomena.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The obvious precedent is provided by Marx’s account of commodification under
            capitalism, which resulted once the status of money altered from a medium of
            exchange to something pursued in its own right. In effect, both the intrinsic and the
            use value of products were replaced by their ‘exchange value’, as defined by the
            money they can command in the market, where the accumulation of money was the
            name of the game. While Marx saw commodification as a negative development in
            the history of humanity, his judgement is not obviously correct (Fuller, 2010a,
            2013). Indeed, the emergence of art has been often explained in terms of a practice
            of originally instrumental value that over time comes to be pursued for its own sake.
            This may be simply because the practice was superseded by more efficient technol-
            ogy, in which case ‘art’ is the name given to technology’s past lives. Alternatively,
            the original practice itself may have been rendered more efficient, leaving time and
            energy to study its intrinsic, or ‘formal’ character. This latter explanation was
            extended by Karl Popper (1972) to account for the existence of ‘objective knowl-
            edge’, notably mathematical objects. Popper clearly regarded this sense of means-
            reversal as a good thing. But how to tell whether a means-ends reversal is healthy or
            pathological?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Popper and Weber were on the same page about most things, but here they parted
            company. Weber was the more sceptical about the means-ends reversal, especially
            when it came to the pursuit of knowledge. This is the context for understanding
            Weber’s apparent ambivalence to ‘positivistic’ vis-à-vis ‘hermeneutical’ approaches
            to the emerging social sciences. Weber welcomed the probabilistic-causal thinking
            that his home field of law was importing from physics at the end of the nineteenth
            century mainly because it encouraged a mode of inquiry that was indefinitely revis-
            able in light of new evidence (Gaaze, 2019). It follows that each scientific paper
            should be treated as no more than a rough draft of some final statement of a truth
            that probably will be penned by someone else. In contrast, the humanities seemed
            to encourage a kind of textual fetishism, perhaps reflecting a theological logos-like
            identification of the ‘spirit’ with the ‘word’, in which certain human works effectively supplement if not supersede the Bible. Stated in such bald terms, the
            view courted charges of idolatry in religious circles. However, the inclusion of reli-
            giously inspired works by Augustine, Aquinas and Dante in the ‘humanistic’ canon
            as it developed in the nineteenth century proved to be an adequate response.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, one countermove to theology’s default textual fetishism is worth
            mentioning. It treats the Bible as a revisable work of science. This move was urged
            upon Thomas Jefferson by his friend, the great Unitarian minister and pioneering
            chemist Joseph Priestley. The result is now called ‘The Jefferson Bible’, a radically
            streamlined version of the New Testament designed to eliminate the superstition,
            error and useless historical detail that impeded the contemporary reception of the
            universal message of Jesus. Notwithstanding its late eighteenth century’s sense of
            the contemporary, the Jefferson Bible continues to resonate sufficiently to be sold
            today in Washington DC museum shops. The Jefferson Bible helps explain the
            emphasis that Weber’s The Protestant Ethic and the Spirit of Capitalism places on
            the role of ‘self-help’ works in translating Biblical teaching into capitalist practice.
            These works, such as Poor Richard’s Almanack by Jefferson’s fellow US founding
            father Benjamin Franklin, similarly highlighted what remains of value in the Biblical
            message after removing the explicitly ‘biblical’ character of the message. Operating
            here was not only a desire to make the Bible relevant but also a keen – perhaps even
            ‘reflexive’ — awareness of the ever-present potential for error in not only our
            reception of God’s word but also its original transmission through human authors
            who were no less fallible.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This profound sense of human fallibility was formative in modern translation
            studies, which took as its cornerstone the diverse strategies used to render the Bible
            relevant to people’s lives across broad expanses of space and time. This in turn fed
            into late twentieth century secular philosophical debates about the ‘indeterminacy’
            and ‘incommensurability’ of meaning, in which both Quine and Kuhn drew on the
            Biblical scholar Eugene Nida (Fuller, 1988: Chap. 5). It helps to explain Weber’s
            rather self-sacrificial and even fatalistic comments in ‘Science as a Vocation’ that
            one’s own research is bound to be superseded over time by others, perhaps leaving
            one’s own contributions forgotten. In this respect, although Weber was generally
            hostile to the capitalisation of academic life, one capitalist concept that he could
            accept is ‘planned obsolescence’. Popper’s falsificationist scientific ethic should be
            sympathetic to this line of argument, but over time Popper came to warmly embrace
            means-ends reversal as a defining feature of knowledge production. Indeed, the
            warmth of his embrace was one that only metaphysics could provide.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Popper came to believe in a ‘world three’ beyond matter (‘world one’) and mind
            (‘world two’) that contains ‘knowledge’ in the sense of the contents of some ulti-
            mate library. Here Popper’s intuition is not so different from that of Bacon, Galileo
            and other champions of the seventeenth century Scientific Revolution, for whom
            nature and the Bible were read as alternative routes to divine truth. Knowledge is
            possible because the world is inherently knowable, but that requires understanding
            the language in which it is written – be it Hebrew, Greek, Aramaic or, for that mat-
            ter, mathematics. Indeed, both before and after the First World War, this world of
            ‘universal documentation’ had acquired renewed vigour as a bulwark for world peace and civilizational maintenance in the hands of such transdisciplinary vision-
            aries as Paul Otlet, Wilhelm Ostwald, Otto Neurath and perhaps most notably
            H.G. Wells (Wright, 2014). Wells famously imagined – as if anticipating today’s
            algorithmically driven integrations of big data sets — that interaction among the
            contents of Popper’s world three would spontaneously generate a world four, which
            Wells dubbed the ‘world brain’ (Wells, 1938; Rayward, 1999).]]>
			</paragraph>
			<paragraph>
				<![CDATA[While Weber might have understood the sentiment behind Popper’s ‘world
            three’, he was loath to fetishize the finished text, which for him epitomized the
            idolatry of modern academic culture. Thus, whereas Popper spent most of his life
            trying to come up with the definitive edition of his first book, The Logic of Scientific
            Discovery, Weber never really seemed to have oriented himself that way, which
            perhaps explains his failed herculean attempt to complete the ersatz magnum opus
            we now call Economy and Society. Popper insisted on conducting arguments on his
            terms (e.g., ‘open vs closed societies’), which meant that unlike his famous students
            Imre Lakatos, Paul Feyerabend, Joseph Agassi, he engaged in relatively few polem-
            ical exchanges. Weber was quite the opposite, as much of his work was developed
            in dialogue if not outright disagreement with various interlocutors. An implication
            of this difference in modus operandi is that Weber’s exact position on topics is much
            more difficult to pin down than Popper’s, since Weber dialectically pivoted in rela-
            tion to the specific interlocutor. He may appear more ‘positivist’ when debating a
            hermeneutician and more ‘hermeneutical’ when debating a positivist. This reflects
            a deeper difference in sensibility, which is related to what may be called an anti-
            monumentalism that Weber took to be central to the pursuit of ‘science as a vocation’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[‘Anti-monumentalism’ alludes to Hegel’s aesthetics, according to which art
            progresses as more of the human spirit is liberated from its material encumbrance.
            In that case, the most primitive stage of art is the ‘monument’, understood as art
            simply standing as proxy for the person, event or idea that it represents. Hegel
            associated this stage with the Egyptian pyramids, which for him literally buried the
            human spirit. The pyramids epitomize a form of art that lacks a life of its own but
            impresses simply as a reminder of — in the case of the pyramids — the pharaonic
            power that originally inhabited it. This was the sentiment that Percy Shelley ironized
            in his poem, ‘Ozymandias’, an updated version of which might take a Borgesian
            turn and be about a scholar whose works fill the shelves of libraries that have fallen
            into disrepair and turned into food for worms. In terms of the famous dichotomy
            that Kant developed in The Critique of Judgement, the pyramids are purposeful, as
            opposed to purposive. Their purpose is transparently available to the observer, who
            simply needs to appreciate how the functioning parts contribute to the overall whole.
            In that respect, the thing’s purpose has been already fulfilled. It has become a ‘take
            it or leave it’ proposition, which marks art at its most primitive stage, a self-contained
            organism whose life and death are two sides of the same coin. In contrast, ‘purpo-
            sive’ implies that the thing’s purpose is not yet – and may never — be fulfilled, yet
            nevertheless the thing displays a sense of purpose, indeed, one that could be resolved
            in various ways.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I shall return to this point at the end of this book, when discussing academic
            performance as an artform, whereby ‘purposeful’ and ‘purposive’ in Kant arguably corresponds to what Nelson Goodman (1968) called ‘autographic’ and ‘allographic’
            forms of art, respectively. In any case, the distinction is related to an ambiguity that
            the Oxford philosopher Gilbert Ryle (1949) noted in the meaning of verbs, many of
            which may be understood as either implicitly referring to an outright achievement
            (cf. ‘purposeful’) or simply to a task in process that may never be achieved (cf.
            ‘purposive’). In my original work on social epistemology, I discussed the implica-
            tions of the ambiguity for the verb ‘translate’, the objective of which has been too
            often presumed to be achieved when in fact it has been merely attempted.
            Deconstruction’s radical break from hermeneutics in the final quarter of the twenti-
            eth century can also be understood as deriving from this awareness – a reassertion
            of the purposive over the purposeful (Fuller, 1988: Chap. 5). And the sense of aca-
            demic freedom defended by Weber in ‘Science as a Vocation’ is similarly predicated
            on academic work being purposive without being purposeful. In the end, I am
            inclined to split the difference between Popper and Weber on the means-ends rever-
            sal, granting Popper the ontological point about expanding the domain of inquiry by
            generating new knowledge objects and Weber the epistemological point about
            inquirers’ deep ignorance of the ultimate significance of their inquiries.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The intellectually honest inquirer really never knows the ultimate nature of his or
            her contribution to knowledge, since its fate is ultimately in the hands of her succes-
            sors, who will determine whether it was ahead or behind the curve of history, or
            simply running with the pack. The image of collective inquiry implied here is not of
            new contributors ‘standing on the shoulders of giants’, as Newton put it, but of their
            being drawn into trying to complete an unfinished work still open to multiple reso-
            lutions, as in the principle of Prägnanz in Gestalt psychology. Moreover, instead of
            embracing fatalism, Weber took this fundamental uncertainty to license a proto-
            Popperian attitude to one’s own knowledge claims – namely, that one should not
            endlessly shore them up but test them to their limits. Needless to say, all this has
            potentially explosive consequences for the normative structure of modern academic
            life, which has come to fetishize the finished product – be it book, article or patent.
            To be sure, the ‘finished product’ is nowadays understood as a commodity whose
            value is determined by national accounting systems, such as the UK’s Research
            Excellence Framework. However, even in Weber’s day, sheer academic industrious-
            ness was valorized to the extent that academics were expected to produce a ‘big
            book’ in their careers. In the Germanized world, such a work was necessary simply
            to qualify for a regular professorship. This book was supposed to be something
            ‘solid’, in the sense of a building block or foundation that would operate as a con-
            straint on what future inquirers can do.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the time of Weber’s famous lecture, the most knowing satire of this sense of
            academic productivity was penned by his younger law colleague, the notorious Carl
            Schmitt (1918). In Schmitt’s Swiftian tale of the tribe of ‘Buribunks’, the type-
            writer — nowadays we would say ‘word processor’ — is venerated as having revo-
            lutionized knowledge production just as the factory system did in the Industrial
            Revolution. Schmitt had in mind all the qualities that Marx associated with com-
            modification, only now given a positive spin as values of knowledge: efficiency,
            definitiveness, cumulativeness. Thus, Buribunkian monumentalism is reflected in the tribe’s core epistemological tenet – namely, that the securest grounds for think-
            ing that something exists is the continual increase in the number of scientific studies
            about it. In recent research policy, a more sophisticated version of Buribunkianism
            has prevailed, albeit without Schmitt’s sense of irony. It focuses on the rate of
            increase in the number of studies, such that a falling rate of scientific productivity
            means that a field is diminishing in utility as a lens for focusing inquiry (De Mey,
            1982: Chap. 7). A consequence of sticking to such a principle is that science turns
            into an elaborate pyramid scheme, whereby fields of inquiry survive only through
            never-ending campaigns to recruit new contributors through doctoral training pro-
            grams (Fuller, 2020a: Chap. 7).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Weber argued that this research-driven monumentalism could be especially
            diabolical if leveraged into the pedagogical arena, where the lecturer may be
            tempted to present his opinions as carrying the weight of history that will propel
            thinking into the future. Here the lecturer’s academic freedom threatens to dominate
            the student’s. In Weber’s day, the most likely suspects of monumentalist pedagogy
            were Marxists, given their claims to a ‘science’ of historical materialism that predict
            capitalism’s imminent collapse as a result of increasing class warfare and the falling
            rate of profit. Against that backdrop, students who might wish to question the
            inevitability of capitalism’s demise risked being charged with ideologically
            motivated resistance. Nevertheless, one academic from the political right, who was
            coming to prominence at the time of Weber’s admonition, pursued a similarly
            monumentalist pedagogy to arguably greater effect on the emerging generation of
            German-speaking students. I refer here to the Viennese political economist Othmar
            Spann, whose academic lectures were public events that invoked the historic
            resources of Germanic culture to project a radical rebirth of a society that had just
            been crushed in the First World War. Moreover, he actually did what many imagined
            Heidegger to have done – namely, to conduct seminars in the forest to foster a
            holistic sense of oneness with others and nature in a common state of being in the
            world. Somewhat surprisingly, given these arguably demagogic gestures, Spann
            supervised the doctoral dissertations of such ideologically diverse and independent
            figures as the pioneering game theorist Oskar Morgenstern, the neoliberal economist
            Friedrich Hayek and the neoconservative political scientist Eric Voegelin.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let us collect the strands of this ‘deep reading’ of Weber’s modus operandi. I
            have focused on the nature of his ‘vocation’, which could mean two different things
            in the wake of the Protestant Reformation. Luther’s meaning stressed a call to obey,
            whereas Calvin’s stressed a call to decide. The former was about submitting oneself
            to a recognised authority, the latter about risking oneself on an unknown outcome.
            Although the Lutheran vocation was arguably the existentially safer option, the
            Calvinist one was truer to the sense of free will that the Bible said humans retained
            even after the Fall of Adam – and which would be instrumental (but in a way that
            only God knows with certainty) in whether one is ultimately saved or damned. In
            ‘Science as a Vocation’, Weber consistently associated the pursuit of free inquiry
            with the Calvinist call. It was pitted against what I have identified as the ‘monumen-
            talism’ of more Luther-driven approaches to the academic vocation, whose respect
            for authority is manifested in an overriding desire to build on it. In this context, Weber feared that the state had not only become the facilitator but also the guarantor
            of academic knowledge production, a proxy for the truth itself. This had been behind
            the decision taken by Weber and several of his German colleagues in 1909 to style
            themselves ‘sociologists’ in contrast to the explicitly policy-oriented ‘socialists of
            the chair’. Moreover, such monumentalism had downstream effects in both the con-
            duct of inquiry and the inquirer’s self-understanding. Academia came to reproduce
            the bureaucratic structure of the state’s administrative apparatus, the civil service, in
            the form of a system of well-defined disciplines that constitute ‘domains of knowl-
            edge’, to which one contributed in the orderly manner satirised by Schmitt in ‘The
            Buribunks’ and later expressed more piously by Kuhn (1970) as ‘normal science’.
            Moreover, this ‘epistemology recapitulates bureaucracy’ mentality was largely jus-
            tified by the ‘Neo-Kantian’ school of German philosophy that was dominant in
            Weber’s time (Collins, 1998: Chap. 13; Fuller, 2009: 81).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Weber’s disquiet over academic monumentalism reflects his antipathy to the
            ‘means-ends reversal’ explanation for the conversion of social practices from tools
            for survival to activities pursued for their own sake. In academia, it involves what
            would otherwise be waystations on the path of inquiry morphing into pieces of
            intellectual real estate – a process I call academic rentiership, which I explore in
            more detail in the next two chapters. In Weberian terms, it amounts to the revenge
            of the Kathedersozialisten, now in the name of ‘expertise’. Experts perform a ‘bait
            and switch’ on the public, invoking their own powers of clarity and control to
            replace the public’s genuinely messy existential concerns with the conceptual neater
            but empirically inadequate cognitive resources they can bring to the problem at
            hand. Thus, the public interest comes to be colonized by unelected representatives
            (Fuller, 1988: Chap. 12). In what follows, I explore the extent to which this de facto
            ‘cognitive authoritarianism’ inhibits dissent, which is the most obvious expression
            of freedom in any collectivized form of inquiry. My touchstone is William Lynch’s
            (2021) Minority Report, a recent comprehensive and fair-minded attempt to give
            epistemic dissent its due in science. Lynch thinks there is something fundamentally
            wrong with how science currently incorporates dissenting voices, yet he does not
            question the legitimacy of the scientific establishment as such. However, as I argue
            below, his version of ‘dialectically engaged science’ is not sufficient to meet the
            challenges posed by the current ‘post-truth condition’, in which distrust in expert
            authority is at an all-time high and expressed by the most scientifically educated and
            most well-informed people in history.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While Lynch (2021) believes that scientific inquiry tends to move in the right
            direction, it needs safeguards so that it is neither distorted by inappropriate private
            interests nor prejudiced against appropriate group interests. Moreover, he recognizes that today’s calls for increased ‘openness’ – that is, allowing more dissent – are
            related to science’s closure of alternative paths of inquiry in its past. In other words,
            the cognitive authoritarianism that characterizes science today is, to a large extent,
            the continuation of past practice. Indeed, a notable contribution of Lynch’s book is
            to revive and extend Paul Feyerabend’s (1979) proposal that the scientific commu-
            nity has a meta-level social responsibility to prevent its own disagreements from
            polarizing into ‘incommensurable’ forms of life that end up feeding into existing
            social divisions. Without this sense of responsibility, Popper’s dream of the ‘open
            society’ gets perverted in the ways we see today, whereby the ‘scientific consensus’
            on a whole host of issues is under siege by dissenters whose refusal of a hearing by
            the establishment results in their own views diverging increasingly from those of the
            establishment, even on matters of fact. Feyerabend realized that the vaunted open-
            minded character of scientific inquiry can easily fold in on itself and become a
            source of cognitive authoritarianism simply by a refusal to communicate with dis-
            senters. The excommunicated are now redefined as the ‘ignorant’. They are ignorant
            in two senses, one direct and the other indirect:
            1. They are formally ignored. Not counting as members of the same community of
            inquirers, their work is no longer cited as contributing to the field’s collective
            body of knowledge. Over time, this stance may be reciprocated, resulting in a
            systematic mutual incomprehensibility that is experienced as ‘incommensurabil-
            ity’. While Kuhn and Feyerabend drew attention to this process in both science
            and art (albeit drawing somewhat different conclusions), the historic precedent
            was set by the ostracism of the Protestant Reformers from the Church of Rome
            in the sixteenth century.
            2. A long-term consequence of this incommensurability is that the two sides
            diverge still further as they develop independently from each other. It results in
            a deformation of perspective on both sides, as the expelled are deprived of access
            to the resources needed to conduct original research, while also being deprived
            of the right to appropriate their shared history to legitimize their inquiry.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Consider an example that Lynch raises and to which I have devoted some attention:
            ‘intelligent design theory’ (IDT) as an alternative to the Neo-Darwinian synthesis
            (Lynch, 2021: Chap. 3; Fuller, 2007c; Fuller, 2008). IDT is basically a scientifically
            updated version of creationism, insofar as it attempts to pursue the line of inquiry
            that is most opposed to the spirit, if not the letter, of Darwin’s theory of evolution –
            namely, that in some sense an overarching ‘intelligence’ is responsible for the origin
            and direction of life. The two theories of evolution competing with Darwin’s in the
            late nineteenth century – Jean-Baptiste Lamarck’s and Alfred Russel Wallace’s –
            were as different from each other as they were from Darwin’s, but both would have
            been friendlier to today’s IDT. Notwithstanding their metaphysical differences,
            Lamarck and Wallace inscribed the human into their accounts of nature as a proxy
            for the divine. In contrast, the feature of Darwin’s theory that has always most dis-
            turbed scientists and non-scientists alike is its presentation of nature as ultimately
            blind to human concerns, control and perhaps even comprehension. This concern
            ran like a red thread through the otherwise consistently robust early defense that Thomas Henry Huxley provided for Darwin’s theory. Nowadays, ‘species egalitari-
            anism’ is Peter Singer’s positive spin on this resolute removal of humanity’s cosmic
            privilege (Fuller, 2006b: Chap. 11).]]>
			</paragraph>
			<paragraph>
				<![CDATA[And so, when Richard Dawkins claims that Darwin’s theory licenses atheism, he
            means that evolution by natural selection implies that there is no cosmic meaning to
            any form of life, including our own. Of course, atheism cannot be logically deduced
            from the premises of Darwin’s theory, yet Dawkins has never said what would lead
            him to reverse his judgement — that is, to countenance an explanation that makes
            room for – if not explicit reference to — nature’s ‘intelligent design’, let alone a
            divine agency informing the process. In short, Dawkins seems to imply that his
            atheism is unfalsifiable. And even though Dawkins would not be able to point to the
            moment when Darwinism already had or will have decisively refuted its IDT rivals,
            he would simply observe à la Laplace that life can be explained without invoking
            any IDT-style hypotheses — albeit only in principle. And that much appears to be
            true – at least for the time being.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Dawkins’ Laplacian move speaks to the Realpolitik of institutional science,
            whereby not only are certain theories dominant at a given time but those theories
            also determine which minority voices are heard in the scientific conversation. Such
            is the totalizing import of what Kuhn (1970) originally called a ‘paradigm’, which
            had so greatly offended Popper and his followers, whose work Lynch substantially
            revisits. It is as if science were the sort of game in which the top teams are permitted
            to interpret their game’s rules in ways that allow them to dictate which other teams
            are fit to play them – and who referees each match (aka ‘peer review’). Larry Laudan
            (1977) tried to put a brave face on this point by equating paradigm shifts with a
            statistical shift in allegiance over a fixed period, about which scientists acquire ‘pre-
            analytic intuitions’, a kind of sixth sense for where the epistemic trail is leading.
            And so, while by no means did all physicists accept Einstein’s relativistic physics
            when he first proposed it in 1905, twenty years later one could not be a front-line
            practitioner without knowing it (Fuller, 1988: Chap. 9). In a similar vein, examining
            the original reception of Darwin’s theory, David Hull suggested à la Kuhn that that
            the twenty-year timeframe corresponded to generational change within the com-
            munity of biologists (Hull et al., 1978). However, the people who performed the
            much vaunted ‘Gestalt switch’ in both cases were simply successors to the chairs,
            editorships and other positions of institutional authority previously held by their
            elders. Their relative youth meant that they lacked a personal investment in the old
            way of thinking and could see certain strategic advantages to adopting the new way.
            But in the end, the story is one of the ‘circulation of elites’, as Vilfredo Pareto would
            say, within a strongly self-contained scientific enterprise, the favored euphemism
            for which is ‘autonomous’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lynch’s general response to this problem is to develop Lakatos’ (1978) strategy
            to expand the pool of competitors in science beyond the dominant paradigm’s natu-
            ral allies and anointed successors. But it requires that potential competitors abide by
            the maxim, ‘No anomaly mongering!’ In other words, alternatives must not simply
            try to undermine dominant paradigm by pointing to its persistent failure to solve
            certain crucial puzzles. In addition, it should also provide testable proposals of their own that aim to either solve or dissolve the puzzles. In that spirit, Lynch opens the
            door to an ‘extended evolutionary synthesis’ that includes such controversial notions
            as group selection and epigenetic inheritance — but not IDT, which he sees as a
            version of anomaly mongering, whereby Neo-Darwinism’s deficiencies are turned,
            ‘God of the gaps’ style, into an argument for an ‘intelligent designer’ behind nature’s
            daunting complexity. As it turns out, Lynch’s demarcation of worthy and unworthy
            anti-Darwinists seems to be shared by the Royal Society of the London, judged by
            the conspicuous presences and absences at its latest attempt to define the frontiers
            of the life sciences — the November 2016 conference ‘New Trends in Evolutionary
            Biology: Biological, Philosophical and Social Scientific Perspectives’. Nevertheless,
            the label ‘God of the gaps’ is just as unfairly applied to IDT as it was to Newton’s
            attempt to justify divine interventions in an otherwise lawlike world. To be sure,
            political proscriptions against referring to divine agency in scientific argument has
            informed such irritating indirectness on the part of both Newton and today’s IDT
            enthusiasts. Those proscriptions had been already written in the Charter of the
            Royal Society, of which Newton was himself an early member. We shall return to
            that foundational scientific institution in the next section but let us first consider the
            long-term consequences of this particular form of repression for today’s scien-
            tific world.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It has helped that the people contributing to the so-called extended evolutionary
            synthesis, whom Lynch presents as exemplary ‘minority reports’ to Darwinism,
            were never so totally expelled from the scientific establishment as to be effectively
            alienated from their own history. Thus, the 2016 Royal Society’s evolution confer-
            ence, while relatively ecumenical in composition, did not rise above the level of a
            troubleshooting exercise for Neo-Darwinism (Zimmer, 2016). While that prospect
            may be attractive to ‘minorities’ who seek acceptance from the ‘majority’, the real
            losers in the scientific game, such as IDT advocates, still didn’t get a hearing. In
            practice, it means that the losers remained disinherited from the past they share with
            the winners. It illustrated Kuhn’s (1970) perceptive observation that a paradigm’s
            ascendancy is marked by a winner-take-all approach to the field’s history, which is
            codified in the textbooks used to recruit the next generation of scientists. This hap-
            pens through a kind of revisionism, in which that common heritage is laundered of
            whatever elements of divergence had contributed to the original schism. Thus, in the
            case of Darwinism’s ascendancy, the fact that many — if not most — of those who
            provided the evidence that Darwin used to forge his original theory, as well as the
            evidence for today’s Neo-Darwinian synthesis, were more metaphysically open
            than Darwin himself to the idea of a cosmic intelligence (often personalized as God)
            is erased from the historical presentation of their contributions — or else treated as
            idiosyncrasies that perhaps even impeded the correct interpretation of their findings.
            (Michael Ruse’s historically informed philosophical underlabouring for Neo-
            Darwinism has been quite explicit on this point.) In short, theology is retrospec-
            tively written out of the official historical record of science, and students are left
            with the impression that all the scientists who contributed to the Neo-Darwinian
            synthesis thought about the world in more or less the same way Darwin did. Kuhn wasn’t kidding when he compared this process to the work of the Ministry of Truth
            in George Orwell’s 1984.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Moreover, the highly technical and largely ahistorical (aka ‘state of the art’)
            character of the terms on which scientific disputes are normally settled has enabled
            the scientific community to protect this history from interlopers who would offer
            alternative histories that draw attention to these moments of closure — as well as
            the repressed alternative futures that might still be worth recovering now. Such
            repressed ‘alt-histories’ are the stuff out of which ideologies have been forged in the
            political sphere, sometimes to revolutionary effect, as when these lost futures pro-
            vide the framework for the revolutionary party’s utopian horizons. Indeed, in the
            nineteenth century, the history of science seemed to be contributing to an alternative
            vision of secularism to that being forged by the emerging nation-states of the
            Europeanized world. Here I mean to include the followers of such disparate thinkers
            as Auguste Comte, Karl Marx and Herbert Spencer. Each in his own way tried to
            leverage the old trans-national framework of Christendom to envisage a world order
            governed by scientific principles instead of ‘natural law’. Of the three, only Comte
            seemed to think that this sense of global governance would still require a ‘super-
            state’ of the sort to which Roman Catholicism had aspired. Marx remained ambiva-
            lent, and Spencer did not think that a state would be required at all in a world where
            humanity’s scientific impulses were channeled into industry rather than warfare.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, as ‘science’, now understood as open yet disciplined inquiry, came to
            be integrated into the teaching regimes of national universities, science’s threat to
            the nation-state as an alternative secular regime gradually receded. By 1870,
            Comte’s ‘positivism’ had become part of the emerging modern academic establish-
            ment; Marxists were still outside the establishment but as its political fortunes
            waned, they too became part of it; and Spencer’s followers have always remained
            somewhere in between, with an increasingly strong footing both inside and outside
            of academia. To be sure, there were various efforts, especially after the First World
            War, to position science as an honest broker of international differences, if not an
            outright consolidator of human interests. The quest for a ‘universal language’ in the
            twentieth century — flying under such banners as Esperanto, Ido, Logical Positivism
            and General Semantics — largely tracks these failed attempts (Gordin, 2015: Chaps.
            4, 5, and 6). And as science became academically domesticated, and ‘scientism’
            became a pejorative for the very idea of science as a political movement, the history
            of science itself became just one more discipline sitting next to the other scientific
            disciplines, a challenge neither to the sciences themselves nor to the larger society
            that sustains science and science shapes. In short, the history of science as a disci-
            pline exemplifies the Orwellian world that Kuhn believed has enabled science to
            thrive. But is it the only world in which science could thrive? And is science thriving
            even in this world?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here is a way to consider these questions: If science itself is to be founded on a
            scientific basis, shouldn’t its own foundations be subject to the same kind of scru-
            tiny as the objects of scientific inquiry? In that case, a ‘science of science’ should be
            as scientific as the science it studies. At the very least, it means that one is concerned
            about the disposition of the evidence that science uses to legitimize itself. And that includes historical evidence. This was the spirit in which Ernst Mach’s Science of
            Mechanics challenged the continued dominance of the Newtonian paradigm in
            physics in the late nineteenth century with his ‘critical-historical’ approach to sci-
            ence (Fuller, 2000b: Chap. 2). In effect, he claimed that the Newtonians had engaged
            in a largely successful two century campaign to suppress countervailing evidence to
            their most fundamental principles. Mach’s larger point – made more by his own
            practice than by explicit argument — was that this sort of critical historiography
            was as much part of the methodology of testing scientific claims as controlled
            experiments in a laboratory. Although Mach expressed his view at a time when
            epistemologically and ontologically sharp differences were being drawn between
            the sciences of Geist and Natur, it would still have been recognizable to his readers
            as re-enacting the prehistory of modernity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[After all, the modern theory of evidence, with its strong forensic orientation, had
            developed in the Renaissance and the Reformation out of concerns that the modern
            Latin-based understandings of a range of authoritative texts from the Bible to the
            Donation of Constantine were false, if not deliberately falsified. Francis Bacon and
            others with legal training applied this critical approach to evidence to testimony
            more generally, resulting in the ‘method’ associated with the nascent experimental
            sciences in the seventeenth century (Franklin, 2001). Moreover, a quick survey of
            Bacon’s ‘idols’ reveals that the mental liabilities targeted by this sense of critique
            cut across the modern science/theology divide. At one level, this should be obvious,
            because the distinction was only in the process of being drawn in Bacon’s day. But
            at a deeper level, Bacon had observed that the ‘idols’ persist, notwithstanding one’s
            formal disciplinary training. In this respect, Daniel Kahneman (2011) and his fol-
            lowers reincarnate the spirit of Bacon in today’s psychology laboratories. Indeed, an
            interesting history of the topic of cognitive limitations and biases could be written,
            starting with Bacon, passing through Marx’s early application of Biblical criticism
            to classical political economy to the early ‘social constructivist’ phase of Science
            and Technology Studies (STS). On the more strictly science side of this trajectory,
            Joseph Priestley’s The History and Present State of Electricity, which inspired
            Faraday and Maxwell, might be inserted alongside Mach’s book, which influenced
            Einstein and Bohr. A couple of books of mine offer some substantial clues about
            how such a history might be executed (Fuller, 1993; Fuller, 2015).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Although he presided over a legal system that was gradually disentangling itself
            from the strictures of Roman law, Francis Bacon remained an admirer of its inquisi-
            torial mode of delivering justice, whereby the judge – not the plaintiff — set the
            terms of the trial. Indeed, this feature of Bacon’s thought attracted Kant, the logical
            positivists and Popper. They liked the inquisitorial mode’s studied neutrality to the
            contesting parties, even in cases where one side had prima facie conceptual or
            empirical advantage over the other. As Bacon himself had astutely realized, such neutrality requires a purpose-made agency — in the case of the ‘new science’, a sort
            of epistemic judiciary capable of settling disputes by virtue of its independence
            from other authorities yet also with the Crown’s backing for its rulings. Bacon’s
            ‘science court’ vision was probably inspired by his role as personal lawyer to King
            James I, who famously justified his own ‘free monarchy’ (aka ‘absolutism’) on
            similar grounds: The King encouraged officials to collect grievances from the peo-
            ple and discuss them in Parliament, but the King’s word would be final on the dis-
            position of cases. In this respect, Bacon conceived of the scientific method as a legal
            procedure for executing the ‘royal prerogative’ regarding epistemic matters (James
            VI and I, 1995).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here it is worth observing that at least since Thomas Aquinas, the role of the
            monarch had been associated with status in a sense more related to ‘state of play’
            (cf. status quo) than social class. The so-called ‘divine right of kings’, on which
            both Aquinas and James agreed, was more about the power to make binding deci-
            sions than any mysterious papal sanctity might hang over them. Bacon’s understudy
            Thomas Hobbes understood this point very well: It is about ending the argument for
            now, drawing a line under the decision taken by the Crown, thereby establishing a
            fact that may be overturned in the future, pending a new challenge. This line of
            thought has provided the metaphorical basis for the modern idea of the state, which
            is responsible for the authoritative readout of the facts concerning its jurisdiction
            (aka ‘statistics’). Thus, the state became the entity in terms of which various social
            groups and political parties jockeyed for position, driving the vectors of policymak-
            ing in the process. In Bacon’s vision, the state would evaluate competing knowledge
            claims by testing them under fair – what we now would call ‘experimental’ – condi-
            tions and keeping track of the outcomes. Bacon held that officers of the state (‘civil
            servants’) should be dedicated to this project because he thought that an ongoing
            forensic inquiry into all domains of knowledge would enhance society’s wealth
            generating capacity — perhaps in the spirit of a stock market, whose price fluctua-
            tions are read as a signal of investor confidence in an array of prospects.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unfortunately, both Bacon and James died only a few years after developing
            their respective ideas about science and politics. The next generation saw England
            engulfed in civil war, resulting in Oliver Cromwell’s brief but potent ‘republic of
            virtue’, which in many respects was more authoritarian than James’ absolutism.
            That train of events greatly reduced the appetite for the sort of ‘absolutism’ origi-
            nally entertained by the Lord Chancellor and his royal patron. The Royal Society of
            London, supposedly Bacon’s institutional legacy, captures this aversion well. Its
            charter constitutes a mutual non-interference pact between science and the state, in
            return for which science makes itself available to the state. Lynch’s (2001) first book
            was devoted to the considerations behind this metamorphosis – if not perversion –
            of Bacon’s vision. In any case, the Royal Society has set the benchmark for ‘autono-
            mous’ science over the past 350 years, not least in the work of Robert Merton and
            Thomas Kuhn – and in so doing, has anchored modern science policy thinking
            (Proctor, 1991).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Note the difference between the Bacon/James vision of the state’s role in
            knowledge production and two alternative visions that have become dominant in the modern era, each of which is intimated in the Royal Society’s charter: (1) The state
            itself is in the business of funding original research. (2) The state lets research rivals
            decide matters amongst themselves so long as they don’t challenge the state. Both
            violate what might be called the state’s ‘concerned neutrality’ that was central to the
            Bacon/James vision, and which John Rawls famously updated in terms of ‘justice
            as fairness’. The political scientist Roger Pielke’s (2003) phrase ‘honest broker’ for
            the science policymaker is worth revisiting in this context. The point is often missed
            because the popular image of ‘absolutism’ is one of monarchical willfulness. And
            while the image may fit its more decadent expressions, absolutism had meant to
            position the monarch as ultimate referee, the final court of appeal. Thus, when King
            James wrote his own job description in 1598 as a ‘free monarch’, he had in mind
            settling differences between parliamentary factions that would otherwise never be
            settled for very long because none of the factions could command the enduring
            respect and trust of the others.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Note that settling a match by an independent referee is neither by mutual
            agreement of the teams (that would be ‘match fixing’) nor by someone who might
            be a player in a subsequent match (as in ‘peer review’). Moreover, the referee’s
            declaration applies only to the match in question and the season of play of which it
            is a part. In that respect, a ‘sunset clause’ is built into the relative standing of the
            competing teams based on a set of referee decisions: The team that tops the league
            table in one season still needs to prove itself from scratch next season. And while it
            is common to speak of consistent team performance over several seasons in
            ‘dynastic’ terms (e.g., Manchester United Football Club under Alex Ferguson),
            those terms are strictly metaphorical, given that match-based tournament games
            such as football are designed to encourage rivals to study their own and their
            opponents’ vulnerabilities in order to improve their own performance in future
            matches.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Now, couldn’t science operate this way? If so, what would that entail? There are
            certainly enough precedents in the history of politics and law to enable it to happen.
            I alluded to the ‘sunset clause’ in much legislation (also much favored by Plato),
            which in the context of science would remove the presumption of indefinite con-
            tinuation to an established knowledge claim (Kouroutakis, 2017). In other words, a
            knowledge claim’s epistemic status would expire, unless it is somehow renewed.
            Theories of democratic politics that take seriously the fallibility of collective
            decision-making have come up with various ways of institutionalizing this idea. The
            most familiar and most game-like procedure involves the constitutional requirement
            of periodic elections, regardless of the current regime’s state of play. Yes, this would
            mean that Newton’s laws would come up for regular review. But that is not as unrea-
            sonable as it may first sound, considering the extent to which scientific research
            design has changed over time; hence the difficulties that arise when one tries to
            replicate an ‘established’ result from many years ago.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Often it seems that scientists today would never have arrived at significant
            findings made in the past, had they been working with today’s methods and
            standards. This is an epistemologically deeper issue than what surrounds the
            increasingly publicized cases of replication failures in science. In those cases, the problems turn on fraud and other malfeasances associated with ‘research ethics’
            (Fuller, 2007a: Chap. 5). However, here we’re not talking about assertions at
            variance with the scientist’s own actions, to which they might be held accountable
            in the present, but rather about assertions at variance with the actions of previous
            scientists, who are no longer around to participate in a reckoning where the standards
            of scientific evaluation themselves had shifted since the actions were first taken.
            While not obviously right or wrong, this slippage in meaning and perhaps even
            verifiability routinely happens under the radar of most scientists and the society
            supporting them. After all, Newtonian mechanics was ultimately overturned, at least
            in part, as a result of improved experimental techniques that brought the theory’s
            anomalies into such focus that they could no longer be ignored or deferred. While
            spokespersons for the dominant scientific paradigm are quick to admit their
            ‘fallibility’, such declarations often seem hollow, if not passive-aggressive, without
            regular testing of the paradigm’s main findings and assumptions. In this regard,
            science might learn from organized team sports on matters of methodological
            vigilance, in order to ensure that minority voices truly have their day in court.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Explaining the emergence and duration of the world-religions, Max Weber (1963)
            famously argued that the great problem of institutionalization is how to manage the
            transition from prophet to priest, or what he called the ‘routinization of charisma’.
            In other words: How does a body of diverse followers stay true to the message once
            the inspirational leader holding them together has left the world-historic stage? One
            way, of course, is for some of the leader’s closest followers to take control and then
            impose ‘rule of law’, which streamlines the chain of command from the rulers to the
            ruled and allows this newly formed corporate entity greater scope for expansion.
            Weber believed that over time this process came to be rationalized as ‘bureaucracy’,
            which itself became self-reproducing, raising all the familiar questions about ‘spirit’
            vs ‘letter’ of the law, understood as the original leader’s message.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the history of Christianity, the process just described fits best the history of the
            Roman Catholic Church – and perhaps explains the inevitability of the Protestant
            Reformation. The Protestants offered a different vision of institutionalization, based
            on an alternative history of Christianity. Here Jesus’ death is marked by his follow-
            ers fully embodying his message as a kind of personal discipline, without any need
            for an elaborate clerical hierarchy. To be sure, such a broadly ‘democratic’ under-
            standing of the faith resulted in the fractious ‘denominational’ history of
            Protestantism, whereby churches formed simply based on common readings of the
            Bible, typically as a result of having broken away from some other church. In this
            context, the difference between prophet and priest is completely erased; hence the
            adventurous nineteenth century US attempts to ‘rewrite’ the Bible by, say, Mormons, Christian Scientists – and even Thomas Jefferson, as we saw earlier. But this policy
            of ‘endless renewal’ or ‘permanent revolution’ tends to result not in an improved
            reorganization along an agreed standard but the proliferation of organizations, each
            operating by its own standard. And so, what began as an attempt to purify Christianity
            of its worldly corruption arguably only served to erect a Tower of Babel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is against this backdrop that Yale law professor Bruce Ackerman (2019)
            proposes to reconcile the above two visions of institutionalization – one elitist, the
            other populist – in terms of revolutionary constitutionalism. According to Ackerman,
            the US exemplifies a successful revolutionary constitution. I shall explore the extent
            to which the modern university might be understood in a similar light. But Ackerman
            himself aims to capture the attitude of the US Founding Fathers towards the remark-
            ably resilient form of republican government they designed. Other revolutionary
            movements across the world have tried to follow its example, with varying degrees
            of success. In Ackerman’s telling, key to the Founding Fathers’ success was that
            they planned for their own obsolescence. Unlike the great prophets (or, for that mat-
            ter, absolute monarchs), they did not, so to speak, ‘die in office’. The Founders tried
            to ensure that the people identified neither with them nor their anointed successors
            but with the Constitution as such, resulting in a ‘government of laws, not men’, to
            recall John Adams’ resonant phrase. George Washington’s refusal to continue as
            President after having served two terms in office symbolized this sensibility at the
            start of US political history. In today’s terms, the people had to ‘own’ the Constitution.
            The Constitution was not a dispensable document that could be easily changed by
            the ‘will of the people’, but more like the rules of a game that outlasts its players, no
            matter how illustrious they might be. Moreover, the players derive their own luster
            from having agreed to play by those rules, albeit in their own distinctive ways.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The theological model for the planned obsolescence of the supreme leader is
            Deism, a kind of post-Protestant secular religion that animated Enlightenment
            thinkers generally, including the US Founding Fathers. To be sure, Deism was
            treated with a certain amount of derision in its day by Voltaire for conceiving of the
            deity as deus absconditus, God fleeing from the scene of Creation, as if it were a
            crime. But truer to the spirit of Deism is to say that it is the death of God by his own
            hand – a thought that no doubt had inspired Nietzsche. Put in cooler academic
            terms, Deism was about a radical revision of the idea of ‘God the Father’, a revolt
            against ‘paternalism’ in the strongest possible terms. Keep in mind that the default
            legal position positioned the father as ‘lord of the manor’, typically based on having
            built the family home or extended it significantly, with presumptive authority over
            all in his domain, especially offspring, until he dies; hence the prima facie legiti-
            macy of the ‘divine right of kings’. Eighteenth century legislation made it easier for
            at least male offspring to escape paternal authority by leaving home and effectively
            establishing a new identity based on their own actions. Thus, Kant’s famous defini-
            tion of Enlightenment focused on humanity’s release from ‘nonage’ — or ‘infantil-
            ism’, as we now say. However, Deism’s conception of divine paternity reversed the
            polarities of this liberal legal tendency: Instead of humans leaving the divine house-
            hold, which could invite atheism, God himself vacates the premises, leaving his
            human offspring to occupy it in perpetuity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This point helps to explain the repeated appeals that the Founding Fathers made
            to ‘natural law’, or the ‘higher law’, as it is sometimes still called in US constitu-
            tional circles. As the late Harvard science historian I.B. Cohen (1995) observed, the
            relevant conception of ‘natural law’ was Newton’s, not Aquinas’. In other words,
            the emphasis was on the ‘law’ rather than the ‘natural’ part of ‘natural law’. The
            Founding Fathers understood ‘natural law’ not simply as a normative order that
            conforms to the nature of things as they normally are, but more expansively as
            God’s great infrastructure project, the comprehensive outworking of a supreme will
            for the purpose of habitation by others who have yet to exist: the realm of the pos-
            sible rather than the actual. The design of the US Constitution was meant to mimic
            the Newtonian world-system, understood as an autonomous machine that can func-
            tion no matter who occupies the various designated roles. We normally think about
            this feature in terms of the Constitution’s intricate construction, with its famed ‘sep-
            aration of powers’ and ‘checks and balances’, which is designed to withstand the
            vast range of interests that might drive not only those who occupy the offices of state
            but also the citizenry at large. And in a democratic republic, the differences between
            these two groups are meant to dissolve over time, as people as a whole develop a
            greater capacity for self-representation. In this respect, the US has made a great suc-
            cess of the Founding Fathers’ vision, insofar as the dispositional difference between
            members of the US Congress and those they represent is now much narrower than
            members of the UK Parliament.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, more subtle but equally important is the complementary effect that the
            Constitution has in disciplining people’s interests, which is not so different from
            how the rules of a well-designed game discipline the players, in terms of molding
            their ‘native’ capacities to the ‘artificial’ field of play. There are several ways of
            thinking about this. In a Freudian vein, we might think in terms of sublimation.
            Thus, the Founding Fathers themselves placed great store by hypocrisy as a political
            virtue, a point driven home in recent years by Jon Elster (1998) and David Runciman
            (2008). A more provocative way of thinking about the matter is to consider ‘method
            acting’, whereby an actor draws on his or her own personal experience to play their
            assigned role in a drama. Konstantin Stanislavski, who introduced this approach to
            theatre in the closing years of the Russian Empire, came to be celebrated in the
            Soviet Union for implying that, in principle, anyone could play any role if they can
            pour themselves entirely into the world dictated in the dramatic script. In practice,
            it is about the actor adopting a frame of mind that enables him or her to speak the
            words in the play ‘authentically’, so that the audience believes that the actor could
            have been the author of the playwright’s words (Benedetti, 1982).]]>
			</paragraph>
			<paragraph>
				<![CDATA[If this entire way of putting matters seems strange, it is because revolutionary
            constitutionalism drives a wedge between authenticity and sincerity in our moral
            psychology (cf. Fuller, 2020a: Conclusion). Epistemologically, the difference boils
            down to whether we believe what we say (authenticity) or we say what we believe
            (sincerity). The Deist shift occurs when authenticity is privileged over sincerity.
            Sincerity presupposes that belief formation happens by some means other than our
            capacity for self-representation. It is anti-hypocritical. Thus, a sincere person’s
            belief tends to be seen as spontaneous and self-certifying, that is, as something they are compelled to believe. ‘Evidence’ has functioned in modern epistemology as the
            catch-all name for that independent source of belief formation. On this view, you
            can’t intentionally make yourself believe something, or even simply decide to
            believe something. On the other hand, one might see this whole way of understand-
            ing belief formation as little more than alienating people from their capacity for
            judgement. Thus, there is equally a line of modern thinkers – Blaise Pascal and
            William James, most notably – who predicate belief formation on the capacity for
            self-representation (Fuller, 2020a: Chap. 12). It is pro-hypocritical. These thinkers
            are about ‘walking the talk’, which is close to the normal understanding of ‘authen-
            ticity’. If you act like you believe something, you end up believing it – ‘owning’ it,
            in that sense. Here the human is envisaged not as a receptacle but a forger of belief –
            an imperfect version, if you will, of the deity who demonstrates his own divinity by
            creating through the word (logos). To be sure, many senses of ‘imperfection’ are
            implied here. Unlike God, humans are not in full control of the consequences of
            what they say, due to their plurality, natural equality and individual finitude. Conflict
            of wills is therefore inevitable. This is the context in which a ‘government of laws,
            not men’, both in the natural and social worlds, truly matters.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the history and philosophy of science, the most obvious analogue to
            revolutionary constitutionalism is the scientific method, especially in the spirit
            originally articulated by Francis Bacon. As personal lawyer to King James I, Bacon
            regarded universities as not simply standing apart from the Protestant monarchy but
            arguably more faithful to the Pope than to the Crown, the result of which might
            inhibit knowledge production in the emerging ‘British’ national interest. Thus, a
            state agency should be entrusted as supreme neutral arbiter. It would be a kind of
            judiciary, but on the model of an inquisitorial rather than an adversarial legal system.
            In other words, this scientific judiciary would actively prosecute cases on its own
            initiative rather than simply wait for plaintiffs and defendants to self-identify (Fuller,
            2007a: Chap. 5). In any case, intellectual litigants would need to translate their
            contesting knowledge claims – which were often informed by fervently held
            alternative readings of the Bible — into the regime imposed by a decision procedure
            that would be itself neutral to the contestants but capable of resolving their dispute
            to at least the temporary satisfaction of all sides – at least until the next contest.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Bacon’s original proposal still defines the baseline understanding of the scientific
            method that science students are taught and about which philosophers of science
            continue to theorize. But more relevant for our purposes is that Bacon wanted his
            neutral procedure to exert supreme epistemic authority, so that everyone would
            know the standing of their respective epistemic claims at any given moment, as well
            as the terms under which that standing could be contested, presumably with the
            hope of improvement. It is surprising that commentators have not made more of the
            game-like character of Bacon’s original proposal, though more modern attempts to
            flesh it out in terms of probability theory, such as Bayes Theorem, capture its flavor.
            For his own part, Bacon spoke of the moments of contestation as ‘crucial experi-
            ments’. This was at a time when ‘experiment’ simply meant ‘experience’. The over-
            all meaning of Bacon’s phrase was carried by ‘crucial’, which suggested a
            crossroads, understood as a metaphor of Jesus’ Crucifixion. The idea was that one had to take a stand, regardless of the consequences. In the case of science, the need
            to take a stand is regularized. The paths presented at any given crossroads may be
            mutually exclusive, but the journey itself involves many crossroads.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is an interesting ‘deep theology’ about this way of thinking about the
            science-religion relationship, especially in light of Bacon’s sponsorship of an
            ‘authorized’ (‘King James’) English version of the Bible and his suspicions about a
            university sector that still oriented towards Rome, notwithstanding the Crown’s
            separation from the Pope. In this context, one might regard crucial experiments as
            the mechanization of divinity, insofar as the prospect of genuine collective epistemic
            advance is tied to the adjudication of specific knowledge claims (Fuller, 2021c).
            (Think about the role that computers often played in early science fiction as arbiters
            of competing human claims and resolvers of human doubt more generally.) But
            more relevant to the current argument is exactly how Bacon envisaged the enforce-
            ment of the supposedly ‘binding’ nature of the results to these crucial experiments.
            Part of the answer lay in the reasoning that Bacon himself offered to justify what we
            continue to recognize as the scientific method. Nowadays we associate this reason-
            ing with a kind of ‘empirical’ or even ‘inductive’ rationality — though Karl Popper,
            the Second Coming of Bacon, rightly balked at ‘inductive’, given what the term had
            come to mean by the time he wrote in the twentieth century.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In any case, clearly more was required – but Bacon himself didn’t provide it.
            Death spared Bacon a mass revolt against the son of the monarch he had served.
            This ‘civil war’ had been fueled by warring Protestant factions, all suspicious of
            each other and foreign Catholic designs. However, Bacon’s private secretary
            Thomas Hobbes survived the conflict and witnessed the Royal Society of London’s
            attempt to institutionalize Bacon’s legacy. Hobbes famously was refused member-
            ship after his various objections to its founders’ interpretation of Bacon’s legacy,
            which together amounted to an outright rejection of the terms of the Royal Society’s
            famed Charter (Shapin & Schaffer, 1985). Hobbes objected not only to the self-
            certification of the Royal Society’s knowledge claims – what we now call ‘peer
            review’ – but also to the Royal Society’s corporate autonomy from the newly
            restored Crown. Notwithstanding its official eschewal of religion, the Royal Society
            was setting itself up as a ‘new Rome’, as in the Catholic Church. Hobbes specifi-
            cally feared that the Royal Society would establish an alternative source of authority
            to the Crown that occluded the absolute power that is required to enable a society of
            free individuals to flourish as a whole. For his own part, Hobbes notoriously drama-
            tized the requisite ‘absolute power’ as a collectively manufactured deity — the
            ‘Leviathan’ — to whom the society’s members grant a monopoly of force in
            exchange for the requisite sense of order for free play. Of course, Hobbes’ succes-
            sors, including Locke, Rousseau and the US Founding Fathers, went on to embody
            this absolute power in various forms. The distinctiveness of the US Constitution lay
            in the explicitness of the Founding Fathers, who clearly wanted to make the ‘social
            contract’ binding for future generations in perpetuity. Theirs was a bureaucratically
            distributed Leviathan.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The phrase ‘popular sovereignty’ captures only part of the underlying idea of
            revolutionary constitutionalism, which helps to explain the default feeling among American citizens that they ‘own’ the Constitution. Yes, the people are the
            Constitution’s ultimate upholders and underwriters. However, the Constitution’s
            specification of their permissible interactions in various individual and collective
            roles suggests something closer in conception to a dramatic script. (The original
            counting of a slave as 3/5 person should be considered in this light.) Of special note
            for our purposes is the care taken to specify how one might enter and exit the roles
            of designated characters in the constitutional script. Of course, in normal theatre,
            the same script may be performed by different actors in different venues, but the
            script does not specify how the actors are chosen or replaced – simply that they
            should perform their roles according to the brief character sketches in the play’s
            notes. To be sure, such non-scripted factors as the popularity or virtuosity of particu-
            lar actors may determine their longevity in theatrical roles. However, the US
            Constitution is written to ‘internalize’ these ‘externalities’, so to speak. In effect,
            you are meant to fully inhabit the role to which you are assigned by the Constitution
            as both citizen and potential office holder. It is a game you play for life with your
            fellow co-habitants, passing in and out of different roles. Thinking about this self-
            transformative character of the US Constitution as an endlessly unfolding drama (a
            ‘soap opera’?) on the world-historic stage has been relatively easy for successive
            wave of immigrants motivated to lose their old identities in favor of new ones. This
            sensibility was epitomized in the Progressive era image of the US as the world’s
            ‘melting pot’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Two countervailing considerations must have weighed on Hobbes’ appraisal of
            the Charter of the Royal Society. On the one hand, there were Plato’s original objec-
            tions to the dramatists, given that their words have the potential to generate an alter-
            native reality that may result in social unrest, especially if the plays are performed
            well. Indeed, this was what Plato meant by calling the playwrights and actors
            ‘poets’, after poiesis (i.e., ‘produce out of nothing’). A former student of the
            Cambridge science historian Simon Schaffer, Adriano Shaplin, vividly drew out this
            point in his 2008 play The Tragedy of Thomas Hobbes, which portrays the Royal
            Society as a theatre that gave the King private showings of its latest productions. In
            this context, Hobbes appears as a carping critic incapable of appreciating experi-
            mental demonstrations as entertainment of potentially universal appeal. On the
            other hand, in a more Calvinist vein, Hobbes may have alighted on the fact that God
            deals most directly with humans as independent agents in both the Old and New
            Testaments through a series of ‘covenants’, which Hobbes himself secularised in his
            own political philosophy as the ‘social contract’. Indeed, he went further, turning
            the social contract into the crucible in which the deity is forged. From this stand-
            point, the Royal Society’s Charter camouflages drama’s potency in the guise of a
            covenant, very much in the style of an established church that remains formally
            separate from, yet unaccountably influential over, the Crown. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[As a point of jurisprudence, Hobbes’ strategy for dealing with the threat posed
            by the Royal Society was straightforward. Its pseudo-covenant with the Crown
            would be replaced by a super-covenant in which the Crown is formally legitimized
            as supreme authority, in virtue of which the Royal Society would then be licensed.
            In short, England (now UK) would be endowed with what it still lacks, namely, a written constitution from which the rights and duties of individual and collective
            bodies are derived. To be sure, the US Constitution aimed to fill that void for the
            original thirteen British colonies in North America. In Hobbes’ case, more justice
            would have been done to Bacon’s legacy had Bacon’s prototype for what became
            the Royal Society, ‘Solomon’s House’, been incorporated as a state agency in the
            social contract. Such an agency would serve two main functions. One would be to
            incubate vanguard projects that promote society’s long-term security and prosper-
            ity. The closest contemporary analogue is DARPA, a product of the US Cold War
            defense strategy (Fuller, 2020b). The other function would be to serve as a kind of
            ‘supreme court’ of knowledge claims, which in the late twentieth century was
            floated on the international level as a ‘science court’. The Royal Society turned out
            to be neither of these, yet it has had an anchoring effect of the Royal Society’s
            Charter on the subsequent institutionalization of science, especially once it was
            incorporated in the university curriculum in the nineteenth century.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The key normative principle of the Royal Society’s Charter that troubled Hobbes
            was its declaration of mutual non-interference with the affairs of state, what is now-
            adays regarded as one of the earliest modern statements of the ‘neutrality’ of sci-
            ence vis-à-vis matters of politics and morals. In practice, it left the door open for the
            Crown to be influenced by the Royal Society, should the Crown be so moved. In The
            Tragedy of Thomas Hobbes, Shaplin presents this prospect by casting the members
            of the early Royal Society as a new species of actors and dramaturges, who are keen
            to persuade the King that experimental demonstrations are the supreme form of
            entertainment. In other words, when the King is not directly attending to the affairs
            of state, his mind should be preoccupied with the diversions of the laboratory. For
            Hobbes, this amounted to an incentive for the King to become beholden to an alter-
            native reality, which had also characterized his understanding of the Church’s
            influence.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Three components of a revolutionary constitution are relevant to the
            reconfiguration of the university: (1) The Constitution reigns supreme, resulting in
            a ‘government of laws, not men’. (2) The Constitution is a social contract across
            generations, which the parties collectively ‘own’ in the strong sense of its being
            constitutive of their identities and the source of their sovereignty. (3) The Constitution
            is elaborated as an all-encompassing exercise in role-playing – as in a game or a
            drama — that potentially never ends but nevertheless possesses a clear sense of how
            one enters and exits a role, and one’s standing within in it at any given time. In
            meeting these three conditions, the revolutionary constitution is ‘purposive’ without
            being ‘purposeful’ in Kant’s sense. It is more about perpetuating certain productive
            attitudes and relations in the citizenry than organizing them to produce a specific
            end, let alone a utopian social order.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The key to regarding the university as a revolutionary constitution is to take
            seriously Francis Bacon’s attempt to de-center the medieval universities with an
            alternative institution of higher learning and knowledge production. Of course, the
            version of Bacon’s vision that succeeded, the Royal Society of London, anchored
            the peer-reviewed, paradigm-driven sense of science that remains dominant today.
            However, Bacon’s private secretary Thomas Hobbes envisaged this alternative institution arguably more in Bacon’s own spirit – namely, as one directly embedded
            in the terms of the social contract. He saw the nascent Royal Society as potentially
            providing an alternative source of authority to the Crown, very much in the perni-
            cious spirit of the established church that would sow the seeds of revolt in the future.
            Hobbes’ objections to the Royal Society can be turned into the basis for thinking
            about the university as a ‘revolutionary constitution’, which is exactly what
            Humboldt did.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Abstract ‘Utopian knowledge capitalism’ signals a re-engagement with the
            spectrum of ‘utopian socialism’ from Saint-Simon to Proudhon that Marx disparaged
            to make room for himself. The US economist Philip Mirowski correctly observes
            that Marxists fail to realize that capitalism taken to its logical conclusion – as
            neoliberalism does – deconstructs the ontological status of ‘knowledge’, just as it
            does ‘labour’. Both are just forms of capital in what neoliberals imagine to be a
            dynamic market environment, whereby technological innovation plays a key role as
            ‘creative destroyer’ of capital bottlenecks that arise from the ‘inheritance’ of wealth
            at various levels, resulting in capital being concentrated, sometimes even
            monopolized. The pre-Marxist ‘utopian’ forms of socialism differed amongst
            themselves whether the relevant sense of ‘organization’ needed to ensure capital’s
            endless mobility should come from above (Saint-Simon) or below (Proudhon). This
            chapter explores what is worth reviving in these relatively suppressed strands of
            socialism’s history, concluding with a reassessment of the Popperian tradition in the
            philosophy of science, which can be understood as exploring a second-order version
            of the same intellectual bandwidth as utopian socialism. In this context, the
            significance of the ‘Gestalt switch’ is highlighted. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the spirit of pruning the tree by sawing off the limb on which one sits, US
            economist Philip Mirowski (2019) has subtly eviscerated the many left-inspired
            critiques of neoliberalism for misconstruing the movement’s true epistemological
            basis, which has effectively ‘rendered socialism unthinkable’ – at least as far as
            Mirowski is concerned. Of course, Mirowski continues to regret neoliberalism’s
            apparent triumph just as much as the critics that he criticizes. Characteristically,
            much of his argument is conducted by ‘persuasive definition’, in which he couches
            in explicitly pejorative terms quite incisive accounts of how neoliberalism works, perhaps to convey a greater sense of conspiracy on the part of the ‘neoliberal thought
            collective’, as Mirowski likes to say, than is necessary. Indeed, one might recast his
            account in a way that makes the neoliberals appear closer to how they have envis-
            aged themselves, namely, as consistent defenders of human freedom, taking the
            battle to the final frontier of unwarranted restriction of access: knowledge itself.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Drawing on the full range of twentieth century capitalist economists – from the
            Austrian school to the neo-classical and even Keynesian schools – Mirowski (2002)
            had earlier identified a tendency to treat the ‘market’ as an all-purpose information
            processor. The motivating intuition is something like the following. Say I produce a
            good – by whatever means, for whatever purpose. What exactly is its value? The
            market answers this question in terms of the price at which I’m willing to part with
            it, which happens when I assume the role of seller and then attract a buyer. At the
            same time, such transactions contribute to a deeper discovery process, as their pat-
            tern reveals the true purpose of the good. After all, some goods are directly con-
            sumed, some contribute to other transactions, and some are simply ‘banked’ or in
            some other way treated as an asset. Moreover, some goods have staying power in the
            market, whereas other goods are soon superseded.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The various capitalist schools differed over the market’s reliability to perform
            these functions, left to its own devices. While the Austrians trusted the market’s reli-
            ability, the Keynesians did not. Indeed, the latter justified the need for the state as
            the producer of certain ‘public’ goods based on the unlikelihood that the market
            would naturally produce them. And while the Keynesian account remains a popular
            justification for state control over health, education and utilities, it is nevertheless
            telling that it trades on a circular definition of ‘public’, which is simply whatever
            cannot be produced by the privately driven forces of the market. Strictly speaking,
            ‘public’ in the Keynesian lexicon is a residual term based on which the state is then
            leveraged as a ‘God of the gaps’ to make up for ‘market failure’. Thus, like the other
            capitalist schools of economics, Keynesianism treats the market as foundational to
            the ‘business of life’, as the English founder of neo-classical welfare economics,
            Alfred Marshall had already said in 1890. ‘True socialists’, as Marxists have always
            regarded themselves, are sensitive to this point, which helps to explain their increas-
            ing suspicion of the ‘welfare’ preoccupations of ‘social democrats’ as the twentieth
            century wore on.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The question of the market’s default standing in economic life arguably reached
            its head in what came to be known as the ‘socialist calculation debate’. This debate
            served to define neoliberalism’s steadfast opposition to a conception of ‘socialism’
            that always threatened to spill over into Marxism (Steele, 1992). The original site
            for this debate were two politically opposed circles – one socialist and one liberal –
            both of which were somewhat outside the academic establishment in post-First
            World War Vienna. The socialist circle, associated with logical positivism, was led
            on socio-economic matters by Otto Neurath, while the liberal circle, associated with
            Austrian economics, was led by Ludwig Mises, though a more junior member,
            Friedrich Hayek ended up having a more enduring impact.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Neurath held that a comprehensive scientific grasp of society’s central tendencies
            allowed the state to administer to people’s needs more accurately and efficiently than a simple reliance on the fallible judgements of individuals subject to market
            forces. Neurath called his position ‘Neutral Marxism’ to indicate that he accepted
            Marx’s socio-economic analytic framework while distancing himself from Marxist
            revolutionary politics (Proctor, 1991: Chap. 9). For his part, Mises updated the
            Austrian school of economics for the twentieth century by stressing science’s inabil-
            ity to capture in any overarching fashion the spontaneously formed intersubjective
            agreements that constitute social relations generally – and which the market epito-
            mizes. Mises and his circle – including Hayek – came into economics through law,
            and so were mindful of the role that privately undertaken contracts played in the
            modern era as a countervailing force to the classical idea of law imposed through
            legislation, either by a sacred or secular sovereign. Contracts (aka ‘prices’ in a
            strictly economic context) historically generated greater levels of freedom, and in
            turn productivity, to which legislators had sometimes responded with authoritarian
            measures, resulting in resistance if not outright revolution. The Mises Circle did not
            want science to be the basis of a ‘New Leviathan’, which is what they feared was
            happening in the newly formed Soviet Union (Hayek, 1952).]]>
			</paragraph>
			<paragraph>
				<![CDATA[As this brief sketch suggests, the two sides of the socialist calculation debate
            read the lessons of the modern era as presenting rather opposed routes to improving
            the human condition. Whereas Neurath saw science as coming to dominate indi-
            vidual decision-making, once people appreciated the improvements to their lives
            that resulted from such delegation to expertise, Mises saw the law’s increasing lib-
            eralization as granting freedom in ever more aspects of life, resulting in people
            being more comfortable with taking risks. While Neurath believed that the state
            could pre-empt unnecessary error, Mises regarded the state as potentially pre-
            empting necessary liberty. Knowledge is implicated in both cases as enabling the
            state to be a vehicle of what I have called modal power (Fuller, 2018a). This theme
            is of course familiar from the work of Foucault, albeit in his characteristically dis-
            engaged way. However, for our purposes, the socialist calculation debate highlights
            the original Platonic point that an emphasis on ‘knowledge’ leads to a concentration
            and asymmetry of modal power – that is, the state is allowed to restrict individual
            freedom to keep the collective efforts of society focused – whereas a de-emphasis
            leads to a more symmetrical distribution of modal power. Plato regarded the latter
            state as doomed to chaos, However, I shall present it below more positively as ‘lib-
            eral interventionism’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When Mirowski rails against neoliberalism’s appropriation of ‘deconstructive’
            and ‘postmodernist’ epistemological tropes, he is referring to just that sort of level-
            ling, which even without the benefit of these trendy French movements, Mises had
            already proposed was better than Neurath’s top-down approach to organizing the
            economy. Here too Mirowski is wise to observe that Marxism’s blindness to this
            move results from its more ontologically grounded, or ‘realist’, conception of
            ‘truth’, which sees knowledge as aligned not merely with specific power relations
            but with material conditions that pre-exist the intentions of the particular agents
            who constitute a field of power. Mirowski also correctly understands Marx’s great
            stress on human labour as a factor of production in terms of this heightened meta-
            physical orientation, one shared by Aquinas and even Locke – but which Mises and the neoliberals, as true moderns, resolutely do not share. But as we shall see below,
            contra Mirowski, to acknowledge the metaphysical blinders on Marxism’s under-
            standing of capitalism is not to indict all forms of socialism. In fact, Marxism is the
            only form of socialism that has openly engaged in ideological warfare against capi-
            talism, even though – in theory at least – it too regards socialism as somehow ‘com-
            pleting’ the industrial revolution begun by capitalism.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My own point in all this is that we should reverse the polarity of Mirowski’s
            value judgements by arguing that neoliberalism’s epistemological horizon is in fact
            more ‘realistic’, but at a meta-level, in the sense of Realpolitik – namely, it is con-
            cerned with, as Bismarck memorably defined politics, the ‘art of the possible’. In
            that respect, the state has one clear superordinate role in neoliberalism, which is to
            increase the realm of possibility. The legal means for this, once again, is to break
            default entitlements, which came to be known in the early days of neoliberalism’s
            intellectual cradle, the Mont Pèlerin Society, ‘liberal interventionism’ (Jackson,
            2009). According to this doctrine, the state breaks up monopolies and other rent-
            seeking socio-economic entities that impede the flow of capital. In the case of intel-
            lectual property, this means ‘knowledge capital’, or more to the point, ‘human
            capital’. ‘Liberal interventionism’ in this sense was the calling card of self-described
            ‘Progressive’ political movements on both sides of the Atlantic in the early twenti-
            eth century, including the Fabian movement, which helped found the UK Labour
            Party (Fuller, 2018b).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Although Theodore Roosevelt and Woodrow Wilson, on the one hand, and
            Sidney and Beatrice Webb, on the other, may seem unlikely comrades in arms, they
            shared a profound aversion to what was passing as laissez-faire capitalism, which
            amounted to the default accumulation of advantage over time without any concern
            for its long-term social value, given the impediments that would be increasingly
            imposed on new market entrants (Fried, 1998). At the very least, this would entail
            an enormous waste of talent, if one operated with an open-minded attitude toward
            the contributions of future generations. Here one needs to recall the supposedly
            scientific basis for the laissez-faire doctrine in ‘Social Darwinism’. The slogan ‘sur-
            vival of the fittest’, a phrase adapted from Herbert Spencer (not Darwin), epito-
            mized the spirit of the movement, which seemed to accept the inevitability of wasted
            talent given a world of scarce resources. But the more politically salient point was
            that Social Darwinism had elided the difference between biological and financial
            advantage by its acceptance of the Lamarckian doctrine of the ‘inheritance of
            acquired traits’, which was intuitively helped along by ambiguity in the colloquial
            usage of ‘inheritance’ and such associated words as ‘dynasty’. Under the circum-
            stances, the Progressives feared that capitalism would turn into a high-tech and
            globalized form of feudalism – that is, unless the state intervened to disrupt what
            amounted to an unlimited license for monopoly formation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowledge itself soon came into the sights of the Progressives, insofar as
            restricted access to both education (via selective schools) and research (via intel-
            lectual property) structurally reproduced the spread of advantage across the popula-
            tion. Theodore Roosevelt came up with a novel solution to part of the problem,
            which would have the big corporate monopolies – Rockefeller, Carnegie, Ford, etc. – channel much of their profits into ‘foundations’ dedicated to research and
            education for public benefit, in lieu of paying taxes. These predated publicly funded
            bodies in the US by two generations, and arguably made the biggest contribution to
            both natural and social scientific innovation in the twentieth century (Fuller, 2018a:
            Chap. 4). The more general Progressive solution was to foster mass education and
            minimize – if not outright eliminate – intellectual property rights (e.g., Nye, 2011:
            Chap. 5). Of course, this was easier said than done. Indeed, over the years several
            thinkers associated with neoliberalism’s ascendancy to power in the late twentieth
            century came to embrace monopoly formation, rendering ‘liberal interventionism’
            in practice a policy for corporate tax relief and market de-regulation, policies asso-
            ciated with the Reagan-Thatcher years (Davies, 2014: Chap. 3). I shall return to this
            curious turn in neoliberalism’s intellectual trajectory, when considering Saint-
            Simon’s brand of ‘socialism’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Once socialism outgrows its nostalgia for Marx, it will regroup around a polarity
            defined by Saint-Simon and Proudhon – the corporatist and the libertarian, and
            thereby ‘socialism’ will be seen as simply the name for alternative capitalist futures.
            Indeed, given that both Saint-Simon and Proudhon completed their most important
            work before Marx, one can read what follows as an updated version of what social-
            ism might have been, had Marx never existed. Basically, the versions of ‘socialism’
            proposed by Saint-Simon and Proudhon were alternative strategies for harnessing
            capitalism’s full productive potential for overall social benefit. As a first approxima-
            tion, the one calls for greater concentration of capital and the other for greater dis-
            persion of capital. Both have their appeal, and as I have suggested, neoliberalism
            has flipped from a more ‘Proudhonian’ to a more ‘Saint-Simonian’ orientation.
            However, unlike Marx, both Saint-Simon and Proudhon celebrated capitalism’s
            entrepreneurship and risk-taking. The two simply differed over the kind of society
            in which this should happen: Should it be governed top-down vs bottom-up? Is
            entrepreneurship best done by one on behalf of all or by everyone as a whole?]]>
			</paragraph>
			<paragraph>
				<![CDATA[These are profound metaphysical disagreements, to be sure. Moreover, the
            associated policy differences between Saint-Simon and Proudhon to some extent
            track the socialist calculation debate. Both were concerned with minimizing waste
            and maximizing productivity, realizing that various sorts of trade-offs had to be
            made between the two. In response to these, Saint-Simon generally favoured scaling
            up and Proudhon scaling down: incorporation vs subsidiarization (cf. Cahill, 2017).
            Nevertheless, despite these profound differences, their attitude toward labour was
            much more ontologically flexible than Marx’s – and in that respect, showed a greater
            appreciation for the implicit metaphysics of capitalism. Here it is worth recalling
            that by the time the labour theory of value had made its way from Aquinas to Marx, it had undergone a curious metamorphosis. The appeal of this medieval doctrine in
            the early modern era had rested on its clear opposition to the value regime support-
            ing slavery and other forms of indentured servitude. Thus, for Locke and Adam
            Smith, the labour theory of value defined the proper reward of an individual’s labour.
            To be sure, Marx went on to re-specify ‘labour’ as a collective factor of production.
            However, Marx retained a metaphysical residue from the original theory. Although
            Marx officially dissociated the value of labour from any theological conceptions of
            human exceptionalism, he still regarded it as qualitatively distinct from the value of
            nature and other forms of capital. Indeed, the Marxist critique of capitalism’s
            ‘exploitative’ character depends on this point.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nevertheless, as greater liberalisation in the nineteenth century enabled people to
            deploy their labour in a wider arena and in various combinations with others, it
            became natural to see labour as itself another shapeshifting form of capital. David
            Ricardo pioneered this awareness by explicitly theorizing the terms – which he took
            to be inevitable – on which the drudgery of labour might be replaced by more effi-
            cient technology. And whereas Marx denounced Ricardo for justifying – if not
            encouraging – unemployment as dictated by the ‘logic of capital’, Ricardo had him-
            self anticipated later neoliberal boosters of the ‘knowledge economy’, whereby
            ‘technologically unemployed’ workers would adapt to changing market conditions
            by acquiring new skills (Fuller, 2019a). Thus, for Ricardo, the real enemy of labour
            is not technological innovation but rent-seeking practices – including trade unions
            and professions – that restrict these renovated workers from entering new markets
            where they might be competitive with more established players. After all, if one
            thinks of labour as a shapeshifting form of capital – indeed, perhaps the most pro-
            tean of all such forms – then the real problem is not that you might lose your job but
            that you might not find another job because the market is dominated by ‘closed
            shops’, the American expression for businesses that hire only union members.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Both Ricardo and Marx were notorious foes of rent as a source of capital.
            However, the above discussion suggests a difference in the diagnosis of rent’s fail-
            ures, which in turn reflects a difference in commitment to the labour theory of value.
            Marx was the stronger adherent to the theory, ultimately anchoring the value of
            labour in the work that people actually do, independently of how many other people
            could do the same job or its exchange value in the market. Rent is a problem from
            this standpoint primarily because it is unearned wealth: Income is accrued from
            sheer ownership without the owner adding productively to what is owned. It amounts
            to a legally protected power grab that gives the owner free rein over how to dispose
            of others. Social justice-based arguments against ‘worker exploitation’ can be easily
            mounted on this basis. In contrast, Ricardo saw rent primarily in terms of what it
            means for other market players – namely, it restricts their access to resources that
            they might use more productively than the current owners themselves. Thus, the
            circulation of capital is impeded, and the economy loses dynamic capacity to deliver
            prosperity for all. In short, the moral force of the critique of rent shifts from freedom
            arrogated (Marx) to freedom obstructed (Ricardo). In a liberal society, the latter is
            of greater normative concern. It is also closer to what I earlier identified as the ‘pro-
            gressive’ position, including what I shall later characterize as ‘Georgism’. Interestingly, some Marxist-inspired economic historians have begun to come round
            to the Ricardian position, albeit in their own tortured way (e.g., Christophers, 2019).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Before turning to Saint-Simon, it is worth observing that historically the
            normative case for allowing rentiers absolute control over access to their property
            rested on the fact that, contra Marx, rentiers do indeed contribute to the value of
            what they own by preventing its degradation, thereby setting a standard of
            maintenance that future leaseholders are obliged to uphold. In his renewed defence
            of this case, Roger Scruton (2012) rightly associates what we would now regard as
            an ‘ecologically sustainable’ orientation with the economic horizon of classical
            conservatism. Moreover, much the same argument is implied in today’s discussions
            about the value of universities as ‘custodians’ of knowledge, notwithstanding the
            barriers that this entails in terms of various ‘entry costs’, whether it be in terms of
            acquiring credentials or simply understanding the content of academic texts. Here
            we need to imagine that the ‘turnover’ of both private land and technical language
            through repeated and extended use generates concerns about the maintenance of
            quality control, which in turn justify the existence of said ‘custodians’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In contrast, Saint-Simon saw the advantage of monopolies from a much more
            dynamic perspective, one that nowadays would be most naturally cast in terms of
            ‘economies of scale’. Moreover, unlike both the eco-conservative and the Social
            Darwinist perspectives, he clearly saw nature as an opponent that can be conquered.
            Indeed, Saint-Simon may be credited with having conceptualised the market econ-
            omy in the sort of explicitly ‘political’ – that is, ‘friend versus foe’ terms – that
            could win over Carl Schmitt (1996). He held that the step change in human evolu-
            tion – from ‘military’ to ‘commercial’ societies – occurred once we shifted from
            exploiting each other in wasteful conflict to exploiting nature together productively.
            Here nature is pitted in decidedly anti-ecological terms as humanity’s ultimate foe,
            the conquest of which is necessary for humanity to fulfil its species potential. Saint-
            Simon’s hostile view of nature – which he inherited from Francis Bacon and from
            whom Marx inherited it whole cloth – is a secular holdover of the doctrine of
            Original Sin, whereby Adam’s Fall amounts to our species’ descent from divinity to
            animality. Perhaps unsurprisingly, Saint-Simon dubbed his philosophy, the ‘New
            Christianity’, which did not sound quite as strange in the early nineteenth century as
            it does today – though the advent of transhumanism may bring this theological
            dimension of Saint-Simonianism back in fashion (cf. Fuller & Lipinska, 2014).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Saint-Simonian vision had been already justified by the medieval jurists who
            invented the universitas, a very interesting Latin concept that is normally – and
            rightly – translated as ‘corporation’, of which ‘universities’ and ‘incorporated com-
            munes’, or ‘cities’, were among the original exemplars (Kantorowicz, 1957). The
            universitas is truly an ‘artificial person’ in the sense that Hobbes later appropriated
            for the secular state and is nowadays associated with androids, à la ‘artificial intel-
            ligence’. What unites these materially quite different cases is a statement of found-
            ing principles, a ‘charter’ or ‘constitution’, that – in the manner of computer
            algorithms today – serves to assemble initially distinct agents into ‘parts’ that con-
            stitute a ‘whole’ whose purposes transcend the particular ends of those who happen
            to be its members at any given time. (One is reminded of the frontispiece of the first edition of Hobbes’ Leviathan.) Unlike, say, a mutual aid society, the sort of military
            expeditions that fuelled the Crusades or even the joint-stock companies of early
            modern Europe, the universitas is not a local and temporary arrangement. Rather, it
            enjoys a life of its own, perhaps even in perpetuity. Thus, the current members of a
            universitas are disposable based on their functionality. In sociological terms, these
            functions – the ‘parts’ that constitute the ‘whole’ – are called ‘offices’, or simply
            ‘roles’, each of which is associated with a procedure for selecting and replacing
            individuals in a line of succession, say, via examination or election. This situation
            poses special challenges for the Humboldtian university, given, on the one hand, the
            indefinite need to reproduce the universitas, which is largely an administrative func-
            tion and may involve the university responding symbiotically to other similarly
            positioned ‘corporate’ players (e.g., the state, business) and, on the other hand, the
            need to allow the academics and students who constitute the university at a given
            time to the indefinite freedom to pursue inquiry.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As far as Saint-Simon was concerned, Adam Smith half-understood what was
            needed to turn the market economy into an engine of human progress – if not human
            redemption. Smith realized that markets allowed people to freely pursue their tal-
            ents in consort with their fellows without the artificial legal restrictions of birth
            rights and monopolies, which serve only to allow future action to be overdetermined
            by past settlements. Moreover, Smith was right to think that this liberalization
            would result in a self-organizing ‘division of labour’, whereby everyone plays to
            their strengths and is rewarded by others for doing just that. Such is the moral basis
            of the market economy, a side-effect of which is the increased prosperity of all.
            Thus, it is no accident that The Wealth of Nations was preceded by The Theory of
            Moral Sentiments. That’s the right way round to understand the moral argument for
            capitalism, as opposed to how his ‘free market’ defenders interpret him, which
            would have the tail of increased wealth wag the dog of mutual respect. Deirdre
            McCloskey (2006) is one free market economist who actually understands this point. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, Saint-Simon observed that Smith’s model does not automatically scale
            up. Historically, markets were centres of action that operated according to the needs
            of the communities housing them. They emerged ‘spontaneously’ as people traded
            their surpluses to satisfy their needs. However, markets were neither continuously
            operating nor coordinated across communities, which could prove problematic if
            the surpluses and/or needs exceeded local market capacities. This was the problem
            that Saint-Simon attempted to solve under the rubric of ‘socialism’. Indeed, by
            ‘socialism’ Saint-Simon meant at least corporate capitalism, perhaps also involving
            a corporate state. Instead of people spontaneously organizing themselves, they
            would be explicitly organized by a change agent – a ‘catalyst’, as chemists would
            say – into what came to be known as an ‘organization’, which is reasonably under-
            stood as an artificial organism. These catalysts are Saint-Simon’s ‘captains of indus-
            try’, the entrepreneurs of social innovation that we nowadays call ‘knowledge
            managers’ (Fuller, 2002: Chap. 3). Moreover, the science of chemistry itself is
            strongly implicated in these developments, well into the twentieth century. Wilhelm
            Ostwald, who had discussed ‘organizing’ in the context of how scientists produce a
            chemical synthesis in the laboratory, also championed a second-order version of this process in terms of the conduct of science itself (Fuller, 2016b). His French contem-
            porary Pierre Duhem (1991) disparagingly dubbed this ‘German science’, but it
            came to be normalized as large-scale, team-based laboratory work led by a ‘princi-
            pal investigator’ who organizes the talent such that the team’s scientific output is
            much greater than what any of them could have accomplished individually, aka ‘Big
            Science’ (Price, 1963).]]>
			</paragraph>
			<paragraph>
				<![CDATA[As I mentioned above, Saint-Simon spoke of humanity’s adversarial relationship
            to nature as requiring ‘exploitation’, a term that Marx later threw back at capitalists
            who, as he saw it, continued to exploit fellow humans just as they exploited nature:
            Both were rendered ‘capital’. Nevertheless, the desire to exploit human nature lin-
            gers – albeit in domesticated form – in the postwar welfare state extension of the
            idea of public goods from roads and utilities to health and education, notwithstand-
            ing the long history of private provision for the latter pair of services. The underly-
            ing intuition was rather different from, say, that of Bismarck, who saw public health
            and education primarily in national security terms. In contrast, welfare economists
            of the mid-twentieth century regarded them in the spirit of human capital develop-
            ment for purposes of increasing national productivity (‘GNP’). This was much more
            in Saint-Simon’s original spirit, whereby ‘peaceful’ economic competition would
            replace warfare. It also helps to explain the backlash that began in the 1960s with
            the University of Chicago economist Gary Becker (1964) and colleagues who
            became the vanguard of ‘neoliberalism’. They questioned whether the return on
            investment justified the elevation of health and education to public goods: Does
            state intervention add value beyond what would result if the market left to sort it
            out? The same questioning was also extended to state provision of research – but
            less vociferously, since the ongoing Cold War was being fought largely on the bat-
            tlefield of scientific and technological prowess.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This last point is relevant to the US government’s increasingly critical posture
            toward Silicon Valley tech giants, whose internet-based social media platforms are
            founded on infrastructure originally provided by the US Defense Department in the
            Cold War as an alternative channel of communication in the event of a nuclear war
            (Fuller, 2020b). However, as throughout the modern period, once the exigencies of
            war were removed (and state debts started to be repaid), such ‘next generation’
            innovation was redeployed to the private sector, resulting in a few enterprising indi-
            viduals taking advantage of the new and potentially very lucrative markets that had
            been opened. They largely still dominate the market more than a quarter-century
            later. In the case of Silicon Valley, the Congressional hostility is interestingly bipar-
            tisan. Consider recent best-sellers by left-leaning Democratic Senator Amy
            Klobuchar (2021) and right-leaning Republican Senator Josh Hawley (2021), two
            Ivy League-trained lawyers who want ordinary Americans to understand the role of
            antitrust legislation in underwriting America’s legacy of liberty and productivity, in
            order to appreciate the injustice now being perpetrated by companies whose modus
            operandi they have come to take for granted in the name of convenience.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Fuelling this animus is the sense that public goods have been effectively stolen,
            or at least Silicon Valley’s stewardship of them is open to serious question (cf.
            Morozov, 2013). The argument’s moral force is informed by the fact that the internet had been manufactured to be a public good – albeit one designed in wartime
            for a post-apocalyptic society. In contrast, most public goods – from roads to
            radio – have come under state stewardship and provision after having had a patch-
            work history under private control. Nevertheless, the fundamental principle that ani-
            mates the two paths to public goods are the same – and very much in the spirit of
            Saint-Simon: namely, that the full expression of human potential is arrested if his-
            torically based entitlements come to dominate our understanding of legitimate suc-
            cession. In this regard, the economic sphere is no different from the political sphere:
            Monopolies are dynasties – and both require a counter-inductive remedy, be it anti-
            trust legislation or periodic elections. Such events offer opportunities for people to
            reconsider lost futures that they might wish to recover. Here academic peer review
            appears regressive, as it aims to reinforce the trajectories of already established
            research, even in cases where novelty is admitted. More to the point it goes against
            the spirit of the Humboldtian university’s emphasis on bringing frontline research to
            the classroom, specifically to inform students of how the future need not be like the
            past – and that one should understand the latest academic thinking even if one does
            not plan to contribute to it directly. Indeed, the Humboldtian university remains the
            most reliable site for converting new information into knowledge as public good.
            But there remains the question of whether the university will stay true to Humboldt,
            if or when the state withdraws its support.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the late 1930s, lawyer-economist Ronald Coase, a product of both the London
            School of Economics and the University of Chicago, reinvented Saint-Simonianism
            for the emerging neoliberal worldview in the guise of the firm, a superordinate mar-
            ket agent that minimizes the transaction costs between first-order market agents by
            granting them privileged access to each other as incorporated parts of a single func-
            tioning unit (Coase, 1988: Chap. 2; cf. Birch, 2017a: Chap. 6). This arrangement
            minimizes potential ‘negative externalities’ from what been the moment-to-moment
            misalignments of supply and demand in a collection of independent agents. These
            costs are ‘internalized’ into what is now a unified system’s normal operation. Thus,
            the concerns raised by Neurath in the socialist calculation debate about imperfect
            information flows and miscommunication in markets are met through the backdoor,
            rendering the firm in effect a protected free market. The European Union was origi-
            nally conceptualized in that spirit. But equally, modern imperialism – with what
            Marxists call its ‘global division of labour’ – can be understood as the logic of the
            firm projected on the world stage. Indeed, this had famously led Lenin (1948) to see
            imperialism as capitalism’s ultimate self-expression. He meant this as an indict-
            ment, but Saint-Simon might have taken it as a compliment – especially if he didn’t
            read the fine print.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, Lenin’s was not the only understanding of capitalism’s relationship to
            imperialism in the early twentieth century. The alternative, due to Joseph Schumpeter,
            gets to the difficulty of capitalism achieving the sort of ‘corporate capitalism’ to
            which Saint-Simon aspired. For Schumpeter (1955), imperialism is simply the rein-
            vention of feudalism in capitalist garb, as it tends to stabilise a ‘global division of
            labour’ on land-based terms (‘colonies’), which in turn become sources of rent
            and – as in historic feudalism – potential targets of military conquest. In the process,
            Saint-Simon’s much vaunted evolutionary transition from military to commercial
            society gets undone. The problem here is not that a global division of labour per se
            exists but that its imperialist version tends to arrest the economy’s dynamism, creat-
            ing artificial monopolies, over which wars can then be fought. After all, targets
            come more easily into view when their boundaries don’t change. In a similar vein,
            Schumpeter regarded the Marxist fixation on classes as an organizing principle of
            capitalism as a holdover from feudal estates. Collective mobilization based on class
            is likely to succeed only if the mode of production is subject to relatively little inno-
            vation, resulting in a static labour market. In short, the appeal of class consciousness
            is inversely related to the prospect of social mobility. This helps to explain the pat-
            tern of Marxist revolutions in the twentieth century, starting with Russia in 1917.
            They typically succeeded in societies that had yet to be fully industrialised – con-
            trary to Marx’s own prediction, of course.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The challenge facing latter-day Saint-Simonians is how to avoid the Marxist trap
            of presuming such a rigid conception of capitalism’s social relations as to ignore the
            inherent dynamism of capitalism’s mode of production. The general Saint-Simonian
            response is to conceptualise class in radically functionalist terms. In order to meet
            demand in response to changing market conditions, firms must be able to regularly
            alter their internal division of labour: Not only role-players but the roles themselves
            must change. This means that any grouping of workers is always expedient relative
            to a job for only as long as it needs to be done: Teams must easily assemble, disas-
            semble and re-assemble. Saint-Simon, who popularised the use of ‘industrious’ to
            refer to a personal quality, understood the increased productivity of labour, like
            Ricardo, as being opposed to the cultivation of a line of work for its own sake, as in
            such residually medieval institutions as trade unions or professions. Indeed, knowl-
            edge management is arguably the only ‘profession’ allowed in Saint-Simon’s world,
            in which universities would effectively become high-toned business schools. Even
            so, how can a firm retain its integrity if its development is likely to require so much
            external and internal change?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Before the existence of the firm, another version of the universitas faced a
            comparable problem on an arguably larger stage: the state. Recall that natural law
            theory, the discursive context in which the universitas was constructed in the Middle
            Ages, presupposed a cosmos governed as a ‘divine corporation’, which amounted to
            a theologised understanding of ecology (Schneewind, 1984). On this view, nature is
            the outworking of divine intelligence in matter, such that life is tantamount to the
            expression of mind. (The intuitiveness with which we accept the idea of life as the
            product of a ‘genetic code’ may be a secular downstream effect of this sensibility.)
            From that standpoint, Saint-Simon’s innovation amounts to asserting that God has delegated to humans the right to operate in a similar fashion, but now within – rather
            than from outside – nature. In this way, the imago dei doctrine of the Bible morphed
            into political economy’s principal-agent theory. An early adopter of this idea turns
            out to have been John Stuart Mill, who made a semi-serious case for the existence
            of a ‘limited liability God’ (Copp, 2011).]]>
			</paragraph>
			<paragraph>
				<![CDATA[But before any of that happened, Thomas Aquinas had already departed from
            Aristotle in envisaging the political order not as a spontaneous feature of human
            nature (i.e., zoon politikon) but as a second-order extension, what today we might
            call an ‘extended phenotype’ or ‘superorganism’ (Fuller, 2016b). To be sure, these
            latter-day conceptions reflect the influence of evolutionary thinking that was
            unavailable to Aquinas. Indeed, Aquinas thought of the ‘state’ in biblical terms as
            imposing an artificial constraint on humans who might otherwise regress to their
            fallen condition. Moreover, his fixation on this superordinate entity as a ‘state’ (sta-
            tus) suggests something capable of maintaining what biologists call ‘homeostasis’,
            which entails a standard of equilibrium between the organism and the environment,
            as well as a means to restore equilibrium when necessary (Kantorowicz, 1957:
            Chaps. 5 and 6). This helps to explain, for example, the right of slaves to revolt
            against their masters if they are mistreated. Such mistreatment, while not providing
            grounds for eliminating the master-slave relationship altogether, does presuppose a
            ‘state’, which combines the sacred and secular features of natural law to redress
            such injustices, thereby restoring a ‘natural’ order in which masters and slaves deal
            with each other with the dignity accorded to their respective natures.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This vision of the divine corporation held sway well into the eighteenth century,
            the last great expression of which was probably Carolus Linnaeus’ theory of ‘natu-
            ral economy’, out of which came the two-named basis of for the classification of
            animals, plants and rocks that remains in modified use today (Koerner, 1999).
            However, that same century witnessed the rise of an ‘epigenetic’ approach to life
            that gradually eroded the intuition that had grounded Aquinas’ conception of the
            state – namely, a static nature that is front-loaded towards the perpetuation of previ-
            ously expressed traits, aka hereditary entitlements. This shift away from ‘inheri-
            tance’ in the broadest sense began in studies of cell development in the embryo, as
            scientists moved from thinking of the nascent life-form as ‘preformed’ to ‘pluripo-
            tential’ – which is to say, capable of various ultimate expressions, depending on the
            embryo’s developmental context. This provided a metaphorical basis for people to
            claim that their ‘potential’ was being held back by adverse conditions that impeded
            their ‘free development’, which soon became a rallying cry for modern revolution-
            ary politics. Two centuries later, Gilles Deleuze and Felix Guattari (1977) would
            retrace this turn to the original disputes over epigenesis, in which they famously
            adapted Antoine Artaud’s phrase ‘body without organs’ for the pluripotential
            embryo – what is nowadays usually called a ‘stem cell’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[And roughly halfway between the rise of epigenesis and Deleuze’s ‘bodies
            without organs’ stands Proudhon, who regarded each individual as an inherently
            pluripotential entity open to a variety of social arrangements, all of which might be
            of potential benefit to humanity as a whole. French sociologist Daniel Colson (2019)
            takes this position to be the metaphysical foundation of the various movements that have styled themselves ‘anarchist’ over the past two hundred years. For our pur-
            poses, Proudhon’s sense of ‘socialism’ as a kind of spontaneously organized capi-
            talism interestingly contrasts with Saint-Simon’s more hierarchical conception.
            Proudhon starts by suspecting the very concept of property as both masking the
            collective nature of any human achievement and stifling the inherent dynamism of
            the modern economy. Indeed, he regarded private property as a feudal residue in
            modern capitalism, whose risk-seeking tendencies he admired as the fount of inno-
            vation. It is thus easy to see why Proudhon instinctively opposed treating the range
            of entities normally covered under intellectual property law as ‘property’ in any
            strict sense that might accrue monopoly benefits to innovators (Guichardaz, 2020).
            More specifically, he accepted Adam Smith’s premise that people are ‘natural
            equals’ at least in terms of not being able to do everything for oneself, which in turn
            encourages a commercial culture based on mutual recognition (McCloskey, 2006).
            It follows that the designated ‘innovator’ is simply the person who configured many
            talents distributed across the population and consolidated them in an invention that
            then employed the efforts of other people to transform society as a whole. The inno-
            vator’s ‘originality’ lies simply in having catalysed this chain of configurations,
            which increasingly extend beyond what the innovator could reasonably anticipate.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For Proudhon, two countervailing points follow from the above, which should be
            read as the broad outlines of a ‘natural history of innovation’. On the one hand, the
            sheer publicity of the innovation removes any sense of the innovator’s proprietary
            ownership, since others may end up employing the invention in ways that make it
            more beneficial if not profitable than what the innovator could have done or even
            envisaged. We might think of this point as the technological antecedent of the late
            twentieth century ‘death of the author’ thesis promulgated by Barthes, Foucault,
            Derrida and ‘post-structuralist’ thinkers, who stressed the extent to which the mean-
            ing and significance of a text always escapes and exceeds its author’s intentions. On
            the other hand, it is certainly true that each innovator provides ‘added value’ as the
            catalyst that consolidates an invention. In that respect, innovators deserve some
            compensation for their work. Proudhon proposed that the state should provide both
            authors and inventors a ‘salary’. Proudhon’s proposal was clearer in its conception
            than its implementation. Nevertheless, the guiding intuition was that anyone could
            be an innovator, but the success of an innovation depends on everyone else. An
            innovation requiring major effort may generate little effect, whereas one involving
            trivial effort may turn out to be very profitable – in both cases, for reasons that are
            largely out of the innovator’s control. A salary would thus strike a middle ground
            between no compensation and a permanent entitlement to the innovation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here it is worth recalling that the distinction between copyright and patent as
            forms of intellectual property had yet to be clearly drawn in the early nineteenth
            century. Indeed, at the 1858 International Congress on Literary and Artistic Property
            in Brussels, authors and inventors were treated as one regarding the central question
            of whether their rights should be specifically understood as property rights (Machlup,
            1950). An influential figure in this context and on Proudhon was the lawyer Charles
            Renouard, who recognized that ‘property’ entails much greater legal protection than
            ‘right’. A right is simply a permission to access, whereas property implies restriction of access as well. Renouard concluded that authors and inventors require
            a temporary property right over their innovations not to reward them properly for
            their ‘genius’, but to ensure that their having been first doesn’t turn into a disadvan-
            tage. Precisely because others could have arrived at the same innovation at roughly
            the same time, they would probably appropriate many if not all its benefits, if not
            explicitly prevented, once the innovation was made public, thereby placing an ironic
            spin on Proudhon’s notorious slogan, ‘Property is theft!’ Thus, for a limited period,
            any such would-be usurper would require permission from the actual innovator
            (Haynes, 2010: 87–89).]]>
			</paragraph>
			<paragraph>
				<![CDATA[While many legal and philosophical justifications have been given over the years
            for extending or restricting intellectual property rights, Renaud’s has remained the
            grounding intuition for assigning them in the first place. It presumes a preference
            for ‘right’ over ‘property’ as the relevant legal category governing innovation,
            which is suited to a process that all everyone agrees is subject to profound contin-
            gency, both in terms of origin and uptake. One might even go a step further and
            suggest that on this Proudhonian view, ‘property’ is subsumed under ‘right’, insofar
            as one’s entitlement to ownership is ultimately predicated on one’s ability to make
            productive use of the object in question, which is to say, it is never a settled matter.
            In principle, everything is collectively owned, but in practice the ‘true’ owners are
            the producers not the inheritors. We shall explore this point in more detail below
            under the guise of ‘Academic Georgism’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The ‘open science’ movement began when academics started to complain about the
            potential lack of access to the journals where they have published or might want to
            publish. The loudest complaints came from universities that could afford to strike
            deals with publishers to enable ‘open access’. From a classical Marxist standpoint,
            the ‘open science’ looks very bourgeois, given the ease with which it can be resolved
            by monetary payment. Moreover, this settlement has been legitimized – and even
            standardized – by public funders requiring that knowledge published in academic
            journals be freely available to anyone. At the same time, many researchers are
            excluded from such arrangements, perhaps due to their universities’ lack of funds or
            simply by their own lack of a university affiliation. They can’t enter what is de facto
            a ‘protected market of open science’, and hence can’t turn the knowledge it contains
            to their own advantage, let alone alter its market dynamics substantially. Thus,
            ‘open science’ may not be as open as one might wish (Fuller, 2016a: 67–71).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Yet, there are also two other senses of ‘openness’ – already exemplified by the
            internet – that have the potential to reorganize the political economy of open sci-
            ence. They pertain to freer entry and freer use. The internet promotes freer entry by
            not imposing an initial intellectual or financial toll on the user. Closely related to that fact is the relatively free rein that users are given in how they operate in the
            virtual knowledge environment. In contrast, the protected market of open science is
            primarily aimed at making conventional academic knowledge transactions (i.e.,
            journal communications) as frictionless and their results as transparent as possible.
            In this respect, the open science movement may be seen as a reinstatement of the
            Charter of the Royal Society on a digital platform. One consequence of such aca-
            demic protectionism is that knowledge producers are valuable simply by virtue of
            being part of a protected market consisting of those universities that subscribe to the
            journals in which the academic publishes. This basic fact is often obscured by the
            ideology of ‘peer review’ which legitimizes academic protectionism and creates a
            halo effect around its public face, ‘expertise’. But as a form of political economy, it
            amounts to rentiership, the bane of both David Ricardo and Karl Marx. They agreed
            that value is intrinsic neither to nature nor even to property, which was too often
            inherited as a ‘second nature’, accruing to its possessors a merit that they do not
            deserve. For Ricardo and Marx, value must be earned through the application of
            labour. Of course, whereas capitalists hold that a free market would incentivize
            property owners to make investments that create opportunities for labour rather than
            simply collect rents, socialists call for stronger, state-based measures, including
            taxation and other more proactive forms of wealth redistribution.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As we saw earlier in this chapter, the US Progressive Era was distinctive in
            turning the anti-rentier mentality uniting Ricardo and Marx into the liberal
            interventionist state, which made it its business to turn capital bottlenecks into free
            markets through various legal instruments. The talismanic intellectual figurehead of
            this movement was Henry George, the political economist and author of the
            worldwide best-seller Progress and Poverty (1879), who argued that the only
            legitimate tax was on land whose owners generate wealth merely by renting it out to
            others who then do the real work of ‘improving’ the land by either developing or
            conserving it. The Financial Times’ chief economics columnist, Martin Wolf (2023),
            has recently recast George’s thesis for its readership as a moral obligation to tax
            resources whose supply avoids the price mechanism, which is capitalism‘s default
            dispenser of justice. Moreover, failure to tax such resources means that what remains
            of the potential tax base for redistribution – earned wealth – results in the rich
            bribing the poor to make themselves richer without enabling the poor to improve
            their lot. It’s effectively a policy of permanent handicapping (Samuels, 1992: Chap.
            5). In this respect, Georgism simply recovers the sense of ‘natural justice’ that first
            motivated Adam Smith and the Marquis de Condorcet to propose the modern
            market-based economy (Rothschild, 2001). And while George followed Ricardo in
            speaking of land rent as capitalism’s original sin, ‘land’ is a proxy for any unilaterally
            protected market that impedes capital circulation. Accordingly, ‘Academic
            Georgism’ would target the various barriers to free entry and free use of academic
            knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Such a policy would have interesting and potentially radical implications for the
            assignment of credit in academia. After all, among the most highly cited people in
            academic publications are those who are dead or if not dead, long ago ceased to
            research in the areas for which they are cited. This helps to explain the highly stratified nature of academic citation patterns, whereby the citation-rich get richer
            and the citation-poor get poorer, nowadays as measured by the notorious ‘H-index’
            (Fuller, 2018c). Such dead and undead folks are effectively reaping indefinite divi-
            dends in areas to which they themselves no longer add value. From a Georgist
            standpoint, their trademarks should be assigned the status of mere labels – or per-
            haps, where appropriate, stylistic inflections of public life. It amounts to the most
            consistent albeit ironic gloss on Proudhon’s slogan, ‘Property is theft!’ After all, a
            thief usually feels under the great obligation – perhaps more than the person from
            whom she stole the item – to protect and transform it, so that others recognize it as
            the thief’s own. The Yale literary critic Harold Bloom (1973) famously wrote about
            this as the ‘anxiety of influence’ that poets feel towards their predecessors, from
            whom they creatively plagiarise and pray not to get caught – or at least, if caught, to
            be seen as having been improvement on the origin (cf. Fuller, 2020a: Chap. 10).]]>
			</paragraph>
			<paragraph>
				<![CDATA[A good way to appreciate the ever-present threat of rentiership to academic
            knowledge production is to recognize that knowledge is not by nature a public good
            but must be made one (Fuller, 2002). Understood as a product of research, genu-
            inely innovative knowledge – that is, a discovery or an invention – is born elite, in
            the sense of being ‘ahead of the pack’. It is at first known by few people, who in turn
            potentially acquire some sort of advantage vis-à-vis fellow researchers, if not soci-
            ety at large. In this respect, all knowledge begins as what economists call a posi-
            tional good, namely, its value is tied directly to its scarcity (Hirsch, 1976). This
            initial condition makes knowledge ripe for rent. For knowledge to become a public
            good, more people need to learn about, use and develop it, without the value of the
            knowledge diminishing – and ideally, with its value enhanced. Humboldt’s great
            innovation was to harness cutting edge research to the reproduction of the society’s
            authority structure to convert the universities from conservers of tradition – that is,
            guardians of rentiership – to dynamos of social change. Humboldt’s renovated aca-
            demic was to present the path of inquiry not as largely charted but as fundamentally
            open, with the expectation that the next generation will not build upon but overturn
            the achievements of the past. As we have seen, Max Weber’s famous 1918 speech to
            new graduate students, ‘Science as a Vocation’, provides a deep and eloquent state-
            ment of what that expectation entails, both personally and institutionally.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I have characterized the conversion of knowledge from positional to public goods
            in terms of the ‘creative destruction of knowledge as social capital’ (e.g., Fuller,
            2002: Chap. 1; Fuller, 2016a: Chap. 1). By that phrase I mean the multiple roles that
            Humboldt-style teaching plays in undermining the competitive advantage – and
            hence the potential for rentiership – that is inherent in cutting edge research. These
            roles include the presentation of difficult ideas in more ordinary terms, as well as
            streamlining the learning process so that students do not need to recapitulate the
            entire history of a field before feeling for themselves or being deemed capable of
            contributing to it. ‘Philosophy’ started to name an academic discipline in this con-
            text, specifically to level the epistemic playing field by forcing even the most estab-
            lished knowledge claims to be justified from first principles that anyone could
            understand. ‘Teaching’ in this Humboldtian sense – the transmission of what is
            already known to those who don’t yet know it – is arguably the most efficient and reliable path to fostering an innovative spirit in society at large because it demystifies
            the ‘genius’ aspect of previous innovations. A claim to genius tends to polarize the
            emotional horizons of audiences: It can either challenge or inhibit any aspiring
            innovators.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The way to understand university ‘teaching’ as producing knowledge as public
            goods is not as technical training but as anti-technical training. Put provocatively,
            the goal of teaching is the very opposite of the faithful transmission of knowledge
            to the student as the teacher learned it. To assume otherwise would be to suggest that
            restricted access is inherent to the nature of knowledge. Knowledge can be manu-
            factured as a public good only if students are enabled to acquire the ‘same’ knowl-
            edge by starting from their own premises. This implies a specific approach to
            teaching. First, teachers need to have a clear sense of what is and is not essential in
            the knowledge they wish to convey to students. Teachers should assume that much
            of how they themselves have come to know something is simply an accident of their
            biography – that is, the fact that they went to certain schools, read certain books, had
            certain teachers. (This is what the logical positivists and the Popperians mean by the
            ‘logic of discovery’.) Those aspects of the innovators’ background do not require
            reproduction. But equally important, teachers need to have a reasonable sense of
            what students already know, as that will be the basis on which they can understand
            the relevant sense of novelty that is presented in what they are taught. The result is
            bound to be a form of knowledge that is sometimes dismissed as ‘popularisation’.
            Nevertheless, it is the sort of knowledge that can be genuinely counted as a ‘public
            good’, simply because excessive access costs to the knowledge have been
            stripped away.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, the contemporary world poses institutional obstacles to turning
            knowledge into a public good. Together they effectively subordinate teaching to
            research, instead of treating them as equal and complementary university functions.
            Two such barriers stand out: academic writing conventions and intellectual property
            legislation. Each in their own way imposes additional restrictions on access to
            knowledge beyond the bare fact that new knowledge is at first available only to a
            few. Such restrictions enable knowledge to become a form of ‘social capital’ that
            accrues power to those able to pay the entry costs to gain access. This is close to the
            sense of ‘knowledge as power’ that the Protestant Reformers found corrupting of
            the Roman Catholic Church. Nevertheless, it is fair to say that Plato and even his
            latter-day followers such as Auguste Comte would have been pleased, since
            knowledge-based power is arguably the least violent way to impose social order
            (Fuller, 2006a: Chap. 4). Of course, I refer here to the technical training required to
            be deemed an ‘expert’ with ‘credentials, the ultimate expression of research-based
            knowledge. This in turn breeds a complementary culture of ‘trust’ and ‘deference’
            on the part of those who lack the relevant credentials. The result is a very explicit
            sense of social hierarchy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Although teaching was hardly mentioned at all in my first book, Social
            Epistemology (Fuller, 1988), the above discussion of teaching – which has figured
            increasingly in my work in subsequent years – is inspired by my long-standing
            interest in translation, the subject of Part Two of that original book. A key distinction recognized in translation theory, due to the Bible translator Eugene Nida, is between
            formal equivalence and dynamic equivalence. In Social Epistemology, I observed
            that it corresponds to the difference between exegesis and hermeneutics of the Bible:
            The former tends to construct the Bible’s authority by distancing the work from its
            readers, while the latter constructs Biblical authority based on its continuing
            relevance to people’s lives. The distinction can be equally cast in terms of education.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching in the spirit of a formally equivalent translation aims to get students to
            see the world as the teacher sees it. This is the essence of technical training, which –
            as Thomas Kuhn rightly stressed – is the sense in which a scientific paradigm
            imposes a worldview on practising scientists. Thus, someone who trains to be a
            physicist implicitly agrees to adopt the cognitive framework of, say, Newton or
            Einstein, as mediated by the instructor. But this arrangement renders knowledge a
            positional good, not a public good. Notwithstanding the significance increasingly
            accorded to the Ph.D. in the modern university, if the university is to remain faithful
            to the Humboldtian spirit, it should be the natural enemy of this approach. In Thomas
            Kuhn: A Philosophical History for Our Times, I made this point in terms of Ernst
            Mach’s vehement opposition to Max Planck’s proto-Kuhnian approach to the teach-
            ing of science in the early twentieth century (Fuller, 2000b: Chap. 2). Here I follow
            Mach in relating education to translation as dynamic equivalence. Thus, the teacher
            aims to get the students to understand the significance of a piece of knowledge on
            their own terms, which means that the teacher must recast it in a way that directly
            addresses the students’ worldview, even if it turns out to challenge, test or critique
            the students’ beliefs. The measure of successful teaching in this context is that stu-
            dents come to see the knowledge that they acquire as relevant to their normal lives,
            rather than as a self-contained expertise that is relevant only in certain ‘professional’
            settings, in which they exert power over others.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Education in the dynamic equivalence mode is clearly difficult for teachers
            whose identity is tied to the way they learned the content they wish to impart. This
            burden perhaps weighs more heavily on teachers with degrees from elite institu-
            tions, which have left them thinking that their training provides them a unique posi-
            tion from which to pursue knowledge. And this is certainly true, sociologically
            speaking, given the ease with which they can secure attractive posts, funding and
            recognition more generally. However, it is much less obviously true in strictly intel-
            lectual terms. In any case, a sign of your intellectual maturity lies in the ability to
            release your knowledge from the context in which you acquired it. The act of teach-
            ing is an opportunity for teachers to reacquaint themselves with what they were
            taught, if only to remain competitive with the students to whom the teachers have
            imparted knowledge. All this activity ‘translates’ knowledge in the sense of adding
            meaning to it through recontextualization. In this respect, grammar – understood as
            the laws that modern states have drafted to regulate language flow within their bor-
            ders – enables language to be turned into an object of rentiership, as barriers are
            placed to what counts as adequate transmission in what would otherwise be the free
            exchange of thought and expression. It was in that spirit that the famous Italian saying, ‘Traduttore, traditore’ (‘To translate is to betray’) was coined in the four-
            teenth century to characterize the French translators of Dante’s Divine Comedy,
            arguably the first classic work in the newly emerging Italian language. It was as if
            the French had stolen something belonging to the Italians by rendering it in their
            own language.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, Chomsky (1971), invoking Humboldt, observed that the very possibility
            of such grammatical boundaries – albeit artificial – between languages implies the
            existence of a ‘universal grammar’ innately possessed by all humans, which is the
            mental equivalent of commonly owned property. From that standpoint, translation
            is less a transgression of the source language than a reappropriation that adds value
            to the original text precisely by being rendered in the target language. Indeed,
            history is full of cases in which a text that was relatively ignored in its original lan-
            guage become a gamechanger for readers in the language into which it is trans-
            lated – often due to the translator’s spin. More speculatively, the filial relations
            among languages suggests that ‘multilingualism’ might be better seen as polyvalent
            monolingualism. This was the spirit in which students of language from the mid-
            eighteenth to the mid-twentieth centuries, across the humanities and nascent social
            sciences, took something called ‘mythology’ to have revealed our species-level
            capacity for the articulated arrangement of words to establish group identity, soli-
            darity and mutual recognition among groups over great expanses of space and time.
            Among the most notable of these scholars of mythology were Max Müller, James
            George Frazer and Claude Lévi-Strauss, all of whom believed that the repeated
            storylines across the world’s mythologies reflected a relatively small number of
            ultimate source languages (Ursprachen), perhaps even resolvable into one, at which
            point Chomsky’s universal grammar comes into the frame. This overall view led the
            great nineteenth-century US Transcendentalist thinker Ralph Waldo Emerson to
            characterize language as ‘fossil poetry’, whereby ‘poetry’ literally referred to its
            productive (poiesis) character to conjure up worlds, à la mythology. Any rebooting
            of the university’s Humboldtian legacy needs to take this profoundly emancipatory
            conception of language into account.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In conclusion, let me return to my original point that knowledge needs to be
            manufactured as a public good, and that this can only happen through the ‘creative
            destruction of social capital’. I have increasingly characterised university tenden-
            cies to privilege research over teaching – and hence reduce teaching to technical
            training – as intellectual ‘rent-seeking’. One of the few things on which capitalist
            and socialist economists agree is that they abhor rent as a source of value, mainly
            because it relies on what they regard as an artificial restriction of access to capital.
            ‘Rentiers’, in Marx’s memorable term, literally try to capitalize on sheer ownership,
            even though the fact that someone owns something valuable now should not by
            itself be the source of future value; rather, ownership should be treated simply as the
            basis on which new value is generated through productive labour. To conclude oth-
            erwise is to mystify the original acquisition process by which someone comes to
            own something. Yet, when scientific paradigms are named after, say, Newton, Darwin or Einstein, origins are effectively mystified. Newton did not merely arrive
            at an impressive number of truths about the physical universe. His methods for
            arriving at those truths became the template for physical inquiry, arguably to
            this day.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This last point recalls what economists call ‘path dependency’ in the history of
            technology, which means the tendency for one innovation to become so entrenched
            that all competitors must play by the rules of its game. Thus, when Henry Ford
            shifted the paradigm of personal transport from the horse to the automobile, future
            competitors aimed to build a better car, not a better horse. However, as we have
            seen, the intellectual trajectory that flows from the Protestant Reformation to the
            scientific method has been rightly hostile to the narrowing of horizons that this ten-
            dency involves. Thus, instead of grounding the legitimacy of the Christian message
            on the dynastic succession of the Popes from St Peter onward, the Protestants
            favoured the approach of St Paul, who spread Christianity to many audiences in a
            more direct and even customized manner. But arguably, path dependency is a bigger
            problem in science today. To be sure, it captures well the emergence and evolution
            of a Kuhnian scientific paradigm, as each new generation of scientists is compelled
            to ‘stand on the shoulders of giants’, to recall Newton’s characterisation of his own
            achievement. Evidence of this may be found in academic citation practice, whereby
            it is near impossible to establish one’s own claims to knowledge without referencing
            specific precursors, on whose work one then claims to build.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But interestingly, science’s instinctive Protestant tendencies have pushed back
            against path dependency. The clearest expression of this hostility is the insistence on
            a sharp distinction between the context of discovery and the context of justification.
            This distinction, associated in my student years with the logical positivists and the
            Popperians, has a long prehistory. It is rooted in the intuition that if science aspires
            to be a ‘universal’ form of knowledge, then its knowledge must be both universally
            valid and universally available. It follows that the originator of any knowledge claim
            is no more than a historical accident, from an epistemological standpoint. Popper
            sometimes even said that the context of discovery was ‘irrational’. What he meant
            was that we should not fetishize the origins of knowledge claims because anyone
            else could have come up with them, under the right conditions – assuming that they
            are indeed valid knowledge claims. This in turn makes the epistemological task
            inherently social, in a way that is related to the role of teaching in the Humboldtian
            university, which is ultimately about demystifying the origins of knowledge to
            enable everyone to use that knowledge pursue their own future. The question that
            remains, of course, is whether this ‘empowering’ view of knowledge, which has
            been central to my version of social epistemology, can be pursued in a way that in
            the end retains a common understanding of ‘society’ sufficient to sustain knowledge
            as a public good. I shall pick up this thread again in the next chapter, but I shall now
            round out the current discussion with a deeper look at the relevance of Popperian
            philosophy to open science.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Although in many respects Saint-Simon and Proudhon presented opposing –
            perhaps even diametrically opposed – visions of socialism, nevertheless both wanted
            to maintain the dynamic character of the capitalist system, which is associated with
            entrepreneurship’s innovative brand of risk-taking. Here they were joined as one
            against the Marxists, who tended to regard that signature feature of capitalism as the
            harbinger of its ultimate self-destruction. The corresponding spirit in the philosophy
            of science to this anti-Marxist stance is the hostility to ‘induction’ as a form of infer-
            ence in the broadest sense, whereby the future is presumed to be overdetermined by
            the past. To be sure, induction is inherent to our default understanding of reality, in
            the sense that we recognize change over time only relative to invariance. However,
            as Karl Popper famously insisted, the progressive nature of scientific inquiry
            depends on our suspending that intuition by systematically disturbing the back-
            ground condition of invariance that which ‘we’ take for granted. And by ‘we’,
            Popper included the scientific community, especially in its institutional understand-
            ing that was made popular in the 1960s by Thomas Kuhn. In this context, the experi-
            mental method functions as the critical foil, as epitomized in the ‘null hypothesis’,
            whereby the default expectation is pitted against a rival newcomer who proposes an
            alternative future – at least in terms of specific predictions – based on a com-
            mon past.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The source of Popper’s instinctive opposition to inductive reasoning lay in his
            original training in Gestalt psychology under Karl Bühler, Freud’s great Viennese
            rival in the 1920s. It played a subtle but significant role that continued to influence
            the philosophy of science for the rest of the twentieth century. Key to this influence
            was the famed ‘Gestalt switch’, an experimental effect whereby an ambiguous fig-
            ure can be interpreted in two radically different ways, depending on how the experi-
            menter contextualises the figure for the subject. What Popper called a ‘crucial
            experiment’ potentially functioned in this capacity if the newcomer hypothesis
            proves to be correct, thereby forcing scientists to reconsider whether their default
            understanding of the phenomenon in question had been flawed all along at some
            deeper level. To put it in Gestalt switch terms, if what you originally saw as a ‘rab-
            bit’ suddenly appears to be a ‘duck’ because the background conditions of the
            ambiguous figure have changed, then was the figure a duck all along? For Popper’s
            great opponent Kuhn, this characterised a paradigm in ‘crisis’ mode, the prelude to
            a ‘scientific revolution’, a Gestalt switch on the level of religious conversion (Fuller,
            2015: Chap. 4).]]>
			</paragraph>
			<paragraph>
				<![CDATA[A striking feature of the Gestalt switch, which emboldened Popper but disturbed
            Kuhn, is the epistemic efficiency with which a radical change in understanding is
            brought about. After all, a scientific revolution – just like the shift from the ‘rabbit’
            to the ‘duck’ interpretation – does not generally involve replacing all the old data
            with new data. On the contrary, it involves reorganizing the old data in light of some
            strategically generated new data (e.g., from a crucial experiment) so as to give the totality of data an entirely new look. And this ‘new look’ may extend retrospectively
            to reinterpreting what past scientists had ‘really’ been talking about. On this basis,
            Popper’s most sophisticated follower, Imre Lakatos, saw the very idea that our past
            knowledge automatically carries over into the future – the presumption of induc-
            tion – as a failure to explore rival hypotheses. From him, a Kuhnian paradigm,
            whereby the dominance of one theoretical horizon restricts the range of scientifi-
            cally viable alternatives, operates more like a closed shop than an open market. In
            response, Lakatos (1978) advanced a ‘rationally reconstructed’ understanding of
            the history of the science, which explored how science could have developed more
            efficiently even within its historical constraints. Some Lakatosians went on to pro-
            mote Bayesian inference, which depicts scientific progress as occurring through
            successive rounds of competing hypotheses responding to common evidentiary
            challenges (Howson & Urbach, 1989).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The epistemological motivation here is like what animated the Progressive
            reformers of the early twentieth century discussed earlier. Both take the sheer fact
            of primacy – that a piece of land is owned by descendants of its first owners, that an
            invention was patented or a discovery made by a particular individual or, indeed, that
            a field of science follows a paradigm based on some foundational achievement – to
            be ‘contingent’ in the strong sense that, under slightly different conditions, the orig-
            inating party could well have been different and the course of events would have
            taken a different path, opening up different opportunities and developments.
            Cognitive psychologists nowadays encapsulate this entire range of phenomena –
            when it occurs in the mind of a single person – as ‘confirmation bias’ (Fuller, 2015:
            Chap. 1).]]>
			</paragraph>
			<paragraph>
				<![CDATA[It would seem to follow, as a matter of ‘progressive’ science policy, that one
            should project that sense of contingency into the future, what Popper (1957) himself
            called ‘reversibility’. His model was regular democratic elections, whereby voters
            are provided the opportunity to change the party of government regardless of
            whether things are going well or poorly. This is the exact opposite of the implied
            policy of his rival Kuhn, who held that the epistemic strength of a science is that the
            paradigm dominating a field’s research agenda is given an exclusive right to fail on
            its own terms – that is, it must persistently fail to solve problems it had set for itself,
            before the sort of fundamental alternative research trajectories that might result in a
            ‘scientific revolution’ are licensed. As the economists today would say, Kuhnian
            paradigms enjoy a license to run ‘path dependency’ into the ground. 150 years ago,
            Renouard and Proudhon would have simply accused Kuhn of encouraging the sci-
            entific community to treat literally its domain of inquiry as ‘intellectual property’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Underlying the liberal and even proactive attitude to scientists fundamentally
            changing their minds that one finds in Popper and his students is a belief that our
            relationship to reality is sufficiently secure that it can tolerate a variety of ways of
            accessing and utilizing it, including ones that at a given time might be deemed
            ‘unfair’, ‘unjustified’, ‘immoral’ or even ‘untrue’. Such was the hidden lesson of
            Galileo’s ‘success’ as told by Popper’s most radical follower, Paul Feyerabend.
            According to Feyerabend (1975), Galileo fabricated his most famous experiments
            and could not explain the optics behind his own makeshift (‘pimped’, in today’s slang) telescope. In that respect, the Papal Inquisition was right to prosecute
            Galileo – and nowadays he would be pronounced guilty of ‘research fraud’. Yet, of
            course, the subsequent history of science proved Galileo to have been largely cor-
            rect, even though his beliefs were not justified at the time he presented them (Fuller,
            2007a: Chap. 5). On this basis, Feyerabend notoriously proposed ‘methodological
            anarchism’ as a principle of science policy, contrary to the position held by most
            philosophers of science, including arguably Popper himself. In short, for Feyerabend
            there is no royal road to the truth. Sometimes a vivid imagination and persuasive
            rhetoric can do the work of rigorous methodology, especially – as in Galileo’s case –
            it inspires others to make up the probative difference.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Feyerabend’s position has proved perplexing for philosophers because it involves
            a head-on collision between the ethics and the epistemology of knowledge produc-
            tion. After all, a cynical lesson that might be taken from Feyerabend’s Galileo tale
            is that deception, perhaps even self-deception, may be a surer route to the truth than
            proceeding in a more ‘truthful’ manner. This goes against the general tendency
            since the end of the Second World War to tie truth-seeking to moral scruples. This
            turn to ‘research ethics’ began in the wake of the 1946 Nuremberg Trial, when sci-
            entists adopted the convention of not citing the results of Nazi research on human
            subjects, due to the unethical character of its conduct. This mentality was extended
            over subsequent decades, such that nowadays publication in scientific journals may
            be prohibited or retracted – and authors may be ostracised from the scientific com-
            munity – under a variety of morally relevant conditions. These include failure to
            declare background financial and political interests supporting one’s research, fail-
            ure to secure proper consent from subjects, failure to acknowledge properly the
            sources of one’s insights and failure to represent properly the means by which one’s
            findings were obtained (Schrag, 2010).]]>
			</paragraph>
			<paragraph>
				<![CDATA[These potential breaches of research ethics reflect two countervailing features of
            the contemporary research environment. The first is a preoccupation with setting
            limits on the access to knowledge claims. Researchers must not overstate their
            achievements. The need to declare interests and secure consent function as a ‘handi-
            cap’ in a research environment that is presumed to be intensely competitive and
            where certain players, due to their initial financial and/or political advantage, might
            otherwise be granted too much authority. Put cynically, the researcher is procedur-
            ally inhibited from taking arguably the most direct path to a desired outcome – say,
            by acquiring all the money and all the power needed to impose it on the research
            community. The second feature, which cuts against the first, is that the knowledge
            claims proposed by researchers are ultimately judged in terms of their face validity:
            Is the experiment or field work described in a way that leads one to believe that it
            took place, and are the outcomes plausible, both in themselves and the conclusions
            drawn from them? Read against the backdrop of the increasingly formulaic presen-
            tation of scientific research, such criteria invite both plagiarism and outright fabrica-
            tion. The only real question here is whether there really has been an increase in
            research fraud or simply more of it has been caught because the stakes – often
            financial – motivate peer reviewers to dig more deeply into the details of the research
            than they would have in the past.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Given these countervailing features of the contemporary research environment,
            one might reasonably conclude that the drive to place ‘ethics’ at the heart of research
            practice is bound to fail. However, I don’t wish to decide the matter here. Instead I
            would simply remind readers that the philosophy of science has long included a
            tradition of thought that is studiously indifferent to the truth of fundamental theories
            and even the truthfulness of the people proposing them: instrumentalism. For the
            instrumentalist, a theory’s scientific significance is simply the span of control that it
            allows over a target range of phenomena. The more one controls, the more one
            knows: knowledge is power. To be sure, the instrumentalist understands ‘knowl-
            edge’ in terms of savoir (‘knowing how’) rather than connaissance (‘knowing
            that’). Nevertheless, for our purposes, what matters is that the scientist need not
            believe the theories she proposes and theories themselves need not be true; indeed,
            she need not even care. At most, so says the instrumentalist, the scientist needs to
            act ‘as if’ her theories were true. Perhaps the moral attitude most closely aligned
            with instrumentalism is hypocrisy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Not surprisingly perhaps, instrumentalism has always been regarded with some
            suspicion. Consider the two leading philosopher-scientists promoting instrumental-
            ism in the early twentieth century: Ernst Mach and Pierre Duhem. The former would
            arguably reduce science to technology and the latter would subordinate it to theol-
            ogy. In both cases, science would be pursued as a means to some ‘higher’ end, not
            as an end in itself. Put bluntly, ‘the end justifies the means’, in which the conduct of
            science itself constitutes the ‘means’. Duhem favoured instrumentalism because it
            prevented science from overtaking theology in terms of setting humanity’s meta-
            physical horizon, whereas Mach favoured the same doctrine because it prevented
            science from ossifying into a secular theology that would then be imposed as soci-
            ety’s metaphysical horizon. (The latter explains Lenin’s demonization of ‘Machists’,
            since Lenin wanted to impose Marxism in just such a fashion.) Philosophers of
            science have tended to ignore these countervailing reasons for embracing instru-
            mentalism because they characteristically fixate on the similarity in content of what
            Mach and Duhem endorsed at the expense of their radically different political moti-
            vations. Mach was a liberal parliamentarian and Duhem a Catholic restorationist.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Interestingly, Popper and his followers – including Lakatos and Feyerabend –
            have also been accused of scientific instrumentalism, yet they have vehemently
            denied it – and with considerable justification (Fuller, 2003: Chap. 4). Yet, the sus-
            picion lingers. The reason, I suggest, is that the Popperians treat ‘science’ in the
            same spirit as both Saint-Simon and Proudhon treated ‘socialism’, namely, as the
            material culmination of humanity’s spiritual journey. The charge of ‘instrumental-
            ism’ functions as the charge of ‘capitalism’ that Marxists made against these so-
            called ‘utopian socialists’. In both cases, the charge boils down to presuming that
            the material world is much more fluid, pliable and biddable than it really is. But
            contra the naysayers, one can take or leave fundamental theories or social structures
            under the right conditions and with minimal effort in the name of ‘progress’.
            Revolutions are ultimately about rearranging the parts of already existing wholes to
            meet new challenges. They are Gestalt switches. The idea that the future will signifi-
            cantly repeat the past – the intuitive basis for induction – presumes that memory is saved rather than lost over time. But this is false to human psychology. In fact,
            memory needs to be actively maintained so as not to be lost. This explains the sig-
            nificance of both education and propaganda as institutions dedicated to the rein-
            forcement of collective memory, as well as the profound and often subtle role that
            generational change has played in the reconstitution of collective memory. Once
            again, the economists got the measure of the situation by reducing the intuitiveness
            of induction to a ‘sunk cost’, a ‘that was then, this is now’ attitude, which should not
            inhibit a change of course in the future (Steele, 1996). In short, there is always
            everything to play for.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pace Fredric Jameson and Slavoj Zizek, it is only Marxists who find it easier to
            imagine the end of the world than the end of capitalism. They can’t get their heads
            around the true nature of capitalism. For the past two decades, this has been the hid-
            den message of Philip Mirowski’s relentless critique of neoliberalism, which unwit-
            tingly has done more to undermine his fellow critics than neoliberalism itself. The
            increasingly baroque appeals to ontology – nowadays called ‘critical realism’ – that
            have characterised accounts of capitalism inspired by ‘Western Marxism’ for the
            past six decades have amounted to a reification of Marxists’ own ignorance of capi-
            talism’s workings, which in the meanwhile neoliberalism has raised to epistemo-
            logical self-consciousness. As Ricardo first realized, once human labour is seen as
            profoundly ‘fungible’ – namely, that it might be done more efficiently by new tech-
            nology – then attempts to protect jobs and secure wages started to look like rent-
            seeking. The logical conclusion, which goes to the ‘spirit of capitalism’, is that the
            ‘human’ is whatever manages to recover and enhance its value by shifting its shape
            in a dynamic market. In this respect, the human is not opposed to capital; it is capital
            in its most protean form, a proposition that transhumanists nowadays promote as
            ‘morphological freedom’ (Fuller, 2019c: Chap. 2). In effect, neoliberalism has man-
            aged to perform a Gestalt switch on Marxism.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Abstract This chapter begins by situating academic rentiership in the deeper issues
            surrounding ‘cognitive economy’ that lie at the heart of the Western philosophical
            tradition. The psychological phenomenon of the ‘Gestalt shift’ exemplifies various
            ways in which the flow of information can be channelled to enable or disable certain
            possible ways of regarding the world, or ‘modal power’. Against this background,
            such familiar concepts from academic practice as ‘quality control’ and ‘plagiarism’
            acquire a new look. Quality control sustains rentiership, whereas plagiarism under-
            mines it. Aesthetics – especially modernist concept of ‘artworld’ – offers guidance
            on how to understand this point. The rest of the chapter is largely concerned with the
            historical swings back and forth between rentiership and anti-rentiership in the his-
            tory of knowledge production. Rentiership’s decline is associated with corruption of
            its quality control processes, which reveals academia’s reliance on equally corrupt
            power elites in society. Moreover, the periodic introduction of new communication
            media over which academia cannot exert proprietary control – from the printing
            press to the internet – has historically served to redistribute the sphere of plausible
            knowledge claims, and modal power more generally. The chapter ends with a con-
            sideration of the stakes in the reduction of knowledge to information, an explicit
            aim shared by Silicon Valley enthusiasts and neoliberals alike.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The ‘normative’ dimension of my version of social epistemology flies under the flag
            of ‘knowledge policy’, a phrase that has always carried strong economic overtones
            (e.g., Fuller, 1988: part 4; Fuller, 2002; Fuller, 2015: Chap. 1). In this respect, I have
            walked in the footsteps of Charles Sanders Peirce, who invented a field called the
            ‘economics of research’ and his latter-day follower Nicholas Rescher (1978, 2006).
            However, it would be a mistake to think that regarding knowledge in economic
            terms is merely a late nineteenth century innovation. On the contrary, ‘economy’ in the sense of efficiency has been endemic to the Western pursuit of knowledge from
            its inception. Indeed, both Plato and Aristotle were interested in ‘explaining the
            most by the least’, though they interpreted this principle in rather opposing ways,
            which have served to determine the subsequent history of philosophy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plato interpreted the principle as calling for a unified understanding of reality,
            very much in the spirit of physicists who continue to seek a ‘Grand Unified Theory
            of Everything’. This meant that the diversity of phenomena that we normally expe-
            rience constitutes an inefficient understanding of reality that must be ‘resolved’ in
            some fashion, say, in terms of the outworking of the laws of nature under specific
            conditions. Such laws are in character quite unlike the phenomena experienced
            because they range over not only actual but also possible states of the world. Thus,
            what we normally see as necessary features of the world – ‘structures of the life-
            world’, as Alfred Schutz might say – turn out to be, under closer and deeper inspec-
            tion, features that are contingent on certain prior opportunities and decisions – ones
            that perhaps faced some cosmic intelligence.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plato and his immediate followers sought to grasp this play of hidden forces
            through the faculty of nous in a manner that is still associated with mathematical
            intuition and thought experiments. However, starting in the Middle Ages, these
            mental manipulations were increasingly performed, not in one’s head but on the
            world itself, in what we now call ‘experiments’ in the proper sense. Accompanying
            this development was a more specified understanding of the Platonic quest for unity,
            namely, that the test of our knowledge of the underlying laws is that we can use
            them to simulate the aspects of reality that we wish to understand – and quite pos-
            sibly improve upon. This is what historians of science call the ‘mechanical world-
            view’, and it survives in all fields where model-building is taken seriously as a route
            to knowledge. For today’s descendants of Plato, the phenomena of the world are the
            outputs of some executed cosmic computer programme, in terms of which scientific
            inquiry amounts to reverse engineering or even hacking.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In contrast, Aristotle saw the diversity of reality as more directly indicative of
            nature’s efficiency, In that case, relatively minimal cognitive adjustment on our own
            part is required to be optimally attuned to reality. What philosophers nowadays call
            the ‘correspondence theory of truth’, first expressed by Aristotle’s staunchest medi-
            eval champion Thomas Aquinas, derives from this tradition. Indeed, Aquinas’ own
            formulation – veritas adequatio intellectus ad rem (‘truth is the alignment of the
            mind to the thing’) – suggests that such a state of mind could arise with relatively
            little deliberate effort. In that case, science is not about the search for hidden laws
            that are far from our normal way of seeing things. Rather, it is a glorified pattern
            recognition exercise whereby we come to see the various patterns which together
            constitute the world. The path from perception to cognition on the Aristotelian view
            is rather shorter than on the Platonic view.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the modern era, the Aristotelian position has been often touted as ‘common
            sense’ realism, a more sophisticated and contemporary version of which is Nancy
            Cartwright’s (1999) ‘patchwork’ scientific ontology. Over the centuries, the sorts of
            hierarchies that represent ‘natural order’ in biological classification systems have
            most consistently expressed Aristotle’s sense of cognitive efficiency, insofar as the relationships between ‘orders of being’ are based on morphological resemblances –
            that is, how organisms appear to the eye. In this context, today’s controversy in
            taxonomy over whether morphological resemblance should yield to genetic overlap
            as the basis for organizing species in the ‘tree of life’ marks a Platonic challenge to
            a field traditionally dominated by Aristotelian sensibilities, as it would allow two
            similar looking species to have been brought about by radically different genetic
            paths (Wilson, 2004). In effect, the taxonomic judgements of the field biologist (aka
            Aristotelian) would have to defer to those of the lab biologist (aka Platonist).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Plato-Aristotle dispute over cognitive economy has been more central to
            Western intellectual history than is normally acknowledged. Consider the role of
            substitution in logic and economics, which is normally rendered as ‘functional
            equivalence’. For the founder of modern symbolic logic (and arguably analytic phi-
            losophy), Gottlob Frege, the ‘Morning Star’ and ‘Evening Star’ are functionally
            equivalent because both refer to the planet Venus, but under different observation
            conditions. Likewise, two qualitatively different goods are functionally equivalent
            in the market if a price is agreed for their exchange. In that sense, the two goods are
            simply alternative ways to spend money; hence, Marx’s critique of ‘commodifica-
            tion’, according to which capitalism turns money into metaphysics, as the exchange
            relation becomes the ultimate ground of being. The supposed difference between
            the logical and economic senses of ‘substitution’ is that the former involves a pre-
            sumed identity, whereas the latter requires identity to be constructed. But is it really
            that neat?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perhaps not, since it took an astronomical inference by Pythagoras to show that
            ‘Morning Star’ and ‘Evening Star’ both refer to Venus. That inference happened at
            a particular time and place, the force of which is masked by using ‘discovery’ to
            describe the event. After all, ‘discovery’ (as opposed to ‘invention’) suggests that
            Pythagoras’ inference could and would have been made at some point by someone.
            To be sure, this is the Platonic stance in a nutshell, and Plato’s own concern was over
            who should take that decision – the one or the many. In the future, the Turing Test is
            likely to be the proving ground for this sensibility, as artificial intelligence-based
            intellectual work begs the question of what is the ‘added value’ of being human
            (Fuller, 2019a). In effect, if something ‘not-human’ passes as ‘human’, then the
            Platonist would welcome this as educating us that what it means to be ‘human’ does
            not require what we had previously thought in terms of substratum. In effect, we
            will have found a different (‘machinic’) path to realizing humanity. Indeed, this
            prospect was adumbrated as ‘functionalism’ at the dawn of the computer age by
            Hilary Putnam (1960). In contrast to this entire line of thought, an Aristotelian
            would regard the mistaking of a computer for a human as simply a misjudgement,
            since the ‘human’ is intimately tied to the sorts of creatures that we normally take
            to be ‘human’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Gestalt psychology provides an interesting lens through which to see this
            difference between Platonic and Aristotelian understandings of cognitive efficiency,
            one that was first surfaced by the social psychologist Kurt Lewin (1931) and later
            taken up by Edmund Husserl (1954). (In both cases, the Platonic position is called
            ‘Galilean’.) The very idea that we tend to see the world as ‘always already’ patterned would suggest an Aristotelian sensibility, were it not for the fact that the pattern we
            see can be so easily manipulated depending on the context of perception, which in
            turn suggests a Platonic sensibility. Thus, in a ‘Gestalt shift’ experiment, we may
            start by seeing an ambiguous figure as a duck but end up seeing it as a rabbit, yet at
            both moments the image appears as an ‘efficient’ representation of reality, both in
            terms of directness of perception and comprehension of experience (i.e., the
            phenomena are ‘saved’). Aristotle may explain the experience of the subject, but
            Plato explains the behaviour of the experimenter. Unsurprisingly perhaps, in the
            mid-twentieth century, the Gestalt shift was a popular means – used by, among oth-
            ers, Ludwig Wittgenstein, Russell Hanson and Thomas Kuhn – to explain concep-
            tual change, especially in science (Fuller, 2018a: Chap. 6).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The meta-lesson of Gestalt psychology is that your perception of the world is
            rendered more changeable once you change your understanding of how that percep-
            tion was brought about. This insight has made Gestalt psychology an endless fount
            of ideas for propaganda and advertising. It has been also used to explain how the
            people behind the early modern ‘Scientific Revolution’ came to shift from an
            Aristotelian to a Copernican (aka Platonic) worldview: that is, from the standpoint
            of the Earth to the standpoint of the Heavens. It involved thinking in radically dif-
            ferent terms about the relationship between what we experience and what we know.
            In effect, these original scientists understood reality from the standpoint of the
            Gestalt experimenter rather than the Gestalt subject – where the experimenter was a
            proxy for a cosmic intelligence, aka ‘The Mind of God’. Optics was perhaps the
            main site of contestation for trying to explain how our senses filter reality, which the
            mind then actively reconstructs (Crombie, 1996: Chap. 16). To this day, philosophy
            frames most ‘problems of knowledge’ in terms of the higher order interpretation of
            visual data.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Now what does the above have to do with rentiership as an economic feature of
            academia? Let us start with an observation about the history of technology but is
            more generally applicable to any strategic decision-making, including knowledge
            policy. For any path-breaking innovation, such as the automobile, there are usually
            at the outset several competing prototypes, with various strengths and weaknesses.
            However, over time one becomes dominant and then sets the pace for the rest. This
            suggests two complementary concepts: opportunity cost and path dependence. An
            opportunity cost consists in alternative states of the world that are made less likely
            if not impossible as a result of a decision taken – such as the market’s decision to
            back Ford’s way of doing cars in the early twentieth century. Path dependence refers
            to the efficiency gains that result from any such decision, as infrastructures develop
            to reinforce its efficacy, removing the original alternatives from serious consideration in the future. In the case of Ford, relatively low prices and simple
            design trump concerns about human safety and environmental protection. These
            anti-Fordist concerns only resurface a half-century later, once the Ford-anchored
            automotive market has stabilized. By that time, they have become ‘negative exter-
            nalities’ that need to be ‘managed’, but more in the spirit of an exception to a rule
            rather than a game changer.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At stake in opportunity costs and path dependence is what I call modal power,
            the manipulation of intuitions about what is possible, impossible, necessary and
            contingent. I regard modal power as the cornerstone of the ‘post-truth condition’
            (Fuller, 2018a: Chap. 2). Opportunity costs look at modal power from Plato’s ‘sec-
            ond order’ standpoint, as the logicians say, while path dependence sees it from
            Aristotle’s ‘first order’ perspective. Classical political economy’s default ‘free mar-
            ket’ mentality – especially its assumption that more competitive markets are ‘freer’
            – may be seen as part of a concerted second-order attack on rentiership as a first-
            order phenomenon, whereby rentiership is understood to be the power that accrues
            to sheer possession, by whatever means it was brought about and to whatever ends
            it might serve. While Marx correctly identified the classical political economists as
            capitalism’s original house theorists, their support of private property was focussed
            mainly on the opportunities for investment that it provided. In that respect, for them
            venture capitalism is ‘capitalism’ in its purest sense. In classical political economy,
            land was valued not as a power base for its owner who could then create bottlenecks
            in the flow of capital, but as a platform for launching any number of projects from
            which all those involved in the transaction might benefit.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is worth pausing to consider the peculiar – some might say alchemical –
            metaphysics that underwrites this alliance of scientific Platonists and free market
            capitalists whom I’ve portrayed as being so vehemently opposed to the modal power
            embodied in rentiership. A commonplace of the modern economic worldview is that
            humans harbour infinite longings but are constrained by limited resources. Such is
            the fate of spiritual beings trapped in a material world – at least that would have
            been the gloss given by Joseph Priestley, William Paley and Thomas Malthus, who
            were among several radical natural theologians who contributed to the foundations
            of classical political economy in the late eighteenth and early nineteenth centuries.
            Nevertheless, and this is the main point, even these natural theologians recognized
            that humanity had already managed to substantially improve its lot over that of other
            animals. They differed amongst themselves over the terms on which one might
            speak of ‘limits to growth’, but they agreed that the ‘Industrial Revolution’ dawning
            in their midst promised much overall growth for the foreseeable future.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Was this apparent human capacity to generate ever greater wealth in the face of
            absolute scarcity an illusion or reflective of some strategy that we had implicitly
            discovered to overcome our material limitations, if not our species finitude alto-
            gether? Adam Smith already started the ball rolling by suggesting that the secret lay
            in the rational organization of labour. A half-century later, Count Saint-Simon
            repaid the compliment by coining the word ‘socialism’ for the policy of governing
            all of society on this basis. The difference between Smith and Saint-Simon was that
            the former believed that people left to their own devices amongst themselves – without the imposition of legal restrictions on land transfers and labour
            mobility – could reorganize peacefully and profitably, whereas Saint-Simon thought
            that this required a special expertise – so-called ‘captains of industry’, professional
            organizers of humanity, who nowadays we might associate with ‘knowledge man-
            agers’ (Fuller, 2002).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Notwithstanding their well-rehearsed differences, the founders of capitalism and
            socialism shared the belief that the way out of human finitude was to disembed
            people from their traditional social relations and re-embed them in contexts that
            made the most productive use of their talents, effectively releasing their hidden
            energies. This way of looking at people amounts to entire societies undergoing a
            Gestalt shift. In other words, people’s capacities remain constant across the shift but
            there is a productivity gain from ‘before’ to ‘after’ the shift as those capacities come
            to be more fully ‘exploited’ (a term that acquires negative connotations only after
            Marx). In Gestalt psychology terms, the ‘figure’ remains the same, but the ‘ground’
            has changed. Put crudely, when a slovenly serf is shifted from the field to the fac-
            tory, he or she becomes a productive worker. Agriculture provided the model for this
            way of thinking: The starting shot for the Industrial Revolution was fired when the
            first person saw a relatively undisturbed part of nature as ‘raw materials’ for
            human use.]]>
			</paragraph>
			<paragraph>
				<![CDATA[An implication of speaking about modern societies as ‘dynamic’ is that they try
            to minimize the opportunity costs of its members having been born a certain way –
            that is, at a particular time and place, to a specific family, with certain capacities,
            etc. They make people more ‘shiftable’ in the Gestalt sense. That someone was born
            of a family of serfs doesn’t mean that he or she must remain a serf forever – and
            hence in an indefinite state of low productivity. One can become more productive
            than that – and thereby provide greater overall benefit – under the right conditions.
            To be sure, this leaves open how exactly those conditions are to obtain, answers to
            which capitalists and socialists then provide competing answers. In either case,
            change is in the cards, with path dependence cast as the ever-present foe, as enslave-
            ment to one’s birth morphs into the slavery of routinized labour or Big Brother. The
            phrases ‘creative destruction’ and ‘permanent revolution’ are vivid expressions of
            the capitalist and socialist antidotes to path dependence. The shared mindset per-
            haps explains why Marx used the word ‘protean’ to characterize capital
            (Fuller, 2021a).]]>
			</paragraph>
			<paragraph>
				<![CDATA[This general line of thought gets complicated in the second half of the nineteenth
            century, as thermodynamics adds nuance to the idea of nature’s scarcity. It is no
            longer simply about the inherent finitude of material reality, which might have
            resulted from the Biblically fallen state of humans, which had been the hidden
            Calvinist stick to prod humans into self-improvement. In addition, our very efforts
            to reorganize matter to increase productivity also raise the level of material scarcity,
            now understood in terms of reducing the amount of ‘free energy’ available in the
            universe. This opened up two radically opposed horizons: on the one hand, pessi-
            mists who believe that the irreversibility of this loss of free energy – aka entropy –
            means that all our efforts are wasted in the long term; on the other, optimists who believe that the secret to beating this tendency – or at least indefinitely delaying the
            inevitable– is to become more efficient (Georgescu-Roegen, 1971; Rabinbach, 1990).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The latter option involves striving to do more with less, which the theologically
            inclined might associate with our heroic striving to mimic God’s original position of
            creating everything out of nothing (creatio ex nihilo), the ultimate feat of efficiency,
            which in secular guise is carried forward as ‘transhumanism’ (Fuller & Lipinska,
            2014: Chap. 2). But it also inspired more downsized expressions in discussions of
            political economy. The drive to minimize ‘transaction costs’ is one of the more cre-
            ative responses, especially as an economic argument for ‘institutions’ as agencies
            whose existence transcends the that of the particular agents involved in any set of
            market exchanges. As originally formulated by Ronald Coase (1937), institutions
            are designed to anticipate and mitigate costs so that trade can flow without substan-
            tial interference – that is, more ‘freely’ than it might otherwise.]]>
			</paragraph>
			<paragraph>
				<![CDATA[And so began the ‘law and economics’ movement, which measures human
            progress in terms of minimizing the costs of harm without necessarily preventing
            the harm itself. For example, if there is an overall benefit to my building a house
            even though it could destroy the value of adjacent land, then I should compensate in
            advance, so as to avoid later complaints that could waste still more time, effort and
            money of all the parties concerned (Calabresi & Melamed, 1972). Of course, such a
            scenario supposes that some superordinate party – typically a judge – takes a deci-
            sion to optimize over all the parties’ interests, which are fungible with respect to the
            prospect of financial compensation. The bottom line is that everyone has got their
            price when it comes to neutralizing harms, and the only question is how to find it
            (Ripstein, 2006: Chap. 8).]]>
			</paragraph>
			<paragraph>
				<![CDATA[While for many this is an intuitively harsh principle of justice, it is nevertheless
            future-oriented rather than past-oriented. There is no presumption in favour of
            retaining the status quo if it potentially blocks a better future for all concerned – at
            least in the aggregate sense of ‘all’. Those who stand in the way of progress should
            be willing to ‘cash out’ their stakes under the right conditions. And in a world inno-
            cent of the long-term effects of environmental degradation, it was easy to see how
            such a policy could hasten the conversion of farms to factories, resulting in the
            rentiers yielding to the capitalists as the dominant economic force. But it would be
            a mistake to understand this mentality as limited to ‘political economy’ as conven-
            tionally understood. It also extends to ‘knowledge production’. In this context, I will
            unpack the economic definition of public good to show that it implies a similar
            hostility to the conservative bias embodied in rentiership.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For economists, something counts as a public good if it would cost more to
            restrict than to permit access to it. The availability of such a good is probably hard
            to restrict by nature, and increased access to it would probably serve to increase
            society’s overall wealth and well-being. An implication is that restricting access to
            the good in question means lower productivity. As we earlier saw in the previous
            chapter, such considerations have historically persuaded judges to rule in favour of
            freeing markets, regardless of which party’s interests actually benefit – as in the
            anti-monopoly rulings in the US Progressive Era (Fried, 1998). This is because the
            judges have thought of markets as ultimately about information, which by nature flows and spreads (Kitch, 1980). It is an idea that Stewart Brand has popularised for
            the Silicon Valley set with the slogan, ‘Information wants to be free’, and one which
            in recent years the economist Philip Mirowski has demonized as the thin edge of the
            wedge for neoliberalism to colonize the academy (Mirowski & Nik-Khah, 2017).
            We shall return to this point in the Postscript to the chapter.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For capitalists and socialists, notably both David Ricardo and Karl Marx, public
            goods in the sense of goods presumed to be free are diametrically opposed to rent,
            which given our earlier analysis can be understood as the economists’ conception of
            evil, as it replaces efficient use with its two polar opposites – on the one hand, sheer
            idleness (i.e., non-use); on the other, excessive effort (i.e., high-cost use). Thus,
            starting with Coase, the law and economics movement has turned the eradication of
            this evil into a positive principle of justice by disincentivizing the rentier’s tendency
            to charge high tariffs to restrict the access of others who might use their property
            more productively. Such universal hostility to rentiership also explains the instant
            positive worldwide acceptance of Thomas Piketty’s (2014) Capital in the Twenty-
            First Century. That book focuses specifically on the role of inherited and unearned
            wealth as the most persistent source of inequality in societies across the world.
            Some of course have gone farther than Ricardo and Piketty – but perhaps not
            Marx – to declare that the institution of strong private property rights is itself the
            economic version of Original Sin, as it creates the legal conditions for the restriction
            of resource use by the bare fact that what’s mine is by definition not yours. Without
            private property, nothing would be rentable, and hence information flow would not
            be blocked at all. This anarcho-communist position, which traces its roots back to
            Rousseau and Proudhon, should be kept in mind when we turn to contemporary
            academia’s obsession with plagiarism.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Of course, ‘rentiers’ do not present themselves that way at all. They see
            themselves as protecting an asset whose value might otherwise degrade from
            unmonitored use (Birch, 2017b, 2020). Thus, the landowners whom Ricardo and
            Marx held in contempt for impeding human productivity are reasonably seen as
            proto-environmentalists for the resistance they provided to factory building on their
            property. This issue of ‘quality control’, which Garrett Hardin (1968) made vivid to
            economists as the ‘tragedy of the commons’, recurs in academia through the idea of
            ‘gatekeeping’, which was originally a set of tolls to channel traffic in privately
            owned lands. The term was then repurposed by Kurt Lewin in the 1940s for the fil-
            tering of messages in a mass media environment. And nowadays ‘gatekeeping’ is
            routinely used to characterise the ‘peer review’ processes that characterise the eval-
            uation and publication of academic research.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is worth lingering here over the idea of ‘quality control’. It is a fundamentally
            uneconomic concept by virtue of assuming that the value of the good in question is
            intrinsic rather than extrinsic. More to the point, ‘intrinsic value’ means continuing
            to respect already known qualities of the good. Thus, the proto-environmentalist
            landowners in the early Industrial Revolution presumed that their property pos-
            sesses a value – say, associated with its specific physical constitution or cultural
            context – that cannot be exchanged for anything else, which in turn justified the high
            tariff levied on any prospective user. These landowners didn’t see themselves as exploiting their relative advantage but as performing a hereditary role as stewards of
            the Earth. A more democratic version of this line of reasoning is familiar today in
            terms of the allegedly irreducible benefits of ‘natural’ over ‘artificial’ food ingredi-
            ents, which informs the largely European resistance to the introduction of ‘geneti-
            cally modified’ organisms into agriculture. To his credit, Roger Scruton (2012)
            locates this mentality at the heart of ‘Conservatism’ as a political ideology, notwith-
            standing the more Left-sounding rhetoric of today’s self-styled ‘eco-warriors’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[However, economics is by nature a ‘Liberal’ science, and from that standpoint
            such ‘Conservative’ arguments for quality control look like attempts to discourage
            the flow of capital by making it harder for competitors to enter the market. After all,
            the lifeblood of capital is fuelled by the prospect that anything that is currently done
            can be done more efficiently and to greater effect – and quite possibly by someone
            other than the current owners of the means of production. The agent of change
            remains an open question: It may or may not be the current owners. The problem is
            that the current owners may not be motivated to step up to the plate. In that context,
            appeals to ‘intrinsic value’ simply grants ideological licence for rents to be added as
            an artificial layer of scarcity on top of nature’s scarcity to limit humanity’s ability to
            rise to the economic challenge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A sign that academia has become more protective of its rentier tendencies is its
            increasing obsession with plagiarism (Fuller, 2016a: 44–46). It reflects a rise in
            intellectual property thinking more generally in academia that is a mark of its des-
            peration in the face of challenges to its authority, which ironically only serves to
            subvert the idea of free inquiry that the Humboldtian university aims to uphold.
            After all, plagiarism is ultimately about syntax fetishism, the heart of copyright,
            which confers intellectual property rights on the first utterance of a particular string
            of words or symbols, even though it could have been uttered by any other grammati-
            cally competent person under the right circumstances. (At least that’s how Chomsky
            would put the matter.) The mystified legal expression for this privileging of the first
            utterance is ‘authorship’, which was subject to much criticism, deconstruction and
            even scorn in the 1960s and ‘70 s, especially in France (Barthes, Foucault, Derrida).
            Nevertheless, automated plagiarism detectors such as ‘Turnitin’, through which stu-
            dents nowadays must submit their papers prior to academic evaluation, uphold the
            syntax fetishism associated with ‘authorship’ in that principled way that only
            machines but not humans can.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unfortunately, no credible philosophy of language supports the policy of
            projecting semantic value from syntactic originality. The meaning of any string of
            words or symbols is ultimately up to the context of use, which inter alia depends on
            the other strings in which it embedded, what Derrida would call its ‘intertextuality’.
            This even applies to students to who cut-and-paste, say, bits of Kant into essays that
            are presented as ‘original work’. To be sure, there are intellectual grounds on which such efforts should fail. But they are less to do with the failure to acknowledge
            sources than simply the failure to meet the demands of the assignment. Indeed, this
            should be the basis on which the teacher is bothered by ‘plagiarism’ in the first
            place – namely, the inappropriateness of the cut-and-pasted bits of Kant to the topic
            that she has asked the student to address. However, a student capable of cutting-and-
            pasting Kant into their texts such that the teacher takes it be part of a good answer
            to an exam or essay question – even if the teacher doesn’t realize that Kant himself
            originally said it – deserves praise, not condemnation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Praise is due not merely because the student had outfoxed the teacher at her own
            game. That sort of response remains beholden to the syntax fetishism that fuels the
            academic taboo on plagiarism. Rather, praise is due because the student had demon-
            strated good judgement regarding what sort of thing belongs in what sort of place –
            in this case, the bits of Kant that end up on a highly graded essay. If any ‘originality’
            is involved, it consists in the ability to pour old wine into new bottles such that it
            tastes different, if not better. (Recall the Borges short story, ‘Pierre Menard, Author
            of the Quixote’.) What we normally call ‘intelligence’ or ‘aptitude’ is primarily
            about that capacity, which has nothing to do with some ultimate sense of originality.
            Indeed, this strategic deployment of plagiarism underwrites the justification of
            multiple-choice tests, where the value of one’s knowledge is presumed to be a func-
            tion of its display in context, not its origin. Indeed, we underestimate the extent to
            which we ‘always already’ live in the world of the Turing Test, where the cheaters
            and chancers can’t be distinguished from the geniuses on a principled basis. Drawing
            such distinctions requires the additional work of embedding test performance in
            larger narratives about how the test takers came to manifest such knowledge, on the
            basis of which the value of their performance might then be amplified or discounted,
            resulting in categories like ‘cheater’, ‘chancer’ and ‘genius’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This anti-proprietary approach to plagiarism recalls the ‘artworld’ theory of
            aesthetics developed by Arthur Danto (1964), who stumbled upon it after wondering
            what made Andy Warhol’s ‘Brillo Box’ a work of art. After all, it would be easy to
            regard the box on display in an art gallery as Warhol’s cheap attempt to profit in
            violation of the ‘Brillo’ trademark (‘Brillo’ is a US brand of scouring pad). Of
            course, most aestheticians would recoil from such a crass judgement, even if they
            wouldn’t rate Warhol’s effort very highly. Yet that crass judgement does approxi-
            mate contemporary academic attitudes to plagiarism. The student, like Warhol,
            would seem to be brazenly appropriating someone else’s intellectual property for
            their own purposes. Danto’s own approach was to say, first, that art is a matter of
            seeing something as art and, second, that this perception requires a context to rec-
            ognize the work as art. In short, we need to inhabit an ‘artworld’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Danto went further with this idea. Nelson Goodman (1968) had famously
            proposed that art may be divided into those works that can be forged (because they
            constitute a unique completed object) and those that cannot be forged (because they
            can be completed in many ways). He had in mind the distinction between a painting
            or sculpture, on the one hand, and a musical score or dramatic script, on the other.
            Against this intuition, Danto (1974) proposed imagining that two artists generate
            paintings that appear the same to the observer but one used Rembrandt’s method and the other Jackson Pollock’s. Goodman might claim that subtle differences
            between the two paintings could always be found, based on which one painting
            might be judged superior and the other even a forgery. One might suppose that
            Goodman’s judgement would depend on suspecting that the two paintings had been
            produced at different times and by different means – and resolving those technicali-
            ties would settle the normative issue of authenticity and hence value. For Danto, if
            you like one, you should like the other. If anything, knowing that they were pro-
            duced differently should enhance not detract from your aesthetic experience. The
            Pollock might even be valued more, given the prior improbability of its result.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Danto’s point was designed to undermine the very idea of forgery – but it works
            equally for forgery’s cheap cousin, plagiarism. For him, unlike Goodman, an aes-
            thetic judgement involved treating not only the future but also the past of a candi-
            date work of art ‘performatively’. Just as we potentially learn something new about
            music or drama with each new performance, the same applies to our unlearning
            ideas about the ‘unique craftsmanship’ of a painting or sculpture upon realizing that
            it can be (and could have been) brought about differently. This sense of temporal
            symmetry dissolves Goodman’s original distinction. Of course, aesthetic judgement
            then gets more squarely placed on the shoulders of the judge – and in that sense,
            becomes more ‘subjective’. Indeed, Danto’s championing of Warhol’s Brillo Box as
            art led many critics to claim that Danto dissolves the concept of art altogether. And
            this brings us back to the challenge that academics are nowadays afraid to face:
            trusting their own judgement.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is perhaps difficult for epistemologists to comprehend that we might normally
            inhabit an artworld: life as one big interactive virtual gallery. However, it will
            become easier in the future. For the past quarter-century, it’s been fashionable to
            speak in terms of our possibly living in a ‘simulation’ (Bostrom, 2003). The uncan-
            niness of this proposal rests on the Cartesian assumption that someone other than
            ourselves – perhaps an evil demon or the ‘Matrix’ – is running the simulation in
            which we live. However, if Mark Zuckerberg gets his way, soon we’ll all be dwell-
            ing in the ‘Metaverse’, subject to the terms and conditions of his arcane Facebook-
            like contracts. But even a demon as benign as Zuckerberg is not really needed.
            Humanity may be routinely running different simulations on itself as history is
            imperceptibly revised to reconfigure future horizons. (In historiography this is
            called ‘revisionism’, often in a pejorative light.) In Gestalt terms, we need to imag-
            ine that the same figures (from the past) are reorganized against a new ground (i.e.,
            a renewed horizon for the future) to constitute a new perceptual whole – what Danto
            himself called the ‘transfiguration of the commonplace’. The previously suppressed
            comes to be salient – and vice versa. Inspired by Freud, Jacques Derrida (1978)
            likened the activity involved in this transfiguration to the writing on a palimpsest.
            He wasn’t far off the mark. A similar sensibility is also expressed in Eagleman
            (2009), a work inspired by recent research in neuroscience.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plato’s anamnesis, the knowledge that comes from recalling what had been
            forgotten, provides a prototype for this kind of activity. However, the relevant sense
            of ‘memory’ is not the strict mental recovery of a determinate past but rather a
            liberalization of our thought about how the past might determine the future. Once again, it may simply involve a rearrangement of what is already known to form a
            new whole. Thus, Warhol did not uncover the hidden truth about a box of Brillo –
            say, that it was an artwork masquerading as a household product. Rather, he saw the
            box’s potential to be reconfigured differently and thereby perform a different sort of
            function – and in so doing, added value to the Brillo box. In this respect, Plato may
            be seen as the original champion of ‘intellectual recycling’, in which case anamnesis
            provides the epistemological basis for plagiarism, especially since Plato regarded
            memory as collectively owned by whichever beings are capable of hosting it. The
            obvious case in point is language, especially if one subscribes to a Chomsky-style
            ‘universal grammar’ approach, which Chomsky (1971) believed that Humboldt
            himself did. In that case, the innate creativity of everyone uttering unique sentences
            is complemented by the capacity that any of us possesses to have uttered those
            sentences. In that case, the ‘added value’ of the person called the ‘originator’
            consists in no more than being at the right place at the right time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This way of thinking has profound implications for our understanding of
            historical significance. Consider Kuhn’s (1970) influential conception of ‘scientific
            revolution’. Even after the great paradigm shift from classical to relativistic physics,
            the achievements of Copernicus, Galileo, Newton and Maxwell did not disappear,
            but they did appear in a different light, which altered their significance vis-à-vis the
            new future horizon of physics. Nelson Goodman (1955) interestingly encapsulated
            the epistemology of the situation as the ‘new riddle of induction’, namely, how the
            same past can logically imply radically alternative futures. In Kuhn’s case, the
            alternatives were that classical physics carries on regardless of its anomalies or that
            it shifts to a new foundation in relativity theory. After 1905 the physics community
            began to take the latter course, after having followed the former course for the
            previous two hundred years. For his part, Goodman presented a more abstract
            example. He posited two competing properties – ‘green’ and ‘grue’ – that emeralds
            might possess. Both can account for the green colour of emeralds before a stipulated
            prediction, but they diverge thereafter, one predicting the next emerald will appear
            green, the other that it will appear blue. While it may be obvious which hypothesis
            will prevail, we really don’t know until the prediction happens. But suppose the
            emerald does turn out to confirm the ‘grue’ hypothesis, what follows?]]>
			</paragraph>
			<paragraph>
				<![CDATA[We might simply say that the test emerald was an outlier, an exception that
            proves the rule – or, we might say that emeralds had been ‘grue’ all along and we
            had been deceived by their superficially green appearance. The latter judgement
            would licence a rewriting of the past. That rewriting would be associated with our
            now having come to see something ‘deeper’ about the emeralds than we previously
            had. This is precisely how a scientific revolution is portrayed by those who back the
            winning hypothesis: That ‘crucial experiment’, as Popper would put it, that pits
            ‘green’ against ‘grue’ triggers the deeper realization that appearances aren’t all that
            they had seemed. Philosophers of science have yet to adequately answer whether
            the historical succession of these ‘deeper realizations’, this trail of ‘grue’ events we
            call ‘revolutions’, display an overall sense of purpose that amounts to a definitive
            growth in knowledge. Is it genuine scientific progress – or, is that conclusion just a
            figment of the narrative imagination, what Kuhn called science’s ‘Orwellian' historiography, which uses history to justify current policies? Kant famously coun-
            selled agnosticism about all such ‘teleological’ judgements, introducing the concept
            of ‘purposiveness’, the mere appearance of purpose. But that may turn out to be
            ‘purpose’ enough, at least for purposes of promoting knowledge production.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The rise of automated plagiarism detectors (e.g., ‘Turnitin’) has nudged
            academics toward treating knowledge as intellectual property more than they
            otherwise might or should do. I have gone further to suggest knowledge would be
            better served by an artworld than the rentiership model that underwrites the fixation
            on plagiarism. However, academic rentiership is not simply a reactionary response
            to contemporary pressures from an increasingly sceptical public for whom ‘Don’t
            trust the experts!’ has proved to be an effective political rallying cry. Indeed, if
            academics were more concerned about spreading ideas than rewarding authors,
            plagiarism would not be the moral panic that it is today. But of course, contemporary
            academia is not simply about efficiently producing knowledge as a public good but
            also about properly crediting the producers. However, these two goals cut against
            each other, resulting in the rather tortured path dependent ways in which academics
            are forced to make their knowledge claims in the professional literature, namely, by
            citing ‘proper’ precursors, which is the functional equivalent of paying rent to
            maintain one’s standing in a domain of inquiry. This ongoing need to publicize other
            authors ends up making academic knowledge claims more arcane than they might
            otherwise be, given that in most cases one could reach similar conclusions by citing
            fewer authors.]]>
			</paragraph>
			<paragraph>
				<![CDATA[131
            historiography, which uses history to justify current policies? Kant famously coun-
            selled agnosticism about all such ‘teleological’ judgements, introducing the concept
            of ‘purposiveness’, the mere appearance of purpose. But that may turn out to be
            ‘purpose’ enough, at least for purposes of promoting knowledge production.
            The rise of automated plagiarism detectors (e.g., ‘Turnitin’) has nudged
            academics toward treating knowledge as intellectual property more than they
            otherwise might or should do. I have gone further to suggest knowledge would be
            better served by an artworld than the rentiership model that underwrites the fixation
            on plagiarism. However, academic rentiership is not simply a reactionary response
            to contemporary pressures from an increasingly sceptical public for whom ‘Don’t
            trust the experts!’ has proved to be an effective political rallying cry. Indeed, if
            academics were more concerned about spreading ideas than rewarding authors,
            plagiarism would not be the moral panic that it is today. But of course, contemporary
            academia is not simply about efficiently producing knowledge as a public good but
            also about properly crediting the producers. However, these two goals cut against
            each other, resulting in the rather tortured path dependent ways in which academics
            are forced to make their knowledge claims in the professional literature, namely, by
            citing ‘proper’ precursors, which is the functional equivalent of paying rent to
            maintain one’s standing in a domain of inquiry. This ongoing need to publicize other
            authors ends up making academic knowledge claims more arcane than they might
            otherwise be, given that in most cases one could reach similar conclusions by citing
            fewer authors.
            The template for intellectual proprietarianism had been already set in the
            seventeenth century in the Charter of the Royal Society of London, according to
            which an agreed self-selecting and self-organizing group of members would decide
            on what counts as valid knowledge without government interference. In effect, the
            Crown gave the Society exclusive rights over the certification of public knowledge
            as long as they kept their own house in order. Accordingly, individuals would submit
            their knowledge claims to ‘peer review’, in return for which they would be able to
            claim limited intellectual property rights as a ‘discoverer’. This setup put a premium
            on being first, and latecomers would be expected either to build upon or work
            around predecessors, as they all staked out a common ‘domain of inquiry’. This
            conception of knowledge informs Kuhn’s (1970) account of puzzle solving in
            ‘normal science’. Thus, open conflict among scientists has increasingly focused on
            priority disputes, which resemble the sorting out of competing entitlement claims to
            a piece of land (Collins & Restivo, 1983). In the long term, this arrangement changed
            the character of referencing knowledge claims and claimants. What might be called
            a ‘Westphalian system’ of epistemic authority emerged, with the aim of establishing
            the jurisdiction of every claim and claimant to knowledge. From this arrangement
            came the mutual recognition exercise – some might say mutual protection racket –
            that is associated with ‘expertise’, which is reflected in today’s credit-assigning
            academic referencing practices, aka ‘citation’. Meanwhile an older referencing
            convention – the footnote – has lost favour. It had been often used to openly contest the epistemic
            authority of rivals, which in turn suggested a sphere of inquiry with multiple overlapping jurisdictions always potentially at odds with each other – in short, a
            ‘pre- Westphalian’ world of knowledge (Grafton, 1997).]]>
			</paragraph>
			<paragraph>
				<![CDATA[One way to tell the history of academia would be in terms of alternating waves or
            rentiership and anti-rentiership. The onset of rentiership may be generally associ-
            ated with the rise of ‘schools of thought’ whose authority is maintained by fidelity
            to a set of ‘original sources’ rather than innovation away from them. I refer, of
            course, to the commentary culture surrounding Plato, Aristotle, the Bible and their
            theological commentators in the Middle Ages. Spaces were introduced in the mar-
            gins of books as new frontiers for claiming intellectual property rights, which others
            would then need to incorporate in their own books. This was the original context in
            which the famous quote normally attributed to Newton – ‘If I have seen as far as I
            have, it is because I have stood on the shoulders of giants’ – when it was first uttered
            in the twelfth century (Merton, 1965). The balance of power between old and new
            academic rentiers gradually shifted over the centuries as marginalia first became
            footnotes and then completely internalized as citations embedded in the aspiring
            rentier’s text.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To be sure, less display – and arguably less mastery – of past work is now
            required to advance one’s own knowledge claims, if one is already in the academic
            knowledge system. Citations, properly arranged, function as currency that one pays
            to be granted a lease on a staked-out piece of intellectual property. Admittedly, that
            constitutes a triumph of ‘efficiency’ in a certain sense – but only for those who have
            paid the high entry costs (e.g., doctoral training) involved in knowing which works
            to cite. This in turn makes it harder for those who have not followed that particular
            path of inquiry to make sense of, let alone evaluate, the knowledge claims in ques-
            tion. Contrary to its own hype, academic knowledge is less a ‘public good’ than a
            club good (Fuller, 2016a: Chap. 2). In a club, several forms of rent are paid, starting
            with the submission of oneself to background checks and sometimes examination,
            followed by periodically renewed subscriptions, alongside the expectation that,
            once accepted, one will provide preferential treatment to other club members in the
            wider society. The acquisition and maintenance of academic expertise follows a
            very similar pattern.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Indeed, the technological infrastructure of academic knowledge production has
            played a diabolical role in rendering the system more club-like. In the roughly half
            a millennium that has passed between the introduction of the printing press and the
            internet, it has become easier for those already in the know to get direct access to the
            knowledge they need to advance their claims, while at the same time keeping out
            those who haven’t yet paid the high entry costs. The relative ease with which books
            and journals have been made available means that that each new knowledge producer needs to reproduce less of the work of their predecessors in their own – a
            citation and a verbal label will often suffice. This allows a quickening of the pace of
            academic knowledge production, a point that was already in evidence by the time of
            Scientific Revolution, less than two centuries after Gutenberg (Eisenstein, 1979).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Until universal literacy became an official aspiration of nation-states in the late
            nineteenth century, the education of the general public was lagging behind the
            advances of the emerging knowledge elites. Indeed, as we have seen, Saint-Simon’s
            original ‘socialism’ justified this new form of social distancing in the name of a
            more rational organization of society. It had certainly contributed to the
            Enlightenment’s self-understanding as a ‘progressive’ and ‘vanguard’ movement,
            which laid the groundwork for the modern culture of expertise, with Positivism as
            its house philosophy. Indeed, the philosophes generally supported the establishment
            of free-standing research institutes that trained future front-line knowledge produc-
            ers – but remained separate from the ordinary university teaching which prepared
            the traditional administrative professions: the clergy, law and medicine. Its legacy
            remains in the academic cultures of such countries as France and Russia, where one
            normally has a dual appointment – one for teaching and one for research, each oper-
            ating quite independently of the other.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is against this historical context that one can begin to appreciate the radically
            anti-rentier cast of Humboldt’s early nineteenth century proposal to reconceptualise
            the academic vocation in terms of the integration of research and teaching in the
            same person called an ‘academic’. But before turning to academia’s historic tenden-
            cies against rentiership, note that even today, the supposed communicative effi-
            ciency of academic referencing erects higher intellectual trade barriers to what has
            become a widely and generally educated public. One needs to know who academic
            authors know before evaluating what the authors claim to know. At the same time,
            most academic publications simply tread water to show that their authors are still in
            the game. The main beneficiaries are the people the authors cite. All told, it perpetu-
            ates an illusion of continuity in fields where people really don’t know what to say.
            And ultimately the power lies in the hands of the peer reviewers, who these days
            often end up being selected simply because they’re the only ones willing to spend
            the time to exploit the uncertainty of the review process. It’s too bad that this point
            is not made more forcefully in the ongoing debates concerning ‘open access’ aca-
            demic journals, which present the false premise that the primary obstacles to the
            public’s utilization of academic knowledge are the prices charged by publishers.
            High prices merely financially disadvantage those academics wishing to improve
            their own chances at epistemic rentiership. Removing that barrier does not remove
            the club-like character of academic knowledge production, which resembles a cargo
            cult fronted by a Potemkin Village.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first major swing away from rentiership in academia began in the sixteenth
            century Renaissance. In effect, the ‘Humanists’ reverse engineered and thereby
            undermined the entitlement structure of medieval scholastic authority by showing
            that the received interpretations of Christendom’s canonical texts were based on
            false translations. (It is worth recalling that the scholastics read nearly everything in
            Latin translation, an epistemic condition comparable to Anglophone academia today.) But two diametrically opposed conclusions were then drawn. Those who
            remained loyal to Roman Catholicism tended to adopt the relatively relaxed if not
            cynical attitude of ‘Traduttore, traditore’ (‘To translate is to betray’), while
            Protestant dissenters adopted the more earnest posture of trying to find the original
            meaning of these texts. This led to a variety of signature modern epistemic moves,
            ranging from more first-hand acquaintance with authoritative sources to the search
            for corroborating testimony, or ‘evidence’, as we would say today, for the knowl-
            edge claims made by those sources. This sparked considerable innovation in the
            independent testing of knowledge claims, which became the hallmark of the ‘scien-
            tific method’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Indeed, Francis Bacon made it quite clear that an important – if not most
            important – reason to submit knowledge claims to independent evaluation was to
            short- circuit the trail of commentary, whereby academics simply build or elaborate
            on misinterpretations of sources that may themselves not be very reliable. Bacon’s
            rather ‘post-truth’ way of referring to the problem was in terms of the ‘Idol of the
            Theatre’, a phrase that evokes the image of academics mindlessly playing the same
            game simply because everyone else does. In his magisterial survey of global intel-
            lectual change, Randall Collins (1998) observed that the periodic drive by intellec-
            tuals to level society’s epistemic playing field by returning to ‘roots’, ‘sources’,
            ‘phenomena’, ‘foundations’ or ‘first principles’ tends to happen at times when the
            authority of the academic establishment has been weakened because its normal
            political and economic protections have also been weakened. This in turn under-
            mines a precondition for the legitimacy of academic rentiership, namely, an accep-
            tance by non-academics of the self-justifying – or ‘autonomous’, in that specific
            sense – nature of the entitlement structure of academic knowledge production.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Renaissance exemplifies what the turn against rentiership looks like in
            action: Charges of ‘corruption’ loom large, most immediately directed at academics
            violating their fiduciary responsibilities as knowledge custodians by transmuting
            the public’s trust into free licence on their own part. This is the stuff of which
            ‘research fraud’ has been made over the past five hundred years – and which of
            course looms large today across all academic disciplines, now as part of an ongoing
            crisis in the peer review system (Fuller, 2007a: Chap. 5). But behind this charge
            lurks a morally deeper one, namely, that academics are not merely protected by
            larger powers but collude with them so as to constitute that mutual protection racket
            known in political sociology as ‘interlocking elites’. The presence of scholars and
            clerics as royal courtiers in early modern Europe is the most obvious case in point.
            In this respect, Galileo remains a fascinating figure because he was always trying to
            turn that situation – his own – against itself (Biagioli, 1993). The modern-day equiv-
            alent is, of course, researchers whose dependence on either state or private funding
            agencies effectively put them ‘on retainer’ yet are then expected to act as ‘honest
            brokers’ in the public interest (Pielke, 2003).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nevertheless, it took the Protestant Reformation to begin to draw a clear line
            under these excesses of academic rentiership. And another quarter-millennium had
            to pass before the point was driven home in a philosophically principled fashion.
            What Immanuel Kant advanced as a ‘critique of pure reason’ was in fact a systematic deconstruction of the academic arguments normally used to justify the
            status quo – that is, the set of interlocking elites in theology, law and medicine who
            would have the public believe that they live in the best possible world governed by
            the best possible sacred and secular rulers. At most such beliefs were ‘regulative
            ideals’ (aka ‘ideologies’) that served to buy time for the elites to carry on. The
            salience of Kant’s critique in his own day lay in the increasingly open contestation
            for epistemic authority in the royal court among the learned professions – the clergy,
            law and medicine – in a period of uneven secularisation. Kant (1996) himself shone
            a light on this matter in his polemical 1798 essay, The Conflict of the Faculties, a
            major inspiration for the Humboldtian university.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Notwithstanding its ever-changing fashions and often to the exasperation of
            fellow academics in other disciplines, philosophy – at least in its pedagogy – has
            stuck doggedly to the Humboldtian mission. But while Humboldt continues to
            inspire the idea of an anti-rentier academy, it only took about a little more than a
            generation for his vision to morph into a ‘re-professionalisation’ of academic life,
            resulting in the discipline-based structure of the university that persists to this day,
            albeit under increasing pressure. This return to rentiership is normally attributed to
            nation-building and a broadly ‘Positivist’ ideology that regarded academic experts
            as the secular successors of the clergy in terms of ministering to the needs of modern
            society. In German academia, it also carried more metaphysical implications –
            namely, that each discipline is regulated by a distinct ‘value’ that it tries to pursue,
            which in practice served as a quality control check on scholarship (Schnädelbach,
            1984). This ‘Neo-Kantian’ turn was associated with an extended training period for
            all disciplines, culminating in the ‘doctorate’ as the ultimate mark of accreditation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[And again, like past rentiers, these new-style academic professionals became
            increasingly entangled with the affairs of state and industry, the high watermark of
            which was Germany’s establishment of the Kaiser Wilhelm Institutes shortly before
            the First World War. This turned out to be a fatal embrace, as virtually the entire
            academic establishment was behind the war effort, which resulted in a humiliating
            defeat. The Weimar Republic that followed has been often characterised as the ‘anti-
            intellectual’, or more specifically ‘reactionary modernist’ (Herf, 1984). However, it
            would be more correct to say that it was an anti-rentier backlash that levelled the
            epistemic playing field between academic and non-academic forms of knowledge.
            Thus, folk knowledges, astrology, homoeopathy, parapsychology and psychoanaly-
            sis – all of which had been regarded as beyond the pale of academic respectability –
            acquired legitimacy in the 1920s and ‘30 s through popular acceptance. They were
            facilitated by the emergence of such mass media technologies as radio, film and
            tabloid newspapers, which provided alternative channels for the spread of
            information.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the same time, academia responded by returning to ‘fundamentals’ in two
            rather distinct ways, which anchored the rest of twentieth century philosophy and
            much of cultural life in the West and beyond. One was existential phenomenology,
            which like the Renaissance Humanists and the Protestant Reformers focused on the
            deception and self-deception (aka ‘corruption’) involved in expert rationalizations
            of our being in the world. Here what Heidegger originally called ‘deconstruction’ played an important role in recovering the original language of being, which in his
            hands turned out to be the aspects of Greek to which German is especially attuned.
            However, I will dwell on the second return to fundamentals, which was via ‘logical
            positivism’ (in the broad sense, to include Popper and his followers), as it has been
            the more comprehensively influential. It is worth noting that while existential phe-
            nomenology and logical positivism are normally portrayed as mortal enemies, they
            were agreed in their near contempt for the leading philosophical custodian of aca-
            demic rentiership in their day, the Neo-Kantian Ernst Cassirer (Gordon, 2012; cf.
            Fuller, 2016c).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The logical positivist strategy was to strip down academic jargon – the calling
            card of rentiership – to data and reasoning that in principle could be inspected and
            evaluated by anyone, including those who aren’t privy to the path that the experts
            took to reach their agreed knowledge claims. In this context, the prospect that the
            future might be like the past – ‘induction’ – was regarded with considerable suspi-
            cion: Even if it appears that the future will resemble the past, why should it? Our
            earlier discussion of Nelson Goodman epitomized this attitude. Indeed, Goodman
            (1955) called sticking with the current hypothesis in the face of an empirically equal
            alternative ‘entrenchment’, a not altogether positive term that implies a path depen-
            dent mode of thought. The larger point is that the act of projecting the future is a
            much more evaluative enterprise than the phrase ‘extrapolating from the data’ might
            suggest (Fuller, 2018a: Chap. 7). By the 1960s this concern morphed into a general
            problem of ‘theory choice’, whereby at every step along the path of inquiry one
            needs to ask why one hypothesis should be advanced rather than another if both
            hypotheses account for the same data equally well. Thus, Bayes theorem rose to
            prominence in epistemology as a vehicle to inspire and record experiments that
            weighed on comparable hypotheses differently. In effect, it kept the opportunity
            costs of carrying on with past epistemic practices under continual review. In the
            philosophy of science, it led to a tilt toward theories that suggested more paths for
            further inquiry and other such horizon-broadening, future-oriented values (e.g.,
            Lakatos, 1978).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Much of the long-term cultural influence of logical positivism relates to its
            extramural interests in overcoming the ‘burden of the past’ in multiple senses,
            ranging from differences in natural languages to the tendency of art and architecture
            to reinforce historical memory. In this respect, the movement was ruthlessly
            ‘modernist’ and occupied the Left of the political spectrum, at least during its
            European phase, when it was seen as part of ‘Red Vienna’ (Galison, 1990).
            Moreover, logical positivism’s transatlantic passage involved a further degree of
            ‘complexity reduction’ that its proponents largely welcomed – namely, the need to
            reformulate in English everything that was originally expressed in German. This
            process, common to other intellectual migrants in the interwar period, was somewhat
            easier than supposed, as the German connotations (‘excess baggage’) were often
            gladly lost in English translation. In this respect, logical positivism’s much vaunted
            respect for ‘clarity’ of expression can be seen as valorizing the very features of
            translation that Heidegger would regard as ‘forgetfulness of Being’. In any case, this was how English quickly became the undisputed language of international exchange
            (Gordin, 2015: Chap. 5 and 6).]]>
			</paragraph>
			<paragraph>
				<![CDATA[To be sure, against this general trend stood the Frankfurt School genius, Theodor
            Adorno, who returned to Germany as soon as he could after the Second World War
            to recover what Judith Butler (1999) has popularised for US academia as the ‘diffi-
            culty’ in writing that is necessary to pursue the deepest issues of concern to human-
            ity. Yet, in many respects, Butler’s appeal to Adorno’s ‘difficulty’ is no more than
            rent nostalgia, by which I mean a longing for a mistaken understanding of a past
            based on rentiership. Adorno himself was a creature of Weimar culture who easily
            mixed disciplines and worked in think tanks both in Germany and the US until his
            postwar German return. Indeed, much of what Adorno published in his lifetime was
            music criticism, which could be understood by a broad range of readers – and even
            in later life wrote a book entitled ‘The Jargon of Authenticity’, a takedown of
            Heidegger’s much vaunted ‘difficulty’ (Adorno, 1973). Moreover, the ‘difficult’
            postmodern French theorists on whom Butler has based most of her own work were
            not nearly as academically entrenched in their homeland as they came to be in the
            United States. The likes of Foucault, Deleuze and Derrida cycled in and out as pub-
            lic intellectual figures in France, each trying to grab attention from the others, which
            helps to explain their flashy style – but not why Americans should base entire jour-
            nals and schools of thought around them (Cusset, 2008). Notwithstanding her cre-
            dentials as a ‘continental’ philosopher, Butler is more parochially American than
            she and her admirers think.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The underlying inconvenient truth is that even though most of the German migrants
            were willing to abandon the academic rentiership of their homeland, they found
            themselves confronted with an American academic system that by the end of the
            nineteenth century had come to emulate the doctorate-driven ‘Wilhelmine’ model
            of academic rentiership, again as part of a nation-building exercise. Most of the
            transatlantic migrants responded as the logical positivists did, already starting with
            Hugo Münsterberg (1908), whom William James had invited to run his psychology
            lab at Harvard and then went on to invent the role of the ‘expert witness’ in the
            courtroom. Having been outsiders in the system they left, these Germans became
            doyens in the system they entered. A key moment was the 1912 presidential election
            of that Bismarck-admiring founder of the US political science profession,
            Woodrow Wilson.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Once Wilson masterminded America’s late but decisive entry into the First World
            War, the sense that Europe was transferring stewardship for a valuable piece of
            intellectual real estate called ‘Western Civilization’ to the United States became
            palpable. The main academic indicator was the rise of American undergraduate courses in the ‘Great Books’, the core of a renovated liberal arts curriculum. By the
            1980s it became the intellectual terrain on which the ‘canon wars’ were fought –
            perhaps the clearest display of academia’s rentiership tendencies at the pedagogical
            level (Lacy, 2013: Chap. 8). The question explicitly put on the table was: ‘Who
            owns the curriculum?’ In this context, ‘identity politics’ opened the door to retro-
            spective intellectual entitlement claims by ‘dispossessed’, typically ‘non-Western’
            cultures, the result of which for better or worse has been to breathe new life into
            academic rentiership. A sign of the times is that ‘cultural appropriation’ tends to be
            seen pejoratively as ‘plagiarising’ rather than extending the value of the appropri-
            ated culture.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The idea that the US was the primary – perhaps even the only reliable – standard-
            bearer for ‘Western Civilization’ reached its peak in the Cold War. Unsurprisingly,
            it was also the period when the ‘Great Books’ approach to liberal arts reached its
            peak, even though the launching of Sputnik in 1957 diverted much of the pedagogi-
            cal focus toward the more usual research-based form of academic rentiership, in
            which students were encouraged to specialize in the physical sciences to help defeat
            the Soviets (Fuller, 2000b: Chap. 4). However, I don’t wish to dwell here on the
            remarkable faith in the projectability of academic expertise onto geopolitics that
            this diversion involved – or the role of the US Gestalt psychologist Jerome Bruner
            in midwifing this development, for that matter. In any case, it resulted in students
            flooding into physics and chemistry courses, the market for which then collapsed at
            the end of the Cold War. Rather, I would turn our gaze to the ‘auxiliary’ knowledge
            enterprises that capitalised by presenting cheaper alternatives to what passed as
            ‘mastery’ of the Great Books. They were the harbingers of the knowledge economy
            we currently inhabit.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I refer here to such Cold War creations as Masterplots, Cliff’s Notes and Monarch
            Notes. Although they presented themselves as ‘study guides’, they were basically in
            the business of supplying the sort of understanding that was needed of a ‘Great
            Book’ to pass an academic exam or sound intelligent in conversation. Implied in
            this multi-million-dollar business, which took advantage of the cheap paperback
            market, was that the knowledge required of even the most classic of works (e.g.,
            Shakespeare) is sufficiently patterned that it can be presented more efficiently to
            today’s readers than in the hallowed originals. Moreover, the strategy seems to have
            worked because an internet-based version started by Harvard students – Spark
            Notes – has flourished since 1999, now as part of the New York-based bookselling
            empire, Barnes & Noble. But more importantly, the internet has permitted a pleth-
            ora of spontaneously generated summaries and commentaries, which when taken
            together undermine the finality of academic authority. This explains the ascendancy
            of Wikipedia as the world’s most widely used reference source (Fuller, 2018a:
            Chap. 5).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let me conclude with a few observations about this development, which even
            now tends to be regarded as unseemly by professional academics, yet it may contain
            the most robust seeds of anti-rentiership to date. I began with a reflection on the
            classical ideal of knowledge as ‘explaining the most by the least’. Historically it has
            involved judgements about what is and is not necessary to include in such an explanation, or ‘modal power’. These judgements have been influenced by the com-
            munication technology available – and, more importantly, who has control over it.
            The modern threat to academic rentiership began with the introduction of a technol-
            ogy over which academics did not exert monopoly control, namely, the printing
            press. To be sure, academics generally welcomed the ensuing legislation – from
            papal imprimaturs to royal licences – that were designed to contain printing’s threat
            to their rentiership. However, the advancement of Protestantism on the back of mass
            Bible reading campaigns seriously undercut these relatively ineffectual attempts at
            censorship. As a result, the great modern political and economic revolutions hap-
            pened typically first in the wider society already capable of making use of the new
            technology and then only grudgingly in academia.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The internet promises to revisit this situation with a vengeance. A general feature
            of modern revolutions from Martin Luther onward is that their instigators were
            trained in the old regime but for whatever reason lacked sufficient personal invest-
            ment to perpetuate it. We now live in a time with unprecedented levels of formal
            education yet widespread disaffection with the judgements taken by its academic
            custodians. Perhaps that is how it should be. The emancipatory potential of educa-
            tion is not fully realized until the educators themselves are superseded. This would
            certainly explain the robust empirical finding that the more people have studied
            science, the more they take exception to expert scientific opinion – yet all along
            without losing faith in science itself. Wikipedia itself is a good case in point.
            Notwithstanding the anonymously non-expert nature of its contributors, it still
            requires adequate sourcing to the claims made on its pages, potentially giving read-
            ers the opportunity to ‘reverse engineer’ the content of the entries by returning to the
            original sources. I have called this phenomenon ‘Protscience’, a contraction of
            ‘Protestant Science’ (Fuller, 2010b: Chap. 4; Fuller, 2018a: Chap. 5).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Like the original Protestantism, Protscience requires an alternative medium for
            expressing what the old regime represses. Step forward the internet, the true succes-
            sor to the printing press (pace Marshall McLuhan), especially once the World Wide
            Web equipped personal computer interfaces with hyperlinks and audio/video dis-
            plays, which have served to fuel the now hyper-educated imagination. This enhanced
            internet publicises, amplifies and democratises the quiet elite revolution that had
            begun with what the media theorist David Kirby (2011) has called ‘Hollywood
            Knowledge’, whereby academic experts freely migrated to that most non-academic
            of media – film – in order to explore their own and their audience’s capacities to
            imagine alternative worlds that academia normally discourages as ‘improbable’ if
            not ‘impossible’. An early case in point is German-American rocket scientist
            Wernher von Braun’s admission that Fritz Lang’s 1929 proto-feminist Weimar film,
            Frau im Mond, inspired the idea for how to launch a rocket into space (Fuller,
            2018a: 38).]]>
			</paragraph>
			<paragraph>
				<![CDATA[If Plato were advising today’s defenders of academic rentiership, he would tell
            them that the biggest mistake that their political and economic protectors made was
            when they regarded emerging alternative communication technologies – starting
            with the printing press – as possibly being of mutual benefit. Thus, instead of Plato’s
            preferred route of censorship, modern power elites have tended either to tax new media or keep them on retainer as propaganda vehicles – and sometimes both, of
            course. While the power elites have thought of this as a win-win situation, placating
            their potential foes while presenting an outwardly ‘liberal’ image, historically it has
            only sowed the seeds of dissent, which eventually led to a turn away from any
            entrenched form of power, including academic rentiership. Here it is worth recalling
            that most revolutions have occurred in societies that had already been liberalised to
            some extent but not up to the liberals’ promoted expectations (Wuthnow, 1989).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The general lesson here is that tolerance for the dissenting views expressed in
            alternative media serves to dissolve the ultimate taboo with which Plato was con-
            cerned, namely, between ‘the true’ and ‘the false’. We all end up becoming ‘post-
            truth’. In this context, ‘fiction’ is ambiguous, in that in the first instance it merely
            implicates a maker in whatever image of the world is presented. And of course, the
            mere fact that something is made does not necessarily mean that it is ‘false’. Thus,
            some further judgement is required, which in turn threatens to reveal the exercise of
            modal power over what people are allowed to think is possible. The modern accom-
            modation has been to regard fiction as ‘literature’ with its own academic space that
            can be judged on its own terms, without reference to other forms of knowledge. In
            this respect, it has simply followed the path of other fields of inquiry that have been
            domesticated by becoming an academic discipline. But of course, lawyers and sci-
            entists who adopt an ‘instrumentalist’ attitude to their activities routinely regard
            their ‘fictions’ as possessing efficacy in the larger world regardless of their ultimate
            ontological standing. To be sure, Plato found this attitude desirable when restricted
            to the philosopher-king but socially destabilizing when permitted in society at large,
            which was his main complaint against the Sophists (Fuller, 2018a: Chap. 2).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The internet adds an interesting dimension to a key feature of this story, which
            requires a backstory. It has been often remarked that many ingredients of Europe’s
            Scientific Revolution were first present in China, yet the comparable Chinese schol-
            ars and craftsmen never interacted in the way they started to do in twelfth century
            European cities such as Oxford and Paris, which over the next five centuries paved
            the way for modern science and technology. The relatively free exchange between
            European scholars and craftsmen allowed ideas and formulas to be seen in terms of
            how they might be realized more concretely, as well as how machines and tech-
            niques might be extended and improved (Fuller, 1997: Chap. 5). This allowed for an
            empowering interpretation of the ‘maker’s knowledge’ principle, famously champi-
            oned by Francis Bacon and which underlies the centrality of modelling in almost all
            branches of science today – namely, that one can only understand what one can
            make. (I say ‘empowering’ because historically ‘maker’s knowledge’ has been
            invoked to suggest both strong limits and no limits to human knowledge.) The inter-
            net enables this largely self-organized activity to bear fruit much more quickly
            through explicit networking, especially ‘crowdsourcing’, whereby invitations are
            issued for those with the relevant aspirations to hook up with those with the relevant
            skill sets. This now commonplace cyber-practice opens up the space of realizability
            by creating the conditions for the surfacing of ‘undiscovered public knowledge’ that
            allows the already existing pieces to combine to solve puzzles that were previously
            regarded as insoluble (Fuller, 2018a: Chap. 4). In short, what looks like a glorified matchmaking service can render the impossible possible, and thereby diffuse
            modal power.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For a parting shot of academic rentiership, consider the following. The only
            difference between an establishment scientist like Richard Dawkins who says that
            the public opposes evolution because they don’t know enough cutting-edge science
            and an establishment humanist like Judith Butler who says that the public opposes
            postmodernism because they don’t know enough cutting-edge humanities is that
            Dawkins is more urgent than Butler about the prospective consequences for society
            if the public doesn’t actively profess faith in his brand of Neo-Darwinism. Whereas
            Butler simply wants to continue reaping the rewards that she has been allowed from
            a relatively insulated (aka ‘autonomous’) academic culture, Dawkins insists on
            nothing less than a conversion of the masses. In a democratic knowledge economy
            that treats academic rentiership with suspicion, neither should be allowed to stand,
            though Dawkins may be admired for his forthrightness. But make no mistake, Butler
            and Dawkins are the two sides of academic rentiership: the parasite (Butler) and the
            colonist (Dawkins) of knowledge as a public good. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Stewart Brand – editor of the Whole Earth Catalogue, founder of the Long Now
            Foundation and godfather of the Silicon Valley mindset – famously remarked in
            1984 at one of the original hackers’ conferences: ‘Information wants to be free’. It
            was apparently meant as part of a formulation of the complex nature of the econom-
            ics of information. Put in its strongest form, the idea is this: As the cost of acquiring
            information is driven down by efficiency advances in technology (aka ‘information
            wants to be free’), the cost of getting the information you need increases because
            you are now exposed to so much of it (Clarke, 2000). A compounded version of this
            formulation describes the quandary facing today’s internet users in our ‘post-truth
            condition’, since information about the missteps and errors of established authori-
            ties is also routinely part of the mix. Moreover, Brand’s insight suggests that a
            market will open for agencies capable of customizing the information overload to
            user needs. In effect, a supply-driven change has manufactured new demand – a take
            on ‘Say’s Law’ for our times. Ever since the advent of the daily newspaper in early
            eighteenth-century England, modern mass and social media have been largely dedi-
            cated to filling this market.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is worth observing that whatever else this dynamic demonstrates, it is not that
            information is inherently a public good. This is not to deny Brand’s premise:
            Information arguably does want to be free. But information may be set free in all
            sorts of ways that then require that people find the means to turn the resulting flow
            to their advantage. Of course, the media – including what is nowadays called ‘social
            media’ – has spontaneously risen to the challenge, but the question remains whether the media truly make information available to all, and even if it does, whether it
            does to everyone’s benefit. Those two conditions need to be met for information to
            be deemed a public good. In other words, information must constitute genuine
            knowledge – and this requires a specific kind of sustained effort that goes beyond
            the removal of barriers to information flow. While the media certainly channel infor-
            mation in specific ways, their modus operandi is different. Trying to increase mar-
            ket share is not the same as aiming for universal coverage. On the contrary,
            depending on the return on investment, a media outlet usually finds after a certain
            point that its resources are better spent shoring up its current market share than
            increasing it still further with greater outreach.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This helps to explain why welfare economists have regarded the state as the
            natural deliverer of ‘public goods’, however they are defined. This reflects a certain
            view about the basis of the state’s legitimacy: namely, that it is predicated on
            universal coverage. This means both that everyone within its borders is subject to its
            rule and everyone expects that the rulers are held to account by the ruled. The latter
            condition has become increasingly important with the democratization of politics.
            Moreover, when welfare economists speak of the state’s role in delivering public
            goods, they often invoke the idea of ‘market failure’, which suggests an interesting
            epistemic relationship between the state and ordinary market players such as
            business firms. The paradigm cases of public goods tend to be about infrastructure,
            such as roads and utilities, without which business could not transpire efficiently, if
            at all. However, no single firm or even group of firms can imagine turning a profit
            from investing in such matters due to all the others who would equally benefit in the
            process. These hidden beneficiaries would be effectively ‘free riders’ to the
            investment, who would also be given an incentive to compete against the original
            investors. However, the state – understood as an epistemically superordinate entity –
            can see how this kind of investment would in fact benefit everyone in the long term,
            including even the reluctant investors. Such a judgement presumes that the state has
            the capacity to see more comprehensively – in both spatial and temporal terms –
            than any potential business firm under its political remit. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[We see once again a politically domesticated – that is, non-Marxist – replay of
            the Socialist Calculation debate of the 1920s, which effectively split the social dem-
            ocratic ranks of Europe into ‘liberal’ and ‘socialist’ wings, a division that grew
            during the Cold War and morphed afterwards into today’s ‘libertarian’ and ‘social
            justice’ tendencies in democratic regimes more widely. At stake here is much more
            than simply whether the state or the market can best provide public goods. It is
            about the very existence of public goods as anything other than an artifice of the
            state. The suspicion is that it is only because the state sets an artificial boundary
            around the sphere of social relevance – namely, the entire human population within
            its jurisdiction – that the prospect of certain people lacking certain goods becomes
            a problem. After all, even more basic than public goods are the ‘natural goods’
            originally identified by Adam Smith, such as air and water. And while Smith was right that in one very clear sense such goods are not scarce at all, they have been of
            variable quality at the point of access – even in Smith’s day: Pollution has always
            been an issue. Indeed, the mortality of humans and other life forms is directly
            related to this variability.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perhaps not surprisingly, by the end of the nineteenth century, some radical
            liberal (aka ‘laissez faire’) economists were proposing that the variability of natural
            goods contributes to Darwinian natural selection, comparable to a market in which
            the buyer must always beware because the seller is relatively indifferent to the qual-
            ity of his goods not only because he monopolizes the market but also because he
            expects a high turnover in buyers. So, even if some are poisoned by the air and
            water, others will soon emerge to replace them. We now call this argument ‘Social
            Darwinist’. Nevertheless, it resurfaced in the recent COVID-19 pandemic by those
            who regarded the prospect of vaccinations as a ‘counter-selectionist’ artifice, as
            libertarians might oppose government subsidies for suppressing the ‘natural’ out-
            working of the market. In that case, infant and geriatric mortality rates should be
            treated in the same spirit as high failed business start-up rates: Both should be toler-
            ated in the name of what epidemiologists today call ‘herd immunity’, as these ‘fail-
            ures’ point to the limits of what constitutes a sustainable population in a given
            ecology (Fuller, 2020a: Chap. 12).]]>
			</paragraph>
			<paragraph>
				<![CDATA[I have raised the nature/artifice dichotomy because it goes to the heart of the
            normative question that underwrites the idea of public good in a strong sense that
            implicates state involvement: Is nature the standard or merely the default position?
            When universitas (‘corporation’) entered Roman law in the later Middle Ages, the
            idea was to manufacture an ‘artificial person’ whose ends transcend the interests of
            those who constitute it at any given time. Such entities originally included churches,
            monasteries, universities, city-states – and much later, nation-states and finally busi-
            ness firms. Before that time, personhood was by default based on parental descent,
            with temporary exceptions made for goal-specific associations (e.g., a military or
            commercial expedition) that required novel social arrangements for their execution.
            So, the exact relationship between ‘artificial’ and ‘natural’ persons in the new legal
            environment was unclear and resulted in an increasingly fractious European politi-
            cal scene. However, Thomas Hobbes shifted the debate decisively in the mid-seven-
            teenth century by explicitly arguing for the superior power of an artificial person
            (‘Leviathan’) over all natural persons. From that point onward, the prospect of the
            state as a reformer, if not outright enhancer, of humanity’s default settings became
            increasingly presumed in politics, even as theorists debated the merits of various
            authoritarian and democratic regimes. And 150 years after Hobbes, it also provided
            legitimacy for Humboldt’s reinvention of the university as central to the modern
            nation-building project. However, as the state recedes as the core artifice for human
            transformation, will there need to be another machine for which the university can
            serve as the ghost? And if so, might that ‘New Leviathan’ be artificial intelligence?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Abstract The Appendix sketches a theory of academic performance for the aspiring
            Humboldtian. Its fundamental principle is that speaking and writing are orthogonal
            media of communication. Both need to be cultivated, allowed to influence each
            other, but without prioritizing one medium over the other. Indeed, speaking and
            writing should influence each other in the spirit of improvisation, very much as in
            music, where it is the wellspring of creativity. This point is related to the original
            humanistic basis of the medieval university and then brought up to date with a pro-
            posal for new set of ‘Three Rs’ for academic performance based on my own per-
            sonal experience: Roam, Record and Rehearse. The Appendix ends with my own
            original academic performance, an address to my university graduating class, which
            foreshadowed the shape of things to come.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Academics spend so much time worrying about how students should present
            themselves as knowers that we rarely set a good example ourselves. We are more
            concerned with what happens in the exam room than the classroom. In short, we fail
            to regard teaching as a performing art, something that is integral to the personal
            development (Bildung) of not only the student but also the teacher. Moreover, this
            failure has enabled the administrative reduction of pedagogy to ‘content delivery’,
            which in turn has invited a host of anti-academic innovations ranging from ‘flipped
            classrooms’ to ChatGPT, the overall effect of which is to diminish the central role
            that teaching has played in the Humboldtian university. It may even threaten
            teaching with extinction as part of the academic profession, given the predominance
            of short-term contract teachers in higher education today and the overwhelming
            focus on research in postgraduate training. It is against this background that I offer
            some guides to the private and public conduct of the aspiring Humboldtian.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is a time-honoured tradition—one that goes back to the ancient Greek
            Sophists — that says that when it comes to important matters, one should always write down one’s thoughts before speaking them (Fuller, 2005: Chap. 1). Many
            reasons are given for this advice. Writing can nail down specific thoughts that might
            be difficult to recall in speech. Writing also allows complex thoughts to be laid out
            in a logical fashion, something that can be hard to do when speaking ‘off the cuff’.
            While this seems good advice, in fourth century BC Athens it encouraged the sort
            of staged public performances that inspired citizens to engage in increasingly reck-
            less behaviour, resulting in the city-state’s downfall, as recounted in Thucydides’
            Peloponnesian War. It left an indelible mark on the young Plato, who as an adult
            came to be a principled champion of literary censorship. To be sure, writing before
            speaking remains the presumed academic modus operandi, but in a way that might
            meet Plato’s approval, since if nothing else, academic writing is ‘disciplined’. But
            this whole way of thinking about the relationship between speaking and writing is
            misleading. In fact, they are orthogonal communicative media.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I learned this lesson in the early 1980s when I was preparing to give my first
            academic ‘talk’, which meant reading from a specially written text. I was in my
            twenties, still working on the Ph.D., never having prepared a forty-minute lecture.
            The event involved my travelling from Pittsburgh to Denver, about 2000 kilometres.
            This was just before air overtook train as the cheapest form of transcontinental
            travel in the United States— and before the portable typewriter was replaced by the
            personal computer as the cheapest means of producing text. Thus, I spent nearly a
            day and a half on a train romantically named the ‘California Zephyr’, banging out
            my paper on an Olivetti portable typewriter. It was not a pretty sight—or sound—for
            fellow passengers. On the day of my talk, I read the text much too quickly, probably
            because I had just written it and so I could anticipate each succeeding sentence all
            too easily. Nevertheless, the event was a reasonable success. The University of
            Colorado at Boulder, which sponsored the talk, became my first employer a couple
            of years later. However, it was clear to me from the audience questions that only
            brief fragments of the talk were understood, but they were sympathetically supple-
            mented with the questioners’ own concerns. This allowed me to connect what little
            they understood of what I said with what really mattered to them. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[I have come to believe that this is the best outcome of an academic ‘talk’, when
            the academic reads a specially prepared text. This is true even for senior academics
            speaking at more distinguished conferences, with presumably better-informed audi-
            ences. Yet, most of these interactions between speaker and audience end up being
            exercises in interpersonal diplomacy. After all, both parties have a vested interest in
            not appearing foolish, and they realize that if one party looks foolish, the other
            might appear foolish as well. To avoid the worst outcome of academia’s version of
            the Prisoner’s Dilemma, a lot of bullshit is generated during the ‘Q&A’ after the talk
            to ‘save face’. I believe that this phenomenon has led many speakers—and not only
            in academia—to regard the Q&A period as a slightly annoying form of public rela-
            tions, specifically as opportunities to repeat the original message to members of the
            audience who may have not quite got it the first-time round. Given this undesirable
            state of affairs, I have concluded that speaking and writing are simply different
            activities that serve different purposes and thus should be judged differently. Of
            course, the two activities may productively relate to each other. The most productive relations occur in the fields of drama and music – and hence Plato’s concerns about
            their deployment. In both cases, writing is a work of composition, and speaking is a
            work of performance. Each draws on the other, but the art that is created in each case
            is distinct. Playwrights and actors are equally but differently valued in the theatre,
            just as composers and performers are equally but differently valued in music.
            However, I want to make the point more broadly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Writing is a solitary and self-involved process. The audience is located in one’s
            own mind. The result can be quite articulate and sophisticated, but invariably diffi-
            cult to follow unless the reader can reproduce the author’s thought process. In con-
            trast, speaking is a publicly oriented process, one best conducted by looking at the
            audience, not a text. The speaker holds forth for shorter periods at a time and is
            compelled to respond sooner after she has concluded her communication. While a
            great writer need not be a great speaker (or vice versa), academics who know what
            they’re talking about should find the move from writing to speaking relatively easy
            because the hard work of writing is inventing a pretext for what one wants to say. If
            the audience already provides the pretext, then it is much easier to cut to the chase.
            Nevertheless, academic culture privileges the writer over the speaker. Popular cul-
            ture is a different matter. We remember the singers and actors rather than those who
            write their songs or script their lines—though all of them may be amply rewarded
            in their own way. Interestingly, law and business—despite the significance that they
            officially attach to the written word—nevertheless also accord considerable respect
            to those masters of the spoken word, courtroom advocates and salespeople. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[I am glad that my training in writing and speaking were conducted separately. In
            the case of writing, I learned how to weigh the selection of individual words and to
            read many words in sequence before passing judgement over whether they were
            true, false or even sensible. Learning to write as a distinct art gave me a tolerance
            for ambiguity in expression. Very few things make sense immediately in writing,
            unless you have previously seen what has been written. To be sure, this often hap-
            pens, because authors repeat themselves and each other. This is why ‘skimming’
            and ‘scanning’ a text is often a successful substitute for properly ‘reading’ it.
            However, repetition is usually considered to be a vice—not a virtue—of writing. (It
            may even count as ‘self-plagiarism’.) Writing is supposed to express a unique
            thought in each sentence, and ‘reading’ is the art of dealing with a text as if that
            supposition were true. The virtues of speaking are very different. Repetition is toler-
            ated—and even encouraged—to keep an idea in the forefront of the audience’s
            mind. This point was first made clear to me when I was taught to recite various
            speeches, poems and songs as a student of Jesuit schooling. Such works include
            ‘motifs’, ‘refrains’ and ‘choruses’ precisely because they have been written to be
            performed in voice. It is against this backdrop that so much of twentieth-century
            music and poetry—and academic writing, one might add—is said to be ‘difficult’.
            These compositions typically lack the performance-oriented features of more clas-
            sically composed works. For example, if the sequence of notes in a piano-based
            musical score is too complicated, then the pianist cannot simply do it from memory
            but must perform at a level below her normal technical expertise, as she is forced to
            read the notes while playing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nevertheless, the virtues of speaking come out clearest in relation to writing.
            When as a student I was required to recite speeches, poems and songs, I had to
            engage in minor forms of improvisation. I had to decide where to place emphasis on
            particular words and phrases to get the point across to a particular audience.
            Sometimes I felt compelled to alter the original composition to improve the flow of
            my performance. There is an art to this too, since audiences, not least the instructor,
            may fail to respond well to your improvisation—I remember many frowns and gri-
            maces. Over the years, I have come to believe that improvisation—that risky zone
            between writing and speaking—is the true crucible of creativity. You need to be able
            to both write and speak well, but then you must play them against each other. Jazz
            is the most obvious art form that is based on improvisation. But I would add teach-
            ing as well. Great improvisers go beyond simply making one text come alive.
            Rather, they can combine multiple texts with surprising results that would not be
            evident from performing the texts individually. In practice, this means that when
            I teach a course, I assign an eclectic mix of readings, the mutual relevance of which
            may not be obvious to the naked eye. However, the point is for the student to attend
            my lectures to see how I weave them together – and then try to do something similar
            in their assigned work.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you see writing and speaking as distinct activities, as I do, then your own
            patterns of written and spoken discourse will change. Writing will become more
            compressed, very much like a musical score, in which no note is wasted. However,
            speaking will become more expansive, very much like a raconteur who can spin out
            a narrative indefinitely. This may create considerable dissonance in how others per-
            ceive you. People generally find me a very difficult writer but a very accessible
            speaker. While the sharpness of this perception surprises me, nevertheless I see
            it—and I believe that others should cultivate it. Because improvisations are by defi-
            nition tied to specific performances, their sense of ‘creativity’ may be difficult to pin
            down beyond the bare fact that a particular audience finds what it hears likeably
            novel. Moreover, audio and video recording has helped to convert improvisations
            into texts in their own right for others to study and improvise upon. This is the stan-
            dard of performance that you should always aim for—what artists and musicians
            call ‘virtuosity’. Your current performance should never be reducible to a past per-
            formance, let alone a composition on which the performance is nominally based. In
            other words, what you say should always be worth recording. You should not speak
            as you write but your speech should be worth writing about.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The value of improvisation is as a public demonstration of creativity. It was precisely
            this aspect of rhetoric that the ancients placed in such high esteem. It manifested the
            free exercise of reason, the hallmark of the self-realized human, someone worthy
            recognizing as a peer in the polity. The Greeks called improvisation heuresis (the
            root of ‘heuristic’) and the Romans inventio (the root of ‘invention’). However, both ‘heuristic’ and ‘invention’ are nowadays terms mainly associated with the
            technological, even the makeshift. To understand the temporal continuity from
            antiquity to modernity, imagine that the ancients saw our individual bodies as
            unique instruments that the mind plays. In the Middle Ages, this attitude was for-
            malized as a set of disciplines that impart what are nowadays called ‘transferable
            skills’. They provided the original basis for the liberal arts and was taught in two
            course packets, named (in Latin) for the number in each packet: Trivium (grammar,
            logic, rhetoric) and Quadrivium (geometry, arithmetic, astronomy, music).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plato’s fingerprints are all over this curriculum, as can be seen by the foundational
            role played by memory (anamnesis) in both courses. Whereas the Trivium trained
            the memory to be delivered initially in speech but increasingly also in writing, the
            Quadrivium added training to the eyes and ears to mediate that delivery. (On another
            occasion, I might propose a ‘Quinquivium’ of five qualitative and quantitative
            research methodologies used across a range of academic disciplines that further
            mediate and enhance ourability to understand and intervene in the world.) Those
            who underwent this curriculum were thus able to judge the world for themselves
            without having to rely on the word of others. They could literally ‘speak for them-
            selves’. This capacity for improvisation would be the mark of their freedom – ‘lib-
            eral’ in its etymological sense. In this context, Plato’s view of ‘memory’ is crucial.
            It is the key to the improvisational character of the liberal arts.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plato believed that memory was ultimately collective, and that an individual
            memory is a ‘recollection’, which is to say, a recombination of an initial set of pre-
            existing possibilities in the collective mind. In that sense, each ‘remembering’ is an
            act of improvisation, whereby the individual improvises their identity by creatively
            elaborating this shared psychology (Bartlett, 1932). In a more scientistic vein, we
            might say that Plato thought about memory as a kind of probabilistic grammar,
            which individuals play out in speech and writing with varying degrees of effective-
            ness. The point of education in the liberal arts, then, is to increase the effectiveness
            of these improvisations through the various sorts of mental discipline offered in the
            curriculum. Thus, the liberal arts increasingly focused on rhetoric, which in the
            Renaissance was epitomized as the ‘art of memory’, whose most distinguished
            practitioners included those icons of the Scientific Revolution, Galileo and Francis
            Bacon (Yates, 1966). But the secret ingredient of this curriculum – about which
            Plato had very mixed feelings – was literacy. Literacy implies that the presupposed
            grammar of possibilities can be stated as a code, knowledge of which enables mul-
            tiple, potentially conflicting, instantiations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It’s hard to improvise if you’re the only source of the code. You must then focus
            your energies on transmitting the code. Thus, in cultures where literacy is either
            highly restricted or non-existent, your identity depends on others regarding you as
            an authoritative instantiation of some part of your culture. This is what is normally
            meant as ‘reproduction’ by both sociologists and biologists. For this reason,
            Marshall McLuhan’s mentor, Harold Innis (1951), somewhat counter-intuitively,
            treated both the stone-based writing of the ancient Near East and the spoken
            Homeric epics as ‘time-biased media’. Innis meant that they principally aim to
            extend the longevity of the message, one through the durability of the medium itself and the other through a system of intergenerational inheritance, whereby invention
            is regarded as imperfect reproduction. This is why in non-literate cultures, speaking
            tends to be quite rigid and directive – at least from the standpoint of those coming
            from literate cultures. Without the external memory storage provided by widespread
            literacy, the personal exercise of memory is the main vehicle for reproducing what
            already exists (aka social structure). Private remembering is a form of self-discipline
            (as in ‘rote memory’) and public remembering a relatively scarce and even sacred
            moment of cultural endowment to the audience, who would have no other way of
            acquiring the culture.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Consider the sense of pastoral security that a literate clergy provided to the
            illiterate laity well into the modern era. Priests enjoyed a monopoly over the sacred
            code, in terms of which they could then justify (or not) the ‘improvisations’ of
            everyday secular life. It was a world that Plato could have endorsed. However, with
            the advent of mass literacy in the Reformation, whereby everyone was encouraged
            to acquire access to ‘The Code’ (aka the Bible), the human mind was gradually
            freed up from having to reproduce the past, which permitted the showcasing of
            novelty, which has increasingly been seen as distinctive of humans as a species.
            Such novelty first flourished in the Protestant splintering of Christianity into denom-
            inations, but over the centuries came to be generalized and secularized as market
            differentiation. Rational choice accounts of religion have been well attuned to this
            phenomenon, which amounts to a downstream effect of Max Weber’s The Protestant
            Ethic and the Spirit of Capitalism (Stark & Bainbridge, 1987).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Improvisation had already become relevant to higher education with the
            institution of the ‘lecture’ at medieval universities. On its face, the lecture hall was
            a factory for mass producing the latest edition of authoritative texts. Specifically, the
            student audience manufactured texts as they copied lecturers’ recitation of their own
            copies of those texts, supplemented by the lecturers’ commentaries. It made the
            establishment of any copyright-friendly sense of ‘authorship’ difficult. New com-
            ments were often incorporated as part of the commented text. This haphazard tex-
            tual accretion has arguably made the history of both factual and fictional literature
            more ‘evolutionary’ in the Darwinian sense than one might have expected or wanted.
            (For a vivid Borges-inspired illustration of what this means, whereby Don Quixote
            ‘evolves’ into Ulysses over four centuries of successive re- and mis- transcriptions,
            see Berlinski, 1996: 29). It also explains the slow pace of intellectual progress until
            the period we call ‘modern’, even though most modern ideas had been incubated in
            medieval universities. But, as Elizabeth Eisenstein (1979) showed, the printing
            press revolutionized the situation by freeing up the mind’s inventive capacity, not
            merely in speech but now also in writing. One no longer had to provide an embel-
            lished repetition of past statements. A simple mention (or ‘link’, as we now say)
            would suffice to conjure up the missing background thatg frames the communica-
            tion. And while the footnote is normally seen as the start of the modern referencing
            system, it began as a way to spatially distinguish the multiple arguments that could
            now be simultaneously pursued on the two-dimensional surface of the written page
            (Grafton, 1997).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The question then became how to spread this ‘meta-capacity’ across the entire
            population. This is trickier than it sounds, because ‘literacy’ is not a univocal skill.
            For example, literacy as a requirement for citizenship is relatively superficial. It
            simply means that you read sufficiently well to provide a competent written
            responses to the certain exam-like questions – and very often ticking a box will do.
            A deeper sense of literacy is required for improvisation. It depends on one’s having
            read enough of what others have written to be able to create something that is inter-
            estingly new. Thus, the more writing to which you are exposed – whether you were
            the original author – the freer you become, as you have more texts to combine that,
            in turn, you will have seen combined in more ways. Indeed, improvisation is most
            clearly understood as an interpretive performance of something written, be it a
            score, a poem, a script – or an academic text. In that sense, it is a ‘reading’ in the
            broad sense that was popularized by French post-structuralist philosophers in the
            1970s – a more metaphysically informed extension of the conventional French aca-
            demic practice of explication de texte.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Jazz is often seen an inherently improvisational genre because performances
            tend to be informed by multiple pieces of music that are artfully played off each
            other to produce a unique result. In this respect, the best teachers are very much like
            the best jazz artists: They are not simply rehearsing one well-read text but are rather
            drawing on multiple texts simultaneously, a feature that is only imperfectly repre-
            sented in an ordinary course syllabus. Students exposed to such a pedagogical
            improvisation can interpret their own task as either to reproduce the performance,
            rendering it a fixed text (which probably means some stylisation on the part of the
            student) or to reverse engineer the performance to reveal the interplay of its consti-
            tutive elements, ideally to produce a new complex that may also exchange some of
            the textual elements at play. I prefer the latter, but it also lays itself more open to the
            charge of ‘mere bullshit’ (Fuller, 2009: Chap. 4).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Improvisation remains one of the few things equally valued by the Humboldtian
            university and the neo-liberal academy. Indeed, only closet authoritarians fear and
            loathe improvisation. In effect, the improviser wants the audience to judge him or
            her sui generis, which is a high-risk strategy, as it simultaneously invites the audi-
            ence to project their own normative models of what the improviser is trying to do.
            Some may see mess, where others see art – and even if they see art, it may not be
            according to the improviser’s own implied standard. Karl Jaspers went so far as to
            describe the lecture as ‘a high point in one’s professional responsibility and achieve-
            ment, finally to renounce all artificiality’ (Jaspers, 1959: 57–8). Unfortunately, aca-
            demics have become less willing to submit themselves to such trials by improvisation.
            Instead, they prefer to be judged by scripts written by others – or themselves in a
            different state of mind from the one in which they do the presentation. When medi-
            eval intellectual life is casually labelled as ‘Scholastic’, this is what is meant. I leave
            it to historians to determine whether that epithet applies to the Middle Ages itself.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nevertheless, ‘Scholasticism’ certainly exists as a free-floating signifier that is
            attachable to a variety of intellectual practices across time and space. In the case of
            the High Middle Ages (I.e., starting in thirteenth century Christendom, largely in
            response to the Muslim threat to Europe), lecturers legitimized their own words by presenting them as interpretations of, say, Aristotle or the Bible. It was a risk-averse
            strategy to shore up a sense of politico-religious authority that might otherwise be
            seen as dwindling. In any case, it is opposed to the innovative spirit of the modern
            Humboldtian university. This is not the deny the intellectual virtuosity demonstrated
            under the medieval conditions. But fast forward to the present, and given today’s
            questioning of the university’s future, we shouldn’t be surprised that Scholasticism
            enjoys a technological facelift in the mindless lip syncing to Powerpoints that rou-
            tinely happens in both the lecture halls and the conference centres of academia.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Several years ago, an enterprising Korean publicist, Douglas Huh, canvassed the
            study skills of creative people in various fields. I responded that my most successful
            study skill is one that I picked up very early in life – and perhaps is difficult to adopt
            after a certain age. Evidence of its success is that virtually everything I read appears
            to be hyperlinked to something in my memory. In practice, this means that I can
            randomly pick up a book and within fifteen minutes I can say something interesting
            about it – that is, more than summarize its contents. So, this is not about ‘speed
            reading’. Rather, I make the book ‘my own’ in the sense of assigning it a place in
            my cognitive repertoire, to which I can then refer in the future. There are three fea-
            tures to this skill. One is sheer exposure to many books. Another is taking notes on
            them. A third is integrating the notes into your mode of being, so that they function
            as a script in search of a performance. If the original 3Rs were ‘Reading, (W)riting
            and (A)rithmetic’, I give you the new 3Rs: Roam, Record and Rehearse. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s start with Roam. Reading is the most efficient means to manufacture
            equipment for the conduct of life. It is clearly more efficient than acquiring personal
            experience. Put better, reading itself is a form of personal experience. In this con-
            text, ‘Roaming’ means taking browsing seriously. By ‘browsing’ I mean forcing
            yourself to encounter a broader range of possibilities than you imagined was neces-
            sary for your reading purposes. Those under thirty years old may not appreciate that
            people used to have to occupy a dedicated physical space – somewhere in a book-
            shop or a library – to engage in ‘browsing’. It was an activity which forced you to
            encounter works both ‘relevant’ and ‘irrelevant’ to your interests. Ideally, the expe-
            rience would challenge the neatness of this distinction, as you came across books
            that turned out to be more illuminating than expected. To be sure, ‘browsing’ via
            computerized search engines still allows for that element of serendipity, as anyone
            experienced with Google or Amazon will know. Nevertheless, browser designers
            normally treat such a feature to be a bug in the programme to be fixed in its next
            version, so you end up finding more items like the ones you previous searched for.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As a teenager in New York City in the 1970s I spent my Sunday afternoons
            browsing through the two biggest used bookshops in Greenwich Village, Strand and Barnes & Noble. Generally speaking, these bookshops were organized according to
            broad topics, somewhat like a library. However, certain sections were also organized
            according to book publishers, which was very illuminating. In this way, I learned,
            so to speak, ‘to judge a book by its cover’. Publishing houses have distinctive mar-
            keting styles that attract specific sorts of authors. Thus, I was alerted to differences
            between ‘left’ and ‘right’ in politics, as well as ‘high’ and ‘low’ in culture. Taken
            together, these differences offer dimensions for mapping knowledge in ways that
            cut across academic disciplinary boundaries. The more general lesson here is that if
            you spend a lot of time browsing, you tend to distrust the standard ways in which
            books – or information, more generally – is categorized.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s move from Roam to Record. Back in New York I would buy five used
            books at a time and read them immediately, annotating the margins of the pages.
            However, I soon realized that this was not an effective way to make the books ‘my
            own’, in the sense of part of my personal repertoire, let alone constitutive of my
            unique personhood. So, I shifted to keeping notebooks, in which I quite deliberately
            filtered what I read into something I found meaningful and to which I could return
            later. Invariably this practice led me to acquire idiosyncratic memories of whatever
            I read, since I was effectively rewriting the books that I had read for my own pur-
            poses. In my university days, under the influence of Harold Bloom (1973), I learned
            to call what I was doing ‘strong reading’. The practice of recording has carried into
            my academic writing. When I make formal reference to other works, I am usually
            acknowledging an inspiration – not citing an authority – for the claim I happen to be
            making. My aim is to take personal responsibility for (‘own’, in today’s lingo) what
            I say. I dislike the tendency for academics to obscure their own voice in a flurry of
            scholarly references that simply repeat connections that can be made by a standard
            Google search of the topic under discussion.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This proactive view of notetaking has also influenced my rather negative view of
            Powerpoint presentations. Powerpoints dumb down the lecturer and the audience, as
            both are led to think that they understand more than they probably do. The ultimate
            casualty is the distinctiveness of the academic encounter, beginning with inter-
            changeable lecturers on short-term contracts and ending with students preferring
            artificial intelligences over humans as lecturers altogether – assuming they can still
            tell the difference. In pre-Powerpoint days, even though the books and articles dis-
            cussed by lecturers were readily available, the need to take notes made students
            aware of the difference between speaking and writing as expressive media. Even
            when lecturers read from ‘lecture notes’, they typically improvised at various points
            that often turned out to be the most interesting things they said. And if they simply
            read verbatim as if speaking from a medieval podium, the students would notice –
            and often not favourably. To be sure, the art of capturing those impromptu spoken
            moments through notetaking is far from straightforward, but there was no consistent
            alternative, even in the 1970s. That was still the era of clunky tape recorders rather
            than recording apps on smartphones. Students like me had to rely on our wits to
            translate meaningful speech into meaningful writing on the spot. This profound skill
            is lost to students who rely on Powerpoint’s false sense of cognitive security.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Finally, let’s turn from Record to Rehearse. Rehearsal already begins when you
            shift from writing marginalia to full-blown notebook entries insofar as the latter
            forces you to reinvent what it is that you originally found compelling in the note-
            worthy text. Admittedly the cut-and-paste function in today’s computerized word
            processing programmes can undermine this practice, resulting in ‘notes’ that look
            more like marginal comments. However, I rehearse even texts of which I am the
            original author. You can keep yourself in a rehearsal mode by working on several
            pieces of writing (or creative projects) at once without bringing any of them to
            completion. In particular, you should stop working just when you are about to reach
            a climax in your train of thought – an intellectual coitus interruptus, if you will. The
            next time you resume work you will then be forced to recreate the process that led
            you to that climactic point. Often you will discover that the conclusion toward
            which you thought you had been heading turns out to have been a mirage. In fact,
            that so-called ‘climax’ opens up a new chapter with multiple possibilities ahead.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some people will recoil from above, as it seems to imply that no work should
            ever end, which is anathema for anyone forced to produce something on schedule to
            earn a living! Nevertheless, even as the author of twenty-five books, from my own
            standpoint, each one ends arbitrarily and even abruptly. (My critics occasionally
            notice this.) And precisely because I do not see the books as ‘finished’, they con-
            tinue to live in my mind as something to which I can always return – and to which
            others are invited to resume and redact. In any case, they remain part of my reper-
            toire, which I periodically rehearse as part of defining who I am to a new audience.
            In a sense, this is my solution to the problem of ‘alienation’ which Karl Marx
            famously identified. Alienation arises because industrial workers in capitalist
            regimes have no control over the products of their labour. Once the work is done, it
            is sold to people with whom the workers have no contact. But of course, alienation
            extends to intellectual life as well, as both journalists and academics need to write
            quite specific self-contained pieces targeted at clearly defined audiences with whom
            they would otherwise have no affinity. Under the circumstances, there is a tendency
            to write in a way that enables the author to detach him- or herself from, if not out-
            right forget, what they have written once it is published. Often this tendency is posi-
            tively spun by saying that a piece of writing makes its point better than its author
            could ever do in person. And that may often well be true – and damning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nevertheless, my own thinking proceeds in quite the opposite direction. You should
            treat the texts you write more like dramatic scripts or musical scores than completed
            artworks – that is, ‘allographically’, in Nelson Goodman’s (1968) terms. They should
            be designed to be performed in many different ways, not least by yourself, which in
            turn explains my liberal attitude towards plagiarism. Whatever is plagiarised is never
            put to the same use as the original, not even in a student assessment context. A student
            who works Kant’s prose seamlessly into an assignment on Kant is doing something
            other than what Kant originally did, because whatever else Kant might have been try-
            ing to do in his texts, he wasn’t trying to show that he understood himself. That would
            have been taken for granted by his intended audience. However, the student is obvi-
            ously in a different position, operating at a distance from the person of Kant and hence
            trying to show that he understands Kant. Kant’s text is incomplete until someone brings it to life. That person might be Kant himself, or equally it might be a student
            plagiarist or an advanced Chat-GPT program – or some combination! Our intuitions
            about this matter are confused because we presume knowledge of the identity of the
            source in each case, which biases our judgement. In this regard, I’m recommending
            that we take a more ‘Turing Test’ approach to the matter and suspend knowledge of
            the author. In any case, the text is always in need of execution, which is another way
            of saying ‘rehearsal’. Taken together, Roam, Record and Rehearse has been a life
            strategy which has enabled me to integrate a wide range of influences into a dynamic
            source of inspiration and creativity that I understand to be very much my own.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What follows is my Salutatory Address delivered to the 1979 graduating class of
            Columbia College, Columbia University, New York. US college graduating ceremo-
            nies are traditionally opened by the person who graduated number two in terms of
            grade point average (all students are ranked together, regardless of subject major).
            This person is the ‘Salutatorian’. The person ranked number one closes the gradua-
            tion ceremony, and hence is called the ‘Valedictorian’. Much of the animus behind
            my speech can be traced to my contempt for pre-professional degrees in law and
            especially medicine. It anticipates the anti-expertise line pursued in these pages. In
            the case of medicine, the better students routinely achieved artificially inflated
            grades based on a bell curve distribution of the marks. It meant that a student could
            do astonishingly well simply by being a couple of standard deviations ahead of their
            average classmate, without the need for the instructor to have set a prior standard of
            adequate performance. (My class valedictorian was a pre-med major.) In contrast,
            virtually all my coursework was judged by instructors who were laws unto them-
            selves when it came to marking. This meant that, at least in principle, everyone
            could do very well or very poorly. Ah, the good old Humboldtian days...]]>
			</paragraph>
			<paragraph>
				<![CDATA[The text is reproduced from the original typescript of the speech with no editorial
            changes. It was delivered two months before the author turned twenty.
            ***
            THE ACADEMY: FROM DIVINITY TO BOVINITY
            (a fabula rasa)
            Steve Fuller, 15 May 1979, Columbia College, NYC
            Once upon a time…
            … Back in the days when teaching was still a marketable profession, its
            bargaining power derived from being the sole distributor of a certain indispensable
            commodity, which we shall call Divinity. Divinity secured this power in several
            ways, each of which capitalized on the structure of the academic establishment.
            First, Divinity was attributed a constant presence in everyday life. This very nicely
            made up for the potential weakness of education lasting too short a period of time
            to make any difference in the student’s life. However, in order to bolster this divine ever-presence, a second trait – that of limited access – was necessary. On an obvious
            level, Divinity appears to be a fabrication since students do not know of its intrica-
            cies until they are educated. Yet, if the academicians argue that its presence is a
            secret one which requires privileged knowledge, then the possibility of fabrication
            gets turned around to emphasize the fundamental stupidity of the students. But there
            is one more element that was needed to seal this power, and that was the explicit
            superiority of Divinity in relation to other possible realities. Making the unseen
            esoteric is not nearly as difficult as making it indispensable. A student may be quite
            willing to accept his ignorance of Divinity in order to capitalize on other virtues,
            such as his sexuality. But the academicians, clever as usual, equated the unseen with
            the initial determining force of the divine agency. They then attributed their own
            esoteric grasp of the matter to the tapping of a natural resource that recreates this
            initial occurrence of Divinity in every action, namely, the soul. If the student became
            content with his ignorance, say by luxuriating in the splendor of sexual awareness,
            his soul was subsequently doomed, which was said to be a rather unpleasant state-
            of- affairs – so unpleasant that it was unimaginable. Admitting this much ignorance
            on the part of the academicians was important so as to guarantee a certain verisimili-
            tude in their teachings, which were those of mortals feebly contemplating the truth.
            In order to be effective, the Divinity had to be separated from the diviners; the
            product from the producers. Producers come and go, but the product always remains.
            We have all heard that somewhere before.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For all these slanderous comments about divine academicians, there is one
            overriding positive note, namely, that Divinity encouraged a consistently critical
            attitude toward anything that exists. There was talk about the illusion of the senses
            that might make us think the usual, undivine route was the correct one. In short, the
            facts never got in the way of the truth. One responded not with examples from real
            life but with possible situations, and it was the logic of the argument rather than a
            majority of consenting adults that carried the day. As keepers of the Ivory Tower, the
            divine academicians found the much-flaunted empirical world to be only one of
            many. Thus, the authoritarian myth of Divinity became a license for all types of
            unearthly thinking and pooh-poohing of current affairs. These ideas then were said
            to have divine powers because they divested one of the baggage that inexorably
            drags down the person committed to following the course of the oh-so-mundane
            facts. A revolutionary transcendence could always be discussed in the Ivory Tower,
            and with the power of Divinity the diviners could actually scare enough people into
            its practice.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But such affective measures could not be expected to last forever. The bottom of
            the market eventually fell out of Divinity, and the diviners were brought down to
            earth – so far down that they were reduced to facing, of all things, the facts. Their
            subsequent ruminations compensated for the prior disregard of the facts by raising
            them to the level of sacred cows, which gives the name to this new academic order,
            Bovinity. Like their mammalian namesakes, the boviners always exhibit a profound
            look of impotence, which is said to be the result of ponderous deliberation that looks
            at both sides before crossing the issue. And crossing the issue is indeed a lot of hard work for these timid creatures. Consequently, the work-value of ideas became very
            significant: Do they work? Which can be translated as whether they take into
            account the ‘hard facts’. Students nowadays consigned to a temporary brush with
            Bovinity have broken up into two classes: the pre-professional and the pre-nothing.
            These two types of boviners can be distinguished by their relation to the facts.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The pre-professional is very clever because he knows what a fact is. As we have
            seen, facts are indeed curious little things. ‘Fact’, as you may know, derives from the
            past participle of the Latin verb ‘to make’, a completed state of making – something
            that has already happened. To say, with the believers in Bovinity, that the facts deter-
            mine the future is therefore quite an endorsement of the way things have been. Pre-
            professionals capitalize on this fact by entry into fields that aim at maintaining a
            social equilibrium or – if I may venture a political word – the status quo. Of course,
            I am referring to law and medicine. Both of these are founded on the fundamental
            weakness of the individual, who is always trying to recoup his losses in order to
            break even. The technical term, I believe, is ‘a standard of living’. The law works its
            wonders by being a constant reminder that the natural condition of the state is a war
            of all against all. Nobody minds their own business because they’re trying to take
            over yours. Medicine is even nobler since it prolongs the amount of time you have
            to break even, which is to say, the amount of time you have to engage in legal ser-
            vices. But naturally you never break even – not even in death – for it is at that point
            that the friendly giants of professionalism come to blows. Medicine won’t let you
            transcend your factual existence so that the law cannot execute what facts remain.
            Pre-professionals see the academic establishment quite rightly as the bovine trans-
            mission of these facts. And they find such things as a classical education very useful
            in that direction, for it neatly maps out the royal road from Solon and Hippocrates
            to Perry Mason and Marcus Welby in such a way as to capture even the imagination
            of the pre-nothings who employ Bovinity as their own standard-bearer. (And, as we
            all know, among comrades the pre-professional will quite openly admit the instru-
            mentality of these bovine features in accomplishing the grand mission, which goes
            under the heading of Bull.)]]>
			</paragraph>
			<paragraph>
				<![CDATA[As for the pre-nothings, such as myself, nothing much can be said. Sometimes I
            think a pre-nothing doesn’t have the attachment to Bovinity that the pre-professional
            does, but I fear that this is not the case at all. Demonstrations are generally conve-
            nient media for making one’s existence felt, and if you’re pre-nothing the transcen-
            dence of nothingness in indeed a pretty tall order. Unfortunately, pre-nothings never
            demonstrate their existence but only some disturbing fact – such as the impending
            nuclear holocaust, covert slave trades in forbidden continents, and the like. However,
            Bovinity as it is can quite readily ruminate a response that restores the balance of
            facts and returns the pre-nothings to a state of nothingness – until the next disturbing
            fact comes along. Again, we break even. If we lived in a divine age, the pre-nothing
            might feel it quite natural to demonstrate about nothing have the keepers of the
            Ivory Tower divine some reasons for the discontent. Surely, a survey of all the pos-
            sible problems in an academic establishment could turn up some rather interesting
            arguments. But then again, that might be a little too divine.]]>
			</paragraph>
		</content>
	</book>
	<book name="Building New Age Games with Rust and Amethyst">
		<content>
			<paragraph>
				<![CDATA[Vіdео games hаvе come a long wау frоm
            thеіr еаrlу dауѕ. The Suреr Mаrіо Brоѕ
            fоr NES rаn оn an 8-bit CPU thаt had a
            1.79MHz сlосk rаtе. Thе game itself іѕ
            roughly 31KB. Nowadays, уоu can easily
            gеt a gaming PC thаt hаѕ аn 8-соrе CPU
            runnіng at 35GHz еасh, аnd games that
            аrе 50-70GB. That іѕ thоuѕаndѕ оf tіmеѕ
            more соmрutіng роwеr аnd mіllіоnѕ оf
            tіmеѕ mоrе storage ѕрасе. Games аrе
            growing more аnd mоrе complex аѕ wеll,
            so thе life оf a gаmе programmer іѕ
            bесоmіng tоughеr thаn bеfоrе.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Rust is роtеntіаllу a great саndіdаtе fоr
            building gаmеѕ. Rust’s lоw-lеvеl mеmоrу
            ѕаfеtу guаrаntее аnd еxсерtіоnаl
            реrfоrmаnсе mаkе іt іdеаl for buіldіng
            rоbuѕt and performant game еngіnеѕ аnd
            gаmеѕ. At the ѕаmе tіmе, its hіgh-lеvеl
            ѕуntаx allows you to wrіtе уоur gаmе
            lоgіс іn a сlеаn and mоdulаr wау.
            Ruѕt іѕ a lоw-lеvеl ѕtаtісаllу-tуреd multі-
            раrаdіgm рrоgrаmmіng language thаt’ѕ
            focused оn ѕаfеtу аnd performance.
            Rust solves problems that C/C++ hаѕ
            been ѕtrugglіng wіth for a lоng time,
            ѕuсh аѕ mеmоrу errors аnd buіldіng
            соnсurrеnt рrоgrаmѕ.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It has three main bеnеfіtѕ:
            -better memory ѕаfеtу due tо thе
            соmріlеr;
            -еаѕіеr concurrency due tо the data
            ownership mоdеl that prevents
            dаtа races;
            -zеrо-соѕt аbѕtrасtіоnѕ.
            If уоu wаnt to do ѕуѕtеm programming,
            уоu nееd thе low-level соntrоl thаt
            memory mаnаgеmеnt рrоvіdеѕ.
            Unfortunately, mаnuаl mаnаgеmеnt
            соmеѕ wіth a lоt оf issues іn lаnguаgеѕ
            lіkе C. Dеѕріtе the рrеѕеnсе оf tооlѕ lіkе
            Valgrind, catching memory management
            рrоblеmѕ іѕ tricky.]]>
			</paragraph>
			<paragraph>
				<![CDATA[
            Ruѕt prevents thеѕе іѕѕuеѕ. Ruѕt’ѕ
            оwnеrѕhір system аnаlуѕеѕ thе
            рrоgrаm’ѕ mеmоrу management аt
            compile-time, mаkіng sure that bugs duе
            tо рооr mеmоrу management can’t
            happen and that gаrbаgе collection іѕ
            unnесеѕѕаrу.
            Furthеrmоrе, іf you wаnt tо dо ѕuреr-
            орtіmіzеd іmрlеmеntаtіоnѕ іn a C-lіkе
            manner, you can do that while expressly
            separating thеm frоm thе rеѕt of thе
            code wіth thе unsafe kеуwоrd.
            Duе tо thе bоrrоw checker, Ruѕt саn
            рrеvеnt dаtа rасеѕ at соmріlе-tіmе.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Dаtа rасеѕ occur whеn two thrеаdѕ
            ассеѕѕ thе same memory at thе ѕаmе
            time, аnd they саn lеаd tо ѕоmе nаѕtу,
            unрrеdісtаblе behavior. Thаnkfullу,
            рrеvеntіng undеfіnеd bеhаvіоr іѕ аll whаt
            Ruѕt is about.
            Zеrо-соѕt abstractions mаkе sure thаt
            thеrе is vіrtuаllу nо runtіmе оvеrhеаd fоr
            thе abstractions thаt you uѕе. In ѕіmрlеr
            wоrdѕ: thеrе іѕ nо ѕрееd dіffеrеnсе
            bеtwееn lоw-lеvеl соdе and оnе written
            wіth аbѕtrасtіоnѕ. Arе these thіngѕ important? Yes. For
            еxаmрlе, аrоund 70 % оf the issues
            аddrеѕѕеd bу Microsoft in thе раѕt 12
            years hаvе been mеmоrу еrrоrѕ. Same
            wіth Gооglе Chrоmе.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whаt іѕ Rust uѕеd fоr?
            Ruѕt bеіng a rather low-level lаnguаgе,
            it’s uѕеful whеn уоu nееd tо ѕԛuееzе
            more оut оf the rеѕоurсеѕ you hаvе.
            Sіnсе іt’ѕ statically tуреd, the tуре
            system hеlрѕ уоu dеtеr сеrtаіn сlаѕѕеѕ of
            bugѕ durіng compilation. Thеrеfоrе, уоu
            wіll tеnd tо use it when your rеѕоurсеѕ
            are lіmіtеd, and when it іѕ important thаt your software dоеѕn’t fаіl. In contrast,
            hіgh-lеvеl dуnаmісаllу tуреd lаnguаgеѕ
            lіkе Python аnd JаvаSсrірt are better for
            thіngѕ like ԛuісk рrоtоtуреѕ.
            Hеrе аrе ѕоmе of Ruѕt’ѕ uѕе саѕеѕ:
            - Pоwеrful, cross-platform
            command-line tооlѕ.
            - Distributed оnlіnе ѕеrvісеѕ.
            - Embеddеd devices.
            - Anуwhеrе еlѕе you would nееd
            ѕуѕtеmѕ рrоgrаmmіng, like
            brоwѕеr engines and, perhaps,
            Linux kernel.
            For еxаmрlе, here аrе a few ореrаtіng
            ѕуѕtеmѕ, written іn Ruѕt: Rеdоx,
            intermezzOS, QuіltOS, Rux, Tосk.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Even though Ruѕt is ѕuреrfісіаllу ԛuіtе
            ѕіmіlаr tо C, it іѕ hеаvіlу influenced by
            thе ML fаmіlу оf languages. (Thіѕ fаmіlу
            includes lаnguаgеѕ like OCаml, F#, аnd
            Hаѕkеll.) For еxаmрlе, Ruѕt trаіtѕ are
            basically Haskell’s tуресlаѕѕеѕ, аnd Ruѕt
            hаѕ very роwеrful раttеrn mаtсhіng
            сараbіlіtіеѕ.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Rust dоеѕ fеаturе mоrе mutаbіlіtу than
            functional рrоgrаmmеrѕ would usually be
            accustomed tо. Wе саn think оf іt like
            thіѕ: bоth Ruѕt аnd FP try tо avoid shared mutable state. Whіlе FP is
            fосuѕеd on avoiding mutable ѕtаtе, Ruѕt
            tries tо аvоіd thе ѕhаrеd раrt оf the
            dаngеr. Ruѕt is аlѕо mіѕѕіng a lоt оf stuff
            thаt would mаkе functional рrоgrаmmіng
            dоаblе іn it, ѕuсh as tаіl саll орtіmіzаtіоn
            аnd gооd ѕuрроrt fоr funсtіоnаl dаtа
            structures. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[All in аll, thеrе іѕ еnоugh support fоr
            funсtіоnаl programming іn Ruѕt fоr
            ѕоmеbоdу tо hаvе wrіttеn a bооk аbоut
            іt.
            Since Ruѕt іѕ fосuѕеd on performance
            аnd dоеѕ nоt uѕе a garbage collector, games wrіttеn in it should be реrfоrmаnt
            аnd рrеdісtаblу fаѕt.
            Unfоrtunаtеlу, thе ecosystem іѕ ѕtіll
            уоung, аnd thеrе іѕ nothing written іn
            Ruѕt thаt would compare tо Unreal
            Engine, for еxаmрlе. Thе pieces аrе
            thеrе, thоugh, and Ruѕt hаѕ a lively
            community.]]>
			</paragraph>
			<paragraph>
				<![CDATA[IЅ RUЅT GOOD FОR WEB
            DЕVЕLОРMЕNT?
            Rust hаѕ multiple frаmеwоrkѕ fоr web
            development lіkе Actix Web аnd Rосkеt
            thаt аrе very uѕаblе аnd wеll-buіlt. In
            particular, if уоu аrе lооkіng fоr рurе
            ѕрееd, Aсtіx Wеb hіtѕ thе tор оf
            frаmеwоrk bеnсhmаrkѕ.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Ruѕt dоеѕn’t have аnуthіng thаt саn
            соmреtе wіth thе есоѕуѕtеm оf
            frameworks lіkе Django and Rаіlѕ,
            thоugh. Since Rust іѕ a rаthеr уоung
            lаnguаgе, a lоt of hаndу utіlіtу libraries
            are mіѕѕіng, whісh means thаt thе dеvеlорmеnt рrосеѕѕ is nоt that ѕіmрlе
            аnd еаѕу.]]>
			</paragraph>
			<paragraph>
				<![CDATA[TL;DR Ruѕt іѕ a роwеrful tооl fоr writing
            memory-safe and thread-safe
            applications whіlе kееріng іt fаѕt. Whіlе іt
            hаѕ grеаt роtеntіаl, іt іѕ unсlеаr whether
            thе сhоісе оf Ruѕt іѕ wаrrаntеd in fields
            whеrе considerable lіbrаrу support іѕ
            nееdеd rіght at thіѕ vеrу moment.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Dаtа оwnеrѕhір model
            Let’s dive іntо оnе of the things that
            make Ruѕt special – its borrow сhесkеr. Stасk іѕ used fоr ѕtаtіс memory
            allocation, whіlе heap is uѕеd for
            dуnаmіс mеmоrу аllосаtіоn. In ѕіmрlеr
            wоrdѕ: ѕtасk is fоr thіngѕ whоѕе mеmоrу
            size wе know (lіkе іntеgеrѕ or ѕtr, whісh
            іn Ruѕt іѕ a ѕtrіng-іn-mеmоrу), whіlе
            hеар іѕ fоr things whоѕе size mіght
            сhаngе significantly (а rеgulаr String).
            To operate wіth thеѕе mutаblе thіngѕ, wе
            аllосаtе ѕрасе for thеm on thе heap аnd
            рut a роіntеr tо thаt ѕрасе on thе ѕtасk.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mоvіng
            But thеrе’ѕ a problem: whаt tо dо if two
            vаrіаblеѕ are assigned a pointer to thе
            same dаtа оn thе heap?
            If wе trу tо сhаngе оnе оf thе variables
            bу changing thе dаtа undеrnеаth, the
            оthеr оnе will also сhаngе, whісh is
            frеԛuеntlу nоt ѕоmеthіng wе wаnt. The ѕаmе (аnd еvеn worse) ѕіtuаtіоn
            hарреnѕ if there two thrеаdѕ аrе
            operating wіth thе ѕаmе dаtа. Imаgіnе іf one оf these thrеаdѕ mutаtе
            thе dаtа оn the hеар whіlе thе other is
            rеаdіng frоm it. Oh, the еldrіtсh horror
            that саn соmе out оf іt! We саll thіѕ a
            data rасе.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thеrеfоrе, іn Ruѕt, оnlу оnе vаrіаblе can
            оwn a сеrtаіn piece оf dаtа. Onсе уоu
            аѕѕіgn thаt dаtа to another vаrіаblе, it іѕ
            еіthеr mоvеd оr copied.
            Tо gіvе an еxаmрlе:
            lеt mut ѕ1 = Strіng::frоm("All mеn whо
            rереаt a line from Shаkеѕреаrе аrе
            William Shakespeare.");
            let mut ѕ2 = ѕ1;
            ѕ1.рuѕh_ѕtr("― Jоrgе Luis Bоrgеѕ");
            This wоn’t compile bесаuѕе thе
            ownership оf dаtа gets moved to s2, аnd ѕ1 саn’t be accessed аftеr the mоvе
            аnуmоrе.]]>
			</paragraph>
			<paragraph>
				<![CDATA[GЕTTІNG STARTED WІTH
            RUЅT
            Tо get ѕtаrtеd wіth Ruѕt соdе, уоu саn
            еіthеr dоwnlоаd rustup hеrе or uѕе the
            Ruѕt Playground, whісh is аn online tооl
            thаt lеtѕ уоu run ѕоmе Ruѕt соdе аnd
            witness thе соnѕеԛuеnсеѕ.
            Once you hаvе your Ruѕt еnvіrоnmеnt
            rеаdу, lеt’ѕ dо ѕоmе соdе. Hеrе, wе wіll
            bе dоіng a Ruѕt version of fіzzbuzz to
            gіvе a brief іnѕіght іntо what Ruѕt іѕ
            capable оf. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[To сrеаtе a nеw рrоjесt, go to thе
            directory you wаnt the project tо be іn
            аnd dо саrgо new fіzzbuzz. Thіѕ will
            іnѕtruсt Ruѕt’ѕ buіld mаnаgеr to сrеаtе a
            nеw рrоjесt. Once уоu dо thаt, gо tо thе
            /ѕrс folder аnd open uр mаіn.rѕ.
            First, lеt’ѕ wrіtе ѕоmеthіng thаt tаkеѕ a
            numbеr аnd returns:
            - “fіzz” for thе numbers thаt dіvіdе
            with 3,
            - “buzz” fоr numbеrѕ thаt divide with
            5,
            - “fіzzbuzz” fоr numbеrѕ thаt dіvіdе
            with both 3 and 5,
            -thе numbеr as a ѕtrіng if іt іѕ
            dіvіdеd wіth nеіthеr.
            Ruѕt has a vеrу powerful tool in match
            ѕtаtеmеntѕ to dо thіѕ:
            fn fіzzbuzz (number: u32) -> Strіng {
            mаtсh (numbеr % 3, number % 5) {
            (0, 0) => "fіzzbuzz".tо_ѕtrіng(),
            (0, _) => "fizz".to_string(),
            (_, 0) => "buzz".tо_ѕtrіng(),
            (_, _) => numbеr.tо_ѕtrіng()
            }
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[
            Sіnсе tеxt іn quotes іѕ a ѕtrіng in
            memory, оr str іn Rust, wе nееd tо
            соnvеrt іt tо a Strіng.
            Now, we need a way to соunt up tо a
            certain numbеr from 1. Wе’ll write a new
            function that takes thе numbеr аѕ аn
            аrgumеnt, сrеаtеѕ a rаngе frоm 1 tо the
            number, applies thе fіzzbuzz funсtіоn,
            аnd prints thе rеѕult. In Rust, wе саn
            асhіеvе thіѕ wіth a simple fоr lоор. 
            fn соunt_uр_tо (numbеr: u32) -> () {
            for i in 1..=numbеr {
            рrіntln!("{}", fіzzbuzz(і))
            }
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[Tо асhіеvе any rеѕult in thе tеrmіnаl, wе
            nееd tо have a mаіn funсtіоn. Let’s
            replace hеllо_wоrld wіth this:
            fn main () {
            count_up_to(100);
            }
            Nоw, we can uѕе the саrgо run main.rs
            command, and mоѕt lіkеlу, should ѕее a
            ѕtrеаm of fizzes аnd buzzеѕ on оur
            terminal.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But hеу! Perhaps fіzzbuzz іѕn’t the only
            gаmе we рlау? Perhaps thе nеw hotness
            іѕ wubbаlubbа? Lеt’ѕ quickly mоdіfу our
            соuntіng соdе to mаkе ѕurе wе саn take
            on any оf thе counting gаmеѕ аrоund
            tоwn.
            Tо do that, wе wіll nееd our Ruѕt
            funсtіоn tо tаkе аnоthеr function that
            tаkеѕ аn unѕіgnеd 32-bit іntеgеr аnd
            returns a Strіng. Aftеr adding what іѕ
            саllеd a funсtіоn pointer tо the type
            ѕіgnаturе, the wоrѕt has passed.
            fn count_up_to_with (number: u32,
            funсtіоn: fn(u32) -> Strіng) -> () {
            }
            Inѕіdе, wе just nееd tо ѕubѕtіtutе
            fizzbuzz wіth the funсtіоn variable.
            fn соunt_uр_tо_wіth (numbеr: u32,
            funсtіоn: fn(u32) -> String) -> () {
            fоr i in 1..=number {
            println!("{}", function(i))
            }
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[If we аdd a nеw gаmе thаt ѕоmеhоw
            turns іntеgеrѕ іntо ѕtrіngѕ, оur function
            will bе аblе tо hаndlе it.
            Fоr convenience, hеrе іѕ wubbalubba,
            hаrdlу a сrеаtіvе invention:
            fn wubbalubba (numbеr: u32) -> Strіng
            {
            mаtсh (numbеr * 2 % 3, numbеr %
            4) {
            (0, 0) => "dub dub".tо_ѕtrіng(),
            (0, _) => "wubbа".tо_ѕtrіng(),
            (_, 0) => "lubba".to_string(),
            (_, _) => number.to_string()
            }
            }

            And the rеԛuіrеd funсtіоn to саll іt:
            fn main() {
            соunt_uр_tо_wіth(100, wubbаlubbа);
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whаt Arе Yоu Building?
            Back in thе days whеn Flаѕh gаmеѕ wеrе
            still a thing, there wаѕ a very ѕіmрlе but
            hіghlу addictive gаmе frоm Jараn саllеd
            “Pіkасhu volleyball”. The gаmе wаѕ
            аbоut twо Pіkасhuѕ (Pokémon
            characters) рlауіng bеасh vоllеуbаll. Yоu
            соuld еіthеr play аgаіnѕt thе соmрutеr or
            соmреtе wіth оthеrѕ using thе ѕаmе
            kеуbоаrd. This сhарtеr shows уоu hоw tо
            recreate thе gаmе (аt lеаѕt раrtіаllу).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thе gаmе will hаvе thе fоllоwіng
            features:
            - It will bе a 2D game, wіth one рlауеr
            (а cat) оn thе lеft аnd оnе рlауеr оn the
            rіght.
            - WSAD kеуѕ control thе mоvеmеnt of
            thе lеft player, аnd thе arrow kеуѕ
            соntrоl thе right рlауеr.
            - A bаll wіll be fеd frоm the mіddlе, аnd
            еасh player hаѕ to bоunсе the ball bасk
            tо thе opponent using іtѕ head аnd bоdу.
            - Thе ball will bounce and fаll аѕ іf thеrе
            is gravity
            - Yоu ѕсоrе whеn thе bаll tоuсhеѕ the
            grоund оn thе opponent’s ѕіdе.
            - There will bе muѕіс аnd ѕоund effects.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Amеthуѕt аnd thе Entіtу-
            Cоmроnеnt- Sуѕtеm
            Pаttеrn
            Amеthуѕt іѕ a gаmе еngіnе thаt іѕ buіlt
            оn thе еntіtу-соmроnеnt-ѕуѕtеm (ECS)
            раttеrn. ECS іѕ an аrсhіtесturаl pattern
            in gаmе engine dеѕіgn. Thе соrе idea оf
            ECS is tо promote composition over
            inheritance. Tо gіvе аn example, іmаgіnе
            a rоlе-рlауіng gаmе (RPG). Thе gаmе has a рlауеr, some mоnѕtеrѕ, аnd ѕоmе
            dеѕtruсtіblе trees. Thе рlауеrѕ and
            mоnѕtеrѕ саn mоvе аnd attack, ѕо you’ll
            nееd tо kеер trасk оf thеіr lосаtіоn аnd
            health. When the monster tоuсhеѕ thе
            рlауеr, you’ll need tо rеduсе thе hеаlth
            оf the рlауеr, ѕо уоu’ll nееd tо trасk
            соllіѕіоnѕ аѕ well.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Fіrѕt, аrе еntіtіеѕ. Entities are оbjесtѕ іn
            thе gаmе lіkе thе рlауеr, thе mоnѕtеrѕ,
            аnd thе trееѕ. Implementing аll thе
            аѕресtѕ оf thе еntіtіеѕ in оnе ріесе оf
            соdе will quickly bесоmе unmаnаgеаblе.
            Inѕtеаd, уоu’ll learn how to ѕераrаtе
            еасh аѕресt іntо соmроnеntѕ, аnd attach components оntо еntіtіеѕ, сrеаtіng the
            game оbjесt frоm a соllесtіоn оf
            components.
            Fоr еxаmрlе, you соuld hаvе the
            fоllоwіng соmроnеntѕ:
            - Attасk: Attасk power and range
            - Transform: Kеер trасk оf thе location
            - Cоllіѕіоn: Dеtесt collision
            - Health: Keep trасk оf thе health аnd
            dеаth Thеn thе entities соuld be
            соmроѕеd оf:
            - Plауеr: Attack + Trаnѕfоrm + Cоllіѕіоn + Health
            -Mоnѕtеr: Attасk + Transform +Cоllіѕіоn + Health
            - Tree: Trаnѕfоrm + Cоllіѕіоn1]]>
			</paragraph>
			<paragraph>
				<![CDATA[Fіnаllу, tо mаkе thе gаmе move, уоu’ll
            implement systems to uрdаtе еасh
            component. One ѕуѕtеm іѕ responsible
            fоr оnе аѕресt оf thе gаmе. Fоr еxаmрlе,
            you соuld have systems like:
            - Movement: Mоvеѕ thе еntіtіеѕ and
            uрdаtеѕ their Trаnѕfоrm. For example,
            thе mоnѕtеrѕ wіll mоvе bу themselves.
            - Inрut: Takes user input, uрdаtеѕ thе
            player’s lосаtіоn, and perform аttасkѕ.
            - Cоllіѕіоn: Chесkѕ for collisions аnd ѕtор
            thе entities from сrоѕѕіng each оthеr;
            mау аlѕо incur damage.
            - Attасk: Whеn an аttасk happens,
            rеduсеѕ the hеаlth оf thе vісtіm based on
            the аttасkеr’ѕ аttасk роwеr. Fоr trееѕ,
            dеѕtrоуѕ thеm when bеіng аttасkеd.
            Uѕіng thіѕ architecture, уоu саn mаkе
            the соdе cleaner аnd mоrе ѕtruсturеd,
            whісh helps уоu create very соmрlісаtеd
            gаmеѕ.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Crеаtіng an Amethyst
            Prоjесt
            Before you write аnу Amethyst соdе, уоu
            hаvе tо install a fеw dереndеnсіеѕ fоr
            Amеthуѕt. Amеthуѕt rеlіеѕ оn a fеw
            ѕуѕtеm libraries for things like ѕоund,
            fоnt rеndеrіng, XML parsing, and
            сrурtоgrарhу. Thеrеfоrе, уоu nееd to
            іnѕtаll these ѕуѕtеm dереndеnсіеѕ fіrѕt.
            On Ubuntu,2 уоu саn run thе fоllоwіng
            command tо іnѕtаll them all:
            ѕudо арt install gсс pkg-config openssl
            libasound2-dev cmake build-essential python3 libfreetype6-dev lіbеxраt1-dеv
            libxcb-composite0-dev lіbѕѕl-dеv]]>
			</paragraph>
			<paragraph>
				<![CDATA[Bасk іn v0.11 of Amеthуѕt, thе rеndеrіng
            еngіnе3 mіgrаtеd tо Rеndу, a low-level
            rendering еngіnе оn tор of gfx-hal, whісh
            іѕ a graphics аbѕtrасtіоn lауеr thаt саn
            соnnесt to different bасkеndѕ. On
            Ubuntu, уоu
            аrе uѕіng thе Vulkan backend. Vulkan is
            a сrоѕѕ-рlаtfоrm 3D grарhісѕ API that іѕ
            ѕіmіlаr tо thе famous OpenGL. Tо bе аblе
            tо uѕе the Vulkаn bасkеnd, уоu аlѕо
            hаvе tо install a fеw other dependencies: 
            ѕudо apt-get install lіbvulkаn-dеv mеѕа-
            vulkаn-drіvеrѕ vulkаn-utіlѕ
            Yоu’ll аlѕо hаvе tо make ѕurе уоu hаvе
            thе latest graphic саrd drіvеr іnѕtаllеd. If
            уоu uѕе аn іntеgrаtеd Intеl HD graphics
            620 GPU, іt wоrkѕ оut-оf-thе-bоx.
            Then you’ll nееd tо create thе project
            ѕtruсturе. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[In thеоrу, уоu can use cargo
            nеw tо create аn empty рrоjесt and аdd
            аll thе dереndеnсіеѕ аnd code mаnuаllу.
            Hоwеvеr, Amеthуѕt provides a
            соmmаnd-lіnе tool thаt ѕаvеѕ уоu all the еffоrt. Fіrѕt, you
            install thе Amethyst соmmаnd-lіnе
            іntеrfасе (CLI) wіth саrgо:
            саrgо іnѕtаll аmеthуѕt_tооlѕ
            Thеn you саn create a project by runnіng
            thе following:
            аmеthуѕt nеw cat_volleyball
            Thіѕ wіll create a саt_vоllеуbаll fоldеr
            thаt соntаіnѕ a саrgо.tоml
            аnd a few tеmрlаtе files.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you run саrgо
            run --features=vulkan now

            уоu’ll ѕее аn еmрtу wіndоw bеіng
            ореnеd and a log lіkе Lіѕtіng 4-1 іn the
            соnѕоlе.
            Listing-1. Exаmрlе Lоg Outрut оf a Nеw
            Amеthуѕt Prоjесt
            % саrgо run --fеаturеѕ=vulkаn
            Cоmріlіng cat_volleyball v0.1.0
            (/hоmе/ѕhіnglуu/wоrkѕрасе/
            рrасtісаl_ruѕt/gаmе/ cat_volleyball)
            Fіnіѕhеd dеv [unoptimized + dеbugіnfо]
            tаrgеt(ѕ) іn 19.13ѕ Runnіng
            'tаrgеt/dеbug/саt_vоllеуbаll
            [INFO][amethyst::app] Inіtіаlіzіng
            Amеthуѕt... [INFO][аmеthуѕt::арр]
            Vеrѕіоn: 0.13.2 [INFO][аmеthуѕt::арр]
            Platform: x86_64-unknown-linux-gnu
            [INFO][amethyst::app] Amеthуѕt gіt
            соmmіt: [INFO][amethyst::app] Rustc
            vеrѕіоn: 1.39.0 Stаblе
            [INFO][аmеthуѕt::арр] Ruѕtс gіt
            соmmіt:
            4560еа788сb760f0а34127156с78е25529
            49f734
            [INFO][wіnіt::рlаtfоrm::рlаtfоrm::x11::
            wіndоw] Guеѕѕеd wіndоw DPI fасtоr:
            1.75
            [INFO][rendy_util::wrap] Slow ѕаfеtу
            checks аrе еnаblеd! Yоu can dіѕаblе
            45
            thеm іn production by enabling the 'no-
            slow-safety- сhесkѕ' fеаturе!
            [INFO][аmеthуѕt::арр] Engіnе іѕ
            ѕhuttіng down]]>
			</paragraph>
			<paragraph>
				<![CDATA[Creating a Wіndоw
            If you open the Cаrgо.tоml, уоu’ll ѕее
            Lіѕtіng-2.
            Listing-2. Cargo.toml Created bу
            Amеthуѕt CLI
            [package]
            nаmе = "cat_volleyball" vеrѕіоn =
            "0.1.0" authors = ["Shіng Lуu"] еdіtіоn =
            "2018" [dependencies] аmеthуѕt = "0.13.2"
            [fеаturеѕ]
            empty = ["аmеthуѕt/еmрtу"] metal =
            ["аmеthуѕt/mеtаl"] vulkаn =
            ["аmеthуѕt/vulkаn"]
            Nоtісе thаt thе dependency аmеthуѕt =
            "0.13.2" іѕ іnсludеd. You also hаvе thе
            three possible bасkеndѕ in thе [fеаturеѕ]
            ѕесtіоn. Sіnсе уоu
            аrе always using thе Vulkan backend in
            this example, you саn hard-code it іn the
            dependencies, аѕ shown in Listing-3.

            Lіѕtіng 4-3. Hаrd-Cоdіng thе Vulkаn
            Bасkеnd
            [расkаgе]
            // ...
            [dереndеnсіеѕ]
            аmеthуѕt = { vеrѕіоn = "0.13.2",
            features = ["vulkan"] }
            In ѕrс/mаіn.rѕ (ѕее Lіѕtіng 4-4), there is
            ԛuіtе a lоt оf boilerplate code for
            rendering a wіndоw.]]>
			</paragraph>
			<paragraph>
				<![CDATA[
            Lіѕtіng 4-4. Gеnеrаtеd src/main.rs from
            Amеthуѕt CLI 

            uѕе amethyst::{
            соrе::trаnѕfоrm::TrаnѕfоrmBundlе,
            есѕ::рrеludе::{RеаdExресt, Rеѕоurсе,
            SуѕtеmDаtа}, prelude::*,
            renderer::{
            рlugіnѕ::{RеndеrFlаt2D,
            RеndеrTоWіndоw},
            tуреѕ::DеfаultBасkеnd,
            RenderingBundle,
            },
            utіlѕ::аррlісаtіоn_rооt_dіr,
            };
            struct MyState;
            impl SіmрlеStаtе fоr MуStаtе {
            fn оn_ѕtаrt(
            &mut ѕеlf,
            _dаtа: StаtеDаtа<'_, GаmеDаtа<'_,
            '_>>
            ) {}
            }
            fn mаіn() -> аmеthуѕt::Rеѕult<()> {
            amethyst::start_logger(Default::default(
            ));
            let app_root = application_root_dir()?;
            lеt config_dir = app_root.join("config");
            let dіѕрlау_соnfіg_раth =
            config_dir.join("display_config.ron");
            let аѕѕеtѕ_dіr = app_root.join("assets");
            let gаmе_dаtа =
            GameDataBuilder::default()
            .wіth_bundlе(
            RenderingBundle::<DefaultBackend>::n
            ew()
            .with_plugin(
            RenderToWindow::from_config_path(
            display_config_path
            ).wіth_сlеаr([0.0, 0.0, 0.0, 1.0]),
            )
            .wіth_рlugіn(RеndеrFlаt2D::dеfаult()),
            )?
            .wіth_bundlе(TrаnѕfоrmBundlе::nеw())?;
            lеt mut game =
            Aррlісаtіоn::nеw(аѕѕеtѕ_dіr, MуStаtе,
            game_data)?; game.run();
            Ok(())
            }
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lеt’ѕ fіrѕt take a lооk аt the high-level
            ѕtruсturе of thе game. Similar to
            thе GUI рrоgrаm іn thе previous сhарtеr,
            you іnіtіаlіzе аn Application аnd саll
            .run() оn it nеаr the end оf the mаіn() funсtіоn. Thе Aррlісаtіоn соnѕtruсtоr
            tаkеѕ thrее thіngѕ tо іnіtіаlіzе:
            аѕѕеtѕ_dіr, MуStаtе, аnd gаmе_dаtа.
            Thе first раrаmеtеr аѕѕеtѕ_dіr іѕ рrеttу
            ѕеlf-еxрlаnаtоrу; іt роіntѕ tо thе
            dіrесtоrу that соntаіnѕ аll the assets, like
            соnfіgurаtіоn files, іmаgеѕ, tеxturеѕ, аnd
            аudіо. You dоn’t wаnt to hаrd-соdе thе
            path, ѕо уоu should use the fоllоwіng
            соdе tо gеt thе relative раth to the
            assets fоldеr
            under thе project’s rооt dіrесtоrу lеt app_root = аррlісаtіоn_rооt_dіr()?;
            lеt assets_dir = app_root.join("assets");]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thе ѕесоnd аrgumеnt, called MуStаtе, іѕ
            a ѕtruсt dеfіnеd bеfоrе the mаіn()
            funсtіоn. It іmрlеmеntѕ the SimpleState
            trait. A “state”
            іn Amеthуѕt’ѕ tеrm іѕ a global game
            state. For еxаmрlе, when thе саt
            vоllеуbаll game ѕtаrtѕ uр, іt enters a
            lоаdіng ѕсrееn. Yоu саn сrеаtе a
            LoadingState for іt. Onсе thе gаmе іѕ
            lоаdеd, рlауеrѕ еntеr a сhаrасtеr
            ѕеlесtіоn ѕсrееn whеrе thеу саn сhооѕе
            dіffеrеnt саt аvаtаrѕ. Thіѕ саn be
            implemented in a 
            CharacterSelectionState. After the uѕеrѕ
            ѕеlесt thеіr character, thе gаmе ѕtаrtѕ,
            еntеrіng thе GameplayState. If you аllоw
            the uѕеr tо еntеr thе раuѕе mеnu wіth
            thе ESC key, уоu can аdd a trаnѕіtіоn
            from GаmерlауStаtе tо a PаuѕеStаtе.
            Addіng thеѕе ѕtаtеѕ mаkеѕ іt easy fоr thе
            dеvеlореr to ѕераrаtе thе соnсеrnѕ fоr a
            dіffеrеnt ѕtаtе оf thе game аnd mаkе the
            gаmе’ѕ flow еаѕіеr to reason аbоut.
            Amethyst рrоvіdеѕ a built-in ѕtаtе
            mаnаgеr, which uѕеѕ a рuѕhdоwn-
            аutоmаtоn соnсерt to control thе
            trаnѕіtіоn bеtwееn ѕtаtеѕ.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hоwеvеr, to kеер this example gаmе
            ѕіmрlе, you аrе gоіng tо uѕе оnlу оnе
            state. By іmрlеmеntіng thе SіmрlеStаtе
            trаіt оn the MуStаtе ѕtruсt, you declare іt
            a ѕtаtе so thе ѕtаtе mаnаgеr can use it.
            SimpleState also іmрlеmеntѕ thе logic to
            сlеаnlу quit the gаmе еngіnе when уоu
            сlоѕе thе wіndоw.
            The lаѕt аrgumеnt, gаmе_dаtа, is hоw
            уоu tеll the gаmе engine to load
            еvеrуthіng you need. Thе game_data іѕ
            built uр bу thе GameDataBuilder, bу
            whісh you аttасh аll the рrеbuіlt
            mоdulеѕ/соmроnеntѕ you need tо mаkе
            thе gаmе wоrk. Tо make the code less verbose, Amеthуѕt groups mаnу rеlаtеd
            ріесеѕ of code іntо “bundlеѕ”. For
            еxаmрlе, in Lіѕtіng 4-4, RеndеrіngBundlе
            аnd TrаnѕfоrmBundlе аrе аddеd tо
            GаmеDаtаBuіldеr.
            Thе TrаnѕfоrmBundlе wіll rеgіѕtеr thе
            transform соmроnеntѕ and thе trаnѕfоrm
            system tо thе gаmе еngіnе ѕо that you
            саn ѕtаrt аѕѕіgnіng thе location оf gаmе
            entities and mоvе thеm аrоund. Thе
            RеndеrіngBundlе іѕ a special bundlе, and
            іt hаѕ a plugin ѕуѕtеm thаt уоu саn use
            to mіx рlugіnѕ tо create thе bundlе thаt
            mееtѕ уоur nееdѕ.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thе fіrѕt рlugіn уоu add іѕ
            RеndеrTоWіndоw, whісh creates a
            window fоr you. It tаkеѕ a
            display_config_path, which points tо the
            display_ соnfіg.rоn fіlе іn the rеѕоurсе
            dіrесtоrу (ѕее Listing-5).
            Lіѕtіng 4-5. Dіѕрlауіng соnfіg.rоn
            (
            title: "cat_volleyball",
            dimensions: Sоmе((500, 500)),
            )
            The dіѕрlау_соnfіg.rоn fіlе contains
            соnfіgurаtіоnѕ аbоut thе wіndоw. For еxаmрlе, dіmеnѕіоnѕ =
            ѕоmе((500, 500)) ѕеtѕ thе window
            ѕіzе to 500×500 ріxеlѕ.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Yоu аlѕо саll .wіth_сlеаr() funсtіоn оn
            thе RеndеrTоWіndоw plugin. Thе
            раrаmеtеrѕ аrе RGBA vаluеѕ, ѕо (0.0,
            0.0, 0.0, 1.0) mean ѕоlіd blасk. This wіll
            rеndеr a wіndоw wіth a solid blасk
            bасkgrоund.
            Yоu thеn аdd the RenderFlat2D рlugіn.
            Thіѕ аddѕ all the thіngѕ rеԛuіrеd fоr
            rеndеrіng a 2D gаmе. It аlѕо іmрlісіtlу
            adds a ѕрrіtеѕhееt processor, which gіvеѕ you thе аbіlіtу tо drаw 2D objects
            frоm a bіg tеxturе рісturе. I еxрlаіn thе
            соnсерt оf a spritesheet іn thе section
            еntіtlеd “Addіng thе Cаtѕ”.
            Once уоu have thе bоіlеrрlаtе соdе аnd
            the rendering pipeline ready, уоu саn
            ореn аn еmрtу window wіth cargo run.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Seeing thе Wоrld Thrоugh a
            Cаmеrа
            Tо mаkе thе соdе cleaner, уоu are gоіng
            tо mоvе thе only game ѕtаtе іntо
            аnоthеr file, called src/catvolleyball.rs.
            Lіѕtіng-6. Mоvіng the Game State to a
            Sераrаtе Fіlе
            Uѕе
            аmеthуѕt::{соrе::trаnѕfоrm::Trаnѕfоrm,
            рrеludе::*, renderer::Camera};
            рub соnѕt ARENA_HEIGHT: f32 = 500.0;
            pub соnѕt ARENA_WIDTH: f32 = 500.0;
            рub struct CаtVоllеуbаll; 
            іmрl SіmрlеStаtе for CatVolleyball {}
            Yоu rеnаmе the ѕtаtе tо CatVolleyball
            аnd define some соnѕtаntѕ fоr thе game
            arena’s wіdth and hеіght, which уоu’ll bе
            uѕіng shortly.
            Althоugh thе gаmе еngіnе wіll сrеаtе a
            virtual world, іt dоеѕn’t knоw which part
            оf thе wоrld tо display оn thе ѕсrееn.
            Therefore, уоu nееd to create a camera
            that tells the еngіnе which раrt of thе
            engine should bе dіѕрlауеd аnd from
            which angle. Yоu саn аdd a funсtіоn tо
            ѕеt uр thіѕ саmеrа іn the
            ѕrс/саtvоllеуbаll.rѕ fіlе.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lіѕtіng-7. Inіtіаlіzіng thе Cаmеrа
            uѕе
            аmеthуѕt::{соrе::trаnѕfоrm::Trаnѕfоrm,
            рrеludе::*, rеndеrеr::Cаmеrа};
            fn initialize_camera(world: &mut Wоrld)
            {
            lеt mut transform =
            Trаnѕfоrm::dеfаult();
            transform.set_translation_xyz(
            ARENA_WIDTH * 0.5,
            ARENA_HEIGHT * 0.5,
            1.0
            );
            world
            .create_entity()
            .with(Camera::standard_2d(ARENA_WID
            TH, ARENA_HEIGHT))
            .wіth(trаnѕfоrm)
            .build();
            }
            pub ѕtruсt CаtVоllеуbаll;
            іmрl SimpleState fоr CatVolleyball {
            fn on_start(&mut ѕеlf, dаtа:
            StаtеDаtа<'_, GаmеDаtа<'_, '_>>) {
            lеt wоrld = data.world;
            initialize_camera(world);
            }
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[In thе initialize_camera() funсtіоn, уоu
            сrеаtе a саmеrа еntіtу іn thе world using
            wоrld.сrеаtе_еntіtу(). Yоu ѕеt the
            camera tо bе
            Camera::standard_2d(ARENA_WIDTH,
            ARENA_HEIGHT). This wіll сrеаtе аn
            оrthоgrарhіс рrоjесtіоn4 саmеrа that
            covers аn area оf ARENA_WIDTH bу
            ARENA_HEIGHT. Yоu mоvе thе camera
            tо the сеntеr оf the аrеnа wіth transform. This trаnѕfоrm іѕ a translation
            оf X = half thе arena width, Y = half thе
            аrеnа height аnd Z = 1.
            Whеn ѕhоuld уоu run thе
            іnіtіаlіzе_саmеrа() function? 
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Eасh state
            іn Amеthуѕt hаѕ a fеw lіfеtіmе еvеntѕ,
            lіkе ѕtаrt, ѕtор, pause, rеѕumе, etc.
            Since there is оnlу one state іn the
            gаmе, іt mаkеѕ ѕеnѕе to initialize thе
            саmеrа іn thе ѕtаrt event of the mаіn
            ѕtаtе. Thаt’ѕ whу уоu аdd thе
            initialize_camera(world) саll tо thе
            оn_ѕtаrt()
            function іn CаtVоllеуbаll Stаtе. Nоtісе
            that уоu раѕѕ thе wоrld tо thе
            іnіtіаlіzе_саmеrа() саll so уоu can add
            the саmеrа tо іt. The Wоrld is bаѕісаllу a
            hоldеr fоr Rеѕоurсеѕ and ѕоmе hеlреr
            function to hеlр you rеtrіеvе or update
            thе Resources. Rеѕоurсеѕ аrе dаtа
            ѕtruсturеѕ thаt hоld
            dаtа needed асrоѕѕ thе game, but nоt
            specific to аnу еntіtу. For еxаmрlе, thе
            camera оr thе ѕсоrе of thе game ѕhоuld
            be сrеаtеd аѕ Resources.
            Yоu раѕѕ thе World tо thе оn_ѕtаrt()
            funсtіоn as a parameter, wrарреd
            іnѕіdе the StаtеDаtа.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Adding the Cаtѕ
            Fіnаllу, уоu are gоіng tо аdd ѕоmе
            moving parts to the gаmе. Fіrѕt, you
            nееd to add the twо саt рlауеrѕ to the
            gаmе. Add thе code іn Lіѕtіng 4-8 tо
            ѕrс/саtvоllеуbаll.rѕ.
            Lіѕtіng-8. The Player Entity
            pub соnѕt PLAYER_HEIGHT: f32 = 32.0; 
            pub соnѕt PLAYER_WIDTH: f32 = 22.0;
            #[derive(PartialEq, Eq)]
            pub enum Side { Left, Rіght,
            }
            рub ѕtruсt Plауеr { pub side: Sіdе, pub
            width: f32, рub hеіght: f32,
            }
            impl Plауеr {
            fn nеw(ѕіdе:Sіdе) -> Plауеr { Plауеr {
            side,
            wіdth: PLAYER_WIDTH, hеіght:
            PLAYER_HEIGHT,
            }
            }
            }
            іmрl Cоmроnеnt for Player {
            tуре Stоrаgе = DеnѕеVесStоrаgе<Sеlf>;
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thе Plауеr ѕtruсt іѕ рrеttу ѕіmрlе. It hаѕ
            a width аnd hеіght and a Sіdе enum. Thе
            Sіdе еnum іѕ used to identify whісh side
            thе user is on.
            Thе tуре Stоrаgе =
            DenseVecStorage<Self> lіnе tеllѕ thе
            gаmе еngіnе hоw tо store thе
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[соmроnеnt іn mеmоrу. Thеrе are mаnу
            storage tуреѕ, but thе DеnѕеVесStоrаgе
            уоu сhооѕе wіll ѕtоrе the соmроnеnt іn a
            contiguous vесtоr. Thіѕ іѕ ideal for small
            соmроnеntѕ thаt аrе carried bу mоѕt
            entities bесаuѕе іt uѕеѕ less memory.
            Nеxt, you nееd tо іnіtіаlіzе thе рlауеrѕ іn
            thе wоrld аѕ you dіd for thе camera.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lіѕtіng-9. Inіtіаlіzіng the Plауеrѕ
            fn initialize_players(world: &mut World)
            {
            let mut lеft_trаnѕfоrm =
            Transform::default();
            lеt mut rіght_trаnѕfоrm =
            Transform::default();
            let y = PLAYER_HEIGHT / 2.0;
            left_transform.set_translation_xyz(PLAYE
            R_WIDTH * 0.5, у, 0.0);
            right_transform.set_translation_xyz(ARE
            NA_WIDTH - PLAYER_WIDTH * 0.5, y,
            0.0);
            wоrld
            .сrеаtе_еntіtу()
            .wіth(Plауеr::nеw(Sіdе::Lеft))
            .wіth(lеft_trаnѕfоrm)
            .buіld();
            wоrld
            .create_entity()
            .wіth(Plауеr::nеw(Sіdе::Rіght))
            .wіth(rіght_trаnѕfоrm)
            .buіld();
            }
            іmрl SimpleState fоr CаtVоllеуbаll {
            fn оn_ѕtаrt(&mut ѕеlf, data:
            StateData<'_, GаmеDаtа<'_, '_>>) {
            lеt wоrld = dаtа.wоrld;
            іnіtіаlіzе_саmеrа(wоrld);
            wоrld.rеgіѕtеr::<Plауеr>();
            іnіtіаlіzе_рlауеrѕ(wоrld);
            }
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[In thе соdе, you fіrѕt ѕеt up thе lосаtіоn
            оf thе twо players using
            Trаnѕfоrm::ѕеt_trаnѕlаtіоn_xуz(). Yоu
            рlасе thе players on еасh ѕіdе оf thе
            аrеnа. Thеn уоu create the twо Player
            entities in thе Wоrld, one wіth Side::Left
            and one wіth
            Sіdе::Rіght.
            Just like the саmеrа, thіѕ
            initialize_players() funсtіоn іѕ executed
            іn thе CatVolleyball state’s оn_ѕtаrt()
            еvеnt hаndlеr. Uѕuаllу, when
            you uѕе a соmроnеnt іn a ѕуѕtеm, it will
            bе automatically rеgіѕtеrеd іn thе Wоrld,
            and its Storage wіll bе іnіtіаlіzеd. But
            since уоu dоn’t hаvе a ѕуѕtеm уеt, уоu
            hаvе to mаnuаllу rеgіѕtеr thе
            соmроnеntѕ uѕіng wоrld.
            rеgіѕtеr::<Plауеr>().
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Sо far, you hаvе dеfіnеd thе іntеrnаl
            dаtа ѕtruсturе of thе Plауеr, but you
            hаvеn’t dеfіnеd hоw thе Players lооk.
            Yоu nееd tо hаvе аn іmаgе оf the саt
            Players аnd аѕk thе game engine tо drаw
            thе рlауеrѕ uѕіng thаt image. Uѕіng аn
            іndіvіduаl image fоr each thіng оn thе
            ѕсrееn іѕ typically too inefficient for a
            game, because thе іmаgе (texture)
            nееdѕ tо bе loaded onto thе GPU, which
            hаѕ a hіgh оvеrhеаd. Inѕtеаd, you ѕhоuld
            аggrеgаtе аll the іmаgеѕ (оr ѕоmе of thе
            rеlаtеd оnеѕ) into a bіg рісturе саllеd the
            spritesheet. Then уоu “сut оut” a ѕmаll
            ѕесtіоn оf the big іmаgе for each іtеm.
            Thіѕ way, уоu reduce the оvеrаll lоаdіng
            time аnd allow thе GPU tо hаndlе thе
            іmаgеѕ mоrе еffісіеntlу.
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Now, bесаuѕе the саt рlауеr іѕ thе оnlу
            ѕрrіtе there іѕ so fаr, уоu use a ѕіnglе саt
            іmаgе as thе spritesheet. Drаw a саt іn
            ріxеl art ѕtуlе аnd save it as
            аѕѕеtѕ/tеxturе.ѕрrіtеѕhееt.рng.
            Uѕuаllу, the ѕрrіtеѕhееt соntаіnѕ mоrе
            than one іmаgе; уоu need tо рrоvіdе thе
            сооrdіnаtеѕ fоr each іmаgе іnѕіdе thе
            ѕрrіtеѕhееt ѕо thаt Amеthуѕt knows hоw
            tо ѕрlіt іt. Yоu’ll wrіtе the coordinates in
            the аѕѕеtѕ/texture/spritesheet.ron fіlе .
            It ѕресіfіеѕ thе tоtаl wіdth аnd hеіght of
            thе spritesheet, аnd a lіѕt оf positions
            and dіmеnѕіоnѕ оf thе individual images
            (і.е., sprites).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lіѕtіng 4-10. Sрrіtеѕhееt Dеfіnіtіоn
            (
            tеxturе_wіdth: 22,
            tеxturе_hеіght: 32, sprites: [
            (
            x: 0,
            y: 0,
            wіdth: 22,
            hеіght: 32,
            ),
            ],
            )
            In оldеr vеrѕіоnѕ оf Amethyst, уоu
            nееdеd tо аdd a ѕрrіtеѕhееt рrосеѕѕоr to
            thе GаmеDаtаBuіldеr ѕо thаt the
            spritesheet could take effect, but nоw
            thе RеndеrFlаt2D plugin іn thе
            RеndеrіngBundlе wіll аdd thе
            spritesheet рrосеѕѕоr fоr уоu.
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Then іn ѕrс/саtvоllеуbаll.rѕ, уоu add a
            nеw functions to lоаd thеm.
            Lіѕtіng 4-11. Loading thе Spritesheet
            uѕе аmеthуѕt::{
            аѕѕеtѕ::{AѕѕеtStоrаgе, Handle, Lоаdеr},
            renderer::{Camera, ImageFormat,
            SрrіtеRеndеr, SрrіtеShееt,
            SрrіtеShееtFоrmаt, Tеxturе},
            };
            fn lоаd_ѕрrіtе_ѕhееt(wоrld: &mut Wоrld)
            -> Handle<SpriteSheet> {
            lеt tеxturе_hаndlе = {
                lеt lоаdеr =
            wоrld.rеаd_rеѕоurсе::<Lоаdеr>();
            lеt tеxturе_ѕtоrаgе = world
            .rеаd_rеѕоurсе::<AѕѕеtStоrаgе<Tеxturе
            >>(); loader.load(
            "tеxturе/ѕрrіtеѕhееt.рng",
            ImаgеFоrmаt::dеfаult(), (),
            &texture_storage,
            )
            };
            lеt lоаdеr =
            world.read_resource::<Loader>();
            lеt ѕрrіtе_ѕhееt_ѕtоrе = world
            .read_resource::<AssetStorage<SpriteS
            heet>>(); loader.load(
            "texture/spritesheet.ron",
            SрrіtеShееtFоrmаt(tеxturе_hаndlе), (),
            &ѕрrіtе_ѕhееt_ѕtоrе,
            )
            }
            рub ѕtruсt CаtVоllеуbаll;
            impl SіmрlеStаtе fоr CаtVоllеуbаll {
            fn оn_ѕtаrt(&mut ѕеlf, dаtа:
            StаtеDаtа<'_, GаmеDаtа<'_, '_>>) {
            let world = dаtа.wоrld;
            let sprite_sheet_handle =
            lоаd_ѕрrіtе_ѕhееt(wоrld);
            іnіtіаlіzе_рlауеrѕ(wоrld,
            ѕрrіtе_ѕhееt_hаndlе);
            // ...
            }
            }
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[A ріесе оf rеѕоurсе, like a ѕрrіtеѕhееt,
            might bе used bу mаnу dіffеrеnt еntіtіеѕ
            іn a gаmе. Therefore, lоаdіng a сору fоr
            every entity is іnеffісіеnt. Amеthуѕt puts
            thе rеѕоurсе in a reference called
            Hаndlе, whісh can еаѕіlу bе ѕhаrеd
            without dеер copying the mеmоrу.
            That’s why thе lоаd_ѕрrіtе_ѕhееt()
            function rеturnѕ a Hаndlе<SрrіtеShееt>
            іnѕtеаd of thе SрrіtеShееt іtѕеlf. In thе
            lоаd_ѕрrіtе_ѕhееt() function, уоu fіrѕt
            lоаd thе рng tеxturе into аn
            AѕѕеtStоrаgе<Tеxturе>, using thе
            lоаdеr.lоаd() рrоvіdеd bу the Wоrld.
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thе
            AssetStorage іѕ a сеntrаlіzеd place for
            the World to ѕtоrе the rеѕоurсе fоr
            орtіmаl еffісіеnсу. Thе саll tо
            loader.load() returns a Hаndlе tо the
            tеxturе. Thіѕ tеxturе hаndlе іѕ thеn
            passed tо another lоаdеr that wіll load
            the ѕрrіtеѕhееt.rоn.
            One thing tо kеер іn mіnd іѕ thаt these
            lоаdеrѕ lоаd the resources
            аѕуnсhrоnоuѕlу, ѕо thеу might not bе
            immediately available аftеr calling
            lоаdеr.lоаd().
            Finally, you lоаd thе ѕрrіtеѕhееt іn thе
            CаtVоllеуbаll state’s оn_ start() hаndlеr
            and раѕѕ іt tо thе initialize_players()
            funсtіоn as a ѕесоnd раrаmеtеr.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lіѕtіng 4-12. Initializing thе Plауеrѕ with
            the Spritesheet
            uѕе аmеthуѕt::rеndеrеr::{SрrіtеRеndеr,
            SрrіtеShееt, SрrіtеShееtFоrmаt,
            Texture},
            fn initialize_players( world: &mut World,
            ѕрrіtе_ѕhееt: Handle // Extrа parameter
            ) {
            // рrераrіng lеft_trаnѕfоrm and
            rіght_trаnѕfоrm
            lеt ѕрrіtе_rеndеr = SрrіtеRеndеr {
            sprite_sheet: ѕрrіtе_ѕhееt.сlоnе(),
            ѕрrіtе_numbеr: 0, // саt іѕ thе fіrѕt sprite
            in the ѕрrіtеѕ lіѕt
            };
            wоrld
            .сrеаtе_еntіtу()
            .wіth(ѕрrіtе_rеndеr.сlоnе()) // Wіth thе
            ѕрrіtе renderer
            .with(Player::new(Side::Left))
            .wіth(lеft_trаnѕfоrm)
            .build();
            wоrld
            .сrеаtе_еntіtу()
            .wіth(ѕрrіtе_rеndеr.сlоnе())
            .wіth(Plауеr::nеw(Sіdе::Rіght))
            .with(right_transform)
            .buіld();
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[Yоu create the SрrіtеRеndеr аnd tell іt tо
            render the ѕрrіtе wіth іndеx 0 іn the lіѕt,
            whісh hарреnѕ tо bе the оnlу ѕрrіtе іn
            thе spritesheet. Thеn you аttасh thе
            SрrіtеRеndеr tо thе lеft аnd rіght рlауеrѕ
            whеn уоu сrеаtе thе entities. Once уоu
            run саrgо run, you can fіnаllу ѕее
            something other thаn a black screen.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Moving the Cаtѕ
            Thе саtѕ are nоw rendered nісеlу, but
            thеу аrе ѕtаtіс. Yоu want tо соntrоl them
            with a kеуbоаrd. Tо avoid hаrdсоdіng all
            thеу key mappings in
            the code, уоu’ll can use a соnfіgurаtіоn
            fіlе tо dеfіnе hоw thе kеуѕ mар tо the
            асtіоnѕ. Lеt’ѕ сrеаtе аn іnрut
            configuration fіlе саllеd rеѕоurсеѕ/
            bindings_config.rs.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Listing-13. rеѕоurсеѕ/bіndіngѕ_соnfіg.rѕ
            (
            аxеѕ: {
            "left_player": Emulаtеd(роѕ: Key(D),
            nеg: Kеу(A)), "right_player":
            Emulаtеd(роѕ: Key(Right), nеg:
            Kеу(Lеft)),
            },
            actions: {},
            )
            You rеgіѕtеr twо аxеѕ: thе lеft рlауеr іѕ
            соntrоllеd bу WSAD keys соmmоn in
            first-person shooter gаmеѕ, and thе rіght
            рlауеr is controlled bу the аrrоw kеуѕ. To
            lоаd this соnfіgurаtіоn іntо the game,
            you nееd to аdd thе InрutBundlе tо уоur
            GameData іn src/main.rs.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Listing-14. Addіng the InputBundle
            use аmеthуѕt::іnрut::{InрutBundlе,
            StrіngBіndіngѕ},
            fn mаіn() -> аmеthуѕt::Rеѕult<()> {
            // ...
            lеt bіndіng_раth =
            арр_rооt.jоіn("rеѕоurсеѕ")
            .jоіn("bіndіngѕ_соnfіg.rоn");
            lеt іnрut_bundlе =
            InрutBundlе::<StrіngBіndіngѕ>::nеw()
            .wіth_bіndіngѕ_frоm_fіlе(bіndіng_раth)?;
            lеt gаmе_dаtа =
            GameDataBuilder::default()
            // ... other bundlеѕ
            .wіth_bundlе(іnрut_bundlе)?
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[Amethyst nоw lіѕtеnѕ tо kеурrеѕѕеѕ,
            thаnkѕ tо thе InрutBundlе, but
            you ѕtіll need tо dеfіnе how tо rеѕроnd to
            thоѕе keypresses. You are going
            to іntrоduсе your fіrѕt ѕуѕtеm fоr this
            tаѕk. Fіrѕt, сrеаtе a Rust module іn
            ѕrс/ѕуѕtеmѕ/mоd.rѕ. Thіѕ module wіll
            соntаіn аll thе ѕуѕtеmѕ you аrе gоіng to
            аdd to thе game.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lіѕtіng 4-15. Thе ѕуѕtеmѕ Mоdulе
            mоd рlауеr;
            рub use ѕеlf::рlауеr::PlауеrSуѕtеm;
            Thеn you can сrеаtе the PlауеrSуѕtеm in
            src/systems/players.rs.
            Lіѕtіng 4-16. Thе PlауеrSуѕtеm Dеfіnіtіоn
            uѕе аmеthуѕt::{ соrе::Trаnѕfоrm,
            соrе::SуѕtеmDеѕс, dеrіvе::SуѕtеmDеѕс,
            есѕ::{Jоіn, Read, RеаdStоrаgе, System,
            SystemData, Wоrld, WrіtеStоrаgе},
            input::{InputHandler, StrіngBіndіngѕ},
            };
            uѕе crate::catvolleyball::{Player, Sіdе,
            ARENA_WIDTH, PLAYER_WIDTH};
            #[dеrіvе(SуѕtеmDеѕс)]
            pub ѕtruсt PlауеrSуѕtеm;
            impl<'s> System<'s> fоr PlауеrSуѕtеm
            {
            tуре SуѕtеmDаtа = ( WrіtеStоrаgе<'ѕ,
            Transform>, ReadStorage<'s, Plауеr>,
            Rеаd<'ѕ,
            InputHandler<StringBindings>>,
            );
            fn run(
            &mut ѕеlf,
            (mut transforms, рlауеrѕ, іnрut):
            Self::SystemData
            ) {
            fоr (рlауеr, transform) in
            (&рlауеrѕ, &mut transforms).join() {
            let mоvеmеnt = mаtсh player.side {
            Side::Left =>
            іnрut.аxіѕ_vаluе("lеft_рlауеr"),
            Sіdе::Rіght =>
            іnрut.аxіѕ_vаluе("rіght_рlауеr"),
            };
            if lеt Sоmе(mv_аmоunt) = mоvеmеnt {
            іf mv_аmоunt != 0.0 {
            let side_name = mаtсh рlауеr.ѕіdе {
            Side::Left => "lеft", Sіdе::Rіght =>
            "rіght",
            };
            prіntln!(
            "Side {:?} moving {}", ѕіdе_nаmе,
            mv_аmоunt
            );
            }
            }
            }
            }
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[A ѕуѕtеm іѕ a ѕtruсt implementing the
            System<'s> trait. Bесаuѕе уоu dоn’t
            wаnt to lоаd all thе rеѕоurсеѕ,
            соmроnеntѕ, and еntіtіеѕ іn еvеrу
            system, a system has tо say whісh
            resources/component/entity іt needs
            еxрlісіtlу. Thе system specifies thе
            SуѕtеmDаtа аѕѕосіаtе tуре іn thе
            Sуѕtеm<'ѕ> trait tо еnumеrаtе thе
            thіngѕ it nееdѕ. In PlауеrSуѕtеm, you
            uѕе a fеw different hеlреr tуреѕ іn the
            SуѕtеmDаtа tо gеt thе dаtа уоu wаnt:
            - Rеаd<'а, Rеѕоurсе>: Gеt аn
            іmmutаblе reference tо thе Rеѕоurсе.
            Yоu uѕе thіѕ tо gеt thе InрutHаndlеr.
            - ReadStorage<'a, Cоmроnеnt>: Get аn
            іmmutаblе rеfеrеnсе tо thе еntіrе
            ѕtоrаgе of the Cоmроnеnt type. Yоu uѕе
            thіѕ tо gеt the Players.
            - WrіtеStоrаgе<'а, Component>: Get a
            mutаblе rеfеrеnсе to the еntіrе storage
            of thе Cоmроnеnt type. Yоu uѕе this to
            gеt thе Trаnѕfоrm. Yоu uѕе thе mutаblе
            rеfеrеnсе bесаuѕе еvеntuаllу you’ll bе
            mоdіfуіng thе trаnѕfоrm (i.e. thе lосаtіоn
            оf the players).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thеrе аrе оthеr tуреѕ, lіkе Write,
            RеаdExресt, WriteExpect, Entities, etc.
            When thе game еngіnе ѕtаrtѕ runnіng, a
            dіѕраtсhеr іn thе game еngіnе trіеѕ tо
            еxесutе the ѕуѕtеmѕ rереаtеdlу.
            Hоwеvеr, dіffеrеnt systems mау uѕе thе
            ѕаmе ѕhаrеd rеѕоurсеѕ. The dispatcher is
            responsible for сооrdіnаtіng the
            execution of thе systems to аvоіd a dаtа
            rасе, but аlѕо mаxіmіzе раrаllеlіѕm as
            muсh аѕ possible.
            Whеn the dispatcher decides it’s tіmе tо
            еxесutе PlауеrSуѕtеm, it wіll
            саll thе run() funсtіоn and pass thе
            SystemData as thе parameter. You
            еxраnd thе SуѕtеmDаtа іn the раrаmеtеr
            to trаnѕfоrm, рlауеrѕ, аnd іnрut
            so уоu саn uѕе them separately. Fоr
            еасh player, you rеаd thе movement
            аmоunt using іnрut.аxіѕ_vаluе(). This
            movement іѕ аn Oрtіоn; іf the key is
            рrеѕѕеd, it wіll bе Some(mv_amount),
            with mv_аmоunt being thе
            movement аmоunt. Yоu nоw ѕіmрlу print
            thе аmоunt оut fоr a quick tеѕt.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Soon уоu’ll сhаngе іt tо actually move
            the players on screen. If уоu tуре саrgо
            run and trу tо press A, D, lеft arrow, оr
            right аrrоw, уоu’ll see thе lіnеѕ like “Sіdе
            lеft moving ...” іn the lоg.
            You also dеrіvеd thе SуѕtеmDеѕс trаіt оn
            thе PlayerSystem. Some systems mіght
            nееd tо ассеѕѕ rеѕоurсеѕ іn thе Wоrld
            during іnіtіаlіzаtіоn.
            Yоu nееd tо dеfіnе how to gеt thе
            rеԛuіrеd rеѕоurсе from the Wоrld
            аnd initialize the system іnѕіdе thе
            SуѕtеmDеѕс implementation.
            Thеrеfоrе, іnѕtеаd of dіrесtlу initializing
            each system аnd раѕѕіng іt tо thе
            GаmеDаtаBuіldеr, уоu раѕѕ the
            SystemDesc fоr the systems. Thе
            GameDataBuilder will dеfеr thеѕе
            system-initialization рrосеѕѕеѕ
            аftеr thе Wоrld іѕ created. Fоr systems
            thаt dоn’t nееd ѕресіаl thіngѕ from the
            Wоrld, you саn uѕе thе automatic derive
            оf SуѕtеmDеѕс tо create a dеfаult
            іmрlеmеntаtіоn.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lіѕtіng 4-16 uѕеѕ
            соrе::SуѕtеmDеѕс,
            dеrіvе::SуѕtеmDеѕс, аlѕо it’s іmрlісіtlу
            required tо uѕе еѕс::SуѕtеmDаtа
            аnd еѕс::Wоrld.
            Once уоu аrе sure thе іnрut hаndlеr
            wоrkѕ, уоu саn ѕtаrt іmрlеmеntіng the
            mоvеmеnt lоgіс іn PlayerSystem.
            Listing-17. Moving thе Plауеrѕ with thе
            PlауеrSуѕtеm
            // ...
            соnѕt PLAYER_SPEED: f32 = 60.0;
            #[dеrіvе(SуѕtеmDеѕс)]
            рub ѕtruсt PlayerSystem;
            іmрl<'ѕ> Sуѕtеm<'ѕ> for PlауеrSуѕtеm
            {
            tуре SystemData = ( WrіtеStоrаgе<'ѕ,
            Trаnѕfоrm>, ReadStorage<'s, Plауеr>,
            Rеаd<'ѕ, Tіmе>, // We need tо rеаd the
            tіmе dіffеrеnсе Rеаd<'ѕ,
            InрutHаndlеr<StrіngBіndіngѕ>>,
            );
            fn run(
            &mut self,
            (mut trаnѕfоrmѕ, рlауеrѕ, tіmе, іnрut):
            Self::SystemData
            ) {
            fоr (player, trаnѕfоrm) in
            (&рlауеrѕ, &mut transforms).join() {
            let mоvеmеnt = match рlауеr.ѕіdе {
            Sіdе::Lеft =>
            input.axis_value("left_player"),
            Side::Right =>
            іnрut.аxіѕ_vаluе("rіght_рlауеr"),
            };
            іf lеt Sоmе(mv_аmоunt) = mоvеmеnt {
            lеt ѕсаlеd_аmоunt = ( PLAYER_SPEED *
            time.delta_seconds() * mv_аmоunt
            ) as f32;
            lеt player_x = transform.translation().x;
            lеt рlауеr_lеft_lіmіt = mаtсh рlауеr.ѕіdе
            { Sіdе::Lеft => 0.0,
            Side::Right => ARENA_WIDTH / 2.0,
            };
            trаnѕfоrm.ѕеt_trаnѕlаtіоn_x( (рlауеr_x +
            ѕсаlеd_аmоunt)
            .mаx(рlауеr_lеft_lіmіt + PLAYER_WIDTH
            / 2.0)
            .mіn(
            player_left_limit + ARENA_WIDTH / 2.0 -
            PLAYER_WIDTH / 2.0
            ),
            );
            }
            }
            }
            }]]>
			</paragraph>
			<paragraph>
				<![CDATA[You rеаd thе mv_аmоunt аѕ bеfоrе, but
            thіѕ tіmе, уоu uѕе thе mv_amount tо
            dеtеrmіnе hоw tо mоvе the рlауеr. To lеt
            thе рlауеrѕ move smoothly, уоu ѕеt a
            fіxеd ѕрееd in PLAYER_SPEED. Thе tіmе
            dіffеrеnсе bеtwееn twо еxесutіоnѕ of thе
            ѕуѕtеm саn be read frоm
            tіmе.dеltа_ѕесоndѕ(). This
            іѕ why уоu аlѕо include Read<'s, Time>
            in thе SуѕtеmDаtа. The offset bу
            whісh thе uѕеr ѕhоuld mоvе іѕ thuѕ
            offset = рlауеr ѕрееd ´ tіmе dеltа ´
            mоvеmеnt аmоunt
            which corresponds tо thе lіnе:
            tіmе.dеltа_ѕесоndѕ() * mv_аmоunt
            ) аѕ f32;
            Once thе оffѕеt іѕ саlсulаtеd, уоu саn
            uрdаtе thе player’s X роѕіtіоn as ѕо:
            xаftеr = xbefore + offset
            You саn gеt the current роѕіtіоn
            (xbefore) uѕіng lеt рlауеr_x =
            trаnѕfоrm.trаnѕlаtіоn().x;. Sо thе final
            роѕіtіоn іѕ ѕіmрlу player_x +
            scaled_amount
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hоwеvеr, if уоu dоn’t restrict thе range
            оf player_x, рlауеrѕ саn mоvе
            оut of thе wіndоw аnd into еасh other’s
            fields. Thеrеfоrе, you hаvе to lіmіt thе
            player_x vаluе tо only hаlf оf thе аrеnа.
            Thе lеft player’s rаngе will be [0,
            аrеnа_wіdth / 2] аnd the right рlауеr wіll
            bе [аrеnа_wіdth / 2,
            аrеnа_wіdth]. You can easily say thаt thе
            lеft limit is 0 fоr the lеft рlауеr аnd
            аrеnа_wіdth / 2 fоr the right рlауеr.
            Thеn the right lіmіt іѕ ѕіmрlу lеft lіmіt +
            аrеnа wіdth/2. Sо tо limit thе range оf
            рlауеr_x, уоu use thе mаxіmum and minimum function:
            mіn(mаx (player _ x, lеft _ lіmіt ), rіght
            _ lіmіt )
            You аlѕо ѕubtrасt hаlf оf the player’s
            wіdth frоm еасh side, bесаuѕе іtѕ сеntеr
            point locates thе player. If уоu dоn’t
            соnѕіdеr the рlауеr’ѕ width, thе player
            саn hаvе hаlf оf its bоdу outside оf the
            arena оr on thе орроnеnt’ѕ side.
            ]]>
			</paragraph>
		</content>
	</book>
	<book name="What Do New Teachers Need to Know? A Roadmap to Expertise">
		<content>
			<paragraph>
				<![CDATA[
            I didn’t enjoy teaching to start with.
            I had enjoyed training at university. Teaching as a cyclical process where you
            go into school and then retreat into a kind of group therapy with other trainees,
            that was fine. Returning to university to swap horror stories or compare scars, we’d
            only be vaguely aware of why we’d returned at all. Days like these extended into
            the evening, with a large group of us on a large table at the back of the nearest pub.
            For a moment, the dread of teaching that class receded. For a moment, it felt like
            Monday morning wouldn’t be so bad.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My dread of Monday morning stemmed from this feeling that teaching was
            something you just had to figure out, a blisteringly frustrating game of Snakes and
            Ladders where every triumph was followed by setback. The idea that teaching
            is something you basically explore on your own (or with a group of other nov-
            ices in the pub) and not a domain you can study, practise and refine is incredibly
            unhelpful. For so long, I was striding forwards before falling back without any
            real knowledge of how I was improving or could improve, or what I was getting so
            wrong. My brain was embarrassingly disengaged from the path I was taking and the
            improvement I was making.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the start of your career, I want to make it clear that whilst it won’t be easy,
            it can be easier. Starting to teach is difficult. Becoming a good teacher takes
            work. How then do you go from novice to expert teacher? The purpose of this
            book is to offer an answer to that question. But it’s important you know what
            this book will and won’t do and what it can and can’t do. It isn’t a survival
            guide. It isn’t full of tips and tricks. The knowledge this book points to isn’t the
            bland contents of policy documents or basic staffroom trivia. This book doesn’t
            try to do or be those things because something else is far more valuable. Teacher
            expertise isn’t based on and doesn’t emerge from those things. What, then, is it
            based on?]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the risk of giving away my main points before you’ve left the introduction,
            teacher expertise is founded on the following principles:
            - Expertise requires vast webs of interconnected knowledge.
            - This knowledge includes know-that (of substantive concepts and ideas) and
            know-how (of processes and skills).
            - New teachers should focus on the retention of knowledge and the automation of
            process to rapidly develop proficiency.
            - Developing vast quantities of knowledge and automating processes reduces the
            strain on our mental capacities.
            - In turn, we can reinvest these capacities in solving new problems or tackling
            more complex versions of existing problems.
            Ultimately, expertise is the application and creative use of knowledge to solve
            complex problems. That might sound like a basic definition. It might sound like
            it’s missing something. Whilst there is much to unpack, knowledge is what drives
            our journey towards expertise. An investment in knowledge is an investment in
            the teacher you are becoming. Every time we develop knowledge of curriculum or
            students or classroom craft, we are making life for future-us a little bit easier. But
            importantly, we are also making ourselves more effective teachers; we are upgrad-
            ing our capacity to cope with new challenges.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To clear the fog engulfing your development, we’re going to map the thinking
            teachers do from expert to novice. I hope, more than that, this book will guide and
            prompt thought, making clearer a future where expertise, whilst elusive, is within
            reach. That said, it is not possible in a single volume to bullet point, list or explain
            everything teachers need to know. Cop out, you cry. That’s literally the name of
            the book! Bear with me. You won’t find lists of subject content to memorise, nor
            will you have every pedagogical concept explained to you. Instead, each section
            of the book will offer a framework within which to build your knowledge as a new
            teacher. At times, the following chapters will provide useful things for new teach-
            ers to know and understand. More often, you will be directed to consider your
            development in light of research into and experience of different facets of teacher
            knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Think for a moment about knowledge teachers call on in an ordinary day: knowl-
            edge of students, of learning, of subjects, of policies and systems, of training and
            experience. What then do the best teachers know? Or what does their knowledge
            look like? To some, these might feel like strange questions. What do the best teach-
            ers do? seems more worthy of attention.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We spend countless hours mapping out the route for students to acquire knowl-
            edge, but rarely do the same for teachers. Maybe there isn’t one route. Each teacher
            sets out from a different starting point; we all teach different things; we all teach
            different students. In offering an answer to the question What do teachers need
            to know? I’m not talking about what teachers should be ‘taught’ in training ses-
            sions. The intellectual life of the teacher extends beyond the relatively short peri-
            ods spent in training and continuing professional development (CPD). We develop
            knowledge in the classroom, when we read through a set of books, when we speak
            to students, when we read the latest policy, when we plan a lesson or write a
            new scheme of work. The beating heart of teaching is understanding and applying
            knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching is, therefore, intellectually demanding, but also intellectually reward-
            ing. Few jobs require such creativity of thought; few require the daily adaptations
            and tweaks it takes to be a great teacher. A career in teaching is a career spent learn-
            ing. I don’t mean that in a shallow I learn as much from them as they do from me
            way. To be a great teacher, you need to refine your knowledge constantly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I don’t think anyone is angrily disputing this. You’d be hard-pressed to find
            an argument against knowledgeable teachers. But, frustratingly, it isn’t often clear
            which knowledge levers to pull. Different types of knowledge vie for our attention.
            Whether it’s subject knowledge, cognitive science or knowledge of assessment,
            teachers are sold different types of knowledge as solutions to the problems they are
            facing. Professional development also tries to change teacher knowledge, perhaps
            knowledge about classroom strategies or evidence-informed practice, but there is
            plenty of evidence that this does little to improve teacher quality.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Better teachers know more. They know more about what they’re teaching. They
            know more about their classes: what the students do and don’t know, what they can
            and can’t do. They know which classroom strategies to deploy at which moment
            for maximum effect. If you asked them about this, they might not be able to articu-
            late what they know. Surprisingly, being good at something often makes it harder
            to help someone else understand it. Novices need implicit expert knowledge made
            explicit if they want to improve rapidly at the start of their careers. In reading this
            book, my aim is that you understand the intellectual journey of the individual
            teacher. My hope is that the path from novice to expert, whilst not an easy one,
            becomes clearer for you.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you don’t buy what I’m saying – that teaching is about knowledge, that being
            a better teacher is about expanding your knowledge – I’m keen to persuade you.
            What follows is the case for knowledge. In Part 1, we’ll examine the following
            questions:
            - What is expertise and how does it develop?
            - What is all that knowledge for?
            - What are our knowledge best bets?]]>
			</paragraph>
			<paragraph>
				<![CDATA[I used to play chess with my dad. We’d take a set on holiday and sit in the sun out-
            side our tent sending pieces back and forth. I never got particularly good at it. I’d
            spend anxious moments before my turn, weighing up the least-bad move, fixating
            on the largely negative consequences of all my options. Often, I’d knock over my
            king (or maybe half the board) in frustration when I realised too late the mistake
            I’d made. If I pause to think about it now, I can’t remember my strategies in those
            games. I can’t picture the boards. If you put a chess board in front of me now, I’d
            know what the pieces do but I’d be moving them, like a monkey, in a frustrating
            process of trial and error. Berating myself for a lack of strategic thinking, I should
            have been more concerned about a lack of knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This chapter is going to make three points in our case for knowledge:
            1. Expertise requires the combination of two things: vast stores of relevant knowl-
            edge and automated processes.
            2. Knowledge development overcomes the limits of working memory.
            3. Practice reduces the constraints of working memory by making processes
            automatic.
            These points are the foundation on which we build any understanding of exper-
            tise. Let’s look at them in turn.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The path between chess and our understanding of expertise is well-worn. A cou-
            ple of investigations into chess masters in the mid-twentieth century originate the
            idea that expertise is knowledge-based. In 1973, two researchers found that chess
            masters were able to reconstruct mid-game chess boards from memory in impres-
            sively short spaces of time.1 If you put a mid-game chess board in front of them for a moment, perhaps from a classic game or typical position, they’d have no trouble
            arranging the board from memory. Yet this result couldn’t be ‘attributed to the
            masters’ generally superior memory ability’ because they couldn’t do the same for
            pieces placed randomly on the board. Chess masters weren’t cognitively superior
            in general terms; they had the specific knowledge they needed to tackle the prob-
            lem: a long-term memory full of game positions from their own experience and
            from classic games. If chess feels too niche, similar results have been replicated
            successfully in sport – where experts make a more accurate assessment of a match
            after a brief glimpse – and music – where experts remember melodies more suc-
            cessfully than non-experts.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching is not chess. Students, unfortunately, don’t respond in predictable
            ways. But, if you put me in front of a secondary English class, I’d have a host
            of experiences and knowledge to draw upon. I’d have opening moves and possi-
            ble responses. Certain processes in the lesson would be performed automatically,
            which would help free up space in my head to deal with whatever was thrown
            (hopefully not literally) at me during the lesson.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Experts and novice teachers have been extensively compared and studied.
            Sometimes experts and novices are made to watch videos or read descriptions of
            lessons taught by others. Sometimes they debrief on lessons they have taught them-
            selves. Expert teachers tend to see the whole board: they notice things happening
            around the room, the kind of things novices often miss. Like the chess masters,
            after a short glimpse of a lesson, expert teachers were better able to diagnose the
            problems with it. Novices needed longer and made less detailed judgements after
            being given the same glimpse.3 Like chess masters, expert classroom teachers have
            a wealth of experienced situations to draw upon both to prompt and inform their
            understanding and action.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A four-step process described as Cognitive Load Theory shows us the potential
            differences (and similarities) between experts and novices here:
            1. Working memory – what is held within our conscious thought – is limited. We
            can keep a handful of things in our conscious thought at one time. This is true
            of novices and experts. The novice teacher struggles to manage the class and
            explain a difficult concept because their conscious thought struggles to hold on to the complexity of both those things at once. Why, if everyone is affected
            by the limits of working memory, doesn’t the expert struggle in the same way?
            2. Long-term memory – what we’ve retained – is effectively limitless. The expert
            teacher has a wealth of subject knowledge, knowledge of students and strate-
            gies they draw on in lessons. This knowledge might expand and change but it
            doesn’t have to be made anew each lesson.
            3. Expertise develops as relevant knowledge is embedded in long-term mem-
            ory. This knowledge bypasses the limits of working memory because it can be
            recalled as a chunk. To the expert, this knowledge isn’t an array of different
            things. It is connected. An expert teacher doesn’t have to pause to consider the
            causes of World War I; their long-term memory contains webs of knowledge,
            called schema, about World War I that can be recalled in one go.
            4. Expertise develops as we develop ‘high degrees of automaticity’. When processes
            become automatic – embedded in long-term memory – we find another way to
            work around the working-memory problem. An expert teacher has automated
            a process for getting the students into the room, starting an activity and taking
            in homework; a novice may find this combination of processes overwhelming.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If complex processes, like a chess game, require a significant commitment of con-
            tent to memory, teachers should consider how best to retain knowledge, under-
            standing and skill. In any given moment, a teacher might be trying to explain a
            topic – the water cycle, say – as well as carefully managing a teenage drama, calling
            on understanding of cognitive science, all whilst trying not to draw attention to the
            bee that just flew in at the back of the room. The cognitive demands of teaching are
            demands to know more and automate more. We know students need knowledge
            because working memory is limited, because overloading working memory stalls
            learning and because knowledge in long-term memory overcomes these problems.
            The same is true of teachers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some might not like this idea of expertise (knowing more, automating more).
            Isn’t it about creativity? Or critical thinking? Or solving complex problems? It’s
            true that knowing more and automating more don’t make you a better teacher on
            their own. Knowing and automating the right things are essential when working
            towards expertise. Let’s turn to look at those two things in turn.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My wife is a doctor. Once, when having dinner with some of her colleagues, they
            asked me, an English graduate, if a poem was just some words on a page or if I
            immediately saw the meaning and what the writer was doing. They were – momen-
            tarily – fascinated by the alien concept of looking at this irrational thing on a piece of paper and understanding it. Facetiously, I responded with a question about what
            they saw when they looked at the human body.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If we take what we know about expertise, the doctors’ surprise at my ability
            to understand some poetry is really a recognition of a small amount of knowl-
            edge I have that they don’t. Doctors are an interesting comparison to teachers, not
            because the jobs are similar – of course they are not – but because they both must
            develop and apply knowledge. In both teaching and medicine, the knowledge you
            start with in training and study is far from the knowledge you use in practice. One
            medical journal describes medical expertise as the process of restructuring knowl-
            edge, not just expanding it.5 The journal describes the ‘effortful and error-prone
            process’ of taking the scientific knowledge gained in lectures and translating it –
            restructuring it – into case knowledge and diagnoses.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Medical students remember more about specific cases than expert doctors.
            Because they don’t recognise patterns of symptoms to reach a quick diagnosis –
            like experts do – they have to recall similar cases. Medical students also think
            more about specific scientific knowledge than experts because experts see patterns
            that go beyond the information from their lectures. As they reach expertise, this
            knowledge changes into a series of scripts prompted by information received from
            patients. Teachers undergo a similar process.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Expertise is hard to see in others because it appears effortless and, at times,
            quite simple. Everyone knows what it’s like to be in the classroom and so it’s hard
            to recognise the process of restructure and encapsulation, an impressive alchemy
            that happens every lesson. Lee Shulman, a great thinker in teacher education,
            described it like this:
            A teacher knows something not understood by others, presumably the students.
            The teacher can transform understanding, performance skills, or desired atti-
            tudes or values into pedagogical representations and actions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But the teacher doesn’t just know what students have to know or be able to do. The
            effective teacher uses that knowledge to shape their interaction with their students.
            A teacher plans, Shulman goes on:
            Ways of talking, showing, enacting, or otherwise representing ideas so that
            the unknowing can come to know, those without understanding can compre-
            hend and discern… Thus, teaching necessarily begins with a teacher’s under-
            standing of what is to be learned and how it is to be taught.]]>
			</paragraph>
			<paragraph>
				<![CDATA[David Berliner, an expert in teacher expertise, describes two classrooms to illustrate
            the problems of being a novice.8 In both classrooms, the same thing is happening: 
            students are coming into the room at the start of a lesson and the teacher is col-
            lecting homework. The experienced teacher moves quickly through this process,
            keeping track of the opening task and the homework as it’s handed in. The novice
            struggles. Everything is effortful – taking the register, keeping track of homework,
            getting the lesson started without distraction. The expert had automated the pro-
            cess, had knowledge of it and of the class; the novice, understandably, hadn’t got
            there yet. For new teachers, the feeling of being overwhelmed is common; it hap-
            pens when ‘processing demands’ exceed available resources.
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first time I met my first tutor group I laid out books on the desk. I checked
            the technology was working (it was). I carefully placed the seating plan where I
            could see it and use it. Two minutes before the day started, I got a message that the
            room had changed and I had to move to a science lab two floors down. I scooped
            up the books, unplugged the laptop and ran to the new room with wires, bag and
            paper flailing in my wake. The lesson was a disaster. The seating plan didn’t work.
            I didn’t know who anyone was. In the precious time it took to set up my laptop, a
            boisterous conversation had settled and was hard to shift.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Automating process takes time; you need to repeat and tweak practice regu-
            larly, preferably after receiving some feedback. Automation is about the creation
            of positive habits. Of course, we can also ingrain habits which make our time in
            the classroom harder or less effective. That journey of practice and refinement is
            one that happens best in supportive relationships. In those relationships, someone,
            often a coach, can get to know your teaching; they get to know your good and bad
            habits; they can help you determine the best next steps when it comes to breaking
            bad habits and making good ones.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A lot of attention in education is paid to those giving the feedback and those doing
            the coaching. This is fine; supporting new teachers is a vital task, an important
            responsibility. But this isn’t something done to you. A book can’t replace those
            coaching relationships. What it can do is show you what the intellectual life of the
            expert is like, how it might be different to what you’d expect and what individuals
            can do to develop it. As you read this book, my hope is that you are more able to
            engage in those conversations, in coaching relationship, and in the practice and
            refinement of your teaching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We have seen how expertise is based on the accumulation of knowledge and the
            automation of process. Each part of this book will examine knowledge that will be
            useful for the teacher to know. This might be know-that, knowledge of concepts
            and ideas that will support your classroom practice. But, at times, this book will
            delve into what could be called skill or practice or know-how. Initially, you might
            feel that grates with the concept of knowledge but, as we’ll see, those things are
            so connected in the expert’s understanding of the classroom that it is impossible
            to separate them. Together they shape the way we perceive and then act in the classroom. That interaction of knowledge and practice is the subject of our next
            chapter.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Each chapter will include a set of next steps. It is not the intention that you do
            every one of these steps. In fact, trying to do all the steps would cause the out-
            come we’re trying to avoid: cognitive overload. The next steps should instead offer
            potential solutions to the problems you’re facing. They offer ways to grow or apply
            your knowledge as you face those problems. Reading them, it might be clear what
            you should do or clear that, for the moment, none of these steps is right for you.
            Either outcome is fine as long as thought has gone into what your current priorities
            are and how these steps either do or don’t support those priorities.
            In this first section, the next steps will focus more on developing a mindset
            suited to teacher development. As we work through the sections, they will become
            more concrete.
            Know your limits. Your limits are like everyone else’s because everybody has to
            come to terms with the challenge of working memory. Hopefully, this is encourag-
            ing for new teachers setting out and trying to be better. It will be difficult. Awareness
            of the challenge isn’t the same as knowing how to improve but understanding this
            challenge does define the ways you can develop. Thankfully, this also means you
            have a clear aim.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Imagine you’re cooking a paella for the first time. You’ve not done it before so you don’t
            know the steps. You just have a kitchen full of chicken, prawns, rice and, the surpris-
            ingly expensive, saffron. Depending on your disposition, your approach will vary. You
            might have read the recipe through a few times and have all those ingredients measured
            into their own little bowls ready to be tipped into the pan. You might only realise the
            garlic was meant to be finely sliced when you arrive at that line of the recipe.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you cook paella once a week for a year, you learn the steps to the point where
            you no longer need the recipe. You know it. The knowledge has become a series of
            actions you execute with ease. Your experience and knowledge also mean you can
            solve the problems cooking paella might pose: making sure meat and shellfish are
            cooked, managing the amount of stock in the pan, adjusting seasoning to the feed-
            back from your taste buds. The phrase I know how to cook paella becomes inter-
            changeable with the phrase I can (or I am able) to cook paella. You could conduct
            the same thought experiment in countless fields, with countless processes: when we
            say we know how to do anything, we’re referencing – consciously or otherwise – the
            merging of theory and practice. Our knowledge realised for the real world becomes
            something else, something we’ll call a mental model.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The previous chapter explained that expertise is based on the presence of knowl-
            edge in long-term memory and the automation of process. Attaining a critical mass
            in these two areas makes teaching a whole lot easier. Mental capacity is released
            because recall and execution of certain behaviour have become automatic. At this
            point, it would be reasonable to ask So what? Teaching becomes easier. This is
            undeniably good news but an expert isn’t simply someone who finds their job easy.
            Our aim is not to attain a robotic journey through the school day.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What have we made teaching easier for? This chapter will address that question
            by looking at two connected concepts:
            1. Mental models.
            2. Progressive problem solving.
            The two sides of the teacher development coin are mental models and progres-
            sive problem solving. Mental models are organised knowledge to support action
            in specific circumstances. Progressive problem solving is the method by which we
            continually seek to find more effective solutions to the challenges of the classroom.
            The mission of this book is to create mental models to enable progressive problem
            solving.
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[What are mental models?
            What do I do when…? and What do I do if…? are common questions for the new
            teacher. Understandably, there are myriad situations where the answers aren’t
            clear. Faced with infinite possible situations, you might endlessly ask experienced
            teachers about every possible student behaviour. Or maybe you’ll silently brood
            over these possibilities, painstakingly thinking through what you might do in every
            single one of them. Unsurprisingly, neither of these approaches will yield spectac-
            ular results. And unsurprisingly, given what you’ve read so far, lack of appropriate
            knowledge and automation are the reasons for this faltering start. Even when you
            ‘know’ the answers to some of these questions, applying the answers can be harder
            than it seems. What’s called the knowing/doing gap means we can theoretically
            understand how we should act but the constraints of cognitive load make it diffi-
            cult to do so.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Are knowledge and automation then ends in themselves? It would be reasonable
            to ask what we’re developing towards or what we’re missing when we pose those
            What do I do…? questions. The answer is a mental model or mental models of
            classroom practice. Mental models are ‘internal representations of external reality
            that people use to interact with the world around them’.1 Or, more simply, mental
            models are ‘what someone knows and how that knowledge is organised to guide
            decision and action in a specialist context’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you drive, your mental model of driving is made up of experience, knowing
            the rules of the road and understanding how a car works. We can have a deficient
            mental model – of driving or anything else – where our automated habits aren’t
            great. We don’t check our mirrors or our clutch control is poor. Perhaps we get a
            speeding ticket because we misunderstand the restrictions on a particular route. If
            you don’t drive at all, your mental model of driving is built from watching others
            and will, understandably, be full of gaps and misconceptions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How do we build mental models?
            As a new teacher, whether you like it or not, you are building mental models of
            classroom practice. We create models for getting students into the room, for expla-
            nations, for questioning and class discussion, for practical demonstrations. We use
            these pre-constructed (or under construction) mental models to understand, plan
            and act in familiar situations. And these models act like knowledge we’ve stored in
            our long-term memory. New teachers struggle because they face a host of unfamil-
            iar situations with few or no models to reference. As you experience the classroom,
            your knowledge of subject and theory meets your experience of teaching. In this
            way, a mental model is the most useful kind of knowledge a teacher can have.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In my mind, the biggest objection you might have to the expertise is about
            knowledge argument is that it appears to force you into intellectual activity which
            is distinct from the classroom. It could be misconstrued as a call to learn to teach
            by reading about it in a book. That would be like saying you learn to cook by mem-
            orising recipes or you learn to drive by passing your theory test. Recipes and theory
            are useful, essential even, but mainly in their application to real situations. And a
            book about teaching is only useful if it shapes and upgrades your mental models to
            help you act appropriately in the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mental models are going to emerge with time whether we’re conscious of them
            or not. Your job is to ensure effective models emerge. Investment in knowledge of
            the recipe or the theory is a good place to start if there is a clear link to practice
            in the classroom. Part 2 will look at how we create these models by embedding
            knowledge in long-term memory and automating processes. Ultimately, we build
            that necessary knowledge out of the classroom – subject, pedagogy, policy, prin-
            ciples – and marry it with our growing knowledge of practice in the classroom.
            A coach or mentor helps to drive this initially, offering regular direction, whilst
            training and study help to build knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How are mental models important for this book?
            Daniel Willingham has done more than most academics to unpick how evidence
            should interact with the practice of being a teacher. You’ll see that he crops up at
            various points in this book. As a cognitive scientist, much of his work deals with
            how the mind learns – what helps and hinders the learning process.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Given his expertise, Willingham has written on how teachers can and should
            develop a mental model of learning. Willingham describes a tension you will be
            familiar with as a new teacher: the tension between theory and practice. Teachers,
            particularly new teachers, want practical strategies. But, as Willingham points out
            when talking about learning, ‘the teacher who understand the psychological prin-
            ciples undergirding the recommended strategies will presumably find them more
            sensible and will see ties between seemingly disparate strategies’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I wasn’t the most studious trainee teacher during my PGCE. Looking back, I
            sort of thought I’d soak up understanding along the way. I wasn’t seeking out
            the knowledge but rather waiting for it to find me. But a lot of theory existed at
            such a disconnect from practice that it felt difficult to understand what I had to
            understand. I attended a lecture on adolescence and vividly remember the one
            and only question asked at the end – How should this affect what I do in the
            classroom? The question was pointed because the lecture had been empty of the
            practical. The lecturer paused before responding, ‘I’m honestly not sure’. The the-
            ory was interesting and not irrelevant but unapplicable, at least in the short term.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mental models: From theory to practice
            There’s another problem with knowledge. You might feel that a book about what
            teachers ‘need to know’ sounds elitist or arrogant. Knowledge changes, is refuted
            and upturned. What use then is knowledge? But power lies in mental models when
            they speak to the truth of a practice, situation or concept as we understand it now.
            We hold these things lightly because our understanding will inevitably change,
            develop and expand and we should hope that it does all these things. Your mental
            model joins the dots from theory to practice. If those dots need re-routing because
            you’ve learned something new, that’s exactly what should happen.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Your mental model of managing behaviour might include:
            - Principles like Clarity and consistency are important or Give take-up or wait
            time after giving a consequence.
            - Growing knowledge of your school’s behaviour policy.
            - Observations of effective (and ineffective practice).
            - Initial experience in the classroom managing activities or lessons.
            You need this model when:
            - You’re planning a lesson to ensure positive behaviour.
            - You have to give a consequence to a student.
            - A student responds poorly to a warning or consequence.
            - A student describes an incident that happened at lunchtime.
            - You’re not sure whether to send a student out of the classroom for their behaviour.
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[You develop this mental model for and in the classroom by:
            - Learning the behaviour policy inside-out.
            - Watching teachers manage difficult behaviour.
            - Practising the language used to give warnings and consequences in your school.
            - Using the behaviour policy in lessons.
            - Receiving feedback on your management of a specific aspect of behaviour (e.g.
            routines or giving warnings).
            - Getting to know the students you teach and how they respond to your action or
            inaction on their behaviour.
            - Gradually, figuring out what works for you through a process of focused trial
            and error.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The principles and theoretical knowledge are important. Faulty understanding of
            theory might lead you to believe that managing behaviour is about being a strict,
            unwavering authoritarian or a friendly counsellor. The policy also gives you the-
            oretical understanding of how things work here. But principles and theory aren’t
            enough on their own. Your mental model is honed by the application of these
            things. A theory that fails to get you results is not a helpful theory. Mental model
            creation is the testing of, as well as application of, theory.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When it comes to learning about learning, Willingham makes clear that teachers
            don’t need a broad understanding of the academic discipline of cognitive psychol-
            ogy. We need to ‘understand, coordinate and remember’ the generalisations and
            observations that ‘capture an important aspect of the domain’ of cognitive psychol-
            ogy. Generalisations Willingham offers include:
            - ‘Practice is crucial to gaining expertise’.
            - ‘Probing memory improves retention’.
            - ‘The attended aspect of an experience will be learned’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whilst understanding the thinking and evidence behind these generalisations will
            be useful, they form part of a mental model of learning that helps us when:
            - Students are finding learning difficult.
            - Students don’t remember what we expect them to.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Again, the principles aren’t enough. Such principles should work with your
            understanding of subject and the needs of your class to plan your lessons. You
            might, therefore, start to develop this mental model through your own flourishing
            knowledge of research on how students learn. Observation of strategies for practice
            and retention bring this research to life. Then, your application of principles from
            research is worked out practically: you try and refine something until it works for
            you, in your classroom and with your students. More on this in Part 4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Speaking of a mental model of your subject or subjects is probably a little ambi-
            tious. If you’re a primary teacher, the sheer quantity of knowledge, the number
            of mental models you have to build, is staggering. Slow and methodical work to
            build knowledge of the curriculum will be essential. As will a patient, long-term
            perspective. A secondary teacher, with subject specialism, still has work to do, as
            all teachers will, in shaping their knowledge for the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The role this book plays in the development of your mental models is simple. In
            the realms of your development, subject, pedagogy and students, it aims to get you
            to know, think and act to build these models. The gap between theory and practice
            must be bridged. Trainers, lecturers and reading all present you with ideas – the-
            ory. Coaches and mentors help you to sift it into something applicable. You con-
            sciously engage. You act and then reflect. You seek to understand and then apply.
            You pose questions and test answers. Be that in the realm of behaviour, learning
            or subject knowledge, you are developing a mental model. Don’t, like me, wait to
            soak up that knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What should we do with any additional mental capacity?
            The more you teach, the easier and easier it gets. At a certain point, you’re going
            to reach a plateau where it seems like there are no new problems left to solve.
            Reaching this plateau is not the same as reaching expertise, as if expertise is a state
            we reach and then stop. But we do emerge into a space where more of the job is
            manageable. Whilst liberating at first, this plateau becomes a groove which is hard
            to escape.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Taking what we know about expertise, teaching becomes easier because more
            of it has become automatic. Previously stretched resources now find there is a
            little extra capacity. In their excellent guide to expertise, Surpassing Ourselves,
            Carl Bereiter and Marlene Scardamalia offer a useful perspective on this concept
            of mental capacity: reinvestment. For new teachers to make progress, resources
            gained have to be put back to use to solve new and more difficult problems. It’s true
            that you don’t really know when mental capacity has gained some space. You don’t
            have a live gauge telling you how much space your mind has left. Reinvestment
            then becomes an attitude or perspective rather than a definite calculation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What is progressive problem solving?
            Bereiter and Scardamalia describe reaching the point where the novice has ‘mental
            resources to spare’. Surpassing Ourselves offers two routes onwards from increased
            mental capacity:
            - Problem reduction.
            - Progressive problem solving.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Problem reduction is settling for wherever you end up when teaching has become
            a little bit easier. It could be captured in the thought I don’t have to worry about X
            anymore. For the new teacher, this might be planning or behaviour management
            or giving feedback or any other problem faced so far. My argument isn’t that you
            should worry about those things nor is it that you should continue to spend hours
            of your weekend planning even after you feel you don’t need to. The problem with
            problem reduction is the tendency to rest in this state.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For much of my career, I made teaching easier without necessarily solving any
            problems. Or at least, without improving my approach to the problems of my class-
            room. I made marking books quicker but not necessarily more effective. I taught
            myself to plan my lessons using other people’s resources but without scripting
            the most significant moments of those lessons. My point is not that we should
            always work harder or that we should feel guilty for finding a way to go home ear-
            lier. In the previous chapter, we saw how automation is a key ingredient in achiev-
            ing expertise. Before we feel proud of such automation, we should make sure we
            haven’t just settled for problem reduction.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In contrast, progressive problem solving starts with using new and additional
            mental capacity to pay attention ‘to other aspects of the problem that previously
            had to be ignored’. Teaching has no ceiling limiting your development. Teaching
            is enjoyable and frustrating because we’re just peeling back layers of complexity
            from an infinite problem. You might have reduced the time it takes to plan lessons.
            Now, focus on improving your explanations. You might have sorted the entry rou-
            tine so that the class comes in calmly and gets started. Now, make sure that what
            they’re getting started on is the best use of their time. You might have got into the
            good habit of asking questions to the whole class (not just those with their hands
            up). Now, work on the habit of listening carefully to what is said and responding
            accordingly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Bereiter and Scardamalia’s concept of investment is useful here. Progressive
            problem solving is about both reinvesting mental resources into new problems
            you’ve not worked on before and on searching out more complex versions of
            problems you’ve already been working on. Learning to teach is beset with new
            problems: you have assessments to mark and respond to for the first time, you’re
            introducing a new topic, you’re taking on the responsibilities of a tutor.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What does progressive problem solving look like?
            More complex versions of existing problems are sometimes harder to spot.
            Behaviour is an area with ever-increasing complexity if you look for it. It’s also at
            the top of the list when it comes to concerns for new teachers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The progression in
            problem solving might look something like this:
            - A new teacher might start by working on getting the attention of the class, set-
            ting activities and making sure students are focused on the task through moni-
            toring and reminders.
            - A more complex version of the previous problem involves managing a class’s
            behaviour whilst navigating trickier processes like modelling under a visualiser,
            explaining a new concept or leading a discussion. Behaviour was less of a prob-
            lem when students were busy but when expectations of them are raised and they
            have to focus on what the teacher or a peer is saying, a host of new problems
            emerge.
            - Having made progress at managing behaviour through more complex aspects of
            teaching, a new problem rears its head. Students are generally well-behaved, but
            they don’t seem motivated. The problems of managing misbehaviour have given
            way to the problems of promoting positive behaviour.
            - Whilst all this is going on, the teacher has had their first run-ins with a couple
            of the more difficult students in the class. The school’s behaviour system has
            helped and the teacher has practised using the language of choice: If you con-
            tinue to talk when I’m talking, you’ll be choosing a demerit.
            - The class is settled; routines are in place. In the event that a child misbehaves,
            the teacher has a process they use… But there’s one child none of this seems to
            work for. They are sent out every lesson. The school’s behaviour policy allows
            for this but the teacher wants to figure out why it is happening and what can be
            done about it.
            You could outline a similar progression for planning or giving feedback or for mod-
            elling an explanation as discrete components of your teaching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How do we progressively problem solve?
            Progressive problem solving can only work where you’re able to identify and work
            on a small number of problems at any given time. Sometimes next steps are obvi-
            ous; sometimes they are far from it. I want to be clear from the start that this is
            something you do, at least initially, with the help of those responsible for help-
            ing you: coaches and mentors, university tutors, training staff, middle and senior
            leaders. Usually, there will be, at least for a time, a single person who is providing
            you with support. What follows is not a guide to leaving those pillars of support
            behind. Instead, this process reflects how you interact with those people and your
            own development.]]>
			</paragraph>
			<paragraph>
				<![CDATA[1. Problems with clear solutions.
            Sometimes we struggle despite knowing the solution. It might sound strange in
            a book about knowledge, but knowing the answer in and of itself is not enough.
            Whatever the answer, it needs to become embedded and automatic. Many
            teachers know they need to do something but struggle to change it. Ingrained
            habits, even in very new teachers, make change difficult.
            What are you trying to achieve? Maybe it’s about the atmosphere of work
            in your classroom. Silence for short periods in the lesson could be a massive
            victory you’ve been working to achieve. Spoiling that silence by talking over it
            is incredibly common. You know when you start to talk over a fragile silence
            or get drawn into a conversation – even a productive one – with a student too
            quickly, you’ll lose it. You know – and you’ve also received feedback – that you
            need to shut up for a minute and just scan the class from the front. A post-it
            note on your desk that just says ‘Be quiet’ can help; a colleague at the back
            holding up a hand for a moment by way of reminder is also useful.
            The gap between knowing and doing is traversed by automating and embed-
            ding solutions you know exist. Chapters 4 and 5 will help focus on how you
            can do this.]]>
			</paragraph>
			<paragraph>
				<![CDATA[2. Problems without clear solutions.
            What is a struggle for you right now? What seems like an uphill battle? These
            will be problems without immediately obvious solutions, or at least not ones
            you feel capable of implementing currently. No-one expects you to be able to
            solve every situation in the classroom right away. You are coached and taught
            and trained because you need help to understand the problems you’re facing
            but you also need help to understand and implement the solutions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Tension exists, therefore, between the independence of a new teacher devoting
            time to solving problems with unclear solutions and the direction they receive
            from mentors and trainers. It’s true that new teachers struggling to fit in planning,
            assignments and rest should not devote additional time to finding and solving new
            problems. The struggles you’re facing are the problems you should seek to solve
            first. Progressive problem solving is still a useful lens through which to view early
            teaching practice.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It’s likely you’ll be receiving coaching regularly. This coaching is designed to
            help you arrive at actionable next steps to help you improve. In a way, coaching
            gives you solutions to problems you may have been unaware of. You could feasi-
            bly wait for feedback and improve by responding to this as you receive it. But just
            seeing your development this way is likely to lead to a complacent fatalism where
            improvement is dependent on waiting for feedback.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whether we’re accumulating knowledge in the lecture theatre, the classroom or
            the staffroom, we’re building mental models. These mental models are frameworks
            of knowledge to support and guide our actions in the real world. Understanding
            mental models helps us understand why knowledge is important (and begin to
            understand which knowledge is important). We build knowledge that faces the
            problems of the classroom. We build knowledge to solve those problems but also
            to make it easy to solve new problems or more complex versions of the ones we’ve
            already tackled.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Equipping a new teacher, or any teacher, with the tools to progressively problem
            solve is about giving them ownership over their progress. Know your job as a
            developing teacher: develop mental models and search for solutions to the prob-
            lems you face. You aren’t simply a receiver of teacher training and development.
            Other people can’t develop your knowledge. The best coach in the world can’t
            automate processes for you. Tracking the problems you’re facing, however small,
            will give a sense of progress and control over your development.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Know your aim. The aim of experts-in-training is knowledge and automation.
            Through these things, we overcome the limits of working memory. Knowing lots or
            automating a lot doesn’t necessarily make you an expert. You can know irrelevant
            things or automate processes poorly. Expertise requires these things to flourish.
            Part of our challenge in the sections that follow is to define the expert knowledge
            of the teacher and look at ways we can develop it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[
            I’ve visited the opticians for pretty much as long as I can remember. The optician’s
            favourite question tends to be, ‘Is your eyesight better like this or like this?’ as they
            change the lenses in the mechanical monstrosity they’ve sat on your head. I always
            found this question difficult to answer. Sometimes the changes between the lenses
            are almost imperceptible but, as you work through them, gradually you reach a
            surprising level of clarity. As you develop knowledge as a new teacher, you reach a
            level of clarity that felt unattainable just a short time previously. But, with all this
            talk of knowledge, it would be reasonable to ask Which knowledge?]]>
			</paragraph>
			<paragraph>
				<![CDATA[To consider ‘best bets’ when it comes to knowledge, this chapter is going to
            make four points:
            1. Teaching doesn’t change as much as you think.
            2. Knowledge helps us to navigate the tension between theory and practice.
            3. It is useful to codify some of this knowledge into categories or problems.
            4. Categories and problems help us to focus our development.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching doesn’t change as much as you’d think
            Not that long ago, a manual for new teachers predicted how technological advances
            would reshape schooling by 2015. The author asked us to ‘imagine’ turning up at
            a ‘newly built school, sponsored by a local software company, on a Monday morn-
            ing’. How would the dizzy technological heights of 2015 shape the teacher’s day?]]>
			</paragraph>
			<paragraph>
				<![CDATA[The author suggests several changes to the average teacher’s morning:
            The first thing you do is to download the homework that has been emailed
            to you over the weekend, along with one or two excuse messages. You check
            over your slide presentation that has been put together for you from your
            notes by the department’s ICT assistant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Technological advances haven’t simply improved the admin tasks of the teacher.
            The ways students learn and interact with their teachers and with each other are
            also irrevocably altered:
            [You] set off to greet your Year 10 tutor group who have been working on their
            personalised learning programme. Six of your group have registered remotely
            from the industrial unit where they are on a work-based learning placement.
            The rest of the group are busy checking the e-conference noticeboard…]]>
			</paragraph>
			<paragraph>
				<![CDATA[It’s not so much that the predictions are wrong – some are close – but underlying
            this vision is the belief that technology would revolutionise school rather than
            just bring new complications. Whilst homework is now often handed in online,
            the idea that your department has an ICT assistant to make a PowerPoint presenta-
            tion is fanciful. The writer also reveals his beliefs about what a child’s education
            should involve and it isn’t a curriculum set by a school or teacher. Students are
            engaged in ‘personalised’ and ‘work-based’ learning. Here, the teacher’s knowledge
            is of technology and its uses, and perhaps the skills students need to enter the tech-
            nological workplace. Knowledge of subject or pedagogy has become less important
            in this imagined scenario, where the teacher is more of a supervisor than educator.]]>
			</paragraph>
			<paragraph>
				<![CDATA[School has the habit of changing a lot whilst, simultaneously, not really chang-
            ing that much at all. Is this a problem? Many will bemoan the old-fashioned,
            ingrained approaches. Hand-wringers make easy comparisons to the Victorian’s
            love of desks in rows and rote learning content devoid of relevance. Why aren’t
            schools modern, like the ones described? If you think about it though, the lack of
            change in schooling is encouraging for the new teacher. You’re entering a relatively
            stable environment.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When we think about the cutting-edge technology responsible for enriching our
            lives, we’re likely to think about our newest purchases. Shiny devices we’re proud
            to own. Economist Nassim Nicholas Taleb offers an alternative view.3 The tech-
            nology we treasure will soon be out of date, in need of an upgrade, and new and
            shiny ideas will soon be replaced by newer and shinier ones. Anti-fragility, Taleb
            explains, is the concept by which disorder and pressure strengthen a system, idea
            or person. Fragile things break under pressure. Robust things cope under pressure.
            Anti-fragile things become stronger as pressure is applied. A knife and fork, or the
            wheel, are anti-fragile technologies, particularly when compared to the technology
            we use which will be soon out of date. They have endured the pressures of time
            and remain useful. In some cases, we’ve found new uses for them or designs have
            improved with age. In contrast, the laptop you used five years ago is probably
            rage-inducingly slow and liable to turn itself off if you expect too much from it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What does this have to do with our best bets for teacher knowledge? Ideas are
            like technology. Taleb contends that ‘the old is superior to the new’, at least in its
            power to work long after newer ideas are dropped. Subject distinctions, sitting
            children at desks, explaining things to a class, having students answer questions
            and complete tasks, rules and order can all feel outdated, a landscape ripe for rev-
            olution. But we erase what already exists in teaching at our peril. Teaching does
            change but the core tasks and activities of the teacher remain largely static. The
            aim of teaching is naturally progressive – social mobility, changed minds, broad-
            ened horizons – but the activities of teaching are largely conservative (with a small
            c) – the passing on of knowledge, inducting students in the traditions and norms
            of a society, defining and exploring subject domains, preparing students for the
            world of work.]]>
			</paragraph>
			<paragraph>
				<![CDATA[These activities imply the jobs and tasks that make up the daily life
            of the teacher:
            - Planning lessons and activities.
            - Managing class behaviour so that they can complete these activities.
            - Explaining concepts, tasks or examples.
            - Asking questions and leading discussion.
            - Helping students who are stuck or don’t understand.
            - Giving feedback.
            And so on.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Now, you might disagree with the conservative view of teaching. The pro-
            gressive seam of education has produced some excellent thinkers who would unpick
            what I have said already. You might agree with Romantic philosopher Jean Jacques
            Rousseau that teachers should give children ‘no verbal lessons’ and that students
            should be ‘taught by experience alone’. Or, like giant of educational philosophy
            John Dewey, you might think teachers should ‘never educate directly, but indirectly
            by means of the environment’. Or you might, like anti-authoritarian Paolo Freire, be
            struggling with the belief that ‘There’s no such thing as neutral education. Education
            either functions as an instrument to bring about conformity or freedom’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[These thinkers are worthy of our attention. They can open our minds to alterna-
            tive views of teaching but generally don’t offer practical advice. They offer a per-
            spective but not the reality of day-to-day life as a teacher. They offer a revolution,
            describing how education could be, without the roadmap of how we get there.
            It’s not that their ideas haven’t shaped education. Each thinker has been influen-
            tial. In some cases significantly so, but the fragility of their ideas means they are
            often picked up, attempted and then dropped when we realise they don’t work
            in practice. Few schools are teaching by ‘experience alone’. Lots of teachers have
            attempted to teach ‘indirectly’ – allowing students to discover content for them-
            selves – and found it close to impossible.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Because the core activities of teaching don’t change rapidly, we can alight on the
            knowledge best bets for new teachers. New teachers need the knowledge that helps
            them to carry out the role and responsibilities outlined above. You won’t find any
            predictions about the future in this book. I’m too embarrassed about the certainty of
            getting them wrong. In reality, though, I’m not doing anything that different. In asking What Do New Teachers Need to Know? we’re looking at what will stand some of
            those tests of time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowledge helps us to navigate the tension between theory
            and practice
            It’s not that you can’t expect revolution from your career in teaching. Teaching
            might feel at odds with this apparently conservative sentiment to stick to how
            things are. You may well witness, or be an instigator of, revolution in education
            through your career. More likely, however, is evolution, a graduated development
            of what works well already and what doesn’t. Therefore, because it gradually
            evolves, the knowledge teachers need is not ever-changing, but broadly consistent.
            Teachers, and especially new teachers, need to develop knowledge of their subject,
            of pedagogy and of behaviour management and more.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Theory is all the new teacher has to go on when starting out. By theory, I mean
            both the lofty ideas and evidence you’re taught – a theory of learning – and the
            strategies you’re pondering – how to get that reluctant but winnable Year 5 boy
            onside. Ideas for the new teacher are all theories to begin with. Suggestions and
            training about how you plan lessons or manage behaviour or lead class discussion
            are all theory until you practice them. Sometimes these ideas, these theories, are
            useful. Sometimes they aren’t. But your job, in the early days at least, is to accu-
            mulate and test the theories. This is a complicated process because the classroom
            is a complex environment.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A theory you might be encouraged to try is Start each lesson with an engaging
            ‘hook’ for the content. A colleague might tell you this in passing or a lecturer might
            dwell on the methods of doing this. If you take this theory to mean, do some-
            thing extravagant and surprising at the start of each lesson, you’ll find the theory
            exhausting and unsustainable in practice. When you enter the class dressed in a
            toga to introduce a topic on the Romans, you notice unhappily that whilst the chil-
            dren find this funny, it doesn’t seem to prepare them for learning. They remember
            the lesson when Sir wore a dress! but they don’t remember much about the topic.
            You drop the theory. But perhaps your theory of ‘engaging’ hooks is grounded in
            planning a question to intrigue students, prompting thought at the start of each
            lesson. You try it, noticing that these questions do help to start lessons, preparing
            the way for deeper thinking.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the start of your career, it’s worth focusing on knowledge building. Knowledge
            building includes knowledge of the theory – from lectures, training, reading and
            advice – as well as the practice of how these things are implemented – from obser-
            vation, teaching episodes, first lessons and coaching. Preferable to going it alone,
            accumulating theories and testing them in practice is a valuable way of seeing your
            job as a new teacher. When a theory – an idea prompting a strategy to use in a les-
            son – doesn’t work, we can go back to the source, if we have them available, and
            ask for clarification or support or coaching. We can reflect and refine.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Taleb describes how knowledge ‘feeds on attempts to harm it’. For example, if
            you use a behaviour management strategy you’ve been taught and it doesn’t work,
            you’ve gained knowledge. Perhaps, you’ve been told to give take-up time, a brief
            period for students to follow your instructions without losing face by having to do
            something the second you asked. You try this with a particular student but it makes
            things worse – they’re more disruptive in the take-up time. It’s easy to feel sensitive
            or like a failure in these moments – I had a strategy and it failed – but what you’ve
            experienced strengthens your understanding of that theory in practice. Take-up
            time could still work for you but with clearer expectations and timeframe, or it
            might not work with that student in the way it works with others. You go back to
            your mentor to discuss when take-up time will and won’t work.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We shouldn’t drop a theory, idea or strategy because it hasn’t worked once. But
            we shouldn’t work ourselves into the ground to make something untenable work.
            Some theories must be dropped. The purpose of developing knowledge is to find
            knowledge that works in practice. Too often, we’re encouraged to keep trying to
            put a theory into practice, when – as Taleb notes – theories come from practice.
            Successful teachers are better able to recognise the balance between the two –
            where theory meets practice, and how they interact.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is useful to codify
            some of this knowledge into categories
            or problems
            If knowledge is important, which knowledge? Whose knowledge? The debate this
            question prompts is not new. When I was a trainee, a lecturer asked us if we wanted
            to teach subjects or children. To me, the question implied further questions. Do you
            want to be a great teacher of children, with a knowledge of what makes them tick,
            how to engage them and how to build relationships? Or do you want academic,
            perhaps dry, and deep knowledge of your subject or subjects? The knowingly unfair
            either-or entirely misses what a teacher does. A teacher takes a subject, or in many
            cases subjects, and makes it accessible and understandable for students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[No one is really asking you to choose and to commit to your choice. But the
            existence of the question troubles me. Hannah Arendt was troubled too; in her
            1954 essay ‘The crisis in education’, Arendt worried that teaching had become
            ‘emancipated from the actual material to be taught’. To her, teachers had come to be
            seen as teachers of anything rather than teachers of something, complaining that it
            often ‘happens that [the teacher] is just one hour ahead of his class in knowledge’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Subject knowledge
            Knowledge, then, hovers in the background of teacher development – necessary
            but often misunderstood or maligned. Various educationalists have attempted defi-
            nitions or categories of teacher knowledge, trying to make overt what is often hid-
            den. In the late eighties, Lee Shulman7 argued that the link between knowledge of
            What are our knowledge best bets? 27
            effective pedagogy, our thinking about classroom behaviour, and subject knowl-
            edge was the ‘missing paradigm’ of teacher education. Lee Shulman chose to cate-
            gorise teacher subject knowledge as follows:
            - Content knowledge: The domain-specific subject knowledge of the teacher.
            - Pedagogical content knowledge: A combination of subject knowledge and the
            ways to apply it in the classroom.
            - Curricular knowledge: Understanding of the content and the materials of the
            curriculum.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In 2020, the Great Teaching Toolkit8 referenced Shulman’s categories of knowl-
            edge, highlighting their enduring relevance, whilst increasing the focus on the
            knowledge of the curriculum and knowledge of how students understand (and
            often misunderstand) what they are being taught. Between Shulman and the Great
            Teaching Toolkit, there are a host of studies looking at these categories: some look-
            ing at the impact of what teachers know, others examining how changing teacher
            knowledge changes (or doesn’t change) teacher effectiveness. As we look at dif-
            ferent categories of teacher knowledge, we’ll delve into some of these studies and
            what they can tell us about effective and impactful teacher knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowledge of Practice
            All this talk of knowledge can feel incomplete. Content knowledge alone, you
            might feel, is not a priority. More concerning is what to do in the classroom. This
            knowledge of classroom craft is a type of knowledge too. Increasingly, this craft is
            being informed by evidence – evidence from studies in school and from the field of
            cognitive psychology. This knowledge of pedagogy and classroom behaviour can
            be broken down and sequenced; it can be taught and then practised. Evidence of
            this is seen in the various ways teacher behaviour has been codified.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Barak Rosenshine wrote a seminal paper, ‘The principles of instruction’, on the
            classroom actions of the most effective teachers. The Making Every Lesson Count
            series of books defines the six teacher behaviours that lead to expert teaching.
            Doug Lemov’s Teach Like a Champion goes further, slicing teaching into a large
            set of concrete techniques whilst offering video content to emulate. A perennial
            criticism of these texts is that they state the obvious to the point that they aren’t
            helpful. But this criticism fails to recognise:
            - The power of getting simple things right all day, every day.
            - Getting the basics right frees up cognitive capacity and enables you to tackle
            more complex classroom challenges.
            - As Stephen Covey has said, ‘common sense isn’t always common practice’. Lots
            of teachers could benefit from attending to the simple and straightforward.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowledge of pedagogy or classroom craft reminds us that much of what new
            teachers need is a set of strategies for the classroom and a growing understanding
            of how to act in the classroom. These strategies form a central part of the mental
            models that we looked at in the previous chapter.
            Lee Shulman had a novel idea for how to ensure teacher knowledge moved
            from the theoretical to the practical. Much like our mental models, Lee Shulman
            describes ‘case knowledge’: ‘knowledge of specific, well-documented and well-
            described events’.9 Shulman envisioned case knowledge being constructed out
            of written descriptions or videoed sections of lessons. Case knowledge would be
            categorised and organised into a curriculum for new teachers, giving them con-
            crete examples of various situations that they might encounter in the classroom,
            and guidance on how to manage them. Unfortunately, you’re unlikely to arrive at
            teacher training to find a library of cases to work through.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Case knowledge points at a gap that can exist in teacher training. When we
            train to teach, we do a lot of background work on subject and planning and theory.
            Sometimes, the foreground – what actually happens in the classroom – is left to
            take shape over time. Whilst we might not have access to a catalogue of cases, we
            can build them with a proactive approach to our relationship with colleagues.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Behaviour management is a good example here. Starting out, your mental model
            is an almost empty space, sparsely populated with principles, ideas and a school
            policy or system for managing behaviour. Experienced and expert teachers have
            a fuller picture of behaviour management, including a richer understanding of
            strategies to deploy as well as nuanced understanding of how to deal with spe-
            cific students. A novice mental model recognises the need to work through the
            school’s behaviour system with a difficult Year 6 student whilst maintaining high
            expectations. Doing this works: the deputy head arrives to support and the student
            spends some time out of the lesson. An expert knows and does all this too but also
            knows the student. The expert knows, perhaps, that this student will calm down
            and re-enter the classroom if they are given a job to do. The teacher sends them to
            collect some resources from another classroom. A mental model is never finished.
            We add to it with our own cases and nuance based on the knowledge we are always
            accumulating.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The limitations of categories
            Of course, the types of knowledge discussed so far can be expanded on or rewrit-
            ten. They aren’t definitive. There are other types of knowledge that are also impor-
            tant to the wider role of teacher. Some might add knowledge of students, or child
            development, or education policy. And the best teachers clearly aren’t those with
            more facts in their heads than everyone else. It would be misreading the research to
            claim that expert teachers are those who have mastered all the information in their
            domains. We’ve all encountered a teacher who knows a subject or pedagogical
            theory better than everyone else but is still a bad teacher. Expert knowledge is not 
            a set of facts but more an indelibly fused web of content, theory and experience – a
            mental model – all working together to construct useful models for each aspect of
            classroom practice.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Categories of knowledge, however, come with a health warning. Professor of
            Education Mary Kennedy describes how, as a profession, we have ‘never reached
            agreement on any partitions’10 of knowledge. Furthermore, the more we seek to
            cut up and categorise teacher knowledge, the more likely we are to ‘reach a stage
            where so many bodies of knowledge are relevant to teaching that the curriculum of
            teacher education becomes unwieldy’. This doesn’t mean we can’t usefully think
            about subject knowledge or pedagogy or cognitive science. However, illusions of a
            curriculum you can work your way through are tempting. If I’ve just studied all the
            right modules, I’ll be an expert teacher.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Kennedy explores the various attempts to categorise even part of what a teacher
            does. One attempted list of teacher activities includes over 1000 items and even
            then doesn’t manage to ‘distinguish what good teachers do, as opposed to poor
            teachers’. Where will this book succeed as others have failed? It won’t try so hard. I
            cannot, nor do I claim to, offer you everything when it comes to knowledge or solu-
            tions. This isn’t a taxonomy or full classification of teacher knowledge. The sec-
            tions on types of teacher knowledge are a starting point. A map is a good analogy
            for what this book can do insofar as it offers a route but there are always choices
            and detours, obstructions and possibilities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[You might wonder why it is important for you to know how teacher trainers have
            considered the categorisation of teacher knowledge. Does a new teacher need to
            worry about such things? Picture yourself, for a moment, in the landscape of your
            development. Depending on your present circumstances, that landscape might be
            a pleasant meadow or a volcanic dystopia. In either case, my goal is to give you the
            toolkit to navigate that landscape. Knowledge is your map insomuch as it helps
            you to face and overcome challenges.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Categories are a useful reflection point. What do I know and what don’t I know?
            But awareness of categories is not our destination. We need to go further. Knowledge
            building needs to be directed at our development as new teachers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Categories and problems help us to focus our development
            What do these categories mean for teacher development? Each section of this book
            examines a type of teacher knowledge. Each type of knowledge is useful insomuch
            as it is a lens with which to view our burgeoning practice. A lens is another way of
            viewing a mental model, with knowledge providing a new mode of ‘seeing’ your
            classroom. In this way, this book doesn’t guide you through the tasks of a new
            teacher: planning a lesson, completing an assignment, creating a seating plan, call-
            ing a parent for the first time. These are useful things to know about but the lenses
            of knowledge make up our narrow focus: creating mental models to face the per-
            sistent challenges of the classroom. Whilst real-life lenses aren’t anti-fragile – they
            crack and break under pressure – a knowledge lens will grow and last. Lasting
            change and improvement are our aim. A commitment to knowledge is a commit-
            ment to anti-fragile growth.]]>
			</paragraph>
			<paragraph>
				<![CDATA[From categories to problems
            As with our discussion of progressive problem solving, Mary Kennedy sees learn-
            ing to teach as learning to solve ‘persistent challenges’. Kennedy argues ‘that
            most observed teaching behaviours can be understood if they are characterized as
            addressing one of these challenges’. Framing learning to teach through the lens of
            challenges leads to a focus on solutions and strategies that teachers learn to apply
            in a range of settings.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Kennedy’s challenges are:
            - Portraying the curriculum. Teachers must find ways to make content ‘compre-
            hensible to naïve minds’.
            - Enlisting student participation. Kennedy frames student participation around
            the paradox that ‘education is mandatory but learning is not’. How, then, do we
            ensure participation that will lead to learning?
            - Exposing student thinking. Teachers must find out whether ‘students under-
            stand, don’t understand, or misunderstand’. Each day, in every lesson.
            - Containing student behaviour. Teachers must manage this behaviour ‘not only
            as a matter of public safety but also to ensure that students are not distracting
            each other, or distracting the teacher, from the lesson’.
            - Accommodating personal needs. For teachers, new teachers in particular, solv-
            ing these problems doesn’t mean you need to become someone else. All teachers
            must find ‘a way to address the first four problems in a way that is consistent
            with their own personalities and personal needs’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowledge is not accumulated for its own sake, though, but as a problem solving,
            direction providing resource. As we examine subject knowledge, we’ll begin to
            consider how we use it in planning to solve the problems of portraying the cur-
            riculum or exposing student thinking. Evidence from cognitive science will offer
            strategies to enlist the right kinds of student participation. Best bets are those areas
            of knowledge that will help us in our early forays to solve these problems. Your
            knowledge, therefore, is not accumulating in a linear fashion.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The problem solving knowledge we require will vary:
            - Pre-emptive knowledge, developed for problems you know you will face. You
            can prepare to solve these problems by pre-emptively developing knowledge of
            the curriculum or banking some classroom management strategies.
            - Reactive knowledge, developed in response to problems surfacing from day to
            day and week to week. Because teaching is so complex, it’s not definite which
            challenges you will face initially. There is some logic in starting with the knowl-
            edge that will support positive behaviour and implementing routines. You can’t
            get much done if your classroom is chaos. Beyond that, certain classes or topics
            or teaching techniques might be causing you difficulties.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching, therefore, involves solving persistent challenges. These challenges are
            unlikely to change. In turn, the knowledge of how to solve them is likely to expand
            but not drastically change. New teachers develop this knowledge. The knowledge
            becomes a theory The theory is applied and tested. Anti-fragility comes back into
            play. For Taleb, our knowledge grows and strengthens under pressure if, and only
            if, we learn from our errors. In classroom problem solving, trial and error is ‘not
            really random’ but based on the rational application of knowledge, ultimately
            ‘what we think is wrong’ (or right).11 Our successful solutions to Kennedy’s prob-
            lems, however we come by them, are anti-fragile. We can only learn from their
            application.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching is both simple and complex. It is a series of problems and a wealth of
            solutions. You can spend your working life unpicking and refining and practising
            your way through those problems and solutions. None of that should hold us back
            from now stepping into the classroom and doing our best for the children in front
            of us with what we know now. That is the simplicity of it. Deep oceans of com-
            plexity swell beneath you and still, even as your knowledge of the route is in its
            infancy, you can chart a course.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Expertise is built on the accumulation of knowledge in long-term memory and
            the automation of process. In themselves, these things aren’t expertise but they
            intertwine to create mental models – ways of seeing situations and understanding
            possible courses of action. Mental models help to reduce cognitive load on us and,
            in turn, we can reinvest our freed capacities to continue to solve the problems
            and face the challenges of the classroom. When it comes to which knowledge we
            should prioritise for our mental models, those categories of enduring use – sub-
            ject, pedagogy, practice, behaviour, knowledge of students themselves – provide a
            wealth of classroom solutions for us.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
            Make aims specific. General aims are hard to achieve and can be unhelpful. The
            aim to be better at questioning is unlikely to yield results because there’s little
            there other than a vague intention. A narrow focus – on putting students’ names
            at the end of your questions – can be practised and a bit of self-awareness, a video 
            or audio recording, or someone in your lesson can give some immediate feedback
            on how successful you’ve been. The same is true of knowledge about our subjects.
            Feeling your teaching would improve if you knew more about human geography
            doesn’t have the same focus as the desire to better understand a culture or case
            study you are going to teach later in the year.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My favourite teachers taught my favourite subjects. Mr Baker, an English teacher,
            was ex-army. Occasionally, he teased us with fragments of stories from life in the
            military but mainly he just worked us hard. And as you’d expect, he was strict but
            we didn’t mind. I saw him once over a decade after I’d left school but such was my
            reverence for him that I couldn’t bring myself to say anything.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mr Doran, a history teacher, was kind and warm and full of knowledge. My
            enduring memories of his lessons are the discussions and debates about history
            and interpretation – his regular invitation to hear what we thought and only gentle
            rebuke when we came close to error. His monologues about a moment in time or
            a book he’d read fascinated me. As a teacher, I’m not much like Mr Baker or Mr
            Doran but I’m grateful for the experience of their classrooms.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Your image of a good teacher will not only be shaped by your training or your
            tentative first moments in the classroom. Like me, you might call to mind a par-
            ticular teacher. Even when we have no positive examples, we draw on a wealth
            of experience. As Mary Kennedy points out, ‘Learning about teaching is different
            from learning about any other occupation, in that our learning begins when we are
            children’. We have all, Kennedy goes on, ‘spent roughly 12,000 hours watching
            teachers through our child-eyes, developing our own conceptions about what the
            job entails and what makes some teachers better than others’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The journey to teacher expertise is clearly not a given just because you have, at
            one time, been taught. Action and activity – yours and those training you – drive
            that journey. Yet it would be foolish to ignore or suppress what you already think
            and believe about teaching. Knowledge you acquire through training isn’t written
            on a blank slate. At times, this knowledge will support the construction of your
            image of an ideal teacher. At others, you might be challenged to re-evaluate this
            image based on new understanding or fresh experience. Bear that in mind, as we
            look at ways to develop knowledge here.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Although you are a novice, I am certain that you know lots already that will
            help you to reach expertise. Your part in that development is to be clear on what
            you know already and how you will use what you know. Even if you do have a 
            clear mental picture of what good teaching is, you need to start at the beginning.
            Your favourite teachers probably didn’t become the ones you really loved by just
            stepping over the threshold of the classroom. In Part 2, we’re going to examine the
            foundations of how teachers learn.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In Part 1, we saw that the essential ingredients for expertise development were
            large quantities of knowledge in long-term memory and automation of effective
            processes. Together, knowledge and automaticity help to create our mental models
            of effective classroom practice. These mental models help us to progressively prob-
            lem solve and to tackle those persistent challenges of the classroom.
            This part answers the questions:
            - How do we retain knowledge in our long-term memory?
            - How do we automate effective processes?]]>
			</paragraph>
			<paragraph>
				<![CDATA[A student asks you a question and you don’t know the answer. How do you
            respond? Talk about something else. Talk around the question. Tell them, I could
            tell you but I want you to work it out for yourself. It’s up to you but I don’t know is
            not a bad option. When we can’t answer a student’s question – from the definition
            of a word to the motivations of a historical figure – it’s quite clear we’re lacking in
            knowledge. The same is also true, however, when we struggle to mark assessments
            because the mark scheme appears to be written in another language. It is true when
            we hesitate to give a consequence because the behaviour policy hasn’t fully crys-
            tallised in our minds. It is true when we struggle to articulate an answer to How
            does this relate to what we were doing before? In each situation, our minds reach
            for knowledge that is absent or undeveloped.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If experts have large quantities of knowledge in long-term memory, our first
            task must be to understand how to retain such knowledge. The mental models we
            develop shouldn’t just consist of what we teach or how we act in certain situations.
            A mental model of how we learn as new teachers offers a valuable insight into how
            we can spur on the improvements we are making. We are, therefore, looking at any
            activity where your aim is the retention of the knowledge that will help you to
            become an effective teacher. This is not so much about ‘studying’ for an exam that
            isn’t coming; instead, it’s about awareness of what you know, how you use it and
            whether you’ve retained it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I hope, therefore, in talking about knowledge development it’s clear that we’re
            not just talking about an optional extra. This book is not just for those who are
            happy to give up evenings and weekends to extra-curricular study activities.
            Knowledge activity is everyday activity for teachers. But there is a problem. You
            don’t have unlimited time or resources – you’re already teaching or training to
            teach. Waiting might not seem practical but spending your evenings deep in study
            isn’t sustainable either. Our aim then is to find manageable things you can do to
            focus your energies early in your career.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Examples of such knowledge activity might include:
            - Reading up on the background of the topics you are about to teach.
            - Examining curriculum materials to make sure you understand what must be
            taught.
            - Training or reading on effective or evidence-informed classroom practices.
            - Getting to know students generally (understanding of student needs or how stu-
            dents learn) or specifically (the students you will be teaching).
            - Applying knowledge of specification or statutory assessment to planning and
            curriculum materials.
            - Planning with the information you’ve gained in the above activities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To ‘build knowledge’, the Education Endowment Foundation’s guidance report
            on Effective Professional Development emphasises the need to manage cognitive
            load and revisit prior learning.1 The principles underlying the strategies in this
            chapter are:
            - Managing cognitive load makes learning more likely.
            - Revisiting what you’ve learned makes retention more likely.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Cognitive Load Theory describes how conscious thought can only hold a small
            number (around four) of new items in your mind at any given time.2 Cognitive
            load increases when the number of items required for a task exceeds your ability
            to manage them. Learning to teach feels like a constant and unrealistic expectation
            has been placed on your cognitive capacities. In many ways, it has.
            Managing cognitive load is about ensuring we can cope with all the new infor-
            mation coming our way. As we’ll see, the ways we ‘revisit prior learning’ will
            determine if we retain or forget what we have learned. Although we might think
            of study techniques as things we teach our students, they can be invaluable for
            retaining the massive quantities of new information we take on as new teachers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Before we begin, it’s important to remember what this knowledge is for. A new teacher
            is forming a variety of mental models: a mental model for classroom routines, a men-
            tal model for teaching the Vikings, a mental model for the application of evidence to
            classroom activity. These models are networks of knowledge we call on to act in
            complex situations. New teachers also face a whole host of problems or challenges:
            the problem of getting students thinking, of getting them to behave, of making content
            understandable and memorable, of checking what they do understand. The idea that
            teachers should develop knowledge or retain knowledge can feel overwhelming.
            Where do I start? With a coach, or on your own if you’re able, work out which
            knowledge will help solve the most pressing problems now.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Narrow your focus
            A sense that you could know something better or do something better is unhelpful.
            We narrow our focus so that we can move beyond good intentions and into genuine
            improvement. That improvement is won incrementally. Cognitive overload can be
            expected for the teacher who decides to improve their questioning in multiplica-
            tion lessons whilst writing new resources and attempting to gauge student under-
            standing. If all of those require improvement, focusing on one at a time is essential
            to avoid floundering.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We also narrow our focus to prioritise immediate needs. Initially, to prioritise
            effectively, help will be essential. A standing item in conversations with your men-
            tor or coach should be devoted to what you are prioritising and what you are, for
            now at least, ignoring. If you aren’t teaching an exam class immediately, the neces-
            sary job of getting to grips with the specification can be delayed in favour of your
            background reading on an upcoming scheme of work. If a class isn’t behaving, it’s
            no use trying to nail a new questioning technique that would require impeccable
            behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Your training programme or the Early Careers Framework will dictate some pri-
            orities by setting assignments and requiring reading or study. Compare your pri-
            orities with these requirements. Ask your mentor what should take precedence. 
            You might want to get to grips with the world of research and cognitive science,
            but your mentor could tell you a more urgent priority is knowing the policies and
            procedures at your school.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perhaps you decide to devote several mornings next week before school to a
            tricky class you’re teaching right now. A class with complex needs and characters.
            The content itself doesn’t trouble you; there are prepared resources and you’re
            happy they’ll work for you. Focusing energy and time on learning names, needs
            and, perhaps from a more experienced colleague, how to approach some of the
            difficult students is likely to reap rewards.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Narrowing your focus means wilfully, if temporarily, ignoring some things.
            Selecting priorities demands we ask the question What am I leaving out? Learning
            to teach is intense; developing expertise will extend beyond the training or early
            careers years. At times, all your time and energy will afford you is the ability to get
            done what must get done. Play the long game. We narrow now to develop breadth
            for the future. Be patient as you plan future priorities.
            Ask yourself:
            - Is this my priority, the thing that will have the biggest impact in tackling my
            most urgent problems?
            - Is there any way to cut this task down further?
            - What other tasks or goals are competing for my attention?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Amy, a new Year 6 teacher, has been working on honing her Reading and Maths
            knowledge. She is beginning to feel like she’s getting to grips with the mammoth
            expectations of the National Curriculum, at least in her year group. But there’s a
            problem. She has to teach music for the first time next week. She doesn’t know the
            curriculum or the school’s resources. More to the point, she doesn’t know music.
            She doesn’t have rhythm. Even clapping in time poses pretty serious problems for
            her. Thinking about it though, she realises her feelings of musical inadequacy are
            getting in the way of prioritising properly. To narrow her focus, she doesn’t need to
            be become a musician by next week. No, Amy needs to understand the materials
            and aims of the first lesson, plan it and then teach it. Maths and English won’t be
            left entirely – Amy will still need to teach these lessons. But music will become the
            planning priority in the next week.
            Ask your coach:
            - I think X is my priority. Does that sound right?
            - Is there anything I can stop doing or do less of to make X my priority?
            - What else should I not ignore or remember whilst working on this priority?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Dan, a secondary Art teacher, is concerned about his new class’s behaviour. He
            goes to his coach after a particularly difficult lesson to try to figure out what to do
            next. Dan knows behaviour is his priority but this doesn’t feel helpful or narrow.
            The coach asks Dan about the lesson and, after a bit of back and forth, Dan realises
            he never really got them settled when they came in from lunch. They work together
            to plan a new routine to manage the post-lunch entry.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A strategy to manage both time and cognitive load, a granular list of the things
            you want to know better can be incredibly helpful. Include everything. Break
            everything down into component parts. Don’t write down Electricity unit if this
            is a topic you are concerned about teaching. Write down each component part:
            include the specific things you need to do to improve your knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you’ve written Learn about the Industrial Revolution at the top of your list
            because that’s Year 6’s next history unit, you might be no closer to adding knowl-
            edge to your long-term memory. Spending a precious PPA trying to immerse
            yourself in the Industrial Revolution by frantically Googling ‘Year 6 Industrial
            Revolution lessons’ is time badly spent. Instead, you might write Read scheme of
            work, Check available resources on the shared drive, List questions (for your men-
            tor), Read the article Jen shared. Once you’ve broken the process down, you need
            to make sure you devote time to them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Blocking is the time management process where you divide your day into blocks
            and schedule activities for those times. Like lots of time management strategies,
            blocking can feel galling to teachers. So much of your day is not yours to direct. For
            a primary teacher, blocks might only be available at the beginning and end of each
            day and during your limited PPA.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Primary or secondary, blocking won’t create more time for you but it remains a
            useful way of making sure your week or your PPA doesn’t get swallowed up with
            just a couple of activities. Jobs have a habit of filling the time we allow for them. If
            you have one hour on a Tuesday afternoon, devote half to admin and deliberately
            move on halfway through. That second half can be devoted to the background read-
            ing for your next topic. Or give the admin the small amount of time you have at the
            beginning or end of the day and devote the entire PPA to knowledge building.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Think hard
            As cognitive scientist Daniel Willingham says, ‘memory is the residue of thought’.3
            That we remember what we think about sounds obvious but it’s easy to think
            superficially about what you want to remember. It’s also easy to confuse activity
            with productivity. Or getting things done with intellectual engagement.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thinking hard only works if you’ve narrowed your focus onto some useful con-
            tent. It’s also not possible to think hard about everything at once; you need to have
            broken down the content into something manageable. Once we’ve done that, we
            can start to think about the kind of thought we devote to the content.]]>
			</paragraph>
			<paragraph>
				<![CDATA[All teachers need deep understanding of the content to be taught. It’s easy to
            confuse thinking about content with thinking about resources or activities. If
            teachers think hard about the resources they’re designing, they might arrive in
            the classroom with only superficial understanding of how to explain the subject
            content. Of course, activities need to be planned. This is not an argument against
            careful thought about what students will do.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mary teaches Politics A-Level in a sixth form but her degree is in International
            Relations. This works for most of the A-Level course but the minutiae of local gov-
            ernment is not something that excites her. More pressingly, she doesn’t know any-
            thing about local government and lessons are coming up on that very topic. She’s
            starting to feel anxious about filling the time. What will students do? And what can
            they do that won’t reveal this is a massive gap in her knowledge? She breaks down
            the process and separates reading and preparation from planning the lessons. She
            spends an afternoon after school reading through a textbook, a revision guide and
            a couple of articles recommended by another teacher. Her sole purpose is under-
            standing the content well. In this way, she thinks hard about the content rather than
            what needs to happen next Wednesday morning when she next sees her group.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To engage with the knowledge we want to understand, we need to devote atten-
            tion to it. But we also need to know what it is]]>
			</paragraph>
			<paragraph>
				<![CDATA[Distributed practice, sometimes called spacing, is ‘a schedule of practice that
            spreads out study activities over time’.4 Putting increasingly long gaps between
            study of a particular topic or process will lead to better retention. Opposite to dis-
            tributed practice is massed practice or cramming. Cramming the week before you
            teach a new topic is only going to lead to deficient knowledge. You’ll only realise
            the extent of the gaps when you’re in front of the class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Your aim is to return to content again in order to remember it. Student names, SEND
            support plans, spellings and definitions you struggle with. It’s unlikely any of these
            will be mastered in one go. Even the plot of stories you know well can feel elusive when
            in front of a class, managing behaviour and preparing students for the next activity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Research isn’t definitive on how long the gaps should be between study ses-
            sions. Instead, the evidence about distributed practice is an important reminder to
            plan ahead when you need to learn something new. Fortunately, the school year
            offers pre-determined deadlines. You can always be working towards something.
            Spaced reading and re-reading can be scheduled throughout a term and will do
            more to keep knowledge in your head than massed or crammed reading. In this
            way, teaching is naturally distributed. Each time you plan in a learning sequence
            you can return to (and retrieve) what you did last time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Jamie is an early careers D&T teacher. He teaches half of Year 9 – 90 students
            across four classes. The students are making bird-houses to go up around the school
            site. Jamie has broken the task down into its component parts. The first thing he
            wants to do is explain and model the measuring and marking wood for cutting –
            his first major practical with a class. He is going to do this four times over a fort-
            night. He writes out the explanation and practises it twice to himself. He does it for
            the first time with a nice group. He gets the technician to watch carefully and jump
            in when necessary. Jamie knows he can do better and practises it again before the
            next class (and every class after that). By the fourth and final time, he has practised
            outside lesson time over ten times and done it three times in front of a class. The
            final explanation is the best – fluent and easy but also built on the learning and
            tweaks Jamie has made from his first tries.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Retrieve
            Trying to recall a piece of information (something you’ve just been taught, read or
            already know) is called retrieval or retrieval practice. Retrieval practice describes
            ‘the fact that information retrieved from memory leads to better performance on a
            later test’.5 In other words, ‘retrieval can slow forgetting’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you want to remember something for a lesson or for future planning, trying
            to recall it without prompts or aids is likely to help you retain that information.
            You could explain a topic out loud to yourself without looking at notes (as you’ll
            have to in the classroom) and then check you included everything you wanted to.
            Making flashcards or a self-quizzing sheet7 for a scheme of work with a lot of con-
            tent which is new to you can be a useful resource to return to.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here, the line between studying like your students and preparing to be their
            teacher might feel blurred. Many new teachers, however, get to the board and
            realise they’re not sure of a spelling of an essential term. Or you forget the name
            of a student who is talking constantly at the back and your strategy for address-
            ing this crumbles. Or a student question stumps you, the answer on the curricu-
            lum documents you printed off and left in the office. None of these are things to
            feel ashamed of. Cognitive load presses heavily on your ability to do everything
            required of you in a lesson. Retrieval, as well as distributed practice, both sup-
            ports your memory and boosts your confidence because of a growing sense that
            you know this.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Carrie has always struggled with spelling. She now teaches Year 5 and it feels
            like they are constantly asking how to spell something. It’s not that she doesn’t
            know how to spell anything they ask her for. It’s just that, put on the spot, she’s
            likely to freeze. Carrie finds it easier to write things out but she wants to be confi-
            dent helping children as she moves around her classroom. Her school’s curriculum
            has a vocabulary list of words students need to learn, both spellings and defini-
            tions. She turns the vocab list into a quizzing sheet. She practises the spellings for
            ten minutes a couple of mornings a week before school. Carrie wants to be able to
            say and spell the words; she mixes practising saying the spelling out loud with
            writing out the spellings from memory. With time, she begins to feel confident and
            notices an improvement when asked for spellings from students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Self-explanation involves ‘explaining how new information is related to known
            information, or explaining steps taken during problem solving’.8 When novices
            lacking in the necessary domain-specific knowledge attempt self-explanation it
            can lead to cognitive overload9 so it’s unlikely to help in a situation where you
            don’t have the prerequisite knowledge. Where the new knowledge you’re learning
            fills a gap in or adds a layer to a topic you’re already aware of, it’s likely self-expla-
            nation could help. If you can narrate the links between the new topic and existing
            knowledge, if you can pose questions about it and answer them, it’s likely you’re
            working to retain this new information.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A physicist teaching photosynthesis for the first time can explain the concept as
            they work through lesson resources or a rehearsed explanation. A geographer teach-
            ing an unfamiliar case study can talk through it systematically. A Year 6 teacher teach-
            ing D&T when their confidence lies elsewhere can work through the concepts they
            will have to explain. In all these circumstances, you are still the expert. Undoubtedly,
            you already know more about the topic than the students you will be teaching. You
            aren’t checking you understand it by simply ‘thinking it through’, a recipe for a wan-
            dering mind. Instead, you’re checking for fluency, for gaps in understanding and for
            your ability to simplify and distil what is necessary for students to understand.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Accept (and seek out) support
            Initially, some new teachers are desperate to put in the work in every area them-
            selves. Resources exist but you want to put your spin on them. A system of online
            homework makes setting and checking homework simple but you want to design
            your own tasks. A textbook gives you a base level of what students should know
            and do but you’re concerned using it is a cop out. The school’s behaviour policy
            gives clear guidelines of how teachers should manage difficult behaviour but you
            want to spend time writing your own rules with a class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The mental resources required to engage in all these activities are a drain and
            despite your best intentions the quality in these areas will suffer if you try to focus
            on all of them. More importantly, you won’t develop knowledge in the same way.
            You’ll likely struggle to retain what you’re working on because you’re trying to
            work on everything. To manage cognitive load, and your time, you need to recog-
            nise early that perfection is not attainable. This does not mean we don’t work hard
            or do a good job. It does mean that we outsource thinking where possible to those
            who have already put in the time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[You could ask Do I really need strategies for remembering? Don’t I need strate-
            gies for understanding? Daniel Willingham explains that when it comes to our
            cognitive architecture, ‘understanding is remembering in disguise’.10 In order to
            deeply understand, we will have to commit some of what we learn to memory.
            Committing this to memory will also reduce our cognitive load, giving us an easier
            route to understanding new knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Why is learning to teach so difficult? The rapid rate of growth expected in subject
            knowledge, education theory, pedagogy, policy, classroom craft and more make for
            massive cognitive overload. To make study effective, you must manage this load.
            Often, you’ll have to reduce it. Embrace help and support available to you as you
            learn to prioritise.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Once teaching, it feels like there is very little time for study. Studying feels like
            something that happens before you become a teacher. But you can’t escape at least
            a kind of study. You can only do it well or badly. You can focus your thinking. You
            can practice retrieving knowledge (like nuggets of subject knowledge or student
            names). You can space out practice on a particular topic and return to it to make
            retention more likely. As we examine different areas of teacher knowledge, we’ll
            look at how to do all this as you go.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
            1. Break down a bigger task into its component parts. If you have a big planning
            task in front of you, an assignment to complete, or a new subject to get to grips
            with, just thinking about the process can be overwhelming. List the compo-
            nents in as granular a way as you can. You’re more likely to focus and retain
            what you’re thinking about if you’ve broken it down. Make a real list – physi-
            cal, in a Word document, in a fancy app that lets you tick off items as you go.
            Work through the list.
            2. Focus your thinking. Once you have a list, make sure any activity you carry
            out focuses your thinking on what you want to remember. Separating out
            tasks and directing your thought is challenging to begin with. It feels like it
            makes sense to do everything at once: plan lessons, learn content, get to know
            a class.
            3. Practise retrieving. Retrieval helps us and, as we’ll see, our students to
            remember. Choose something you want to remember in the classroom.
            Student names, spellings, processes, formula – all can benefit from retrieval.
            Remember, retrieval isn’t just reading through a list of terms or poring over
            pictures of students. Cover the content you want to remember and then say
            the content out loud or write it down. Check your answers and repeat the ones
            you get wrong.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching is all about relationships. Your day is made up of human interactions.
            You aren’t simply delivering knowledge and skill to the young people; it isn’t
            transmitted from your mind to theirs. Planning is preparing to communicate
            something – a topic, a process, a concept – to students. Primarily, this planning
            deals with the explanations, examples and activities that will help students to
            understand what they are learning. Part of the excitement of teaching is discover-
            ing how planning plays out in the classroom with real-life human children. The
            warmth and the humour of teaching emerge as we spend each day with these
            people.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Where does automation fit into this view of teaching? Surely automating the
            processes of the classroom is a cold way to view your interaction with students?
            There is another problem with automation: how can it work with the infinite
            interactions, situations and problems posed by the classroom. Can we really auto-
            mate anything useful for an environment which is constantly changing? Before
            we look at the practical ways to automate processes, it’s worth addressing these
            concerns.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Is automation a cold way to view teaching?
            It’s difficult to imagine a good teacher who doesn’t care for their students. That
            said, caring is often misunderstood. Caring is not the same as being friends with,
            nor do we have to care by going beyond the boundaries of the teacher–student rela-
            tionship. To care, we have to want the best for our students but we also have to do
            our best for our students. Doing our best can be confused with working harder than
            it is reasonable or healthy to expect a teacher to work. Caring for students doesn’t
            have to mean that all weekends and evenings are devoted to work. In part, caring
            for our students should be about doing what has a good chance of working in the
            classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Picture two classrooms. In the first, a Year 7 Science class, as the students come
            into the room the teacher asks them about their weekends, jokes with a couple of
            boys about their football team’s recent failure and congratulates one student on
            their performance in a recent concert. Once the lesson begins, the teacher fumbles
            through the explanation of a new topic. When the class move to practical work, the
            teacher hasn’t thought through the transition and, on top of being a little too lively,
            students have no idea what they are doing: the explanation was lost in the clatter
            of movement.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A similar lesson is happening in a second Year 7 Science class down the corri-
            dor. This time the class aren’t greeted in the same way; they enter quietly as is rou-
            tine and get started on an initial task. The teacher has practised an explanation of
            a new topic at her desk whilst planning by saying it out loud before making some
            final edits. The fruit of that planning is a short list of words, an aide-memoir, writ-
            ten in her planner but also the familiarity won from practising the explanation. Not
            only did this teacher practise the explanation, she thought hard about how to move
            from desks to practical. She wrote out the steps and practised going through those
            instructions, again out loud in the room by herself. Explanation and practical both
            go well but, looking at her notes, the teacher notices a couple of things to tweak for
            next time based on how the class responded.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Which teacher cares more for their class? Of course, it’s an unfair question. You
            can be a bit of both teachers. My point in posing the question is not really to force
            a choice but to be clear that caring as a teacher means more than the stereotype. If
            your classroom is more like the first, you can care more deeply for your students by
            automating some elements of your lessons. And yes, if your classroom is more like
            the second, you can show you care for your students by taking an interest in them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Is automation possible in a changing environment?
            Nowhere in the world is quite like a classroom. The atmosphere ranges from exu-
            berant debate to knife-edge tension to studious silence, often in the space of a sin-
            gle lesson. It’s true that students are individuals but classes also have a personality
            of their own. Even teachers who repeat similar lessons across a year group find that
            these lessons branch off in different directions with different classes. How then
            does automation meet the needs of this varied environment?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some events in the classroom are hard to predict. I hope I can’t be blamed for
            not expecting two boys to continue their snowball fight right into my lesson. But
            more can be prepared for than we might think: explanations, examples, models,
            classroom routines are all good places to start. Students will enter your room each
            day. You can automate your position in the room, perhaps by the door, what you
            say and what you expect of students in that moment.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Behaviour management is an understandable concern for new teachers and one
            where automation seems difficult. It isn’t possible to practise for every potential
            student behaviour but we can rehearse for different situations. If you’re speaking
            to a class from the front of the room, you can automate moving to a position where
            you can see the whole class and you can automate looking at each chair as you
            speak. For giving consequences when students don’t meet your expectations, a
            scripted response can help alleviate anxiety. A good behaviour management sys-
            tem often gives you the steps or language to rehearse until automatic.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Recognising that the classroom is a changing environment is not an argument
            against automation. Because the classroom is a complex environment, we make
            automatic those processes that will allow us to manage the complexity. We don’t
            automate processes to the extent that we sleepwalk through each day. We automate
            processes so we can respond more effectively to the routine as well as the tangen-
            tial or unexpected. Automation frees up mental capacity to attend to the problems
            of the classroom and apply our burgeoning solutions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The mental models we create of the classroom are not just made up of con-
            tent knowledge or theoretical knowledge. These models include the strategies we
            deploy in the classroom, and how we react to expected and unexpected moments.
            A mental model is not reality; it exists inside our head. But the fuller our model of
            the classroom – of the Year 5 classroom, the English classroom, the D&T workshop –
            the easier we find it to react in the moment.
            Let’s turn to look at the practical ways we automate processes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Observe
            Working closely with more experienced, knowledgeable or effective teachers is
            likely to make you better at your job.1 Early in your career, you are more suscepti-
            ble to the benefits of having expert colleagues around you. But what do we learn
            from colleagues? Some of that learning is focused on how to navigate the inner
            workings of schools: the politics and practicalities of the day to day. If, however,
            all we learn from our colleagues is how to unjam the photocopier, we’re missing
            out on an incredible resource.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching staff at your school are a cavernous library of accumulated wisdom
            and practice. Classroom craft, subject knowledge and pedagogical understanding
            don’t just – and don’t mainly – exist in what we read; these domains are alive in
            those we work with. A small number of new teachers see experienced teachers as
            out of touch or unaware of recent developments. This belief always totally misun-
            derstands what we can gain from our colleagues.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One study goes as far as to say that ‘learning from colleagues is at least as
            valuable as formal training’. In this study, teachers were placed in ‘skills-
            matched’ pairs: one teacher needed to work on a particular aspect of class-
            room practice, the other teacher had been identified as proficient in this area.
            Pairs discussed practice and observed one another. Teachers who needed to
            improve were accountable to their partners for the improvements made. Long
            after the study had ended, the improvements made in these relationships were
            still being felt.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you let it, being a new teacher can become a chain of these ‘skills-matched’
            pairs. You start out understandably deficient in a host of classroom activities. Your
            resources are the teachers around you. A mentor or leader at your school can point
            you in the right direction, towards the teachers with those strategies for behaviour
            management or questioning or explanations. Whatever it is you need.
            How do we make sure this kind of observation is useful?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Go for something specific
            It’s foolish to observe a teacher just because they are seen as excellent at everything.
            Beware of being told in hushed tones You should definitely observe Mark without
            any clear direction as to why.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Narrow down what you’re looking at with your mentor, coach or the person
            you’re observing. Ideally, this will be based on what you’re working on, the feed-
            back you’ve received recently or the problems for which you’re currently searching
            for solutions. Often we go to someone (or get sent to someone) who has everything
            sorted. Behaviour isn’t a problem; beautiful work is produced daily. We leave
            deflated because learning from what looks like perfection isn’t easy. Go and see
            someone managing a difficult class, someone in the process of changing their class-
            room routines, someone teaching some content for the first time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Looking at something specific helps you in two important ways:
            - It helps to manage your cognitive load. Trying to learn everything from a teacher
            in one go is too much.
            - It saves you time. You don’t need to come for the whole lesson. Ask roughly
            when the teacher will be doing what you want to see and turn up at that point.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Watch the teacher and the students
            It’s hard to know what you’re looking for when first observing teachers. Achievements
            that could be a triumph for you appear ordinary in other classrooms.
            Depending on what you’re looking for, consider making a note of:
            - What the students do without being asked.
            - The apparent routines or habits of the classroom (students and teacher).
            - What the teacher specifies about how students complete tasks; what is left for
            them to decide.
            - What happens in books (or wherever/however the work is being produced).
            - Anything the teacher does that doesn’t chime with your experience – something
            you wouldn’t do or find difficult to.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As you observe, collect questions about anything that doesn’t make sense to you in
            the moment. Base these on the notes you made using the prompts above. Watching
            the start of a lesson, you notice the teacher says almost nothing to the class and yet
            students come into the room, hand out resources and get started silently. The act of
            watching the lesson isn’t helpful on its own; the story of how the teacher arrived
            at that point can be.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To build knowledge of classroom craft, you need to ask questions of what you
            observe. Ask for those stories and you’ll begin to piece together the journey you
            might take from where you are to where you could be. You need someone to take
            the implicit and ongoing aspects of classroom behaviour and make them explicit.
            Note taking is a good idea but it’s difficult at first to unpick what you’re seeing.
            If you’ve been directed to watch another teacher manage whole class discussion,
            it’s worth going with the person doing the directing. They can provide useful com-
            mentary. As questions arise, ask them or debrief afterwards.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Know the limits of observation
            Observation is a useful way of building up a mental model of effective classroom
            behaviour. Until the observation, a particular strategy had felt too theoretical. Now,
            it feels both tangible and achievable. Observing teachers builds a sense of what
            your future might look like. Unfortunately, it can also make you feel inadequate
            as you struggle to understand how the teacher has arrived at an apparently effort-
            less level of classroom practice. As Anders Ericsson has said of sporting expertise,
            ‘extensive watching is not the same as extensive playing’.3 Mental models don’t
            develop simply through watching because you aren’t trying to become an expert
            watcher-of-lessons.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Practice
            David Feldon, education researcher, reassures new teachers that ‘feeling over-
            whelmed by the amount of simultaneous activity in a classroom is a common
            experience’. We all experience cognitive overload when ‘the total processing
            demands… exceed available attentional resources’. We can only pay attention
            to so much; when we reach the limit, we stop being able to ‘adapt effectively to
            complex classroom dynamics’. Why don’t experienced teachers struggle to adapt?
            Why don’t they reach the limits of the ability they’re able to process? Experienced
            and expert teachers have broader knowledge and effective automated processes.
            To break through the limits of our cognitive architecture, Feldon recommends that
            we focus on making effective behaviour automatic whilst minimising ineffective
            behaviours. Practice is what promotes effective behaviour whilst also ridding us of
            ineffective behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For a new teacher, thinking about practice is helpful because it makes explicit
            the often-implicit routines and habits of your classroom. If you’re already able to
            think of your classroom behaviour, ‘I do that a lot and I don’t like it’, you’re on
            the road to a level of self-knowledge the best teachers need. Your own simple but
            annoying behaviours like saying ‘Perfect’ after everything every student says start
            to grate on you. Or harder to shift habits like screaming to regain control from a
            boisterous, noisy class. The solution to both problems is practice.
            In a way, we’re always practising. If you’re in the classroom, you can’t escape
            practice. Each day, habits are being ingrained; processes refine and stagnate. As
            we repeat, we embed. This is an absolute necessity for a teacher. Practice leads to
            automation. Automation brings a process closer to effortless. Pockets of effortless-
            ness make your day more manageable but this is not necessarily the same as getting
            better.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Anders Ericsson, the undisputed king of practice, describes a very particular
            kind of practice:
            Deliberate practice presents performers with tasks that are initially outside
            their current realm of reliable performance, yet can be mastered within hours
            of practice by concentrating on critical aspects and by gradually refining per-
            formance through repetitions after feedback.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Practice is not simply about making more of your day automatic. Practice is also
            about fighting against the sub-par things that are already automatic. We practice
            away those bad habits. For Ericson, the main challenge in ‘attaining expert level per-
            formance is to induce stable specific changes that allow the performance to be incre-
            mentally improved’. The challenge for the lone new teacher is twofold: identify what
            needs to change and know when it has changed sufficiently to be able to move on.
            Deliberate practice includes the following.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Selecting a narrow behaviour
            Initially, your mentor or coach – preferably in conversation with you – will direct
            the behaviours to practice. Don’t be afraid or disheartened to practice the narrowest
            and apparently most basic of skills. Small changes make a big difference. Practice
            in the following areas can collectively make a massive difference:
            - Where you stand during a particular activity.
            - How you phrase a question.
            - The order of your instructions.
            - What you look for when you walk around the room.
            - How you praise students for doing the right thing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Don’t baulk at being encouraged to master a specific behaviour before moving onto
            something more complex. Teaching well is the sum of specific, narrow behaviours,
            implemented well.
            In various classrooms, students have to move from their seats to another area
            in the room. In D&T, the students move from the benches to a demonstration
            at one piece of equipment. In Music, students move from desks to keyboards.
            In some primary classrooms, students move from tables to the carpet. In one
            sense, these are things that students must practice, that students must automate.
            As teachers, however, we can practice the preparation and management of that
            process.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Decide on a process; be clear with students about what will happen. For example:
            In a moment, we’re going to move to a demonstration on the front bench.
            You will stand in one wide circle round the bench. No one will be standing
            in front or behind of anyone else. If there isn’t room, you will move to find a
            space.
            Write out the specific instructions:
            Everyone stand silently behind your chairs… Good. Table 1, in silence walk
            and find a space around the front bench… Okay, good… Table 2 do the same.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Because classes are unlikely to get it right first time, it’s worth practising what will
            happen when they don’t:
            Table 3, you aren’t walking in silence. Go back. We’ll come back to you in a
            moment. Or, I asked for one line around the bench and we’ve got a rabble so
            we’re all going to go back to our seats and try again.
            Practise these steps in your classroom, on your own. Or in your car on the way to
            school. Or, perhaps most helpfully, to another teacher who can help you tease out
            any ambiguities or problems. Clarity won from practice makes the early attempts
            to manage these transitions easier.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Defining success
            When working with a specific, narrow behaviour it should be clear what you and
            your classroom will look like when it is successful. Be careful not to receive the
            success criteria from a coach or observer without seeking to understand it. Ask
            questions of it. Why do I need to X? Ask about alternative scenarios. What happens
            if Y? Your job is to make sure you understand why you need to achieve this success
            criteria and how to be successful. At the defining success stage, observe colleagues
            who are already successful to build your understanding of success.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the transitioning around the room example from above, success is a smooth,
            orderly transition from one part of the room to another. You can practise the word-
            ing and the clarity without the children in the room but ultimately success is only
            achieved in the classroom when students follow your instructions (or are corrected
            when they don’t).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Practice
            Once the behaviour is defined, we can start to think about how to practise it. Sometimes,
            this can be done in an empty classroom: in front of your mentor or at your desk as you
            plan a lesson. An explanation of a very specific set of steps, for example, might benefit
            from a rehearsal outside of the lesson. Be careful of confusing practice with overthink-
            ing. You can overthink a set of instructions, dwell and stew in them. If you aren’t active –
            writing, saying, getting feedback – you likely aren’t really practising.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Certain elements of your teaching are one-off – like the introduction of a new
            topic – and therefore quite high stakes. To allay concerns about getting this right,
            you could write a script with a more experienced colleague. You could rehearse
            it in front of them until it becomes close to automatic. We begin to make a pro-
            cess automatic when we practise it outside the lesson. Just make sure the practice
            matches what you’ll be doing in the lesson. I used to write copious notes for my
            explanations or instructions in early lessons but, when the lessons arrived, I’d
            struggle to use them – too much was going on. I should have practised before the
            lesson and not worried so much about relying on the notes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is possible to overstate what can be achieved before a lesson. Practice is not
            simply this period outside of the lesson; it isn’t just a rehearsal. It includes what
            happens when you take your focus back into the complex environment of the class-
            room. Here, your responsibility for practice becomes even more important.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some
            of the following reminders might feel silly but they may help you keep practising
            when you’re on your own:
            - Write a note in your planner or on a post-it and leave it on your desk or where
            you’ll see.
            - Say to yourself out loud just before the lesson, I am working on X or Today, I’m
            going to try to…
            - Tell the children what you’re working on. Today, I’m going to try to ask more
            questions with hands down, Year 7. If I ask three questions with hands up,
            remind me to ask with hands down. WARNING: I’d only do this with a class you
            already feel comfortable with and only with specific behaviours that students
            can observe. I’d also avoid talking about how you’re working on improving your
            behaviour management – Today, Year 7, I’m working on using the language of
            choice with Ben because he can be a real pain. If in doubt, ask your mentor, a
            coach or line manager. Anything you want students to get better at is a good bet
            for this one. Today, we’re going to try to nail our transitions from the desks to the
            keyboards so we can spend as much time there as possible gives a short explana-
            tion of what you want them to work on as well as a reason why.
            - A coach (see below) can also help you in a lesson. For example, I know a new
            teacher who was working on achieving silence before he begins speaking to a
            class. His coach kept her hand raised at the back of the room whilst students
            weren’t yet silent as a reminder to him to wait and look. Ask for this if you think
            it would help.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Feedback in deliberate practice should be directly tied to the narrow behaviour
            you’ve been practising and the success criteria defined at the outset. If you feel
            your observer is straying beyond the boundaries of what you’ve been practising, do
            let them know. It’s not that what they have to say won’t be useful. To focus practice
            on rapid improvement, you need to hear specifically about what you were practis-
            ing. You could ask to hear specifically about what you’ve been practising first and
            leave the rest for any spare time at the end.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whilst difficult to achieve in isolation, some forms of practice are possible
            on your own. Perhaps you’ve left training and early careers status behind and
            no longer are being observed as often. You can still home in on a specific skill.
            Imagine you’ve become aware that classes are slow to settle when you want to tran-
            sition between activities. Asking repeatedly for quiet and raising your voice hasn’t
            helped. Having talked to colleagues and observed another teacher, you’ve decided
            to practice narrating your countdown. You write what you want to say on a post-it,
            mainly aiming to reiterate simple instructions as you countdown. This isn’t high
            stakes: you’re going to aim to narrate the countdown at least three times each les-
            son. You audio record your narrated countdown and notice, in your first try, your
            pauses are too short. The narration isn’t getting through. You try again, focusing on
            the pause. The class settle. Developing a rhythm of ongoing practice is important
            for the individual teacher. Stepping out of yourself, analysing your behaviour and
            finding ways to alter it are essential activities for all successful teachers. Doing this
            on your own is sometimes possible but not ideal.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whilst some forms of feedback work in isolation – video or student response, say –
            it’s unlikely these things will work indefinitely, particularly as a very new teacher.
            Practice is an effective strategy for making a change in the classroom but we don’t
            always know what needs to change. We don’t always see every corner of our class-
            room. We don’t hear every whispered misconception. This is why it’s vital someone
            is supporting you to see beyond what is easily visible. This is why we need a coach.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coaching
            Any comments about coaching or mentoring should be an extension of the com-
            ments about practice. Initially, a significant role of the coach or mentor is to help
            you practise. Learning to teach is difficult because you are just developing aware-
            ness of what is happening in your classroom. A coach helps to solve this problem.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Instructional coaching, a form of responsive and, at times, directive coaching, like
            practice, attempts to increase awareness of your classroom actions and behaviour
            before offering specific ways to change. One definition describes coaching like this:
            a one-to-one conversation focused on the enhancement of learning and devel-
            opment through increasing self-awareness and a sense of personal respon-
            sibility, where the coach facilitates the self-directed learning of the coachee
            through questioning, active listening, and appropriate challenge in a support-
            ive and encouraging climate.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whilst generic forms of coaching exist in education, instructional coaching is at the
            core of current thinking about how teachers improve. But definitions – and your
            experience – of instructional coaching will vary. Most debates about instructional
            coaching centre on how directive it should be. Jim Knight describes how ‘instruc-
            tional coaches teach others how to learn very specific, evidence-based teaching prac-
            tices’.7 For Knight, instructional coaches add this more directive element to more
            traditional coaching techniques like ‘dialogic questioning’ and ‘effective listening’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Paul Bambrick-Santoyo, a prominent voice for a more directive version of
            instructional coaching, emphasises the need for an ‘action step’, a clear, well-de-
            fined next step for the teacher. In Bambrick-Santoyo’s model, this action step is
            given by the coach, modelled by the coach and then practised, often in an empty
            classroom, by the coachee in preparation for the real event. It’s likely you’ll swing
            between a directive model and a responsive one as you bring ideas to the coaching
            relationship at times and, at others, go to your coach desperate for any solution.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It might seem strange to talk about coaching when it feels like it is something done
            to you. Too often, the focus in education and CPD is on what the coach will do. To
            maximise your understanding of what good teaching is – your knowledge of classroom
            craft – you need to know how to act in that coach/coachee relationship. With this in
            mind, it is worth going into a coaching relationship knowing what you’re getting.
            Beyond understanding the coaching process, you participate in it by receiving
            feedback. No one really tells you how to do this, which is strange because receiv-
            ing and responding to feedback well is the main way you will get better. You often
            don’t know how you’re going to take feedback until you’re receiving it, and in
            training and early careers, you receive a lot. Depending on how you’ve arrived
            in teaching, you may have only received generally positive feedback in your aca-
            demic and extra-curricular pursuits. How you receive feedback also depends on
            your sense of yourself. If you’ve dreamed of being a teacher since your first day of
            school, being criticised for your first lessons can be incredibly demoralising.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I don’t know what kind of feedback you’re going to receive. However that feed-
            back is given, even if it could be better, kinder or more productive, you can make
            the most of it. In their excellent book, Thanks for the Feedback, Sheila Heen and
            Douglas Stone offer three feedback triggers – three ways we can reject feedback –
            and what we can do about them.
            The triggers are:
            1. Truth Triggers – We feel ‘indignant, wronged and exasperated’ because the
            feedback is ‘somehow off, unhelpful or simply untrue’.
            2. Relationship Triggers – Our reaction to feedback is ‘based on what we believe
            about the giver’.
            3. Identity Triggers – The feedback makes us ‘feel overwhelmed, threatened,
            ashamed’ because we’re ‘suddenly unsure what to think about ourselves’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Identifying our feedback triggers, which may vary depending on who is giving the
            feedback and what it is, is the first step to learning as much from feedback as pos-
            sible. You might recognise yourself in one or all of those triggers. Just knowing our
            triggers is not the same as being able to deal with them.
            Some strategies for making the most of feedback include:
            Pause. Don’t immediately jump to the defence of whatever behaviour is the tar-
            get of the feedback. Don’t immediately explain away what has been noticed even
            if it feels like there is an explanation.
            - Ask questions. It’s important to pause before questioning so that we don’t just
            attack the feedback or be seen to. Ask your observer to explain it again. Ask them
            to give examples. Ask them to give you a reason why what they’re talking about
            is so important.
            - Separate the feedback giver from the feedback. When you start out, it’s likely
            that you’ll be receiving feedback on your lessons from a very small number of
            people. If, for whatever reason, you find one of these people difficult to receive
            feedback from, there is a real danger this difficulty slows your progress as a new
            teacher. If there’s someone you dread getting feedback from, imagine the feed-
            back from that person is coming from someone you really respect. How would
            your response be different?
            - Acknowledge how the feedback is making you feel. In the moment, your
            heart can start to race; you can feel your face turn red with frustration or
            disappointment or anger. Recognise how you physically respond to feedback.
            Recognise that physical reaction is momentary. Perhaps thoughts about the
            amount of work you’ve put in or the progress you know you’ve made enter
            your mind. It doesn’t feel like your coach gets that. Separate the feeling from
            the feedback where you can. Make notes if you need to so you can return to
            the feedback when you’ve had time to process the physical and emotional
            reaction to it.
            - Remind yourself feedback happens to help you get better. You’re not meant to
            feel under attack when receiving feedback but you can. Feedback is not the end-
            point. As Heen and Douglas remind us, feedback should be seen as ‘welcome
            input rather than upsetting verdict’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If, after following this advice, you still feel feedback is unhelpful, unrealistic or
            hypercritical, raise it with those giving the feedback. It is your responsibility to do
            all you can to make the most of feedback. It is the responsibility of those giving
            feedback to make it useful, possible and productive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
            Teachers automate effective processes by:
            - Seeing and understanding effective teaching practices.
            - Narrowly defining specific behaviours to practice.
            - Practising before and during lessons.
            - Receiving feedback and refining in response to feedback.
            Observations, practice and coaching overlap and intertwine; you don’t move neatly
            from one to the next. As we begin to look at different types of teacher knowledge,
            these ideas will be relevant to specific aspects of pedagogy and subject. My aim in
            writing this chapter has been to give you a way to be an active participant in your
            training and development rather than a passive receiver.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
            1. Plan an observation. Decide what you need to see. Ask others to observe for
            that focus. Before the observation, arrange a time afterwards where you can
            talk to the person you’ve observed. Go into that conversation with questions.
            Annalise describes preparing herself to receive feedback.
            I had prepared myself to feel really down and out or to feel like I had a lot of work to do.
            I’m my biggest critic.The way I went into it was to try to be specific in what I wanted to
            know. So don’t go into some feedback and say How did I do? On a scale of what?
            So I’d try to say so I’ve just done this task. Can you let me know if I could have done it
            better? I have done it this way.Would you have done it differently? So you present them your
            thought process and then you can receive theirs. And they know where your focus is.
            2. Find at least one way to take ownership of your practice. Others are responsi-
            ble for directing and developing you, but that doesn’t mean you aren’t equally
            responsible. Take steps to make sure your practice extends beyond those out of
            lesson times. Make visible notes and reminders for yourself. Record or video
            your teaching. After a period of practice, return to your coach and ask for clar-
            ity if you’re not sure what to do next.
            3. Talk through your feedback triggers. You could do this with your coach if you
            have that kind of relationship but you don’t really need to discuss this with a
            teacher. It would be equally useful with a friend or family member you trust.
            Articulating your triggers to feedback raises your awareness of them. This is
            the first step in being able to tackle them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Early in my one of my first teaching placements, I was left alone – for a moment –
            with a class for the first time. The class were already working quietly, a benefit of
            their usual teacher’s presence, so I waited anxiously for her return. Whilst she was
            gone, one student decided to start flicking little balls of paper off his desk and onto
            the floor. I tried to mask my terror at encountering behaviour I was not prepared
            for by saying, as firmly as I could muster, ‘Stop doing that’. The student looked up,
            sighed, and asked, quite reasonably, ‘Or what?’ It was a question for which I was
            entirely without an answer. Having made his point, he quietly continued and the
            teacher returned soon afterwards. To begin with, behaviour management felt like
            something I wanted to avoid. I wanted my classes to be well-behaved. I wanted not
            to have to deal with paper flickers or their ilk.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Behaviour management, for me, was a minefield of misconceptions. Knowledge
            of behaviour management often begins in challenging these misconceptions, pre-
            conceptions and stereotypes. Here are a few of them.
            - You need a certain type of personality to manage behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I have never been an outgoing, shouty extrovert. Those were the people, so I thought,
            who would find all this effortless. I, on the other hand, was exhausted by just the
            possibility of poor behaviour in my lessons. Even when I became a senior leader,
            I felt that I would never have the authority of those who could command a play-
            ground of children with a whistle and a loud voice.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Behaviour management, however, is not about personality type. It’s a set of
            strategies and these strategies can vary from person to person. Unlike other
            areas of teaching, these strategies feel more tied to our personality. Are you
            more likely to defuse a situation with humour or lay down the law? But these
            are quite superficial questions when compared to the complexity of planning for
            and managing behaviour across a class because so much of behaviour manage-
            ment is about how you plan and organise your lessons. You do need to be able
            to talk with authority, loud enough for a class full of children to hear you. You
            do, at times, need to act decisively for the sake of the children’s safety or the
            success of your lesson. None of that is the same as saying behaviour management
            is the skill of the extrovert. Believing you can manage your behaviour through the
            force of your personality is a recipe for burnout.
            - You need to have a response ready for every possible scenario.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As a trainee, behaviour management felt like a huge set of scenarios that I would
            have to plan for, rehearsing responses. I never plucked up the courage to do this
            but I was always eager to sit my mentor down before a lesson and ask endless What
            if questions. Comfortingly though, behaviour generally falls into categories that we
            can prepare for.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Your first line of defence against low level and off-task behaviour is the organ-
            isation of your classroom and the planning of your lessons. It’s not so much that
            your lessons have to be ‘fun’, ‘relevant’ or any other vomit-inducing sentiment.
            Rather, we plan for students to behave by creating a classroom built on routine.
            That routine includes a response to students who don’t adhere to expectations, a
            response that should be guided by your school’s behaviour management systems
            and policy. It isn’t cruel or boring to make the foundations of our lessons routine.
            Whilst unglamorous, this sort of planning makes the most out of our time with the
            young people in our classes. Routine doesn’t mean we can never be spontaneous,
            never deviate. It is simply the foundation on which we build.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Even when behaviour is more extreme, there are routines and scripts we can embed
            to help us prepare. A student in one of my lessons once threw a book across the room
            and then proceeded to kick it into confetti against the wall. I hadn’t been trained for
            this and I confess I just watched in frightened awe. It would have been foolish to
            spend hours considering how to prepare for future book-kickers. He was the only one.
            - You shouldn’t smile until Christmas.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Smile as often as you normally would. Smile when a student gets something right.
            Smile when they do something funny. I’m not sure this advice really exists in any
            real sense but perhaps some new teachers go into lessons believing they need to be
            an ogre. Being strict, at least in some quarters, is back in vogue and so it is likely
            a few will confuse this with draining your face of personality until spring. Even if
            you don’t hear this advice, it points to something unhelpful about behaviour man-
            agement instruction. Smiling or not smiling will have very little impact on student
            behaviour. Behaviour change is borne out of more fundamental change in how we
            plan for and act in the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Contained within this unhelpful advice not to smile, though, is perhaps a nugget
            of truth. You aren’t there to be a friend. Chummy banter with the students might
            elicit a positive reaction initially but will make your life difficult in the long run.
            As we’ll see, the strategies we use to manage behaviour are quite different to those
            we use to win friends.
            - For students to behave, you need to show them you care.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Almost inverse to the previous statement, some believe that students can’t learn
            anything until they feel loved. Of course, you should care for your students.
            Teaching would be a surprising career choice if you didn’t. But there does seem to
            be some confusion about what care really means here. It does mean you’re inter-
            ested in them as people. It does mean you want them to do well. It does mean you
            work hard to ensure they meet their potential. Caring doesn’t mean putting up
            with offensive, disruptive or aggressive behaviour. It should never mean working
            yourself into the ground to make tiny gains with a student who is endlessly rude
            or deliberately spoils your lessons.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The twin statement of you need to show them you care is teaching is all about
            relationships. Teaching is absolutely about relationships but those relationships
            aren’t friendships. A teacher–student relationship works best when students know
            exactly what to expect from you. Everything, at least in the realm of behaviour,
            comes back to creating clarity around routines and expectations, as well as cer-
            tainty of response when students don’t meet those expectations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[On our tour of teacher knowledge, I’m focusing on areas where we can develop
            real understanding of and make tangible changes to our behaviour management.
            Whilst whole books have been written on the subject of behaviour, it’s my hope
            that these chapters give you a guide to begin building mental models for behaviour
            management. This section won’t answer every question you have about behaviour
            because it can’t. Instead, you will be directed to the two areas in which you can,
            right now, develop knowledge to improve behaviour.
            In this section, we’ll answer the following questions:
            - How do we prepare for students to behave?
            - How do we manage behaviour in-lesson?]]>
			</paragraph>
			<paragraph>
				<![CDATA[None of the answers here guarantees perfect behaviour nor are these answers
            switches you flick to get immediate good behaviour. Behaviour in a school is also
            not wholly down to you. We must be careful not to abdicate responsibility to man-
            age our classes well, but school systems, policy and routines also play an impor-
            tant role. The following chapters focus on you and what you can do whether you’re
            in a school that manages behaviour well or badly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perhaps the biggest misconception about behaviour management is that it is
            something you master before moving on to your loftier goals. Teaching is always
            going to be about the management of behaviour because students are young people
            and, well, they behave in ways you wouldn’t expect or predict.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When I was a new teacher, I felt the benefit of teaching English in a school that
            mandated five minutes of silent reading at the start of every lesson. Not many
            schools do this now, probably because – with entry, settling and getting the stu-
            dents to actually read – 5 minutes often became 15. Starting my lessons was easy:
            students came in, got their books out (or got a battered one from the box at the
            front) and started reading. I had some time then to hand out books and resources,
            to set up my computer and do odd jobs I should have saved for later. Starts of my
            lessons were anything but good but the reading masked that, and I carried on, clue-
            less about what could have been better.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Great Teaching Toolkit describes how ‘Great teachers plan activities and
            resources so everything works smoothly’.1 In the myriad of ways we can achieve
            smooth running of the classroom, a larger proportion than you’d expect happen
            before students have even entered the classroom. A crucial question to ask is
            What should I base my decisions about behaviour on (before the lesson or
            otherwise)?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Behaviour management is both proactive and reactive. Your mental model of man-
            aging behaviour is proactive in that you can prepare for behaviour in advance
            outside of the classroom. It is proactive in that you can teach students positive
            behaviours rather than just respond to negative behaviours. We shouldn’t kid our-
            selves, though: there are times when we will have to react. We can prepare for these
            times but that doesn’t mean that they will be easy. Whilst the next chapter will
            deal with the reactive side of behaviour management, this one looks at the proac-
            tive steps we can take. These steps stem from a question: What type of classroom
            do I want to create? Such creation doesn’t happen because of wishful thinking. It
            happens through careful planning, thoughtful reaction and intensive work with
            our students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Behavioural economist John Elster describes social norms as ‘emotional and
            behavioural propensities’ – behaviours and inclinations, driven by motivation to
            fit in. Often, such behaviours are not rational but instinctual. Every day, teachers
            see in their students the tensions between instinct and rational forward thinking.
            Secondary students often know intellectually that school is useful and that educa-
            tion is valuable but they still act in ways that seem to discount this. In fact, adults
            also tend to discount the potential gains in the future compared with gains in the
            present but, developmentally, teenagers are more susceptible to this.]]>
			</paragraph>
			<paragraph>
				<![CDATA[So what? That children can act irrationally hardly needs stating. We don’t
            need a body of evidence to remind us to be the voice of reason in the classroom.
            Fortunately, this is not all we can learn from behavioural economics. In this field,
            we find methods not just to recognise the norms I’ve described but also to create
            them through habits, routines and nudges.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In being proactive about behaviour management, we’re not simply trying to pre-
            pare for the worst – the worst behaviour or our worst expectations. Rather, we
            want to make changes to our classroom, our planning and our delivery that make
            it less likely that students will misbehave. Ultimately, we’re trying to create hab-
            its or norms. Logan Fiorella, Professor of Educational Psychology, describes how
            ‘beneficial habits will continue effortlessly despite fluctuations in motivation or
            willpower’.4 We can’t overstate the importance of such knowledge for teachers:
            even when students don’t feel like or don’t want to learn in your classroom, habits
            trump apathy and disengagement.
            Before you consider what norms you are trying to create, consider the following
            questions:
            - What do you value as a teacher and how do you want to make that a routine?
            - What norms already exist in your school? Or, what norms is your school trying
            to establish?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Remove the friction from doing the right thing
            When the British government’s Behavioural Insights Team produced a report on how
            to nudge behaviour in an intended direction, their first piece of advice was to ‘make it
            easy’. As this sounds a bit obvious – of course I want it to be easy for students to behave –
            it’s worth considering from another angle: How can I reduce the ‘friction costs’, the
            ‘seemingly irrelevant details that make a task more challenging or effortful’? ]]>
			</paragraph>
			<paragraph>
				<![CDATA[As behavioural scientist Katy Milkman explains, ‘An engineer can’t design a suc-
            cessful structure without first carefully accounting for the forces of opposition (say,
            wind resistance or gravity). So engineers always attempt to solve problems by first
            identifying the obstacles to success’. At times, the origin of friction or obstacle can
            be difficult to identify. In some lessons, it might all feel like friction, making choosing
            one behaviour or routine difficult. Consciously looking for friction might help but a
            supportive observation (or recording) can highlight what you would otherwise miss.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A large part of the answer to the question of behaviour depends on what your
            school does about it. Your school may have a particular routine about entry to the
            classroom – they line up, they go straight in, they stand behind chairs. Such rou-
            tines make your life and the lives of your colleagues profoundly simpler. If you are
            lucky enough to find yourself in such a school, embrace the routines even if they
            feel like they grate against your freedom-loving sensibilities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Constructing your own routines, where a school does not dictate one, is defi-
            nitely possible. Your aim is to remove the friction in the process of students enter-
            ing the room and starting whatever the first task is. When students have to go and
            get their books or resources at the start of every lesson, when they have to look for
            them because they seem to move around, we’re making it harder for students to get
            started. We’re making it more likely that students will act out.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Behavioural Insights Team encourage us to ‘harness the power of defaults’.8
            You might not like the idea of every lesson starting in the same way but there
            is power in a routine that happens (at least almost) every lesson. Routines may
            feel constricting but their effects are often liberating: time has been saved, behav-
            iour has been managed almost effortlessly, students spend lesson time on the right
            things. A lesson entry is a good time to create a default: students come in, sit
            straight down, push books along a row and then get started on a task on the screen.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For a while I was teaching in five different rooms. I know teachers who have
            had to teach in so many more (the highest I’ve heard being around 13). Five still
            felt like a lot to me. I didn’t know what to do at first; I’d arrive panting, sweating
            and stressed from one room to the next. Behaviour in the initial moments of those
            lessons was always precariously close to chaos as I set up resources, frantically
            searched for books hidden by another teacher and started the first task.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you’re moving from room to room, it’s likely you won’t have a say about the lay-
            out of the space. I would have liked to change the layout of some of those rooms I was
            teaching in but I couldn’t. Instead, I focused on what I had control over: resources
            and first activities. I got into school early and logged on to each computer in turn and
            set up my lessons for the day. That way I just needed to log in for students to see the
            first task on the screen. I made different piles of books for each row in the classroom
            and told students on the end of those rows it would be their responsibility to pass
            the books along. Because these strategies worked so well across five rooms, I contin-
            ued using them even when I went back to teaching in one. My argument isn’t that
            you should get in early and pile up books in a specific way, but a plan of some kind
            can help massively. I’ve known teachers print out a task on a slip of paper so that
            students can complete something whilst the room is prepared. I’ve known teachers
            breeze into a room and write a question on the whiteboard to similar effect. Try out
            different approaches but aim for a routine that works over time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Training schemes usually, if only for a time, mandate a lesson planning proforma.
            These often have a box for what you’re working on, some information about the
            class (which you can copy and paste from the previous plan), and the sequence of
            activities for the lesson. These plans can have you playing Nostradamus when they
            ask you to set out the length of time each task will take, which is a brutally unfair
            thing to ask a new teacher to decide.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What is missing from these forms tends to be what will happen in the gaps or
            transitions between the activities. Any time you set a task or transition from one
            activity to the next, there is both the possibility of calm and chaos. Planning tran-
            sitions and structuring the distribution of resources can feel like additional work,
            additional planning, but these things will save you immeasurable time and hassle
            in the future. You can prepare for transitions by:
            - Checking all resources are ready before the lesson.
            - Putting out resources for the next task during the current task.
            - Giving time reminders and warnings. For example, In five minute’s time, we’re
            going to be packing away. By then, I’d expect you to…]]>
			</paragraph>
			<paragraph>
				<![CDATA[Dead time between tasks is a breeding ground for off-task behaviour and even if
            students do behave as you set up the next task, the time you’ve spent doing this
            could have been folded into a prior moment in the lesson. Time can be saved and
            learning maximised. Equally, if instructions are given but are unclear, students
            may start a task but fail to engage properly or quickly – at best asking lots of ques-
            tions, and at worst disrupting the lesson.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What, then, avoids this potential disruption or loss of time? In short, clarity and
            specificity. Knowing you need clarity and specificity or trying to remind yourself
            of those things in the moment is unlikely to be helpful. We prepare for behaviour
            when we plan our expectations around a task. Going into a lesson, for the tasks you
            will set, you should know:
            - How long the task will last? I’d make this a rough estimate and respond to what
            you see in the students’ work but if you struggle with timing, you may want to
            use a stopwatch.
            - Where and how the students will complete the task? On the sheet? In their
            books? In full sentences? In notes? What does ‘notes’ mean? One way to
            answer these questions – particularly for more complex tasks – is to model
            how you expect students to approach a task. This can be particularly effec-
            tive under a visualiser when you demonstrate work in the same format as the
            students: if they are using a sheet, show them how to work on the sheet; if
            they are working in an exercise book, use your own exercise book under the
            visualiser.
            - What are the expectations of and constraints on student behaviour during the
            task? Start with noise. Are students allowed to talk? Who are they allowed to talk
            to? Are they allowed to move around the room? If students get stuck, what should
            they do?]]>
			</paragraph>
			<paragraph>
				<![CDATA[The questions above can help you to create scripts like the examples below.
            Remember, your aim is friction reduction.
            Example 1: You have 10 minutes to complete all of the questions on page 120
            of the textbook. Your answers should be written – with working out – into
            your exercise book. You must not talk any louder than a whisper to the person
            sitting next you, and only to that person if you get stuck. Go.
            Example 2: Complete the passing drill I just showed you with your group as
            many times as you can in two minutes. You should only be talking to people
            in your group. Off you go.
            Example 3: Write your own paragraph, like the one I’ve modelled. If you get
            stuck, you’re going to use the sentence starters and notes in your book from
            this lesson to help you. You won’t put your hand up. This task will be com-
            pleted in silence because I want to see what you can do on your own. You
            have ten minutes. Begin writing now.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In addition to the above, we remove and reduce friction when the starts and ends
            of tasks are clear. Notice each of the examples has a clear start to it. Deciding a
            word or phrase to start a task is as much for you as it is for the children. If you say
            ‘Off you go’, emphasis on the go, every time a task starts, it helps you to stop
            continuing to talk until your instructions fade out as you find yourself wandering
            around the class. After the ‘Go’, pausing at the front of the room can help you to
            assess if your instructions have been successful. If everyone looks puzzled or if the
            instructions of where to complete the task – Not on the sheet! – aren’t followed or
            if everyone starts talking even though silence was the expectation, then you have
            grounds to halt the task and reiterate instructions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[
            Ending tasks
            Just as we shouldn’t continue talking after our planned instructions have ended,
            we shouldn’t end a task by starting to talk, gradually winning the attention of
            small numbers of students at a time. Instead, make the ending clear. If you
            use a countdown timer, that could be the buzzer noise as it ends. The upside
            of the buzzer is that it is definitive and clear to students; the downside is that
            once you’ve started a countdown, you cede control of the end of the task to the
            buzzer.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teachers often use a verbal countdown at the end of a task, perhaps because
            they’ve heard others use one. Countdowns can be a source of friction rather
            than a solution if you get to zero and the class aren’t doing what you’ve asked
            of them. Classes can be taught to respond appropriately to the countdown
            but we should expect to put in the hours of communication, practice and
            repetition.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Narrating the countdown is often effective at getting students to follow an
            instruction in a short space of time. A narrated countdown communicates and
            reiterates the expectations between the numbers: Silence in 3… pens down in 2…
            everyone looking this way, silence in 1… zero. Early in my career, this became my
            signature move to the extent that, at one school, a student used to yell, ‘Silence in
            THREE’ at me across the playground. Although students yelling at you across the
            playground is usually not a good sign, I was cautiously optimistic that this particu-
            lar student had become aware of the routine.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Planning a set of instructions or narrated countdown can be scripted and then
            practised as discussed in Chapter 5. The more you do this in the short term, the
            less you’ll have to do it in the long term. Friction against the behaviour we want is
            likely to deter students from it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Decide on common cues to prompt the right behaviours
            Habits are sometimes described as the behavioural responses to cues in our envi-
            ronment. Although it’s probably more complex than this – habitual behaviour
            interacts with our goals and personality9 – cues can be useful in the classroom.
            Certain contexts demand certain actions; habit researchers Wendy Wood and
            David Neal give the example of buckling your seatbelt, an action demanded by
            the act of getting into a car but not one you think a great deal about.10 In your
            classroom, you don’t have the luxury of relying on an already universal action like
            the buckling of a seatbelt. Work is required to get anywhere close to that kind of
            automaticity. Cues are your way of highlighting the easy behaviour and removing
            the friction.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As students enter the room – whatever the time of day, whatever they’ve just
            done – set it up so the cues are visible. Exercise books are out on tables. Resources
            are easily accessible. A resource is handed to students as they enter. A countdown
            tells students to start quickly. Settled, immediate work is the aim. One-off usage
            won’t turn any of these into a cue or achieve the result you’re after. Repetition is
            the breeding ground for habit cues.
            Some poor behaviour is the cued response to the friction you’re working to elim-
            inate. Students have nothing to do or instructions are unclear; students engage in
            some kind of off-task behaviour: they talk, they move around, they shout across
            the room.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Keep at it
            If your aim is to establish a behaviour as the norm, you need to be in it for the
            long haul. We can always move away from behaviours we don’t think are getting
            the desired result, but we shouldn’t be quick to move on to the next thing until
            students are in the habit we want them to be. One study found it took around six
            weeks, the length of an average half-term, to establish a regular gym habit.11 We
            should probably expect something similar in our classrooms.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If, for example, you want students to pack up in a particular way, you need to
            introduce the new system to the students, make a big deal of it for a long time, get
            students to do it again when it isn’t right and praise it when it is. If we’ve made
            any change or introduced a new strategy with a class and it doesn’t feel like it’s
            working, ask yourself how long you’ve been at it, how much you’ve practised it,
            and how often you’re returning to it. Even weeks after you’ve introduced a routine,
            demonstrate to students that it’s still important and that you still care about it. Set
            a reminder or schedule an email to yourself; find one way to force yourself back to
            a routine a few weeks after you start it. Reminders like these could pleasantly sur-
            prise you when they come at a time when you have established something in your
            class you can be proud of. They could also remind you before you lose something
            that it is worth pursuing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Distinguishing between a norm you should keep going with and one that can
            be shelved is difficult. You have sunk time and energy into creating it. Base your
            decision to leave a norm behind on:
            - The realisation you don’t want to or can’t create this norm.
            - The discovery of a better way (through observation, conversation or training)
            that you can achieve this norm.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Use the power of timing
            I love meeting a new class at the start of the year. These are moments of such poten-
            tial. Every time is an opportunity to reinvent yourself as a teacher or, if that is too
            strong, to take at least one step forward. And, whilst I’ve rarely lived up to my own
            expectations of what I could achieve in an academic year, I’ve also found it easier
            to make at least some kind of change for the better. I’ve got a little bit stricter, per-
            haps. Or worked hard to teach a new topic well.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Katy Milkman describes the ‘fresh start’ effect, a phenomenon where people find
            it easier to change at natural transitions or landmarks in the calendar. Milkman
            describes how these fresh starts are ‘helpful for kick-starting change’ but warns
            that they can disrupt ‘well-functioning routines’,12 particularly where those rou-
            tines are newly established. The ‘fresh start’ effect has two important applications
            for teachers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A new term is a good time to introduce a new habit or break an old one
            I once worked for a headteacher who would make no major changes to school pol-
            icy until September. If the behaviour policy was proven to be woefully inadequate
            in February, you just had to buckle up and hold on until September. Perhaps this
            headteacher, like I had with my classes, realised a new term is a good time to make
            a change. Unfortunately, not all changes can wait until after a holiday.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A new term, therefore, is a good time to make the big changes you want to make
            to your practice. You’ve realised you’ve been too lenient and now you want to raise
            expectations. You’ve been tentative about live-modelling under the visualiser but
            now you’re going to start. You’ve wanted to discuss and share more student work
            during lesson time but have felt a barrier to doing so – a new term is a fresh go at
            breaking down that barrier.
            Before the end of a term, consider what changes you want to make in your
            classroom. Consider the norms you want to create or recreate. Consider the cues
            you need to set up for yourself and for your students. Note these things down for
            future-you to be reminded of when you return after the break.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A new term is an important moment to review and consolidate habits
            you want to keep
            The end of a term is also a good time to reflect, consolidate and take pride in the
            success you have achieved. Unfortunately, habits and norms only recently formed
            are at their most fragile when we take a break from them. If a questioning tech-
            nique, an exit routine, a behaviour management script had a modicum of success
            during the term, make a note of that success before you leave. Leave yourself a note
            to explain to future-you what you’ve been doing and why you think it’s worked.
            Don’t leave to chance your ability to sustain success into a new term.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Prepare for the class(es)
            Part 6 will go into more depth on knowing your students. For now, it’s worth not-
            ing that to make actions easy, to use the power of identity, to use praise sincerely,
            you need to know your students:
            Know their names. Knowing a student’s name when they don’t feel you should
            never ceases to have impact. Place a seating plan in front of you whilst teaching
            and use the names as often as you can. Photos help you to learn names over a
            brief period of quizzing.
            Know their needs. Support plans and pupil passports (named differently from
            school to school) offer strategies to work with some students. At times, advice
            given is vague: break content down. Concrete direction can be followed easily:
            Sit at the front or Don’t get this student to read aloud. Following such plans
            reduces friction.
            Know the data. Bias and assumption cloud knowledge of students. Past perfor-
            mance tells us something but not everything. Data might need to be explained
            to you. A conversation about specific content and how to pitch it is likely more
            useful than a spreadsheet at this stage.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Be wary of ‘getting to know you activities’, or at least spending time excessively on
            them. Teachers aren’t short of time with students; relationships don’t form because
            we’ve played a couple of games with a class. Teachers are responsible for knowing
            their classes but those relationships are best developed within the boundaries of
            the rules, expectations and norms of the classroom. We’ve spent time consider-
            ing routines and habits so that you have the space to teach and develop those
            relationships.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
            Behaviour management is more than the sum of the routines you build but you’re
            unlikely to manage behaviour well without routine. If you look around your class-
            room and despair at the lack of routine, don’t panic. Effective teachers aren’t usually
            managing behaviour through the force of personality. They’ve created successful
            routines that happen automatically. Within the framework of those routines, we’ll
            still have to respond to student misbehaviour as it arises whilst promoting positive
            behaviour. That is what we’ll turn to next.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
            1. Be clear with yourself on norms and routines. If you haven’t done it before,
            write down as much as you can about how you’d like students to behave in
            your classroom. Consider which of these are coming from you and which from
            your school’s procedures (prioritise whole school behaviours first).
            2. Identify a point of friction. Where and when do students behave in a way you
            don’t intend? Is it when they’re handing out resources? During class discus-
            sion? In extended independent tasks? Of course, identifying friction is only
            half the battle. Once identified, talk to others about how they deal with similar
            friction. Invite your coach or mentor to look at this area specifically.
            3. Script and practise a clear instruction. You can’t do this for every instruction –
            imagine how long that would take! But, with new or complex activities, a
            script can help clarify – for you and them – what is expected. An unrehearsed
            script is unlikely to help. Practise by yourself or with a coach to reveal possible
            issues.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are so many stories I could start this chapter with, examples of my inability
            to predict or manage student behaviour. The time I knew a student was about to
            start a fight, stood in the way and saw it start anyway. The countless occasions stu-
            dents have said something witty and withering and I’ve just stood there, stuttering
            and red faced. I once tried to convince a boy that he was sure to injure himself if he
            continued climbing the fence. By the time I’d explained how dangerous it was, he
            was over the fence and walking home.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perhaps you’ll already have your own stories of situations that could have gone
            better. Struggles with behaviour are universal yet somehow our own difficulties
            can become a personal source of shame. Managing behaviour gets easier but that
            doesn’t mean it’s easy. You can slip into a rhythm with classes that feels effortless,
            only to be jerked from it when a student arrives riled from the events of break and
            ready for confrontation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this chapter, we’ll look at how you can manage behaviour in your classroom,
            during lessons. We’ll work through everything from the minor to the more extreme.
            In our search for knowledge that can help us face the challenges and problems in
            the classroom, we’ll focus mainly on your school’s behaviour policy. To do that,
            we need to step back and consider your responsibilities and the responsibilities of
            leaders in your school.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whole school versus individual teacher
            You’ve done some of the work to mitigate poor behaviour. Your classroom is
            ready. What more can individual teachers do to tackle poor behaviour? And what
            is the responsibility of leaders? Behaviour is sometimes passed back and forth
            between teachers and school or pastoral leaders. A student misbehaves in the
            classroom; the teacher, rightly, sanctions them. A leader looks at a spreadsheet
            of this week’s sanctions and wonders why so many sanctions are being given in
            particular classes to particular students. Leaders mostly don’t look at behaviour
            in this way to get at teachers; they have a wider responsibility to create a pos-
            itive culture across the school. Behaviour tsar Tom Bennett describes the ‘key
            task for the school leader’ as creating a culture. When Bennett describes the fea-
            tures of the most successful schools, he focuses more on leader responsibilities
            than staff behaviours. Leaders are a visible presence. They set clear expectations.
            They support staff.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Further research supports the idea that leaders play a vital role in managing
            behaviour. For example, leaders use knowledge of students’ lives and difficulties
            to pre-empt poor behaviour. Teachers don’t always have this knowledge. Behaviour
            interventions, activities run outside of lessons, are also tools in the leaders’ arsenal
            to tackle poor behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Leaders’ responsibilities could be summarised as follows:
            - Creating a culture of positive behaviour.
            - Providing clarity on what is and isn’t acceptable.
            - Being a presence around the school, challenging poor behaviour and promoting
            positive behaviour.
            - Supporting staff to manage behaviour, including the delivery of training.
            Teachers’ responsibilities include:
            - Keeping children in their classrooms (and around school) safe.
            - Following the school’s behaviour policy.
            - Creating a classroom environment where learning can happen.
            - Challenging poor behaviour in and out of the classroom.
            - Knowing the students you teach, forming relationships built on consistency and
            clarity of expectation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Simply put, it is your responsibility to manage behaviour as best as you are able,
            including following the school’s behaviour policy. It is a school leader’s responsi-
            bility to support you as you do this.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I thought I was terrible at managing behaviour until I worked in a school with a
            clear policy. In the school where I worked for the first two years of my career, a
            large comprehensive in Bristol, teachers managed behaviour through the strength
            of personality. For the first time in my life, I realised mine wasn’t that strong.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In a school of almost two thousand students, I regularly found a child doing
            something inappropriate, only to have them run off when I started speaking. I’d see
            them again months later when it felt rude to bring up their past misdemeanours.
            My tutor group had its base in the science labs and, to the enduring irritation of the
            technicians, would turn on the gas taps when they got bored of PSHE. Our aim was
            to keep students in lessons for as long as possible, or until they did something so
            terrible that our defence for sending them out could be cast iron.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My second school was quite different. There was a clear system, a graduated
            approach, we were to use with all students in all classes. The headteacher was
            clear: he didn’t want anyone doing anything else. I used the system and, to my
            surprise, found I could manage behaviour quite well.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Sometimes there are two behaviour policies: the intended policy and the enacted
            policy. The intended policy sounds good but the way it is enacted leaves much
            room for improvement. Or, even when the enacted policy works quite well, it feels
            distant from what is written down. The knowledgeable teacher seeks to under-
            stand the policy, intended and enacted. To do that, we read it. We talk to leaders
            (ideally, this would be part of your induction). Most importantly, we use it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Learning about the policy is important but it isn’t possible to understand it until
            you’ve seen it in action. If you can, observe teachers using the policy. Don’t observe
            the easiest classes. Root out those classes and students that could pose a challenge
            and examine what experienced teachers do about it. If a consequence is given or
            a warning appears to be withheld and you’re not sure why, ask. Understand the
            enacted behaviour policy by engaging with those who do the enacting each day.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I once worked with a new teacher who said the first thing she wanted to do
            was to co-construct a set of rules for her classes. A nice idea but one that failed to
            acknowledge the school already had a set of expectations and a process for dealing
            with poor behaviour, alongside a process for recognising effort, achievement and
            positive behaviour. There may be rare occasions when creating your own set of
            classroom rules is necessary but the first port of call is always the behaviour policy.
            Why are we talking about a policy when the aim is to understand how you
            should act in lessons? The behaviour policy, when properly understood (and when
            properly implemented by school leaders), should clearly direct your behaviour in
            lessons. It won’t tell you what to do in every situation, but it should be your map
            to traverse the range of behaviours you are likely to encounter.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Practice prepares us. Not for every eventuality, but for the processes we’ll repeat.
            It’s true, you can’t prepare for everything, but warnings, consequences, the act of
            resettling a class or calming a student are all scriptable moments.
            Your practise should answer the following questions:
            - How do I promote positive behaviour through clear instructions?
            - How do I settle a class when needed?
            - How do I give warnings?
            - How do I call for support when needed?
            - How do I give a consequence or send a student out of the classroom?
            - How do I respond to (rare cases of) extreme behaviour (e.g. a fight)?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unfortunately, you can’t practise in VR. When my wife was training to be a GP,
            they’d hire actors to play patients. These actors would cry on demand or get cross
            when a diagnosis wasn’t to their liking. Whilst a helpful colleague may play the
            part of the recalcitrant youth, it is often the case that you can’t rehearse what is
            unpredictable. Teacher trainers (and schools) don’t seem to have the budget for
            actors. Perhaps it’s the difference between one actor (who can play many patients)
            and the 30-odd you’d need for a realistic classroom. The predictable part of behav-
            iour management is not the students’ behaviour. It is yours.
            If your training, policy or growing experience can provide effective answers to
            the questions above then these can be rehearsed to the point of automaticity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What if my school’s behaviour policy doesn’t work?
            A policy may fail to answer the questions above so, before we turn to what we will
            be practising and how, it’s worth considering what to do in those circumstances.
            Equally, it’s worth considering why some behaviour policies go unused or ignored.
            Teachers, not just new teachers, fail to use school behaviour systems for all sorts of
            reasons. They don’t like the system. It’s too strict or not strict enough. They want
            to do their own thing. At times, there might be genuine concerns that the behaviour
            policy doesn’t manage behaviour well. If it doesn’t work, why should I use it?]]>
			</paragraph>
			<paragraph>
				<![CDATA[I once worked in a school where the behaviour policy had buckled under the
            weight of detentions set. Students walked into detentions months after wrong-doing
            totally unaware of what they had done wrong. At another school, we weren’t even
            able to set detentions. That was done as a last resort by staff above my pay grade.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In these ways, behaviour policies can fail because staff are deterred from using
            them. When a leader, or anyone else for that matter, asks why you’re giving so
            many sanctions or detentions, it can feel like sanctions or detentions are actively
            discouraged. Even when, in reality, these things are mandated by the behaviour
            policy, a codified expectation of what staff are meant to do.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A policy not working will also look different in different schools. One sign is,
            obviously, student behaviour, but a more telling one is how students respond to
            the language, sanctions and consequences of the policy itself. If one student shrugs
            when given a detention, it doesn’t mean much. If no student cares about the conse-
            quences set, the policy isn’t having the desired effect. The knowledge you develop
            of the policy is not just what is written on the page.
            If it feels as though your policy behaviour policy isn’t working, there are several
            actions you should take.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Talk to and observe other teachers
            If others don’t feel as you do, seek to understand before critiquing or rubbishing.
            A behaviour policy at your current school might be different to one at a placement
            school. Different doesn’t mean terrible, ineffectual or wrong. Our aim is knowl-
            edge, an understanding of how better behaviour has been achieved in the same
            corridors and classrooms we inhabit.]]>
			</paragraph>
			<paragraph>
				<![CDATA[List the questions you have. What happens when I send a student out? What
            kind of behaviour gets a student a suspension? Is it acceptable for a student to…
            Ask colleagues who know the policy well. Ask to watch them. I once watched a
            teacher who was so much better at behaviour management than me; it was a reve-
            lation. Her classroom was calm. Her students were happy, engaged and committed
            but also compliant. Observing her, I noticed how little she said. Even when giving
            a warning, her language was precise and to the point. She didn’t have to say more
            because she was clear, was known to follow through and, therefore, was respected
            by the students. The point of this story is not to say less and, as we’ll see, asking for
            clarity is not enough; we need to know what we’re being clear about. But, watching
            her, I started to understand where my enactment of the policy was haphazard or
            ungainly. I didn’t just take the principles of clarity or precision from the observa-
            tion. I took phrases I could use later that day, in lessons of my own.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Give feedback to leaders
            The best leaders want to know as much as possible about what it is like to learn
            and teach at their schools. They want to know this because, for them as well as
            you, knowledge is the key to expertise. Like all humans, your school’s leaders
            might bristle at the thought of hearing that the behaviour policy isn’t working.
            Receiving such feedback from a new member of the profession might not be easy
            either. Leaders might not believe your feedback. A leader’s identity might be tied
            up in the success of what they’ve implemented.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Your feedback could take the form of questions, requests for advice or support,
            or observations you’ve made about the enacted behaviour policy. Firing off a feed-
            back email to a senior leader can feel like a quick and easy way to ‘help’. Email,
            though, bats a problem to someone else and misses our opportunity to engage and
            understand the intentions of a behaviour policy. Go to see someone; arrange a
            meeting if you need to. Be in the same room with them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Be clear on your own expectations in the meantime
            Even where a policy is non-existent, or feels that way, where sanctions feel ineffec-
            tual, where feedback leads nowhere, you are still the teacher of your class. You have
            a responsibility to manage it as well as you can, a responsibility to the quiet majority
            who turn up, behave and want to learn. Read back through the questions listed above.
            Your answers are your expectations. Clarity for students begins with clarity for you.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If this section has frightened you with the thought of the behaviour policy crum-
            bling beneath you on day one, please don’t panic. Of course, most school behaviour
            systems are neither perfect nor are they awful. A symptom of seeking to manage
            complex behaviour in a complex organisation is a set of practices that will often
            work most of the time, for most students. If you’ve worked through the steps above
            and feel unsupported, unsafe or burned out, if possible, it might be time to look for
            another school or talk it through with your training provider.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When I was a trainee, I was fascinated by a homemade ‘Wheel of Noise’ a teacher
            had stuck on their classroom wall. A, now quite floppy, cardboard arrow could
            point around the wheel at various ‘levels’ of sound from silence through to whis-
            per and then to group conversation. The wheel didn’t give a section for ‘Chaotic
            Hubbub’ but, in that school, it was hard to achieve anything else.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Certain types of behaviour are often called low-level disruption. ‘Low-level’
            reflects the minor, often quiet, nature of the behaviour: chatting rather than work-
            ing or quietly distracting others, for example. ‘Low-level’ is an unhelpful label,
            though, as this behaviour wastes hours of lesson time if it goes unchecked. Teachers
            try to manage this behaviour by shushing, by shouting and by creating ineffectual
            craft projects that hang neglected on the wall.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Part of the problem, as it so often is, stems from the need for clarity for the stu-
            dents and ourselves. If we say ‘Quiet’ when we mean ‘Silence’, we confuse students
            when we get cross about an expectation that goes unfulfilled. A classroom rule I’m
            fond of is that students should not get out of their chairs without permission. At
            times, I’ve got cross with students getting out of their seats only to remember I hav-
            en’t actually taught or told them not to. Like a lot of behaviour management, then,
            managing low-level disruption works better proactively rather than reactively. A
            teacher once told me about all the strange ways he’d try to get a class’s attention
            when they went off-task – standing on tables, clucking like a chicken, that sort of
            thing – but he was reacting to behaviour rather than planning for it. I would not
            recommend these strategies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As we saw in the previous chapter, managing low level disruption becomes part
            of our planning. Scripts for instructions include direct comments on behaviour
            expectations. Such scripts don’t always lead to the desired behaviour but they
            make it easier to respond to undesired behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For example, if you’ve asked for silence, quite clearly, and not got it:
            1. Narrate a countdown to focus back on you. Silence in 3… Looking this way
            with pens down in 2… 1 and zero.
            2. Watch and wait to ensure expectations are met.
            3. Reiterate explanations and warn class of consequences where they aren’t met.
            Year 9, the expectation was silence. We’ve not managed that. We’re going to start
            the task again. Anyone talking will immediately receive a warning. Off you go.
            4. Follow through. If students start talking when you’ve asked for silence, they
            should receive whatever warning or consequence mandated by your behaviour
            policy. Alice, you’re talking. That’s a warning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The same principles apply to classes who are only meant to be talking to their
            groups, only meant to be on one part of a court or field, only meant to get out of
            their seats to get resources.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Clarity of expectation, warning and consequence
            In the previous chapter, we looked at how planning clear instructions prepare
            the ground for students to behave. Clarity makes positive behaviour easier for
            students and managing behaviour easier for you. A student speaks when silence
            was the expectation. A student shouts out when your expectation is hands-up.
            A student is rude or unkind about another member of the class and the school
            expectations are clear. Clarity from you or school policies helps to define which
            behaviours will be encouraged and which will be challenged. Clarity of expecta-
            tion with no follow through when those expectations aren’t met is likely to erode
            student trust in your ability to manage their behaviour. If you’ve said students
            should ask to get out of their seats and they’re constantly getting out of their
            seats with no consequences, some of them will start to believe that what you say
            doesn’t matter.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowledge of how to use this clarity is really knowledge of students and sit-
            uations. Who needs a quiet warning, given so only they can hear? Who needs
            time to make the right decision? When does the class need to hear you and see
            you give the warning? Behaviour is a sequence of problems set by questions like
            these. The sections below offer strategies that do work but you need to learn how
            they work for you, when they work and with which students. This comes with
            practice.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Clarity of warnings
            Students need to be clear on where a warning will lead, even when the school pol-
            icy dictates what you do. Ways to make warnings clear include:
            - Use the language of the behaviour policy (for example, if you’re meant to say C1,
            warning, demerit etc., say those words).
            - Use the language of choice – give students ownership and responsibility for
            their decisions.
            - Be clear about the consequences.
            Example 1: If you continue to talk over others, you’ll be choosing a lunchtime
            detention.
            Example 2: You’ve chosen your first warning because you haven’t completed
            the task.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It’s worth noting down the warnings that you give to avoid getting lost in what
            you’ve given and what you haven’t. At times, it’s enough just to say That’s a warn-
            ing. Brief and to the point works well when you know a student won’t react and
            you can return to them later.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Our aim with consequences is to diffuse rather than escalate. If we’re too quick to
            jump on every behaviour that follows the warning or consequence, students can go
            from ten minutes at lunch to a five-day suspension. That said, certain behaviours
            can’t be ignored: direct rudeness and defiance need consequences.
            When giving a consequence:
            - Use the language of the policy.
            - Continue to use the language of choice.
            - Remain calm and in control. Delay giving the consequence if this isn’t possible.
            If a student has wound you up to the point that you’re cross, it’s better to move
            away from that student for a moment (if their behaviour allows), returning to
            them when you can calmly and clearly give the consequence.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unlike a warning, it may not be possible or desirable for all these things to be
            shared with students in the moment. For example, you may give the consequence
            as set out by a policy (a C, a detention, whatever) but give the student a chance to
            calm down before talking through their behaviour.
            In the first instance, you may just make clear the consequence.
            Example 1: You’re talking over me for the second time. You’ve chosen a C1.
            Example 2: I’ve now had to stop the lesson twice because you were talking
            over myself and others, you’ve chosen to lose five minutes of your break
            time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you have the chance, then or afterwards, to talk through the student’s behaviour,
            discuss how it affected the lesson and ask lots of questions to prompt their under-
            standing, all the better. Such conversations are usually not possible, or not done
            well, during the lesson because you will still have to be teaching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The ultimate consequence in the classroom is the removal of a student. In some
            schools, sending a student out is a rare occurrence; in others, it is a daily battle.
            Whilst our aim should never be to get students out, a teacher who is tentative about
            removing persistently disruptive, rude or dangerous students is likely to have a
            chaotic or unsafe classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Without meaning to sound like a broken record, your school’s behaviour policy
            should dictate how and when a student is removed. Often the behaviour policy
            will answer questions about when the student is removed, where they are removed
            to and who is notified. If it doesn’t answer these questions, seek your own answers
            from leaders.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Preferably, sending a student out should be a well-signposted act. The students
            in the class and the student in question should be unsurprised by the removal.
            Here, warnings serve a vital function. Warnings set a destination the student is
            heading towards unless their behaviour changes. That way, a sending out can be
            framed as an unfortunate result of the student’s continued choices.
            Example 1: Unfortunately, as warned, you’ve continued to distract others.
            You need to stand outside.
            At times, you’ll want to make the reason clear for the student and the class. At
            others, it’s more effective to leave it brief.
            Example 2: John, stand outside please.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Brevity has the benefit of removing any anger or frustration you feel from the lan-
            guage used. If a student is leaving, it helps no one to hurry them up or remind them
            they’ve ruined your lesson or your day (even if they have). If the student refuses to
            go, you’ll need to call for additional support.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As always, we need to use, and practise using, the behaviour system and the
            language it encourages.
            Example 3: John, that’ s a red card. You need to go and sit outside Mr Smith’s office.
            At its best, the system removes the need to come up with your own snappy
            phrases. You know you’ll have to say ‘red card’ or ‘C3’ or ‘removal room’ or ‘Mr
            Smith’s office’. Your job is to put those into a clear, concise phrase. Although you
            might feel a little silly, practising the phrasing in your classroom by yourself is
            really helpful]]>
			</paragraph>
			<paragraph>
				<![CDATA[When the student returns the next lesson, the slate must be clean and students
            must have the opportunity to re-enter the classroom and improve without past
            behaviour hanging over them. When possible, a quick chat with the student before
            they return – even if it’s on the door as they come back in – to repair and rebuild
            can be really valuable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How do we promote positive behaviour?
            Discussing behaviour management can feel overly negative, constantly preparing
            for the worst. We do need to plan for good behaviour but we can also plan for, and
            practise, the promotion of good behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Use praise
            When you’re first teaching and a student does something you ask or answers the
            most basic of questions, you can begin to gush with praise. ‘Perfect’, ‘Thank you so
            much’, ‘What a wonderful idea!’ I began to talk in a way so far removed from my
            usual style of communicating I was worried they would realise I was being insin-
            cere. Perhaps they did.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A knee-jerk view of reward sees it is the physical – often packs of sweets or
            chocolate bars you spend your own money on. Logan Fiorella cautions us that ‘a
            feeling of accomplishment [is] generally superior to relying on expected external
            rewards’.3 He goes on to say that simple praise ‘can play an important role in habit
            change if the rewards are uncertain or unexpected’.4 Further research suggests that
            young people will respond better to the immediate, rather than distant or future,
            rewards.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Praise is the main way you can motivate through reward. Although praise is
            given in the moment, and although it often isn’t planned, used well it is a way of
            proactively creating the environment you want in your classroom. To avoid insin-
            cerity, this praise should:
            - Be (or appear) spontaneous.
            - Be specific, related to aspects of what the student has been learning.
            - Focus on creating a sense of accomplishment.
            - Point students towards the intended identity of the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Examples of ‘spontaneous’ praise are hard to show you because, in their nature,
            they will emerge from the moments in your lesson, the responses from your stu-
            dents and work that is produced. Praise can both focus on the content students are
            learning and the positive behaviours they exhibit.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Subject-focused praise
            Example 1: Lucy, your use of the word ‘timbre’ there was spot on. You used it
            correctly and without me reminding you to! I want to hear more of our subject
            terms in class discussion. Thank you, Lucy.
            Example 2: Sammy, you’ve converted these fractions to percentages with no
            mistakes! That’s incredible. I bet you’re ready for something even more chal-
            lenging now.
            Subject-focused praise can feel distant from management of behaviour. But praise
            is motivating.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Positive behaviour praise
            We need to be careful we don’t praise students for things that should just be
            basic expectations. That said, where possible, thanking students for getting
            it right should be heard more often in our classrooms than reprimand and
            correction.
            At the start of a lesson we can use praise to set the right tone for the class:
            Example 1: Thank you, back row. You’ve all settled down and have written
            the title and date really nicely… Thank you, Travis – it looks like you’ve
            already started which is excellent… Thank you to the vast majority of the
            class – already silent and working.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It might feel silly to thank students for what you’ve just asked them to do but giv-
            ing instructions and then noticing the students who follow them well is incredibly
            powerful. Try it.
            During independent writing, we can use praise to encourage classroom habits
            we are teaching.
            Example 2: Year 4, you are using your purple polishing pens beautifully. I
            really like what I’m seeing in Meera’s work. She’s corrected the spellings like
            we talked about but she’s also replaced one of her adjectives with an even
            better word.
            Sometimes this kind of praise works well if you can project a student’s work
            under a visualiser or show the class in some other way.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Use the power of identity
            Intrinsic motivation is the motivation to act that comes within ourselves as opposed
            to a response to external reward. Habit expert, James Clear, explains that the ‘ulti-
            mate form of intrinsic motivation is when a habit becomes part of your identity’.6
            Identity is a powerful motivator to act or not and schools face an uphill battle to
            create an identity focused on learning rather than social approval.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The language we use is a powerful tool in creating an identity norm. High per-
            forming schools seem to have their own language as well as a set of ingrained rou-
            tines. I recently visited Yate Academy in South Gloucestershire, a school which
            has achieved results amongst the best in the country. Leaders at Yate have created a
            set of ‘mantras’ outlining the commonly held beliefs and aspirations of the school:
            ‘At Yate, we…’ The mantras cover the punctual, hard-working, community-fo-
            cused nature of the school.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At a whole school level, properly embedded, you can see how a set of mantras
            like this could use the power of identity to promote positive behaviour. Schools,
            leaders and teachers can pay lip service to such strategies without putting in the
            work to embed them in the long term. The Behavioural Insights Team recognise
            that we are motivated by ‘mutual support’ and committing to others. Harnessing
            the power of language, then, supports the creation of a shared identity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[An individual teacher doesn’t have the power of the whole school approach
            and, in your first year, it may feel like you don’t have the headspace to create a set
            of mantras for your classroom. This is fine. As elements of teaching become easier,
            though, your use of language is one route to consider as you progressively problem
            solve behaviour and motivation issues in your class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I was recently talking with a trainee about the behaviour issues in her class. We
            talked about routines and sanctions, aspects of her teaching she was aware needed
            more and continued practice. Our conversation ended and dwelt upon the source
            of the negative behaviour as she saw it: students struggled with Science and gave
            up quickly. She was aware that, with hard and focused work, she was on the road
            to achieving compliance from the class but she wanted more than compliance. In
            your first years of teaching, with some classes, compliance is a massive victory, but
            there’s a horizon beyond compliance: commitment. It’s unlikely that consequences
            or detentions on their own will reach the commitment we want from students
            (that’s not a reason to stop issuing them).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Out of our conversation, and from her not me, came the idea that scientists
            make mistakes and learn from them. This trainee wanted that nugget of truth to be
            understood, to be the lived reality of the class. We acknowledged that the Science
            classroom is quite different from the lab beyond school. We weren’t trying to turn
            the former into the latter. She started playing around with the language of the
            classroom: ‘In this class, we think like scientists’, and ‘Scientists make mistakes;
            so will we’. I’m not arguing that using phrases like this will solve your behaviour
            problems. But shaping a students’ identity whilst they’re in your classroom will
            take conscious effort.]]>
			</paragraph>
			<paragraph>
				<![CDATA[None of this means we should apply a label to a student until they believe it.
            Labels should be approached with caution in education, but the language of iden-
            tity can be incredibly powerful. Katy Milkman describes how ‘When we’re labelled
            “voters” (instead of people who vote), “carrot eaters” (instead of people who eat
            carrots whenever they can), and “Shakespeare readers” (instead of people who read
            Shakespeare a lot), it influences how we act, not just how we describe ourselves’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
            Knowledge new teachers develop spans from the lofty academic heights of the
            curriculum to the internal and existential questions of what we actually want. You
            might be ambitious for your students to develop strong intrinsic motivation to do
            well in your subject. Until we’ve laid the groundwork – and that is our main aim
            starting out – we can’t achieve those ambitions. The behaviour policy – intended
            and enacted – is what we need to know to solve the initial problems of behaviour
            in our classroom. I won’t lie and tell you that knowing the behaviour policy well
            will prevent all poor behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Behaviour management is both proactive and reactive. Proactively, we plan
            for clear routines and social norms which promote positive behaviours. We get
            to know our behaviour policy well so we can manage behaviour consistently. Our
            knowledge of policy should drive our instructions, warnings and consequences
            towards clarity and consistency of response to student behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
            1. Check your understanding of the behaviour policy. It’s easy to think we know
            something because we’ve been around it for a while. Do you know how, when
            and why consequences are given in other classrooms? If it’s new to you, lami-
            nate a summary of the policy and stick it where you’ll see it.
            2. Script and practise warnings and consequences. Whilst you can’t rehearse
            every type of behaviour that might come up, you can make automatic the
            92 Knowledge of student behaviour
            language of the policy. Make it easy for your mouth (and brain) to get through
            language you’re likely not used to (demerit, red card etc.).
            3. Tally up the ratio of praise to reprimand. All this talk of consequences and
            warnings is essential but can present an unbalanced view of the classroom.
            How are you also working to create a positive atmosphere in the classroom?
            Use praise and the language of identity. If the positive side of behaviour man-
            agement is proving difficult to embed, script and practise it as you have for the
            instructions, warnings and consequences.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When I was training to teach, I had close to no idea what pedagogy was. People – our
            trainers and other trainees – would ask, ‘What’s the pedagogy behind that?’ and I’d
            just fluster and change the subject. Or they’d say ‘Pedagogically speaking…’ and I’d
            zone out for a minute because what they were saying seemed unhelpful for planning
            tomorrow’s Year 9 lesson. Discussions about pedagogy, to me, seemed like a distant
            piece of theory I had to endure rather than something relevant to the moment. Of
            course, I was wrong. Whilst pedagogy can become theoretical and distant, we can’t
            escape it. Every time we plan, every decision we make about how to teach in the
            classroom is underpinned by pedagogical thinking. We can either do this implicitly,
            drifting along without intellectually engaging, or we can make this thinking explicit.
            Pedagogy is the thought underpinning your behaviour in the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There isn’t a curriculum of pedagogical knowledge for teachers. Perhaps such
            a curriculum is not possible. Too often this means that new teachers’ pedagogi-
            cal thinking is strung together from the strategies left over after a long period of
            trial and error. Some teachers and researchers have sought to break down teacher
            behaviour – pedagogy – into its component parts.1 These attempts at codification
            can be helpful but they generally give a set of broad behaviours to follow rather
            than a framework with which to make pedagogical decisions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pedagogy is not synonymous with evidence-based practice; they haven’t always
            gone and won’t always go hand in hand. But, in seeking to offer you a framework of
            knowledge for your pedagogical decisions, we can look to no more helpful a place
            than what evidence says about learning. That hasn’t always been the most obvious
            direction to look.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Education has, even in the recent past, seemed resistant to evidence. More
            recently, the tide – in England at least – has begun to turn. Pointing to a time,
            a place or a person as originator of this sea-change is difficult. Recognising the
            effects of the change is much easier. Founded by the Sutton Trust in 2011 with a
            grant from the Department for Education, the Education Endowment Foundation
            (EEF) aims to tackle disadvantage through the application of evidence. To achieve
            this, the EEF trials strategies and reports on the results, produces guidance reports,
            and offers a toolkit of evidence-based approaches for schools. Brought on the same
            tide, founded in 2013, researchEd runs conferences that aim to ‘bridge the gap
            between research and practice in education’.2 At these conferences, you’re just as
            likely to hear from a teacher as you are from a researcher or policy maker. Whilst
            both the EEF and researchEd delve into evidence beyond the classroom – they
            cover everything from attendance to teacher training – both have forced pedagogi-
            cal decisions to become about the evidence available to us.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pedagogical thinking will always have a limit. It’s clear why. Lee Shulman
            describes how pedagogical skill is ‘useless’ if it is ‘content-free’.3 You can’t think
            pedagogically without considering the content to be taught. Some might even argue
            persuasively that thinking pedagogically is next to useless4 and that thinking about
            subject and curriculum are more worthy of our time. Christine Counsell, one of
            the UK’s leading curriculum thinkers, warns against ‘an intransitive pedagogy, a
            pedagogy without an object’. An established teacher may well think more in terms
            of the merging of content and pedagogy. A new teacher should build up a mental
            model of the behaviours which promote learning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To do that, this section will seek to answer three questions:
            - What does ‘the evidence’ say about learning?
            - How should Cognitive Load Theory affect our pedagogical decisions?
            - What changes long-term memory for our students?
            What you won’t find in this section is an explanation of every teaching behaviour,
            what you need to know about it and how to do it. These chapters will, instead, pro-
            vide a framework to think about pedagogy. They will help you to create a mental
            model of learning and the learner which can direct future decisions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My friend turned to me one Inset Day and said, ‘The word “Research” has become
            a bit meaningless in education, hasn’t it?’ I can’t remember what we were being
            asked to do in the name of The Evidence. It was almost certainly workload induc-
            ing, untenable and destined to be quietly dumped onto the scrapheap of bad ideas.
            But it was, we were told, backed by Research. Infuriatingly, The evidence says… is
            not the helpful phrase it could be. In some utopian dream of education, research
            provides teachers with The Answers. Even more infuriatingly, a side-effect of the
            evidence-revolution in education has been the rise of those who will use vague
            references to research as the flimsy support for their flimsier ideas.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Part of the problem is the breadth of what might be considered ‘evidence’. Is a
            study based on interviews of teachers in small, rural primary school evidence? Is a
            large quantitative study of a classroom strategy in several secondary schools useful
            for primary teachers? A national survey of students might be considered evidence
            but should such a survey inform decisions I make in my classroom? And where
            does all this evidence come from? The problem of breadth is really a problem
            of different kinds of evidence. Randomised controlled trials might be conducted
            using classroom or whole-school approaches. With these trials, we can say X is
            likely to work (or not). Teaching also draws on evidence of what works from cogni-
            tive and behavioural science, which provide principles which underpin effective
            learning. How far, then, can teaching and teachers learn from the evidence from
            fields beyond education?]]>
			</paragraph>
			<paragraph>
				<![CDATA[These questions prompt several important acknowledgements:
            1. Evidence rarely provides an absolute answer. As the new teacher – you –
            develops knowledge of evidence-based pedagogy, arrogance about the right
            classroom strategies should be avoided at all costs. Instead, this knowledge
            gives us a lens through which to view our planning and our practice. This
            knowledge might hint at why what we’re doing is working, or why it isn’t. This
            knowledge might guide but it rarely gives a single answer to a question about
            classroom strategies.
            2. It’s easy to expect too much of evidence. Sometimes teachers ask questions of
            evidence we know can’t really be answered. What does ‘the evidence’ say about
            the types of questions I should ask Year 9 on Friday afternoon? What does the
            evidence say about how to teach boys [insert particular problem topic]? You
            might get lucky and find a specific study on just the question you were think-
            ing of but, invariably, niche questions in teaching and learning don’t sit upon
            mountains of evidence.
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[3. Evidence doesn’t offer one way to teach. On the one hand some research might
            confirm that certain styles of teaching aren’t effective in promoting learning.
            For example, inquiry learning – where students must discover information or
            processes for themselves, with minimal input from a teacher – has widely been
            discredited.1 Regularly asking students to learn about topics or acquire new
            skills with minimal guidance is a bad idea. To some, the evidence about the
            failures of minimally guided instruction makes the case for a type of teach-
            ing where, whilst the teacher doesn’t simply lecture the students, lessons are
            firmly led from the front, by the teacher. Even if we, as I do, see merit in this
            argument, what this teacher-led approach looks like can vary massively.
            4. But we ignore the evidence at our students’ peril. Arrogance is a real danger
            when we increase our understanding of how learning happens. At the other
            extreme, if we remain indecisive or ideology clouds our view of what the evi-
            dence is telling us we take the risk of making decisions that put the brakes on
            learning in our classrooms.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What
            can we get from ‘the evidence’?
            The four points above offer a negative view, focused on what the evidence can’t
            or doesn’t help us with. If evidence is only viewed this way, we can end up overly
            cautious of seeking evidence out or applying what we find. Tim Cain, Professor
            of Education, has a particular interest in how research meets the practice of the
            classroom. He offers several ways that teachers can forge a positive relationship
            with research.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Firstly, teachers should look beyond ‘the personal experience of individual
            practitioners or the cumulative assumptions and practices of a profession, both
            of which tend to be untested’.2 Secondly, by providing ‘conceptual frameworks’,
            research can support teachers to understand and reflect on student learning. These
            ‘conceptual frameworks’, like the mental models we have been discussing, gener-
            ate insights and rules of thumb as evidence is understood further. It is not that evi-
            dence trumps experience every time; instead, you plan and teach lessons in light
            of both evidence and experience. This section on pedagogy will help to build your
            conceptual framework of learning by focusing on foundational principles – things
            we must know – to plan for effective learning. Finally, Cain argues that research
            ‘can raise the quality of debate within a school and thereby improve the school as
            a learning organisation’. In such a debate, we must hold lightly to our position and
            be prepared to have preconceptions challenged.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Project Follow Through
            American history tells a cautionary tale. In the late sixties, one of the largest exper-
            iments ever to be conducted on education in America began. Across more than 180
            schools and with upwards of 70,000 students, researchers examined which teach-
            ing methods had the greatest impact on student learning. This was Project Follow
            Through. Ostensibly, the experiment pitted child-centred, child-directed learning
            against Direct Instruction from a teacher. Child-centred teaching was about free-
            dom for children to learn at their own pace, to dictate their own learning journey.
            Direct Instruction was the opposite: a teacher followed a carefully structured and
            often scripted curriculum engaging all students in the same activities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the child-centred approach, in a reading class for first graders, children largely
            direct their own learning:
            [Children] might be found scattered around the room; some children are
            walking around, some are talking, some painting, others watching a video,
            some looking through a book, and one or two reading with the teacher.3
            In turn the teacher has a facilitative role:
            The teacher uses a book that is not specifically designed to be read using
            phonics skills, and, when a child misses a word, the teacher will let the mis-
            take go by so long as the meaning is preserved to some degree.4
            In contrast, in the Direct Instruction classroom:
            Some children are at their desks writing or reading phonics-based books. The
            rest… are sitting with the teacher. The teacher asks them to sound out chal-
            lenging words before reading the story. When the children read the story, the
            teacher has them sound out the words if they make mistakes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In which class do students more effectively learn to read? Is it close? No. It isn’t.
            Students who direct their own learning don’t learn as much as those taught directly.
            The two groups were compared with a control group. In some cases, the child-centred
            approach didn’t fare as well as the control group. In everything from problem solv-
            ing in Maths to reading comprehension, the Direct Instruction group achieved the
            best results. There were even hints from the research that the Direct Instruction
            approach has a positive effect on students’ self-esteem.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Why hasn’t Project Follow Through had the seismic impact it deserves? It’s com-
            plicated. The Direct Instruction approach is complicated; it isn’t just a teacher
            telling students things (as it is sometimes caricatured). Intensive development and
            training time are required to make Direct Instruction approaches viable. Whilst the
            experiment was large, it covered measurable subject-areas – English and Maths in
            particular. Applying the lessons of Project Follow Through would not be as simple
            as deciding to apply those lessons, particularly for the individual teacher.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But some would offer less diplomatic reasons for its failure to make inroads.
            In the angrily titled ‘Why education experts resist effective practices’, Douglas
            Carnine bemoans the entrenchment of unevidenced positions based on ideological
            rejection of ideas because they come from the wrong voices or draw the wrong
            conclusions. Recalling the failure of Project Follow Through to win the argument
            for Direct Instruction, Carnine describes education as an ‘immature profession’
            because expertise is based on ‘the subjective judgments of the individual profes-
            sional’. You can see his point. Why should opinion trump evidence?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Direct Instruction has come to mean many things, from the scripted lessons from
            Project Follow Through (with capitalised letters – Direct Instruction) to any teach-
            er-led approach (often called, confusingly, direct or explicit instruction). Evidence
            supports the behaviours of direct teachers, things like reviewing content regularly,
            checking student understanding, modelling processes and obtaining a high suc-
            cess rate. Often, when we list these behaviours direct teaching can seem simple or
            obvious. Don’t all teachers do this? The short answer is no. A longer answer might
            explain that, as in Project Follow Through, certain types of pedagogy are treated
            with scorn for reasons other than their efficacy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The labels child-centred and Direct Instruction imply something that isn’t so
            clear cut. What is child-centred about teaching in a less effective way? Teachers
            imbued with a sense of their moral purpose must seek out the most effective
            teaching methods. The knowledgeable teacher recognises what evidence does and
            doesn’t say, what it can and can’t do. The question is learning. The tentative answer
            is evidence-based pedagogy. What that answer looks like might vary depending on
            how we define learning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What is learning?
            It’s an obvious question with a slightly elusive answer. Teaching is quite clearly
            about making learning happen. When you go into the classroom, you are trying to
            achieve learning. But what is learning? What will it look like if anything? What
            tangible changes will there be in students because of your teaching? Let’s look at
            some definitions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[An internal model of an external world
            Neuroscientist Stanislas Dehaene sees learning as forming ‘an internal model of the
            external world’. Helpfully, Dehaene’s definition recalls the mental models you’re
            already familiar with. These models help us to navigate the world, reduce errors,
            discover possibilities, recall information quickly and hypothesise. Dehaene explains
            that this learning is often unconscious. A common complaint about our school days
            is that we remember nothing from them. Dehaene’s expansive definition of learning
            includes the perceptions and ways of thinking that make up our mental models.
            Dehaene’s understanding of learning is a bird’s-eye view; there’s some distance
            between it and the day-to-day practice of our classrooms. The next definition
            brings learning a little bit closer to the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A change in long-term memory
            A straightforward, but regularly controversial, definition of learning is that it is
            a change in long-term memory.10 In a paper drawing on evidence from cognitive
            science, Paul Kirschner, John Sweller and Richard Clark use this definition to chal-
            lenge minimal guidance approaches to teaching. Provocatively, they go on to say,
            ‘if nothing has changed in long-term memory, nothing has been learned’. Think
            about that for a moment. At the risk of sounding ridiculous, How does that defi-
            nition make you feel? I ask because it’s not uncommon to bridle when confronted
            with what feels like a cold view of learning. It’s not uncommon to feel uncomforta-
            ble. I initially felt uncomfortable with this definition because, looking back on my
            early lessons as a teacher, I’m sure lots of my students wouldn’t have remembered
            much at all. It’s not uncommon, in our discomfort, to reach for examples that will
            challenge or undermine the definition.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whatever our feelings about it, this definition can radically affect what we do
            in the classroom. Before we get to that point, we need to understand what the
            definition doesn’t mean. Talk of memory somehow usually leads to talk of facts.
            Focus on facts leads to accusations of rote learning. And soon this is no definition
            of learning but a description of regurgitation. In this way, the definition can be
            dismissed without ever being refuted. But this is not simply about factual learning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As we’ve seen already, knowledge extends beyond the parameters of what we
            can list on the page. A student practising their free-throws in basketball is commit-
            ting the process to memory. A student who has been taught orthographic drawing
            in D&T, who has practised it to automaticity, has committed it to memory.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A change in long-term memory is a powerful definition because it points to
            some tangible ways in which we can change what we do. It forces us to move from
            ‘I taught it; they learned it’ to ‘I taught it; how do I make sure it sticks?’ It’s ok to
            want a more expansive definition of learning as long as we let this one challenge
            us. Cognitive science offers an answer to why humans often forget and sometimes
            remember what they’re told or what they do. Before we turn to those strategies,
            let’s spend a moment considering what can’t be defined as learning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What isn’t learning?
            I was in an interview for Assistant Headteacher of Teaching and Learning. The
            Chair of Governors had just asked me what learning looked like and I could tell
            my answer wasn’t going well. None of it was going well. I continued talking about
            what learning looks like even though I couldn’t quite make sense of the question.
            Looking back, I think I spoke a bit too much – for her liking – about what I do as
            a teacher to make learning happen. She stopped me mid-sentence. Always a good
            sign in an interview…]]>
			</paragraph>
			<paragraph>
				<![CDATA[‘Yes’, she said, pointedly. ‘But what does learning look like?’
            As it turned out, she meant: if learning is happening, what can you see? It
            dawned on me suddenly that viewing learning as an invisible process taking place
            in the mind would make it difficult to answer this question well. Mercifully, they
            sent me home at lunch.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The governor’s question reveals a problem we all encounter when we talk about
            learning. To what extent are we talking about learning? To what extent are we
            talking about fluff that sounds exciting but ultimately isn’t learning? Professor Rob
            Coe has helpfully created a list of the things we can often mistake for learning. He
            calls these ‘proxies for learning’, things that look like learning but don’t guarantee
            it. These proxies aren’t bad things; it’s not that we must avoid them. They’re just
            easy to mistake for learning. They are:
            - Students are busy: lots of work is done (especially written work).
            - Students are engaged, interested, motivated.
            - Classroom is ordered, calm, under control.
            - Curriculum has been ‘covered’.
            - (At least some) students have supplied correct answers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We want our classes to be engaged and ordered. We want students to get the right
            answers. We want to get through the curriculum. But none of these things mean
            that students have learned anything.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coe offers a ‘better proxy for learning’: ‘Learning happens when people have
            to think hard’. Of course, this is still a challenge. My three-year-old daughter has
            already mastered the tilt of her head, her hand to her chin and an exaggerated
            ‘Hmmm’ to signal she is thinking. It would be easy just to turn thinking into another
            proxy where we look out across a class and ask ourselves ‘Do these children look
            like they’re thinking?’ Perhaps we could count up the number of hands reaching
            for chins. In this way, I don’t see how ‘thinking hard’ can be a proxy for learning
            because, unlike other proxies, it’s difficult to see.]]>
			</paragraph>
			<paragraph>
				<![CDATA[So thinking hard becomes something we don’t necessarily look for during the
            lesson; it’s something we plan for before the lesson. Understanding some evidence
            about teaching or learning can’t answer every single pedagogical question your
            classroom poses. This understanding gives us a lens through which to view our
            actions and those of our students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Planning for proxies
            To take Coe’s proxies as an example, if you’re planning activities for a lesson you
            could ask yourself two sets of questions. In the first set, you ask:
            - How can I make sure the students are engaged?
            - How should I keep the students busy for the whole hour/day/morning?
            - What sorts of activities will keep them calm?
            - What do I need to cover?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Asking these questions isn’t wrong; it’s just that there isn’t a direct line between
            the answers and student learning. Coe’s ‘better proxy’ would lead us to ask differ-
            ent questions:
            - What do I want students to learn?
            - How can I get them to think hard about that?
            - What might get in the way of thinking hard and what could I do about it?
            - How will I know that they are thinking hard?]]>
			</paragraph>
			<paragraph>
				<![CDATA[After we’ve asked these questions, we could turn back to some of the others if they
            seem useful. Grounding your thinking in what learning is and what it isn’t is vital
            to getting your planning right. Armed with these definitions and proxies, we have
            a target to aim at and a measure of our success.
            An important caveat lingers behind our answers to these questions. In a school
            where behaviour is poor or difficult to manage, we celebrate those times when stu-
            dents busily engage in lessons and get through content. The second set of questions
            are easier to answer and the answers are easier to put into practice when behaviour
            is well managed. Whilst as a new teacher it’s important to develop a solid mental
            model of learning, this will only help you in a classroom where students are ready
            to learn. If necessary, return to Part 3 and look at the steps you can take to embed
            positive behaviour.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
            We could go on about evidence in education for much longer. In many ways, this
            chapter skirts the sides of a mountainous terrain of viewpoints and controversies.
            No teacher can expect evidence to answer every question about the classroom. No
            teacher should blindly follow every new trend packaged as evidence. We must
            question and interrogate. We should, however, do this from a place of knowledge,
            knowledge of what learning is and isn’t, what evidence does and doesn’t suggest.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thinking hard about evidence-based learning leads us to plan lessons where stu-
            dents must think hard. Perhaps even more helpfully, evidence-based thinking
            helps us to avoid getting confused about proxies for learning, potentially good
            things that can masquerade as learning.
            To really use our understanding of what learning is and isn’t – and how we get
            more of it in our classrooms – we should delve further into what makes learning
            happen. If learning is a change in long-term memory, we need to understand what
            prompts and causes those changes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
            1. Reflect on proxies. Look back at a lesson you taught today or this week with
            the list of proxies:
            - Students are busy: lots of work is done (especially written work).
            - Students are engaged, interested, motivated.
            - Classroom is ordered, calm, under control.
            - Curriculum has been ‘covered’.
            - (At least some) students have supplied correct answers.
            To what extent have these things driven your view of learning in those lessons?
            Look ahead to lessons coming up tomorrow or later in the week. How will you
            ensure these proxies don’t colour your view of success in the classroom? Stick
            them up somewhere as a reminder to yourself.
            2. Plan for thinking. Rob Coe’s better proxy was ‘thinking hard’. Whilst this is
            exactly a proxy because it’s difficult to observe ‘hard thinking’, it’s worth plan-
            ning for thinking. As Daniel Willingham tells us, ‘memory is the residue of
            thought’. Look at upcoming lesson plans or resources. Student thought will be
            directed by activities you plan, the resources they use and your explanation
            and instruction. Unthinkingly planning an activity that keeps students busy
            but doesn’t direct their thinking is easy to do in a rush. Slow planning down to
            focus solely on the thinking students will be doing. To grapple with content –
            to think deeply about it – students need content-rich lessons. Reading, clear
            and planned explanations, rich demonstrations and well-structured talk can
            all give students time to dwell on what they are learning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[You often hear students tell you I learn best when… but how the sentence is fin-
            ished usually speaks to preference rather than efficacy. I learn best when I’m lis-
            tening to music. I learn best when I’m sitting with my friends. I learn best when I’m
            eating. Students will often come into your room and make you feel like the only
            teacher who doesn’t let them do X. Miss Clark always lets us sit where we want,
            eat what we want, do what we want, and listen to music. Why don’t you? Although
            students won’t always accept it, your answer is that you know more about the best
            conditions for learning than your students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Constructing a mental model of learning based on the evidence available to us
            is vital. Our intuitions about learning can be limited or unsupported. Cognitive
            Load Theory has quickly become an all-encompassing way of understanding the
            learning process. What follows is a brief introduction with some practical steps for
            implementation in the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What is Cognitive Load Theory?
            The originator of Cognitive Load Theory, John Sweller, describes how Cognitive
            Load Theory is founded on the premise that ‘human cognitive processing is heav-
            ily constrained by our limited working memory which can only process a limited
            number of information elements at a time’.1 Because of this, our cognitive capacity
            becomes overloaded when we attempt to process too much new information at
            once. As a guide for teachers explains, Cognitive Load Theory is grounded in two
            principles:
            The first is that there is a limit to how much new information the human
            brain can process at one time. The second is that there are no known lim-
            its to how much stored information can be processed at one time [my
            emphasis].]]>
			</paragraph>
			<paragraph>
				<![CDATA[Cognitive Load Theory doesn’t simply diagnose a problem. It points us towards
            the solution: long-term memory. The conclusions we draw from Cognitive Load
            Theory are both simple and complex:
            - We must teach in a way that doesn’t overload cognitive capacities. It isn’t
            helpful to speak of minimising cognitive load. To learn, as we’ll see, we need
            to think hard. Zero cognitive load probably means you aren’t thinking much
            at all.
            - We must teach in a way that takes account of what we know of long-term mem-
            ory. This will be the focus of the next chapter.]]>
			</paragraph>
			<paragraph>
				<![CDATA[These conclusions are simple because they are straightforward and logical. They
            are complex because you could spend your career trying to optimise your teaching
            to achieve these two ideas and still have work to do.
            At a glance, Cognitive Load Theory is an appeal to simplify but there is hidden
            complexity to our applications of it. Cognitive load affects the mind and the learn-
            ing process in different ways; there is intrinsic load, the focus needed to learn, the
            required load during a task, and there is extraneous load, anything superfluous or
            unnecessary for learning. But cognitive load research goes beyond simply pointing
            out how students are likely to become overloaded. As we’ll see it offers teachers
            specific strategies and principles for how to teach.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Before we get there, let’s just look at some examples. A Year 5 student has been
            tasked with comparing the National Parks they have studied. In separate les-
            sons, the student has learned about the Lake District and Pembrokeshire. Ideally,
            because of those lessons, knowledge about the parks is now ‘stored information’.
            The intrinsic load in the comparison task is the generating or finding of the
            similarities and differences between the parks. Extraneous load could come in
            several forms: superfluous information in resources or unnecessary, distracting
            images.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In a Year 6 music lesson, students might be learning about rhythm by tapping
            out a repetitive beat. Students have learned what the word rhythm means and have
            watched their teacher tap a rhythm with two fingers on the edge of a desk. Now
            it’s their turn. Paradoxically, the intrinsic load – what students must focus on and
            learn – is the beat they are tapping but this may also be a source of extraneous load
            as other students’ unrhythmic tapping becomes a distraction. Involve ukuleles and
            the extraneous load is likely to become unbearable.
            Cognitive Load Theory underpins the work we’re doing in this book to
            develop expertise. Your working memory is limited too. Whilst you have more
            embedded in your long-term memory than your students, making teaching
            manageable is about embedding knowledge and process into your long-term
            memory.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How should the principles of Cognitive Load Theory inform
            my teaching?
            The problems posed by the classroom, the persistent challenges of teaching, start
            with how we will get students to learn content they don’t know and aren’t inter-
            ested in, all while those students are distracted by the environment, their peers or
            even the teacher. You may have students who, lesson to lesson, don’t seem to recall
            what you tell them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mary Kennedy’s persistent challenges remind us we need to ‘portray the cur-
            riculum’ and ‘enlist student participation’. Whilst knowledge of behaviour (from
            Part 3) and knowledge of curriculum (from Part 5) both will help us to face these
            challenges, addressing the challenges offered by cognitive load help to cut to the
            heart of how learning happens, and whether it’s happening in your classroom. As
            you read this introduction to cognitive load, there’s a real ironic danger it backfires
            and leads to cognitive overload for you. In particular, what makes sense on the
            page might be overwhelming to apply.
            My advice to stop that happening would be:
            - Read the chapter through once, taking in what you can.
            - Once you’ve done this, turn again to the chapter as you plan a single lesson.
            Refer to the principles as you decide on content and activities for your lesson.
            - Refer once more to the chapter as you plan another lesson. Look again at the
            strategies for managing cognitive load. Plan a lesson considering at least one of
            the following: a Worked Example, a Completion Problem or the Split Attention
            Effect.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plan for intrinsic load
            Daniel Willingham’s maxim ‘memory is the residue of thought’ takes on richer
            meaning in the light of Cognitive Load Theory. Coupled with Rob Coe’s encour-
            agement that learning happens when students have to ‘think hard’ we have a good
            idea of what it means to maximise intrinsic load. Two things determine intrinsic
            load: how complex the information is and how much the learner already knows.3
            In this way, intrinsic load is not a given for a topic or task. Not every student
            experiences intrinsic load in the same way. Knowing this gives us two ways to
            think about intrinsic load. In the long term, we can increase a student’s capacity to
            manage more complex information by ‘changing the expertise of the learner’4 (in
            other words, by teaching them). In the short term, we can change ‘what needs to
            be learned’. Through planning, we can focus on the specific components we need
            students to learn; we can break content and tasks down to make the learning pro-
            cess more manageable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is the aim of all lesson planning: to focus students’ minds on what it is we
            want them to learn. Look at your planning or resources for upcoming lessons.
            Consider the question What will students need to think about here? at every stage
            of the lesson. Whilst planning for intrinsic load is our guiding principle, the fol-
            lowing strategies shed more light on how we do this.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Manage element interactivity
            Cognitive Load Theory contains within it the truth that human beings find it diffi-
            cult to hold more than four5 new pieces of information in their minds at one time.
            Without meaning to, we can give students too much to juggle in tasks and activities
            we plan. Something which seemed straightforward to us strains the limits of their
            working memory. As John Sweller and colleagues point out, ‘Working memory is
            simply incapable of highly complex interactions using novel… elements’.6 Sweller
            concludes that teaching which requires the combination of lots of unfamiliar ele-
            ments is likely to fail. Element interactivity, therefore, is the term used to describe
            how different knowledge, instructions and information interact.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What does element interactivity look like? Firstly, it can be described as high or
            low, depending on the design of a task or the content to be learned. Sweller gives
            the example of learning a language. Learning individual bits of vocabulary from
            a new language can be difficult but the element interactivity is low. One word in
            the target language interacts with an equivalent (or two) in the native language.
            A languages teacher would be quick to tell you that they don’t spend lessons just
            working through the dictionary one word at a time. Understanding the grammar
            of a new language forces language learners to manage a dangerously high number
            of elements. Not only is vocabulary important; now word endings, tense, voice:
            the relationship between words becomes the thing the learner is holding in their
            minds. Students will be more than capable of tackling complex grammar when
            their knowledge of vocabulary and tense – built separately and over time – have
            been embedded in long-term memory.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When we plan a task, the process is usefully seen through the lens of element
            interactivity. All we’re doing here is assessing the complexity of the task; to begin
            with, we’re not simplifying it nor are we making it more complex. Ask:
            - How many new elements do students have to think about at once?
            - How much will students have to combine or use the new elements together?
            - What knowledge or skill do students have embedded in long-term memory that
            they can use in this task? How can I prompt them to do this?]]>
			</paragraph>
			<paragraph>
				<![CDATA[If students will have to use various things which are new to them in combination,
            it’s likely they’ll be overloaded. This doesn’t necessarily mean they all won’t be
            able to do the task. Some might but it probably isn’t worth the risk. We can
            always accelerate the learning where students are finding it manageable. A room
            full of students with their hand up because they feel they can’t do it is
            counterproductive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Assessing element interactivity is not even half the battle. We have to do something
            about it. Whilst it is unproductive to make things easier for students, where our
            tendency is towards complexity, we should consider how to break content down.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It’s easy to tell you to break content down but harder to specify by how much or into
            what kinds of pieces. Here again, Cognitive Load Theory challenges our intuitions
            and the preferences of our students. Students might prefer to play a match than
            practise passing drills. They might prefer to write the full story rather than practise
            individual sentences. They might prefer to ferociously debate the interpretations of
            a historical event than encounter each of them in detail and gradually. We too might
            prefer to give them the freedom of the former rather than spend time in the latter. In
            doing so, though, we guarantee slow progress towards proficiency in netball, story
            writing and debating. It’s true that we may demotivate students if the drill and the
            practise never lead to a finished product. But success too is a motivator and success
            is unlikely without breaking down and practising the component parts.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are two broad approaches to consider when breaking content down into
            manageable stages:
            - The ‘part-whole approach’ – ‘where the individual elements of the material are
            introduced to the learner first, before the integrated task is introduced’.7
            - The ‘whole-part approach’ – the whole or bigger task is introduced to students
            initially but the teacher breaks it down into sub-tasks or elements.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Even when you’ve settled on an approach, How far should I break this content
            down? is a fair question and one we will return to below. If we remember that
            learning happens when students are thinking hard, we must direct their thinking.
            Deciding on how far to break content down will take a bit of trial and error.
            It’s also important to remember we can both break down content – what we are
            teaching – and tasks – the way we are teaching. We need to see this distinction
            because it’s possible to have lots of mini-tasks focused on big unwieldy bits of con-
            tent. Breaking down both will help students to manage cognitive load. Let’s look
            at two examples, one for breaking down content and one for breaking down tasks,
            both focused on teaching a play.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When teaching Macbeth as a GCSE text, I want students to know something
            about King James I. I could give a short explanation about all those things and then
            expect students to know them. Perhaps I’ll find some reading that reflects what I
            want them to know. But there’s a problem. In my mind, King James I might be one
            thing about Macbeth. In the students’ minds, King James I is a subtopic of Macbeth
            with several strands. To break this content down, it first needs to be defined (by me
            or by the curriculum):
            - King James I had become king three years before Macbeth was written.
            - King James I was Scottish.
            - King James I was Protestant.
            - King James I believed in the divine right of kings.
            - The year before Macbeth was written, an attempt was made on the life of King
            James I by a group of Catholics – the Gunpowder plot]]>
			</paragraph>
			<paragraph>
				<![CDATA[If I make a list like this or if the curriculum provides one for me, I can start to see
            what is manageable together and what should be separated. Students can handle
            learning when King James I became king and that he was Scottish in quick succes-
            sion. I may group other content that works naturally together. For example, I could
            save the concept of the divine right of kings until we reach Act 2 Scene 2 where
            the concept relates to what we will be reading. The Protestant and Catholic conflict
            that acts as the backdrop to his reign will, however, take more careful thought and
            planning. Students will need to understand the terms Protestant and Catholic but
            also the historical background. The introduction to the Protestant/Catholic content
            will, therefore, be broken down into its own lesson, a topic worthy of attention in
            its own right. Even this lesson needs to be broken down. Definitions of words and
            reading of historical context offer further divisions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s turn our attention to the breaking down of tasks and processes. If I want
            students to write analytical paragraphs about the play we’re reading as a class,
            I could just get them to write the paragraphs (with lots of sentence starters for
            support) but if I really want them to retain and re-use what they’ve learned, I’ll
            focus on the different teachable elements. We might spend a lesson on introduc-
            tory sentences where we try to accurately express a supported, personal interpre-
            tation about Macbeth, ambition or evil. Following this, a lesson on how to use and
            embed quotations will be necessary to embed and manipulate the content students
            are discussing. Of course, the aim is fluent analysis and I should not teach the parts
            without aiming to reach the whole.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The following sequence of questions might help you to consider how best to
            break content down:
            - What end goal am I working towards?
            - What component skills and knowledge is that goal made out of?
            - Which components are priorities for teaching individually? These could be the
            things students need to get right to reach the goal. They could also be things
            most relevant to future topics and tasks.
            - Having chosen a priority to focus on, how can I get students to think about or
            practise just that part? Which activities will best create intrinsic load focused on
            the relevant component?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Minimise the extraneous
            In my misspent early years as a teacher, I kept a few activities held in reserve for
            difficult classes I wanted to keep busy (and behave). One such activity was the
            board game sequence of lessons. My students made board games about anything I
            could vaguely relate to the topic at hand, including some truly awful ones about
            Shakespeare and English grammar. I vividly remember the first time a particularly
            difficult class were cutting and sticking in near silence. If only for a moment, I
            thought I’d cracked teaching but, of course, the class were learning nothing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If this sounds like fun to you – I can assure you it wasn’t – I’m not against some
            project-based or open-ended activities for students. But setting students off on
            open-ended projects over the course of several lessons is an inefficient use of time
            and of the teacher’s expertise. From a cognitive load perspective, the core prob-
            lem with these approaches is their failure to minimise the extraneous. Students
            get bogged down in design and complex systems of rules; they barely think about
            Shakespeare or grammar at all.
            Extraneous load can be found in the design of our tasks, the detail of our expla-
            nation, the presentation of our resources. The teacher who shares a self-deprecating
            joke over a silent and focused class is adding unnecessarily to their cognitive load.
            Perhaps this is where the cognitive load informed classroom starts to feel to you
            like a humourless vacuum but it doesn’t have to be. Maybe the class can handle
            the joke and what they’re doing. It isn’t necessary to remove your personality from
            every moment of your lessons. It isn’t necessary to stop telling jokes. I’ve told a lot,
            particularly bad ones. But our knowledge of Cognitive Load Theory forces us to
            reflect on our decisions and actions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When planning tasks or designing a resource, even when thinking about your
            classroom, consider how to minimise extraneous load. Anything that is likely to
            distract students from thinking about what you want them to is worth cutting back.
            In planning, this is about considering each moment of the lesson and looking for
            any point where students may have their attention drawn to something which
            won’t aid learning. Resources should not contain the superfluous or distracting,
            both in terms of visuals and writing.
            To some, the idea of ‘designing your classroom’ will feel alien and unwelcome.
            It certainly did to me. In my third year of teaching, I had clearly done so bad a job
            of making my room look nice that two colleagues came and did all my displays for
            me, unintentionally teaching me to leave the ‘design’ of my room to others for the
            rest of my career. If someone tells you that your displays or room could have a bit
            more colour or excitement, look them straight in the eye and say, ‘I’m doing my
            best to minimise extraneous load’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teachers concerned about extraneous load should be careful not to eject every
            interesting story or tangent from their lessons. Minimising extraneous load is
            not about removing the colour, the detail or the life from the topics we teach.
            Shakespeare offers fascinating tangents, stories and language to explore. As does,
            in my opinion, English grammar. When our tasks or resources take us too far
            from these things, we’re likely to be minimising the intrinsic and maximising the
            extraneous.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Aim for independence
            Cognitive Load Theory has become the main bit of research underpinning the argu-
            ment that teaching should be an explicit or direct affair. I agree that what is called
            explicit or direct teaching – highly interactive teaching led by the expertise of
            the teacher, full of lots of questions and practice – is the best bet and a suitable
            classroom norm. That doesn’t mean you can never set projects or work in groups.
            Whatever, and however, we’re teaching, our aim should be to build those chunks
            of content back into something recognisable as knowledge or skill.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At one school I worked at, some students engaged in an extended project that
            culminated in an exhibition in the sports hall. I wandered that exhibition in awe,
            listening to students explain their projects, confidently rich in detail. One boy in
            particular sticks in my mind because of his passion for black holes, something
            about which I know almost nothing. I asked question after question only to have
            them all answered energetically. Schools can and should make space for this kind
            of independence and see it as the natural destination of education.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What classroom strategies does Cognitive Load Theory
            encourage?
            Thus far, our focus has been principles but it would be a reductive reading of the
            Cognitive Load Theory research to just look at these. Much of this research inves-
            tigates specific strategies that manage cognitive load well whilst embedding new
            knowledge and process into long-term memory. Practical applications of Cognitive
            Load Theory are not simply the good intentions to manage load a bit better. It
            should prompt real change in your classroom. Some of the strategies below are
            things you can fold into your planning from tomorrow.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Again, consider how they might solve the problems you’re facing right now.
            If they don’t seem to, return to them in the future. Students struggling with com-
            plex processes might benefit from the Worked Example Effect. Students who ‘don’t
            know how to start’ can engage in Completion Problems, initially being guided
            through the steps of a task. Students who are overloaded by your explanations
            might benefit from your knowledge of the Split Attention Effect. Knowledge of
            these things expands your repertoire of strategies for guiding students towards
            competence and skill. None of these are tips or tricks to try in the classroom in
            that superficial way. They are solutions to the complex problems of thinking and
            learning. Consider whether they solve the problems you are facing before throwing
            them at your planning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Worked Example Effect
            Worked Examples are completed examples of problems or questions. Worked
            Examples are not simply resources available to students if they are struggling.
            They are objects of study. Steps are revealed; strategies highlighted. You could see
            how Worked Examples could be powerful in Maths where tasks frequently aim at
            a specific solution but require mastering several steps. Complex division or multi-
            plication are processes where students can benefit from the specificity afforded by
            a clear Worked Example. However, Worked Examples can be used across subjects.
            Where tasks or questions must be approached in a particular fashion, the Worked
            Example can provide the blueprint for independent practice.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Worked Examples work best when:
            - They make clear every step a student will take.
            - They are a focus for discussion.
            - Discussion brings clarity about a process.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some discussion questions might focus on the approach in general, like ‘Why
            might this strategy work?’ Or, for two comparative examples, ‘What are the differ-
            ences between the approaches? Which approach do you prefer and why?’ Worked
            Examples should also have questions that focus student thinking on the subject
            content being taught. In an example of an analytical paragraph on Oliver Twist,
            questions could explore how the paragraph communicates Dickens’s intentions or
            how the paragraph unpicks the quotation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It’s unlikely your first attempts will produce perfect Worked Examples but they
            should highlight what students might struggle with and where your examples or
            explanations have been unclear. As you start to find Worked Examples that work,
            you can collect them for future use. Research also suggests that two or more Worked
            Examples might be better than one so don’t expect a breakthrough with students
            after one short example.8
            A Worked Example can’t complete a task for a student. We need to hand over
            responsibility – at times, gradually – to our students. That is where Completion
            Problems come in.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Completion Problems
            In a Completion Problem, part of a solution or process is revealed, leaving the rest
            to be completed by the student. An almost finished diagram, a table with some data,
            a question with the beginnings of an answer, the first two sentences of a paragraph.
            One frequent classroom refrain is, ‘I don’t know how to start’. The Completion
            Problem gives the student the help they need to get going. Our aim is to gradually
            hand over responsibility to students so that they can do such tasks with no Worked
            Example or Completion Problem necessary.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A geography teacher might want their students to confidently analyse climate
            graphs. In the last lesson, climate graphs have been introduced. In this one, stu-
            dents will have to work independently to draw inferences from the graphs. One
            way to do this would be to reiterate what a climate graph is – a graph showing
            average temperature and rainfall – and get students to answer a couple of questions
            about them. Some students might struggle; others might have missed or misunder-
            stood the explanation. Cognitive load from the multiple sources of information –
            the task, the graph, the new concepts – is probably too much. Instead the teacher
            projects the climate graph and the question which follows it under the visualiser.
            The teacher starts to answer the question, modelling their approach (more on this
            in Chapter 14). A sentence is started – everyone copies it – and then students are
            expected to finish it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A Completion Problem is a good middle step between a Worked Example and
            independence. Have a look at an upcoming lesson plan. Where is independence
            expected? If students are ready for it, fine – let them carry on. At times, though,
            what they need is the bridge between an example and competence. They need
            either one or more Completion Problems.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Split Attention Effect
            Students often must split their attention between two sources of information:
            the board and their book, the teacher and their friend, the clock and their work.
            The Split Attention Effect refers to a more specific tension: between the modes of
            communication used to explain and teach. A diagram often has labels helping to
            explain it. When the labels are separate from the diagram, John Sweller explains
            that, ‘Learners must mentally integrate the two sources of information in order to
            understand the solution, a process that yields a high cognitive load and hampers
            learning’.9 Integrating labels with a diagram reduces this split attention.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Implications of split attention extend beyond an integration of labels with dia-
            grams. Two researchers of the Split Attention Effect differentiate between visual
            working memory and auditory working memory. Visual working memory scoops
            up what we’ve seen, including what we’ve read. Auditory working memory does
            the same with what we hear. Integrating labels with a diagram may reduce split
            attention but both labels and diagram enter visual working memory. Students may
            need to combine and manipulate information from a diagram. Working memory
            may buckle before students think in a way that will support learning. Students
            manage cognitive load better when information is channelled to both visual and
            auditory working memory. Not only that, students are able to connect and think
            about content when we use both channels.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If this all feels a bit theoretical, consider what students are expected to attend
            to in a single lesson. Attentional issues abound where resources, particularly
            PowerPoint, brim with competing information. Initially, evaluating a resource’s
            attentional demands before a lesson or reflecting on explanation after a lesson can
            weigh heavily on your cognitive load. Focus on:
            - Integrating text and diagram in resources.
            - Avoiding text heavy resources that you plan to explain verbally.
            - Considering the balance between auditory and visual working memory in
            explanations.
            - Scripting and then practise explaining a diagram or picture with no text.
            - Removing PowerPoint slides or other aspects of a resource that are for you. Print
            and use them as an aide-memoir if you need. Don’t submit students to text heavy
            resources just there to remind you what to talk about.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowing about Cognitive Load Theory will only matter if it affects what you do
            in lessons. Recalling vaguely that splitting students’ attention is unhelpful as you
            explain a diagram with a complex key and set of notes improves nothing. Our
            understanding of Cognitive Load Theory should help us to pre-empt the pitfalls of
            cognitive overload in the planning stage of lesson delivery.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How should this affect my teaching?
            Improve your reflection
            We’ve been examining the knowledge needed to face the challenges of the class-
            room. Ideally, this knowledge would prevent most of the problems we face but
            often it works differently: we look back at a lesson, initially unsure of what went
            wrong. Students got stuck or didn’t get through as much as we expected them to.
            Students failed to answer what we considered basic questions in a mini-assessment.
            Students struggled with content they know well but perhaps in an overly compli-
            cated task. Cognitive Load Theory is a core element of your mental model of how
            students learn. Apply it to your classroom and shine a light on student learning
            and these problems.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In lessons, and after them, we can reflect on learning by considering how well
            we (and our students) are managing cognitive load in any given moment. You may
            realise a crucial task had students handling too many elements at once and decide
            to return to that topic in a subsequent lesson.
            When you grow in confidence, you can start to do this in the lesson. As an
            explanation lands you may realise you’re expecting too much of students in one
            go. Pause it and start at least part of a task. If students were going to answer five
            questions or complete a task with several steps but this seems too ambitious, break
            it down in the moment.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pre-empt cognitive overload
            Better than reflection, plan with cognitive load in mind. Look at upcoming les-
            sons. Is content broken down into manageable chunks? How will element inter-
            activity affect student thinking here? Am I demonstrating processes using Worked
            Examples? Am I handing over responsibility using Completion Problems? Are
            there any points in a lesson where student attention will be split?]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
            Knowledge of cognitive load adds a layer of thinking to your planning, making it
            richer and, in all likelihood, more effective. You continue, of course, to think about
            content. Activities and classroom routines are still important. Not every activity
            can include a Worked Example or Completion Problem. It isn’t always possible to
            completely focus student attention. Cognitive Load Theory, in this way, doesn’t
            solve all your problems but it gives insight into the learning experience that you
            quickly find you can’t do without.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Cognitive Load Theory can feel both obvious and profound. Obviously, we
            should spend time breaking content down for students. Obviously, we should
            gradually hand over independence to students. But these things haven’t always
            been obvious. Rather than planning a series of gradual steps, teachers have been
            rushed towards extended and open tasks: writing full stories in English, conduct-
            ing experiments in Science, making an argument in History, problem solving in
            Maths. Cognitive Load Theory provides both a set of principles and strategies to
            shape your planning. Applicable immediately, these principles and strategies will
            take a career to master.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
            1. Optimise, don’t minimise, cognitive load. Students need to think hard in les-
            sons. The application of Cognitive Load Theory is not to make things easier. If
            students really know the content we’re looking at in a lesson, we should expect
            more from them. If we’re introducing a concept for the first time, we should
            break it down and carefully manage element interactivity.
            2. Plan some Completion Problems. In lessons coming up, where would students
            benefit from a bridge between your explanation and their independent prac-
            tice? Don’t simply plan to do a Completion Problem. Include in your plan the
            task you’ll focus on, what you’ll write or say and where you’ll leave students to
            continue. Where you leave students will depend largely on how new this is to
            them and how much they’ve covered before. After modelling a process for the
            very first time, you might use a Completion Problem where students are taken
            through the whole process again with only the very final step left for them to
            complete. If students have completed similar problems before or spent time in
            the content, you might just give them the very first step.
            3. Plan against split attention. Upcoming resources are a good place to look for
            possible split attention. Pre-empt it where you can. Do this by considering
            where students are being asked to pay attention – the screen and their books,
            a printed knowledge organiser and their textbook, the labels of a diagram and
            the diagram itself. Reduce the moments where students must move from one
            source of, potentially unfamiliar, information to another.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A common classroom conversation goes something like this:
            I say, ‘You’ll recognise this from last week –’
            ‘What?’ says at least one student.
            ‘Last week. We introduced this. We’re just recapping’.
            ‘I’ve literally never seen this before in my life’, says the student, gesturing
            at the PowerPoint or resources. ‘I’ve never even heard of…’
            Even when I get them to turn to the page, to the date, in their exercise book when
            they were in and they did the task, they still look blankly, happy to continue
            arguing that they’ve never heard of what I’m talking about. Memory is a funny
            thing. And students don’t have it easy – their experience of school is different
            to ours. We focus on one subject or one class. We spend more time anticipating
            lessons and more time reflecting on them. Students move from lesson to lesson,
            to break time drama, to home, to all the other activities of their lives. I can’t guar-
            antee you won’t have conversations like the one above but knowledge of memory
            provides a foundation to explore the problems and solutions of learning in the
            classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If learning is a change in long-term memory, we must seek to understand what
            changes long-term memory. I recognise for some that is still quite a big if. Remember:
            - Long-term memory contains more than facts. This isn’t simply about learning
            dates, concepts or terminology.
            - If learned meaningfully, facts don’t sit in mute isolation; they connect and form
            webs of knowledge called schema.
            - Other definitions are available. Dehaene’s forming ‘an internal model of the
            external world’ is a useful contrast whilst driving at a common truth.
            - Even if you still don’t fully accept the change in long-term memory definition, it
            usefully informs some pedagogical decisions.
            - Cognitive Load Theory is based on evidence that working memory is lim-
            ited and long-term memory is limitless. Students can fail to learn when we
            overload their working memory capacity. We must, according to Cognitive
            Load Theory, attend to how students retain information in their long-term
            memories.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When we looked at Developing Teacher Knowledge, the following strategies were
            introduced.
            1. Retrieval practice, our attempts to retrieve memories without (or with mini-
            mal) prompts or aids.
            2. Spaced practice, our attempts to practice a specific, narrow skill with feedback
            that helps us to improve.
            However, we’ll examine them now considering the classroom rather than our per-
            sonal study. Our aim here is that we understand the basic principles, can apply
            them to our classrooms and have a plan for how we will do this.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Retrieval practice
            What is it?
            Testing, particularly high-stakes testing, is commonly seen as a Bad Thing because
            of the pressure it places on students to demonstrate what they’ve learned. Testing
            can be a quiz at the start of the lesson. It might be an end of unit assessment of
            what students have learned during a half-term. Testing is also a method of gaining
            knowledge in order to adapt to what students show us they can and can’t do, and
            do and don’t know, known as formative assessment (more on this later). When
            surveyed on their views of the uses of testing, teachers still tend not to link testing
            to memory and retention.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The ‘Testing Effect’ describes the observable phenomena where people remem-
            ber more of what they are tested on compared to non-testing. In turn, ‘retrieval
            practice’ refers to how, as teachers, we exploit the Testing Effect to benefit long-
            term learning. Every time a memory is searched for and found without significant
            direction or support, our ability to retrieve that memory strengthens. When we try
            to remember something, we conduct a kind of imprecise search for it. This search
            extends from a source or prompt, like a piece of terminology, towards a target,
            the definition. Retrieving the definition strengthens its connection to the termi-
            nology we started with. Benefits of retrieval extend beyond individual memories.
            When we retrieve, the connections between the network memories in the same
            topic strengthen. And memory here doesn’t solely mean individual, isolated facts.
            It includes concepts, complex ideas and understanding of processes. All can be
            retrieved and strengthened.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What mistakes can we make with retrieval practice?
            Despite a well-established body of evidence and growing use in the classroom, 
            quality of retrieval practice varies wildly. Knowledgeable teachers will see past the 
            superficial and potential pitfalls outlined below.]]>
			</paragraph>
			<paragraph>
				<![CDATA[See it as one part of the lesson
            Too often, helpful concepts backed by evidence get diluted in their transition to the 
            classroom. Retrieval is one such concept. The idea of testing leads understandably 
            to the idea of a test but then, for many, testing and retrieval become a quiz, and then 
            a five-question quiz at the start of the lesson.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Retrieval includes quizzing but it also includes any activity where students are 
            forced to recall something they have previously learned without, or with minimal, 
            support.
            Some examples include:
            - Explaining an unlabelled diagram to a partner.
            - Writing out everything you can remember about a topic.
            - Describing the connections between two previously explored names, terms, 
            dates or ideas.
            - Summarising a concept in a specific number of points.
            Retrieval happens well at transition points in the lesson. The start of the lesson 
            may work for routines but the middle or the end may work for the content you’re 
            teaching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Move to retrieval before learning
            Retrieval won’t work where students don’t know something. It’s a method of review 
            not delivery. If retrieval reveals a lack of knowledge or ability, the task should be 
            stopped and we should focus on re-teaching activities. Where we are concerned 
            that students aren’t ready for retrieval, we can pre-empt potential retrieval failures by increasing the cues and prompts. For example, in a language task where 
            students had to translate English words into another language, providing the first 
            letter or letters of the target word led to effective retrieval and long-term retention.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Spend too much time on it
            Starting a lesson with a retrieval activity is not necessarily a bad use of time nor 
            is it inherently a good use of time. Some schools mandate quizzing at the start 
            of lessons. This is fine if the quizzing is high quality. However, such a quiz can
            eat into time for new content to be introduced. A five-question quiz can quickly 
            take up 15 minutes of a lesson once students have come in, done the questions 
            and corrected answers. One temptation is to rush them through it. Another is to 
            avoid feedback, perhaps thinking the process of retrieval is enough without sharing answers or corrections. 
            Both temptations should be avoided. Even when students have retrieved something accurately, it helps retention to give feedback and 
            share answers or ideas. Wherever retrieval takes place in our lessons, it must be 
            efficient. Specific parameters are given; time limits are rigid; feedback is quick and 
            clear. Before moving on, let’s look at how to use time efficiently in two different 
            retrieval activities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Quizzes
            If you’re using a quiz for retrieval, don’t be restricted by an arbitrary number of 
            questions. Ask questions focused on what you want students to remember. I can’t 
            tell you how long a certain number of questions will take to answer and give feedback on: it depends on the questions (and the answers). We’ll re-examine what we 
            want students to remember when we look at the curriculum in the next section. It’s 
            worth planning more than one quiz at a time so that you can map questions into 
            the future rather than dwell on what will be asked in a single lesson. Quizzes tend 
            to target an individual’s ability to retrieve on their own. For this reason, silence 
            should be the norm. Explain the quiz. Explain what it is for (and keep on explaining, until the students are bored of hearing it, that retrieval strengthens memory). 
            Explain the time limit and that silence is expected.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This might sound something like:
            We’re starting today’s lesson with four questions to see what you can remember from the last two weeks. The more you try to remember these things, the 
            easier you’ll find it to remember them in the future. You have two minutes to 
            answer the four questions on the screen. You will do this in absolute silence. 
            Off you go.
            ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pause at the front of the room. Don’t be tempted to move around. Don’t interrupt 
            the silence. Show the class you’re watching them and do watch them. If there are 
            more questions, or students need more time, once the class are settled and silent 
            you can move around to check their answers. When time is up, go through the 
            answers quickly but clearly. Hands-down questions will give you a good idea of 
            how the class have got on. If an answer seems to have stumped everyone, you have 
            a couple of choices. You can pause the lesson and re-teach or revisit a concept in 
            some depth again, possibly adding another question to check understanding further. Alternatively, you can give the correct answer and move on, knowing there 
            will be time to return to the mistake in the future. If you’re unsure, it’s worth just 
            giving the answer and moving on, making a note of what they struggled with.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pair discussion
            One way for students to retrieve is to explain what they’ve been taught to someone 
            else without the use of notes or too many other reminders. Unfortunately, pair discussions can waste a lot of time. They waste this time when pairs don’t talk about 
            what they’re meant to, when they have too long to talk, and when the responsibility between the two speakers is unbalanced.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Because it’s likely you’ll explain the pair discussion in the moment (unlike the 
            quiz questions you’ve thought about beforehand), it’s vital you’re clear on what 
            you want students to retrieve. This could be the underlying principles behind a 
            scientific concept, the tone or tempo of a musical genre, the correct usage of a 
            new piece of vocabulary or something else. Be clear with students about what 
            you’re wanting them to retrieve – I want to see if you can remember the factors 
            influencing the declining number of Sweat Shops. Be clear about time limits and 
            give less time than you think they need – You have 30 seconds to explain to your 
            partner… Sometimes it’s useful to label individuals (1 and 2 or A and B) and give 
            direct instructions to each person in the pair. For example, A might talk during the 
            30 second time limit but B might be expected to feedback to the class. It’s important that both retrieve the target content otherwise the task won’t have the desired 
            effect.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A final mistake made with retrieval practice is rushing to it before embedding 
            what has been taught. At times, retrieval is not the answer; further teaching, practice, checks of understanding and feedback are required. Students are better served 
            by depth of study in the present, not the assumption they’ll get it or top up their 
            understanding when it comes to retrieval.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Retrieval is only meaningful as an ongoing strategy rather than a one-off. It may 
            help students remember something in the following lesson or following week, but 
            our eyes should be fixed on the long-term impact. The curriculum should define 
            what needs to be retrieved and which memories require strengthening. Where 
            the curriculum is well-planned, it (or the leader in charge of it) may direct what 
            retrieval should focus on. It may not.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If schemes of work or resources don’t arrive to you with this kind of definition, 
            you may need to make decisions about retrieval yourself. Too often, in reaching for 
            something to retrieve (usually for a quiz) we simplify content or place too much 
            importance on the basics. Even worse, we can make retrieval trivial through a 
            focus on the surface or tangential features of recent lessons. A story might have 
            helped students to understand a concept but, if we don’t expect them to recall that 
            story in the long term, questions about it should feature in our retrieval.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Curriculum should drive retrieval. If we hold a narrow view of knowledge – that 
            it’s just facts and vocabulary – we might find our retrieval contorting into a misshapen and deficient version of our curriculum.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Increase the complexity
            It’s no bad thing to get students to recall definitions for important vocabulary but if 
            this is where our retrieval rests, we’ll find students unprepared for the complexity 
            of thought and understanding our curriculum demands. It might be quicker and 
            easier to design a quiz full of definition recall but we should also expect students 
            to use those words accurately, to use them in combination, to understand correct 
            and incorrect uses.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Students forced to recall basic individual items regularly are likely to remember 
            them. Unfortunately, stringing a few bits of rudimentary vocabulary or content 
            together in a student’s mind won’t ignite creative or critical thinking. Asking students to repeat a definition they’ve heard every few lessons will automate the definition but not necessarily the understanding. Retrieval practice can progress from 
            recalling a term, through to using it, and beyond to using it in conjunction with 
            other important bits of content. Retrieval tasks increase in complexity when they 
            incrementally expect more of students: a multiple-choice question might be a good 
            start to check and strengthen a student’s understanding of a word or process.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A possible unintended consequence of delving into the research on retrieval is the 
            fear that a ‘right’ way to do retrieval is always just out of reach. Another study or 
            paper or speaker may come along (or already exist out there) ready to give definitive insight. Nerves about applying research with fidelity are understandable but, 
            in this area, probably unwarranted. A recent large review of the retrieval research,
            examining different types of retrieval practice, found that all classrooms and 
            schools ‘yielded a benefit from retrieval practice’. The researchers concluded that 
            ‘educators should implement retrieval practice, with less concern for the precise 
            format or timing’ of retrieval tasks set.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Spaced (or distributed) practice is the repetition of any activity where the gap 
            between repetitions is greater than nothing.5 Repetition leads to better retention 
            than massed practice, an extended period of uninterrupted practice. The optimal 
            gap between practice sessions is debated and disputed, with some arguing the 
            longer the better while others warn that left too long nothing will be remembered. 
            Whilst the definition of spacing includes any length of gap, there is evidence that 
            spacing within lessons – practising something at the start of the lesson and returning to it at the end – does very little to aid retention.6 Reaping the benefits of spaced 
            practice appears more likely when we take the long view, embedding it over time 
            rather than during a single lesson.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teachers should have some control over what is practised in their lessons, but not 
            all the control. A curriculum defines the direction students are travelling in, and 
            their ultimate destination. Practice tasks should be obvious as you examine the 
            resources of the curriculum. It should be obvious but it isn’t always. Even when a 
            rigorous curriculum sets out what your students should practise and when, if your
            students struggle with a process, responsibility to fix it lies with you, their teacher.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Writing an essay isn’t, initially, useful practice. Getting students to play badminton for an hour isn’t practice. Designing a product from scratch, with no direction, 
            isn’t practice. This is all sink or swim. Practice demands, as Cognitive Load Theory 
            does, that we break content down into the components that make up the whole.
            A badminton match is made up of stance, serve, forehand, backhand but also 
            position on the court, perception of opponent, knowledge of rules and potential 
            strategy. Even in this list, further dissection is possible. A teacher’s job is to break 
            tasks down into practicable chunks.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Ask how many different elements – knowledge, steps in a process, instructions 
            – students will have to juggle to do what you are asking. Focus on practising these 
            separately. Our aim is to string them together, to reach alchemy of knowledge and 
            skill. Students who have been taught your subject for some time don’t need every 
            single element broken down. They are likely to find this frustrating and tell you so. 
            We don’t practice to make everything easier for students; we do it so that students 
            master what we teach them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Return to practice regularly
            Time is tight. Each lesson is overfilled and colleagues warn you there isn’t enough 
            time to ‘get through’ the content. Is it worth spending lesson time returning to what 
            you’ve already studied?
            For a long time, teaching analytical writing, I would teach a paragraph structure. 
            Students would then write paragraphs to varying degrees of abject failure. To me, a 
            paragraph was a reasonable chunk to practise. For the students, I may as well have 
            been asking for an essay. A paragraph is so many individual things: the knowledge underpinning it, rules of structure (and how you’re allowed to break them), 
            grammatical and sentence level skills (like embedding quotations). Practising the 
            whole before students have mastered the parts is overwhelming for you as well as 
            for them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Reading through my students’ early paragraphs, I was struck by how they struggled with so much. Opening sentences were a problem. Students used phrases that 
            made me shudder – This makes the reader want to read on – and phrases I would 
            never have taught them – The writer says the quote… Confusion about quotations 
            revealed fundamental misunderstanding. Efforts to get students to write these fully 
            formed paragraphs too soon had wasted time. Students didn’t know more. They 
            couldn’t do more and yet time had moved on.
            We don’t have time to return to everything so we decide based on two questions: 
            What do we want students to do automatically? and What are students struggling 
            with? The former can be asked proactively, informed by the curriculum. The latter 
            we return to regularly and adapt based on our findings.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We don’t teach the components forever. Practice of the individual aims at mastery 
            of the whole. As students attain proficiency, combine elements. My failure with 
            early paragraphs wasn’t permanent – students should never write paragraphs – nor 
            was it universal – students shouldn’t write any paragraphs.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Our aim is not that students do the same task ad nauseam until they’ve got 
            it. Practice works cumulatively into more complex tasks. Practice also varies 
            when we vary the type of task whilst focusing on the same component. My 
            realisation that students couldn’t learn everything they needed for effective 
            paragraphs by writing lots of paragraphs doesn’t mean we leave every complex task until we’re sure students can do it. Approaching these tasks with our 
            knowledge of practice, we teach and focus on an element and recognise that our
            focus will rest there.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At a certain point, all teachers will ask themselves why a class or a student can’t 
            remember something they’ve been taught. If this is you, don’t worry. One of our 
            persistent challenges in the classroom is ‘exposing student thinking’. It’s not 
            uncommon to be disheartened by a lack of thought or understanding or retention. 
            After a whole term of teaching her, one of my students couldn’t remember the 
            name of the play Macbeth starred in so you’re probably not doing too badly. It’s 
            easy to feel that problems of retention shouldn’t trouble the research-informed 
            teacher. Unfortunately, they do.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If we’ve experimented with the strategies in this section and some groups or some 
            students are struggling, we shouldn’t be surprised. We should turn back to the principles of progressive problem solving. Progressive problem solving harnesses our 
            knowledge to solve our problems. At first, we use retrieval or spaced practice because 
            we know these things are good bets. Because the classroom is complex and so are 
            they, at least some students are likely to struggle to remember some of what they’ve 
            been taught. Defining the problem is essential. Is it that students don’t remember 
            what we want them to? Is it that their memory is superficial or surface level?
            Which additional avenues can you explore when you feel that practice and 
            retrieval aren’t leading to memory?]]>
			</paragraph>
			<paragraph>
				<![CDATA[The problem we’re trying to solve is likely to 
            fit into one of these categories:
            1. We haven’t been practising the right things in the right way, a possibility even 
            when we think otherwise. Michael Pershan, a Maths teacher, describes the 
            conflict between students doing ‘a bunch of problems on a page’ and genuine 
            retrieval or practice of Maths facts.7 Students should be able to remember basic 
            addition and multiplication facts, helpful knowledge to think and problem 
            solve in Maths. Pershan explains how, depending on their method, students 
            might be retrieving and practising or they might be counting or laboriously 
            noting down their working. Both routes will get the answer but neither will 
            strengthen the memory.
            2. We haven’t taught the knowledge or skill sufficiently in the first place. Retrieval 
            and spaced practice only work, only can develop in complexity, when students 
            have learned something in the first place. Where they haven’t, retrieval and 
            practice become a sort of second teaching where students grasp some knowledge just a little bit tighter.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Refocus your practice
            Picture a sports hall filled with badminton nets. Students start the lesson by grabbing a racket and a couple of shuttlecocks and serving to their partner. Serving has 
            been a problem for the teacher. Students just weren’t getting it so this extra practice 
            has been planned for them. Except there’s a problem. Students aren’t practising
            serving or at least not practising it well. No expert badminton player steps up to 
            serve thinking, How can I direct this serve at my opponent’s racket?]]>
			</paragraph>
			<paragraph>
				<![CDATA[To refocus this practice, the teacher explains where students can serve that will 
            pose an immediate challenge for their opponent. She demonstrates these one at a 
            time before students practise them. Monitoring and individual coaching corrects 
            potential errors and misconceptions before the class move onto the next serve. 
            Teaching in light of the research on practice doesn’t mean that students won’t play 
            mini-matches at the end of the lesson. They will simply do so with a variety of 
            serves beginning to enter their repertoire. The same research reminds us that students will need to return to these serves later for them to be useful. With the same 
            group the following week, the teacher starts with serve practise again but calls out 
            the names she has given the serves to move quickly through them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[So often, we think students are practising when they aren’t. Or, more accurately, 
            students practise and learn a skill counter to our intentions. We want students to 
            work something out in their head but they write something down. We want students to write unaided but sentence starters remain on the board. We want students 
            to shade for definition competently in art without asking us before every stroke of 
            the pencil. Audit the tasks you’ve planned, checking they practise the right knowledge and watching for loopholes and shortcuts that undermine your aims.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Prioritise reading
Cognitive scientist Daniel Willingham argues that ‘Books expose children to more 
facts and a broader vocabulary than virtually any other activity’.8 Willingham’s case 
is not simply that reading supports general knowledge; he makes it clear that the 
knowledge gained from reading provides the foundation for the skilled activities we 
want students to master. Simply put, ‘Factual knowledge must precede skill’.
Students need a rich experience of subject matter before we expect them to 
retrieve or practise in that domain. Moving too quickly to quizzes and narrow 
practise activities may be the cause of shallow knowledge, weakly connected in 
our students’ minds. If students are struggling with a particular topic, go back to 
the resources that introduced the topic. Were students exposed to complex and 
interesting texts? Were these texts accompanied by lively, interesting discussion 
or debate? There are other legitimate ways to enrich and introduce content but it’s 
hard to argue any of them are as important as reading.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Use homework for spaced practice and retrieval
Homework varies from school to school and many have not yet decided what 
exactly it is for. Although homework can serve more than one master, the one it 
should be most devoted to is retention of taught content, an aim achieved best 
through spaced practice and retrieval. In some schools, homework is set for you. 
If this is the case, make sure you’re clear on what is set and what it is for. Where 
freedom allows, set in line with the principles in this chapter. Define what you 
want students to know and do well, based on what you teach in the curriculum. 
Decide what students can do on their own. Set simple tasks which retrieve and 
practise. Several online platforms offer ways for this to largely be done for you but 
the idea remains the same: students should spend additional time retrieving and 
practising as well as learning new content. As homework is unsuited to the introduction of new content, it is ideal for the review of what has already been covered]]>
			</paragraph>
			<paragraph>
				<![CDATA[Memory is vital to learning. Don’t, as I foolishly did as a new teacher, make 
the argument from ignorance that I don’t want students to remember. I want them 
to understand. How will they understand if they don’t remember? This chapter 
has focused on two main strategies because they are straightforward and easily 
adopted. Dwelling on these strategies is not the same as saying your lessons should 
only or mainly be filled with quizzes and narrow slices of practice. They shouldn’t. 
Teaching is more than these things but it’s unlikely to get very far without them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
Pedagogy is the thinking underlying our decisions and strategies employed in the 
classroom. Where possible, our pedagogy should be informed by the best available 
evidence. Whilst this section has not been a comprehensive summary of ‘the evidence’, it has highlighted some important pillars to any mental model of learning:
- Evidence can’t answer all our questions but it can open some avenues to pursue 
whilst closing others down.
- Learning changes our long-term memory. More expansive definitions of learning 
are available but this one has the benefit of focusing our minds on what might 
bring about tangible change in the classroom.
- Things that teachers often fixate on in the classroom – activity, busyness, completion of tasks – are proxies for learning, not bad in and of themselves but no 
indicators of learning.
- Cognitive Load Theory is one of the best explanations of why students learn and 
remember, and why they don’t. In both the principles it sets out and the strategies it promotes, Cognitive Load Theory should underpin our planning.
- Retrieval and practice both offer applicable ways to make learning stick. We must 
not pay lip service to these strategies but embed them with thought and care.]]>
			</paragraph>
			<paragraph>
				<![CDATA[So often, I’ve just got students to do some activities and 
not carefully defined what I wanted them to practice. Maybe this is a symptom 
of teaching English where content and skill aren’t clearly defined. Nebulous 
approaches where we expect students to gather up skills almost incidentally 
from our lessons are bound to lead to failure in retention and frustration for 
students. Pick out one skill you want to practice. Make it definable. Create a 
clear success criteria. Get students to practise that thing and nothing else. Set 
up a task so you can give feedback to all or almost all, even if that feedback is 
simply, ‘You’ve got it, well done’]]>
			</paragraph>
			<paragraph>
				<![CDATA[Views about teaching have changed dramatically over time. One such change provides insight into the role of subject knowledge in teacher training. In the late nineteenth century, elementary teachers in California were expected to pass a rigorous 
written exam to qualify. Prospective teachers had to be well-versed in arithmetic, 
grammar, geography – so far so good – as well as penmanship, industrial drawing 
and vocal music.1 Whilst your average primary teacher might still just look down 
this list, nod and get on with it, such tests are almost entirely absent from teacher 
training today. What is interesting about this test is its unwavering focus on subject 
knowledge. Nothing in it examines generic teaching skills or pedagogy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What would you put on a test for teachers? It’s hard to know, isn’t it? As a 
new member of the profession, which areas of your knowledge received the most 
scrutiny in your training? Perhaps it has been your subject knowledge. Or your 
knowledge of educational theory. You may feel that the greatest scrutiny has been 
directed to your actions in the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Around a hundred years after that Californian test of subject knowledge, 
American assessments of trainee teachers had changed dramatically. A test usually 
still existed for trainees but it now assessed knowledge of planning, organisation 
and policy. Subject knowledge had gone. I’m sure the trainees and the trainers considered the subject but it had lost its core status. The trend in America is mirrored 
around the world. Subject knowledge has been important, less important and now, 
it seems, is regaining prominence in the teaching profession. Two separate but parallel narratives have brought subject knowledge back into focus after that period 
in the cold.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One Christmas, I received a book that genuinely changed how I saw teaching. 
It didn’t change anything immediately. This was no Damascene conversion. 
Gradually, as I went back to it, I realised I’d got teaching completely wrong. The 
book was Why Don’t Students Like School? by Daniel Willingham. Willingham, a
cognitive scientist, promises to ‘answer questions about how the mind works and 
what it means for the classroom’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I was never – as far as I can remember – taught how to teach. We didn’t work 
through the ‘learning to teach’ curriculum from start to finish because such a 
curriculum did not exist. When I started teaching in 2009, truths about teaching 
were cobbled together from moments in lectures and initial lesson feedback. One 
observer told me she loved how little I spoke to my classes. I didn’t explain that 
I was terrified of speaking to them so I kept students as busy as possible. A lecturer gave us a complicated equation we could use to figure out how long we were 
allowed to speak to students in a lesson dependent on their age. It became clear 
that learning was something you tricked students into.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There were so many texts I had looked forward to teaching but I realised quickly 
that English teaching wasn’t concerned with specific texts, periods or movements 
of literature. Content was less important than the skills students developed along 
the way. It didn’t matter if you were teaching Romeo and Juliet or rap lyrics because 
we weren’t teaching the content. What mattered was students’ ability to analyse or 
express an opinion or create something new.]]>
			</paragraph>
			<paragraph>
				<![CDATA[So when Willingham told us, based on research from cognitive science, that 
‘Factual knowledge must precede skill’,3 he was challenging an ingrained and 
long-standing status quo. He was describing a well-founded principle of cognitive 
science: we need knowledge to apply skill. When I was training to teach, the absolute pinnacle of good teaching was ‘higher order’ thinking. Creating, synthesising, 
analysing, problem solving and others were all more important than simply knowing. It’s hard to disagree. I haven’t met a teacher who doesn’t want their students 
to reach the dizzy heights of the ‘higher order’. A difficulty arises when we try to 
skip the steps of ‘lower order’ thinking. You can’t synthesise knowledge you don’t 
have. You can’t problem solve in an area you don’t understand. Strange as it is to 
say it now, that’s exactly what teachers were asked to do when I was training. Skip 
the knowledge and go straight to the complicated stuff. It doesn’t make any sense 
now, and it didn’t then.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowledge, and in particular the knowledge held by the teacher but not their 
students, became incredibly important. But this didn’t happen, for me, right away. 
For a long time, I struggled to see how facts could be important to the English 
curriculum. The word itself – facts – is possibly part of the problem. In English, I 
thought, students needed understanding rather than facts. We tend to see facts as 
an isolated, potentially interesting but ultimately minor part of cognition. Your 
mind doesn’t see things this way. Knowing when Romeo and Juliet was written, 
its genre or themes, its historical background – none of this is isolated in the mind 
of a student of literature. Instead, a web of knowledge, a schema, builds the more 
you get to know.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I realised students need to know things. To some, this might feel like an underwhelming realisation. To me, it was ground-breaking. Whilst the profession has 
moved on, it would be wrong to miss the turn cognitive science has caused. 
Cognitive science made knowledge, and therefore curriculum, central but that 
didn’t end the conversation. Cognitive science prompted the questions Which 
knowledge? and Whose knowledge? but couldn’t answer them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Cognitive science has encouraged curriculum into the limelight, but it can’t take 
sole credit. There are plenty of voices who argue for the supremacy of the subject and the curriculum whilst paying little attention to the cognitive arguments. 
Despite wanting the same thing, or at least a similar thing – subject and curriculum 
to be central – these arguments are often made from drastically different positions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Many have looked to Matthew Arnold, nineteenth-century school inspector and 
poet, as they’ve sought to reignite the curriculum fires. Michael Gove borrowed 
Matthew Arnold’s phrase and argued that students should learn ‘the best that has 
been thought and said’.4 We should return, thought Gove, to teaching students a 
traditional curriculum unashamedly. Being a fairly blunt instrument, Gove didn’t 
seem to wrestle with the less palatable views of Arnold’s expression. The idea of 
‘best’ is often exclusionary and elitist. Arnold’s overriding desire is to civilise the 
barbarian. Teaching the ‘best’ in this way becomes, for many, an imposition of values and ways of thinking.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Gove’s curriculum legacy has been baked into the most recent Ofsted inspection 
framework in England. Amanda Spielman, Ofsted’s Chief Inspector, also a fan of 
the ‘best’,5 put curriculum at the heart of school inspection. Ofsted must ascertain 
whether schools ‘construct a curriculum that is ambitious and designed to give 
all learners… the knowledge and cultural capital they need to succeed in life’.6
Teachers, in turn, must ‘have good knowledge of the subject(s) and courses they 
teach’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To be fair to the curriculum, these arguments had already been made outside 
of the corridors of power. Over a decade before Spielman’s inspection framework, 
in his aptly named Bringing Knowledge Back In, Sociologist of Education Michael 
Young argues from the left that knowledge is the right of the many, not the few. 
Where knowledge has been the preserve of the elite and of private education, 
Young argues knowledge offers an avenue to equality. Knowledge doesn’t belong 
to the powerful; knowledge offers power to those without it. In this view, subject 
and curriculum aren’t an imposition of values but a way of understanding and 
accessing the world and all its opportunities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To some, these narratives have run their course. Curriculum, they say, is a fad. 
Or, if not the curriculum, then the work that has gone into it. Carefully crafted 
(and wholly ignored) statements of curriculum philosophy written by harried 
middle leaders are read by no one. Low-rent art projects present the curriculum 
as a tube map or an incomprehensible web (where a bullet point list would do). 
Narratives rise, fall and are replaced. If subject knowledge or curriculum were fads, 
we could expect to see them fall again as rapidly as they had ascended. Of course, 
the curriculum – like assessment or behaviour management – can never be a fad. 
The current focus on curriculum may be dropped as faddish but that would be our 
fault for allowing one of the central aspects of what we will always do to become 
superficial and cheap, or overwrought and unsustainable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Difficult though it is to imagine, curriculum will give way to another inspection 
focus, another politician’s priority, another solution to an ill-defined problem. If 
you’ve made it this far, I hope it is clear that knowledge is vital for the expert 
teacher. Which knowledge could be more vital than the knowledge of the content 
being taught? If some new fad grips the profession soon, trying to wrestle your 
attention from the curriculum, remember that subject is central. This is the peak, 
the focal point, of our examination of teacher knowledge. Pedagogy without content is empty. Knowledge of how students learn is useless unless we direct this 
knowledge to the content learned.]]>
			</paragraph>
			<paragraph>
				<![CDATA[All that said, there is confusion about what kind of subject knowledge makes a 
great teacher. With that in mind, this part will seek to answer these questions:
- What types of subject knowledge do we need and how do we develop them?
- What do we need to know about the curriculum?
- How does knowledge of the curriculum affect our planning?
- How do you use all that subject knowledge in the classroom?]]>
			</paragraph>
			<paragraph>
				<![CDATA[I decided to train as a teacher when I realised that after university, I wouldn’t be able to 
talk to people about poetry all day. Unfortunately, I found early in my career that English 
teachers don’t just spend their days talking to people about poetry. The knowledge I’d 
gained at university didn’t seem relevant. I stood in the store cupboard trying to choose 
a text to teach Year 7 from a selection of books I’d never read. I was given a half-term’s 
topic titled Animal Poetry but had to muddle through after being told I could choose 
the poets, poems… or animals. It wasn’t all like this. My favourite moments became the 
times I had to think deeply about the texts I was teaching and make plans for students to 
think deeply too. I didn’t know if working on that knowledge made me a better teacher 
but I knew I wanted more of those times and more of that thinking.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Being Head of English allowed me to devote large chunks of time to reading, 
thinking and planning around the topics I loved. As I made being a Head of English 
more about the content of what was taught, my team followed – some willingly 
and some begrudgingly – and we found knowledge to be addictive. The more we 
sought to know deeply what we were teaching, the more we wanted to know. There 
were department meetings where we read an essay about Macbeth and discussed 
how it should shape our teaching of Year 10. There were discussions and debates 
about what students should study and when.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My point is that teaching becomes a rewarding job when knowledge is central, 
and nowhere is this truer than subject knowledge. This isn’t a one-time thing, nor is 
it something only for the uber-keen. Not only is it rewarding; it’s essential for your 
continued development, particularly – but not limited to – the start of your career.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this chapter, we’ll look at three types of subject knowledge:
- Degree knowledge – The knowledge we gain in higher education.
- Subject knowledge – This is your knowledge of the whole domain of the subject 
you teach. Often the domain is different to, or at least much larger than, the 
curriculum you teach.
- Pedagogical content knowledge – This is your knowledge of pedagogy – your 
classroom behaviour – merged with your knowledge of your subject. If your 
subject is writing heavy – like English or history – your pedagogical content 
knowledge might be full of writing frames and models, how they can be used 
as well as their limits. Every subject will have content to explain and some 
useful ways to think about explanation. Questions will vary in use from subject 
to subject.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Degree knowledge
No teacher takes the same route into teaching. Training and prior qualifications 
are diverse. In secondary school, teachers often teach outside of specialism and 
must develop subject knowledge on the job. Even if you have a relevant degree, 
if you’ve entered teaching from another profession, it may have been years since 
you’ve had to think about the object of your study. In primary, being a subject specialist, at degree level, across the curriculum is impossible. Concern 
about how prepared you are to deliver the content of the curriculum well is 
understandable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It’s worth pausing to consider how this study prepares you and how perhaps it 
doesn’t: to what extent does what we arrive in teaching with shape our success and 
the success of our students? Reflect, for a moment, on the mental model you have 
of your subject or subjects, or had when you started teacher training. The extent 
to which this mental model maps well onto the school curriculum will depend on 
the subject and on your route into teaching. It’s natural to feel, as a subject expert, 
you should be ready to encounter relative novices in the classroom. Remember, the 
most useful knowledge is that which solves the problems or addresses the challenges of the classroom. Degree knowledge, whilst useful and enriching, might do 
that but not all the time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Academic degrees are often framed as a kind of foundation, particularly for secondary teachers. You might even believe subject knowledge is less of a priority 
because it is something you have studied recently. Pedagogy, on the other hand, is 
probably entirely new to you. Whilst there is some evidence that a better degree 
will lead to better teaching, there is little consensus. For example, one large-scale 
study in America saw no link between university test scores and eventual teacher 
quality.1 The same study found little evidence that having done postgraduate qualifications predicted more effective teachers or teaching. More recent research does 
challenge this, arguing that having specialised or done postgraduate study in the 
subject you teach is correlated with better student outcomes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Why then is this type of knowledge useful or necessary? Lee Shulman describes 
what he calls ‘syntactic knowledge’; this is the ‘structure of a discipline [and] the 
set of ways in which truth or falsehood, validity or invalidity, are established’.3
Like grammar, syntactic knowledge provides the rules of a discipline. A good 
teacher doesn’t just know what the students are going to learn really well. A good
teacher understands the reasons why the things they’re teaching are important or 
true. At times, depending on your subject, you might feel like a degree is next to 
useless, irrelevant and detached from what students are learning in your classroom. What you’ve gained from it may feel like it is lying dormant but this knowledge shouldn’t be underestimated. Where you’ve studied the subject you will be 
teaching, making a list of this background, syntactic knowledge as you encounter it can usefully bring to the fore those things you already understand that are 
helping.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What then of teachers who don’t arrive in teaching with a relevant degree? Or of 
primary teachers who could never have studied all the things they are going to 
teach? Many very successful teachers start their teacher careers as ‘non-specialists’. 
You or a mentor or a training provider can assess what the absence of a particular 
qualification might mean for your development. Syntactic knowledge can still be 
developed but be careful of seeing further study as the silver bullet.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Conflicting research in this area reminds us of something we can easily ignore: 
degree knowledge is not the same as teacher content knowledge. It would be foolish to suggest that we don’t need or want highly qualified teachers. It would be 
foolish not to go into teaching pleased that you have built the prerequisite knowledge through study (or slightly concerned that you haven’t). But this research about 
qualifications and degree knowledge:
- Reminds us that a good degree or further study is no guarantee you will be a 
good teacher.
- Encourages us that whatever our background we can be successful (even if we 
don’t have the ‘right’ degree).
- Warns us that we can’t rely on previous study. We’re breaking new ground. We 
can use what’s come before but we’ll need to do something with it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A qualification points to something: subject knowledge. The degree or degree result 
is always only a proxy for knowledge. What about the knowledge itself? Even then, 
the types of knowledge we need, the types of knowledge we want for our students 
vary further.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As you’d imagine, no one is arguing against this content knowledge. Research 
papers and investigations don’t stack up against content knowledge. And researchers have been fascinated by teachers’ content knowledge for a long time. Subject 
knowledge has been assessed, compared, tested, improved, all with the aim of 
understanding teacher expertise. The message from this research is that ‘more 
content knowledge is always better’.4 This knowledge is a ‘core component of 
teacher competence’.5 This knowledge is not just necessary; it is a ‘significant 
predictor’ of student achievement.6 The essential thread running through teaching, and through this research, is content knowledge – knowledge of the subject 
or subjects taught.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But that isn’t the only message or a complete message. The Great Teaching 
Toolkit has, whilst reviewing this research, described the relationship between 
teacher content knowledge and student learning as ‘modest to weak’.7 Writers of 
the Toolkit are not arguing that subject knowledge is unimportant. Improving subject knowledge is not no-impact but it can be low-impact. Extensive research into 
Maths teachers, for example, did link improving teacher subject knowledge with 
improved student learning but, understandably, the biggest effect was felt by those 
with the worst knowledge to begin with.8 You can see the issue. When a teacher 
starts out, a boost in subject knowledge leads to a boost in quality of teaching. If 
you’re a non-specialist, a primary teacher or just beginning to teach with understandably large gaps in knowledge of a much larger domain, subject knowledge 
will help you.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Where further development is concerned, however, there is a problem. The content has no borders. If we just stroll around this borderless domain, we’ll pick 
up some knowledge along the way but the progress may be slow – for us and our 
students. It will help you because, however much you’ve studied, there will be 
elements of the domain which are incomplete in your mind but it will help slowly. 
Subject knowledge development, therefore, is a good bet for new or non-specialist 
teachers but it will not remain a good bet on its own if our aim is expertise.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Subject knowledge is:
- Important for effective teaching but…
- Not the be all and end all – continued development will see limited immediate 
returns.
- A massive field – whatever the discipline – and something we’re never going to 
master]]>
			</paragraph>
			<paragraph>
				<![CDATA[How do we develop subject knowledge?
To start, a warning: growing your subject knowledge can be what you enjoy the 
most. It can also be something that offers limited returns the more you develop it. 
Time spent developing your subject knowledge doesn’t necessarily equate to a proportional development in your teaching ability. You could read a whole book on 
your subject and find that little changes in your practice. Not reason to stop reading 
altogether but certainly a reminder to be realistic, to pace ourselves. Perhaps, when 
it comes to subject knowledge, we should look to the long term. Subject knowledge 
development, in this way, probably doesn’t help us solve the immediate problems 
of the classroom unless we truly don’t understand what we’re teaching this week. 
Rather, it prepares us to solve future problems by expanding the options, ideas, 
concepts, analogies, stories – the knowledge – we can call upon in future.]]>
			</paragraph>
			<paragraph>
				<![CDATA[You can build subject knowledge by:
Auditing what you know
When I was completing my PGCE, I had to fill out a subject knowledge audit. 
It massively depressed me. There were vast swathes of English Literature that I 
hadn’t covered or had barely covered in my degree or previously. I went into a 
meeting with my university tutor with trepidation, concerned about the amount 
that I’d have to get done to ‘catch up’. The tutor looked at it, said it was fine and 
we spoke about something else. For some time after, I felt a bit bitter about that. 
I thought he should have challenged me more. But should he? I had limited time 
and maybe it was shameful I hadn’t read any Sylvia Plath but I wasn’t immediately 
teaching her poetry. In this way, a broad audit can be unhelpful.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As a new teacher, the curriculum at your first school is the best mirror you can 
hold up to your subject knowledge. The curriculum helps narrow the domain so 
you’re not auditing everything. Colleagues can help narrow that further by directing you to the priorities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As you examine what you’ll be teaching, you are prompted to reflect on what 
you know (and what you don’t). To manage your time, and your cognitive load, 
you’ll have to narrow down your focus to what is most immediately important. 
Examine each unit, topic or subject you’ll have to teach. It might be clear which 
bits of content need immediate work. There’s also a danger that it feels like every 
aspect of the curriculum is in urgent need of attention. Focus on the areas you 
haven’t ever studied, read or learned anything about. Make a list in order of importance – what is coming up and where is your knowledge worst? Remember, this 
isn’t just about confidence levels. Instead, you’re assessing whether you have the 
necessary knowledge to teach a topic. Don’t ask Do I understand the resources?
Ask Could I explain this topic? Could I model a process from this topic confidently? Don’t confuse developing subject knowledge with developing knowledge 
of lesson tasks and processes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Reading widely
Teachers should be readers. We should read about strategies for the classroom. We 
should read about subject. We should seek to understand how young people learn. 
Being a critical teacher-reader means interrogating what we’re reading. It means 
asking Does this chime with my experience? Will this help in my classroom? Is this 
my understanding of the subject matter or the best way to communicate it?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teachers should be readers because we should be intellectually engaged in 
what we do but we must acknowledge a couple of things. First, not everything 
you read will be transformative. Second, reading is a long-term strategy. 
Re-reading is unlikely to support retention or improved understanding in the 
short term.9 Teachers should be career-long readers but that doesn’t mean several weighty tomes are the only answer to the gaps in knowledge for next term’s 
scheme of work.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you’ve audited your knowledge, you might decide you need to read around 
two upcoming topics. As a time-poor teacher, your choice of reading material is 
crucial. Your choices of when to read and when to stop are equally important. If 
reading about your subject feels like a leisure activity, it might fit effortlessly into 
your evenings and weekends. If the thought of picking up a book on a weeknight 
feels like attending a lecture or research for an essay, subject knowledge reading 
might best be left for the brief time available in the school day. In this case, articles 
and summaries serve you better than, even highly recommended, books.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A chasm stretches between subject knowledge and the classroom, and what we 
know doesn’t always feel like what we need for teaching. What kind of knowledge 
could bridge that gap?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pedagogical content knowledge
Knowledge of subject is a bit of a Russian doll. We study a subject through various tiers of education, and perhaps informally. The residue of this study is subject knowledge but even then, we must admit that subject knowledge gained from 
study isn’t quite like subject knowledge used in the classroom. The question then 
becomes not Is subject knowledge important? but What kind of subject knowledge 
is important for the classroom?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lee Shulman describes pedagogical content knowledge as ‘subject matter 
knowledge for teaching’ [his emphasis] and the ‘ways of representing’ the subject 
that help students to understand it.10 Pedagogical content knowledge includes any 
knowledge that enables analogies, explanations, illustrations and demonstrations.
Teachers with this knowledge recognise which aspects of a topic will be particularly challenging and what strategies will best meet this challenge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pedagogical content knowledge takes the generic elements of pedagogy – like 
questioning – and turns them into classroom behaviours. It acts as the bridge 
between the background – what we’ve studied, learned and understood – and the 
foreground – the classroom where that knowledge will be used. But it isn’t just 
about what you do; your knowledge changes your perception and understanding 
of what students say and do in lessons. A teacher with pedagogical content knowledge will be more able to recognise a student’s mistake11 or interpret a student’s 
answer.12 Vitally important to effective teaching is understanding how what you 
know of a subject is becoming what students know of it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[New teachers must develop pedagogical content knowledge whilst planning and 
delivering early lessons. There’s a sense in which it will just happen if you wait for 
it. As someone looking to accelerate their journey towards expertise, this won’t do. 
It’s not uncommon to hear about a new teacher with a PhD or other further study 
who is struggling to break through to the simplicity required to teach successfully. 
Like many anecdotes, the lesson here doesn’t reflect the reality or possibility for all 
new teachers. There’s no reason why serious qualifications and study should hold 
you back from being a great teacher; they may well help. The problems with large 
quantities of subject knowledge arise when coupled with deficient knowledge of 
how novices progress in your domain.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The curse of knowledge is a cognitive bias born of the assumption that others 
possess the same knowledge as you. Expertise in your subject – whether signified 
by a qualification or not – in this way can lead to a blind spot. Experts can struggle 
to verbalise a thought process because working, problem solving and thinking in 
the domain of their expertise has become implicit.13 Not that such a blind spot 
should convince us to shun content knowledge but it should cause us to reflect on 
how ready our knowledge is for the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Developing pedagogical content knowledge should change something in the 
classroom – an explanation, feedback to students. Perhaps because of this, at least 
some evidence indicates that developing pedagogical content knowledge is a good 
bet for improvement for new teachers (and a better bet than simply developing 
your subject knowledge).14 Pedagogical content knowledge is most likely to be 
lacking from your current mental models of the classroom. Compare what you 
know – the topics you’re going to be teaching – with how you’d teach it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pedagogical knowledge:
- Is amalgamation of subject knowledge and classroom actions and behaviours.
- Includes explanation, modelling, feedback, questions and adaptations to student responses.
- Won’t necessarily follow from large quantities of subject knowledge (but large 
quantities of subject knowledge can help us to develop it).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Although pedagogical content knowledge has a lot to say about what we do in the 
classroom, it also speaks to how students encounter the subject. Teachers need, 
Shulman explains, ‘an understanding of what makes the learning of specific topics 
easy or difficult: the conceptions and preconceptions that students of different ages 
and backgrounds bring with them to the learning of those most frequently taught 
topics and lessons’.15 Pedagogical content knowledge is forged through understanding how students will experience, misunderstand and make progress in your 
subject, allowing this knowledge to shape our decisions in the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One of the main challenges we face in the classroom is making content meaningful for children. It’s a problem that will never leave us. Developing pedagogical 
content knowledge is a method of trying to solve that problem, taking something 
abstract and turning it into a tangible classroom resource, activity or explanation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Co-planning
Colleagues’ subject knowledge and knowledge of planning is invaluable. Whilst 
it’s not possible to observe the planning process in the way we might observe a 
lesson, there is much to learn from interrogating how experienced teachers plan. 
Most schools will have at least partially resourced curricula. Resources offer a 
framework or suggestion for what might happen in your lesson. Seeing the route 
the resource will take into your classroom is not always easy. At times, it can feel 
like these resources are a drain on your time rather than a help. Early in my career, 
I would look at the resources available – usually a PowerPoint presentation for 
each lesson – and start to delete the slides I didn’t like or activities I didn’t think 
would work. Slowly, I’d reach the point where all that was left was the title slide 
and maybe a bit of reading. Some of these resources weren’t fit for purpose, like the 
hour of Citizenship I was meant to fill with a single slide that said ‘Democracy?’ 
on it. But, with others, I lacked understanding of what to do to make them work.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Your expert and experienced colleagues may well have forgotten their early steps 
into the difficult process of navigating from a point in the curriculum through the 
resources available to a real and successful lesson. At times, this makes recently 
qualified teachers the font of wisdom for the new teacher. Teachers who have
recently qualified are able to call upon the recent experience of learning how to 
adapt the resources or curriculum documents into teachable lessons. The onus is 
on you to ask them and investigate how they do this. Much of what they offer will 
explain how subject knowledge is brought into the classroom. How much content 
can fit into a lesson? How challenging should I make this? How many questions 
should I set for them to practise?]]>
			</paragraph>
			<paragraph>
				<![CDATA[In some schools, PPA is shared or planning has its own semi-regular meeting. 
Often, this is called collaborative planning. Embrace these times if you’re lucky 
enough to experience them. If these aren’t available to you, manufacture them. 
Collaborative planning is not someone else planning for you. If you can plan with 
a more experienced colleague, make the most of this process by being proactive: 
plan something and talk through it with them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Ask your mentor which teachers are best to talk to about certain topics or areas 
of teaching and learning. Talk to the teachers who use nothing but the whiteboard 
and a pen. Talk to those who use PowerPoint or booklets. Talk to the teachers who 
weren’t subject specialists when they arrived but are now thriving.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Engaging in the subject community
Beyond the walls of our schools, communities of teachers offer help in everything 
from planning to applying research. Subject communities bridge the gap between 
the academic subject and its curricula counterpart. Ruth Ashbee defines a subject 
community as ‘the people involved in the discourse around a subject both formally 
and informally, through organised groups and ad hoc arrangements’.16 The dual 
emphasis within the definition is useful: there are formal groups and institutions 
we can turn to as well as an informal web of knowledgeable educators, generously 
willing to offer us their support.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Many subjects have their own subject association you can join. Benefits of 
these organisations – and levels of quality – vary wildly so check with your 
colleagues if they’ve used, or found useful, resources from them. If the formality 
of these subject associations is not for you, plenty of informal networks have 
sprung up in recent years, reflecting the increased connectedness of teachers. 
CogSciSci describes itself as ‘a grassroots group of teachers and other education professionals looking to promote the use of cognitive science in school 
science teaching’.17 The group freely offer training, resources and guides for 
teachers across a range of Science-based topics. LitDrive, an informal English 
community, ‘provides an extensive bank of classroom materials for teachers to 
upload, share and adapt for use in their classrooms’, as well as articles, CPD and 
mentoring.18 Core subjects have it easier. The Science and English groups here 
aren’t replicated at the same scale in D&T or Music. Equally, a large group is no 
guarantee of quality. Whilst you might find a Facebook group full of specialists 
in your subject, realising it’s just a forum for moaning about the exam spec can 
be a bit disheartening.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Formal or informal, these groups should:
1. Support your ability to plan effective lessons. This is not simply, or even 
mainly, about taking someone’s resources to save you time although the impact 
of that cannot be underestimated. More than reducing the workload burden, 
resources from other teachers demonstrate how to approach a particular topic, 
examples of sequence, explanation or models. Such resources might give a flavour of the amount of practice another teacher gives to a topic or the way new 
content is connected to old. It’s not unusual for these groups to include blogs 
or articles from teachers on how they’ve dealt with planning certain topics. 
Subject communities expand the number of colleagues’ minds you have access 
to which can only help you as you plan new units and encounter subject areas 
for the first time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Formal or informal, these groups should:
            2. Make links between the domain and the classroom. Curriculum giant Michael 
Young gives us further cause to consider engaging with the subject community. 
To Young, this is not simply about gaining lesson ideas from others who teach 
our subjects. Young expects all teachers to have a ‘relationship with knowledge’ and for those teachers to want the same for their students. For Young, 
this relationship is not simply built on transmission of knowledge to our students. Such a relationship is a relationship with the ways that knowledge is 
created, articulated and perpetuated.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
Subject knowledge isn’t easy to pin down. It’s what remains from your school 
and university days. It’s what develops as you teach and plan and learn from 
colleagues. Subject knowledge on its own won’t make you an expert teacher 
because great teachers aren’t those with deep but detached knowledge of subject. Subject knowledge you arrive in teaching with must be funnelled through 
the curriculum.
We could continue examining subject knowledge from further angles or create 
further divisions. In fact, two more chapters will be devoted to one more type of 
subject knowledge: curriculum knowledge. Depending on the subject you’re teaching and your prior study or qualifications, you might find that your knowledge 
doesn’t map well onto what is taught in your school. Knowing your curriculum is 
more than just knowing what will be taught.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Questions for reflection
- Have your qualifications (A-Level/degree) prepared you to teach your subject(s)? 
Where might they be deficient and why?
146 Subject knowledge
- How closely related are the content you have studied and the school curriculum? If you’re not sure, talk to someone in your department or phase team or 
whoever’s running your training.
- Have you conducted an audit of your subject knowledge? For the subject or 
phase you’re teaching, what would an audit like this look like? Often, this is 
part of a training programme. If it hasn’t been, talk to your mentor about how 
this could work.
- If you have conducted an audit, what has it revealed? In broad domains, like 
History or Literature, you might feel like there’s too much that you don’t yet 
know. Be careful of feeling like you need to rush to develop every area.
- How confident do you feel about taking what you already know and explaining 
it or adapting it to be manageable and understandable for students? Don’t worry 
if you aren’t sure about this; the next chapter will look at some strategies.
- If you’ve begun to teach, how have you already tried to develop this pedagogical knowledge? Where have there been barriers to students understanding what 
you’ve been explaining or modelling?]]>
			</paragraph>
			<paragraph>
				<![CDATA[When I was Head of English and planning a curriculum with a team of English 
teachers, I made a lot of mistakes. I tried to do too much at once. We tried to reinvent a curriculum in the space of one academic year. We soon realised that this 
was a bad idea. We could create resources easily enough but the curriculum is not 
simply the sum of your resources. Curricula exist as a shared understanding, often 
written out in a document, of what will be taught and when it will be taught and 
why it will be taught in this order. Shared understanding does not emerge from a 
rush of resource creation. It emerges over time, through careful planning driven by 
ongoing dialogue.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the outset of this chapter, it’s important to make something clear: I don’t think 
a new teacher should be in the business of curriculum creation. The idea that you 
can learn to teach – an incredibly complex task – whilst learning to create a curriculum from scratch – an equally complex task – is nonsense. And you don’t have 
to master the latter to achieve the former. Teacher training rarely gives time to the 
theory and practice of curriculum creation, nor should it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[That said, intellectual engagement with the curriculum, as you find it in your 
school, is vital. It would be very unusual to find a school where little to no curriculum thinking had happened, but levels of work will vary from an overview or 
broad plan right through to resources for every lesson you teach. Your job is not 
to plan a curriculum from scratch; it is to teach the curriculum well. With that in 
mind, it is worth asking What do I need to know and understand about the curriculum? and How do I use that knowledge and understanding in my planning? We’ll 
answer the former in this chapter and the latter in the next.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Curriculum metaphors
The complexity of curriculum thinking and planning prompts us to often see the 
curriculum through a metaphor lens but metaphors for the curriculum are bewilderingly varied.
Archipelago
Andy Tharby, English teacher and writer, uses one metaphor which diagnoses the 
problem with a lot of curriculum thinking. An archipelago curriculum, Tharby 
explains, ‘is taught in atomised topics that are vaguely linked by a shared discourse or a generic set of “skills”’.1 More problematically, ‘there is little expectation that children retain what they have learnt beyond an end-of-term assessment, 
the subject becomes more like a pleasurable holiday cruise around the islands than 
a learning experience’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When I was a new teacher, a shared spreadsheet dictated when each of us taught 
different topics through the year. So one person taught the novel Private Peaceful
in September; another would teach it in July. Sometimes the bland unit on comprehension could be whistled through in the short May half-term; sometimes you’d 
have to stretch it over one of those long, dark Autumn terms. We cruised round 
these islands – many of which were worth a visit – but these journeys were incoherent meanderings.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The archipelago is a good metaphor for many curricula, but it doesn’t capture 
every subject well. A Music or Art teacher may recognise the archipelago in the 
curricula they’ve taught. A Maths or Science teacher probably won’t. A Maths 
teacher’s curriculum is not unconnected and, while some elements are diverse and 
disparate, most hang together in a logical sequence.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Spiral
To capture a journey that repeats and revisits, Jerome Bruner used the image of a 
spiral. In a spiral curriculum, you visit a topic in one year and return to that topic 
with added complexity the following year. Bruner was keen for students to understand ‘the fundamental structure of whatever subjects we choose to teach’.2 This 
poses an interesting question – What is the structure of my subject or subjects?]]>
			</paragraph>
			<paragraph>
				<![CDATA[To some, subjects fall into two broad categories: hierarchical and cumulative. In 
hierarchical subjects, knowledge is connected through a sequence of progression 
that demands students master one element to get to the next. To an extent, in hierarchical subjects, at least parts of this sequence are settled and common between 
schools because of the progression students must make. Maths is often used as 
an example of a hierarchical subject, where students require mastery of one step 
before they reach the next.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Cumulative subjects tend to allow more curricula decisions: What shall we 
teach Year 7 in term one? has a range of possible answers. In English, one secondary school might choose one structure – a chronological study of literature, say, 
with lots of grammar and writing practice built in – whilst another could structure 
the curriculum completely differently – perhaps around thematic episodes like 
Outsiders or Power, where tasks and texts are selected based on their link to the 
theme. In both models, students will develop knowledge of genre and theme as
well as writing ability cumulatively as they revisit topics, adding layers of complexity to their understanding.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Bruner’s spiral, like the terms hierarchical and cumulative, may be a useful 
guide or way of seeing. A problem with strict belief in the cumulative/hierarchical 
divide is that it doesn’t quite work. At best, it can be an illuminating starting point. 
‘Cumulative’ subjects might lend themselves to Bruner’s spiral metaphor because 
the teaching of them allows for this gradual repetition. Teachers of ‘hierarchical’ 
subjects would rightly resist a spiral structure that doesn’t respect the structure of 
their subject. The deeper you go into subject structure, the more you realise these 
terms can only ever apply to parts of a subject. At times within the same subject, 
you’ll encounter hierarchical elements alongside cumulative.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How useful Bruner’s curriculum metaphor is might depend on the subject you 
teach. To the history or English teacher, the idea that we can return to the complexity of Shakespeare or the Anglo-Saxons is a comforting reminder that there 
is always more to get from these subjects. To the languages or Maths teacher, the 
idea of leaving mastery of a topic to another year might feel woefully inadequate 
as a way of ensuring students progress within the subject and the curriculum. That 
said, Bruner recognised the importance of the knowledge of the teacher, exhorting 
us to attain ‘the most fundamental understanding’ of the subjects we teach.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Narrative
Christine Counsell gives us one of the pithiest definitions (and metaphors) of curriculum when she describes it as ‘content structured as narrative over time’ [her 
emphasis]. Narrative, Counsell reminds us, ‘is full of internal dynamics and relationships that operate across varying stretches of time. Those dynamics and relationships realise the function of every bit of content’. Or, more succinctly, ‘every 
bit of content has a function’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Once we understand this, the importance of each bit of what we teach becomes 
more apparent, whether we’re teaching a physical skill or activity or a body of 
knowledge full of complex, interconnected facts. Knowledge and skill snowball as 
new connections are made and new abilities are harnessed.
When the narrative is working, every element joins together in a marvellous 
coherence of subject matter. Occasionally, a student will come along to shatter 
the illusion that this narrative is working perfectly. Why are we doing this now? 
You never taught us that. Shouldn’t we spend more time on… Whilst the narrative 
is the aim, narratives vary in success and clarity. If there are incongruous bits of 
content that don’t seem to fit the narrative, ask about them. Become a curriculum 
thinker early by considering how the narrative fits together well and where perhaps it feels disjointed.]]>
			</paragraph>
			<paragraph>
				<![CDATA[New teachers are rightly taught to plan their lessons, sometimes having to fill 
in cumbersome proforma to evidence the work of planning. What is often missed 
in this focus on planning is the understanding of the bigger picture. Lesson length
is arbitrary and variable. If you’re constantly overrunning, you may need to look 
at your planning, but extending the study of a topic over two lessons or five and a 
half lessons or anything else is perfectly acceptable. Keep an eye on your place in 
the narrative as well as on the clock in your classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Curriculum metaphors, and I’ve shared only a small number with you, will only 
ever convey part of what we need to know about the curriculum. As you approach 
the curriculum – in examining resources, in planning, in teaching – such metaphors and characterisations can begin to feel detached or incomplete. It’s worth 
spending some time understanding the big picture, the overarching narrative of 
your curriculum, but more time, for the new teacher at least, should be spent on 
the granular level of what your curriculum requires you to teach. That is what we 
will turn to now.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To get to know your curriculum, there are some questions you need to seek answers 
for. In some schools, these will be provided for you, possibly in painstaking detail 
on your first visit. In others, the curriculum will be barely held together by a single 
disused document that vaguely outlines what is taught and when. Either way, our 
job is to know enough to plan, teach and assess our students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What type of knowledge am I teaching?
Substantive knowledge is described by Christine Counsell as ‘the content that 
teachers teach as established fact – whether common convention, concept or warranted account of reality’.4 Facts, terminology, understanding of concepts all fit into 
the substantive knowledge within a domain. Easy to dismiss, knowledge like this is 
essential for students to build schema and tackle complex tasks within a domain.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Counsell describes disciplinary knowledge as ‘what pupils learn about how 
that knowledge was established, its degree of certainty and how it continues to 
be revised by scholars, artists or professional practice’.5 Disciplinary knowledge 
includes understanding of how discoveries are made and theories are established 
in science, how sources are used as the basis of an argument for truth in history, 
and how art is assessed as effective (or beautiful) in art.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Procedural knowledge is the skill or know-how of a subject. All subjects have 
some skill to practice and embed, from the lay-up shot in basketball through to the 
introductory sentence of an analytical essay. Too often, skill is framed broadly to 
the point of uselessness. Critical thinking, analysis, creativity are not really skills 
you can teach, at least not in one go. Not only will these big ‘skills’ look different 
from subject to subject, but they are also made up of component parts which can 
and should be defined and taught. And these component parts can probably be 
broken down further until we have defined behaviours students can rehearse to the 
point of automaticity before stringing them together in order to tackle the larger, 
more complex tasks of the curriculum.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It would be strange for a curriculum to only dwell on one of these: most visit 
each in turn, or more often, intertwine the different elements of them. Ask how 
these types of knowledge are defined in your subject at your school. A follow up 
question would be How well defined are these areas of knowledge? which leads us 
to our next set of questions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What is essential? What is desirable?
Whilst the curriculum is in the spotlight because of fresh reading of evidence 
around learning, because of renewed belief in curriculum entitlements for all and, 
cynically, because Ofsted have become more interested in it, what you encounter 
in curricula in different schools and subjects will vary massively. Nowhere is this 
variety between schools and subjects likely to be felt more keenly than in the level 
of specificity with which content is defined in the curriculum. Some curricula 
define everything that should be taught and learned. Others give a vague set of 
directions. Check with whoever has developed or leads on the curriculum. Is there 
a set of essentials which must be covered? Are they listed? Must some content be 
mastered before students progress to something else?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Asking what’s essential only gives partial understanding when it comes to considering what to teach. Ask teachers what they like to include, what they include
            if they have time, which stories and examples they use to illustrate the points 
they make. Christine Counsell calls this hinterland, a ‘supporter and feeder’ of the 
essential content of the curriculum.6 To Counsell, this is not just an added extra; it 
is an essential way we enrich and elucidate the curriculum. Where suggestions or 
guidance aren’t forthcoming, this hinterland is hard to come by for new teachers 
because it is won through experience. You might find just the right explanation, 
story, video or activity which is not essential but does clarify and embed the essential knowledge in a way that wouldn’t have been possible without such hinterland. 
A catalogue of such things is likely to be developed over time. Where you can, plan 
with others and ask for their examples.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How is the curriculum codified? And why?
It would be foolish to think that curricula simply exist in the heads of those who 
write them. Equally foolish is the idea that the resources, materials or curriculum 
paperwork are the curriculum. A useful contrast is found in the terms ‘intended 
curriculum’ – what those devising the curriculum hope and plan to see – and the 
‘enacted curriculum’ – what actually happens in the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Curriculum codification is the springboard from which the intended curriculum 
can begin to be enacted. As Ruth Ashbee explains, codifying the curriculum is the 
process of bringing together – at times ‘writing down’ – ‘into maps, booklets and 
other working documents’.7 In turn, teacher time is freed up to spend on ‘subject 
knowledge and pedagogy planning’. Unfortunately, as a new teacher, you don’t 
have much control over whether this time has been freed up for you. Arriving in 
your school, you may find clarity and precision, or their opposites.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To gain the necessary knowledge of the curriculum you are teaching you should 
ask:
- How are knowledge and skill defined in the curriculum? Are there lists or maps 
that students need to have covered by certain points? Does this definition exist 
in a document, in resources, a textbook, somewhere else or nowhere?
- What are the relationships between the knowledge and skill taught? Does the 
curriculum mandate the links that should be made between different facets of 
the content taught?
- How do students progress through the curriculum?]]>
			</paragraph>
			<paragraph>
				<![CDATA[I recognise the previous questions are theoretical, focusing on the thought underpinning the curriculum (or very much hoping such thought has taken place). It’s important to understand those things but it’s also important to understand answers to:
- What resources are available? How do these resources link to other curriculum 
documentation – maps, schemes of work, plans etc.?
- What is the expectation to use (or deviate from) such resources? Are teachers 
allowed to teach however they like as long as certain content is covered? Or are 
the resources essential?
- How are these resources updated and what is the process for updating?
Ruth Ashbee reminds us that the codification of the curriculum is in a constant 
state of ‘flux’. Not because it is not good enough but because review, refinement 
and updating are expected features of curriculum maintenance.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What are the destinations, assessments and aims of the curriculum?
A six-week block is an arbitrary length of time. In an ideal world, getting through a 
unit of work or the teaching of a topic would take as long as it takes. It is right for 
departments to consider how best to use time without being constrained unnecessarily by the structure of the school year. However, time constraints also mean 
that we can’t spend forever on a single topic. Calendars dictate moving on points 
or assessment windows we must be aware of.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
Curriculum knowledge prepares us to tackle a whole host of challenges thrown at 
us. How students will learn and connect what they learn is sequenced clearly and 
overtly signalled in the best curricula. Mary Kennedy describes ‘Portraying the 
curriculum’ and ‘making it comprehensible to naïve minds’ as a persistent challenge of teaching.8 Hundreds of decisions about what to teach – which activities, 
explanations, demonstrations – assault us before we’ve even considered how we’ll 
ascertain what students have learned. Knowledge of the curriculum doesn’t exist 
in a vacuum. We know it to plan with it. The subject of the next chapter, planning 
is a challenge made simpler by deep knowledge of the curriculum.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the end of my university-based training year, they got a newly qualified teacher 
in to talk to us about her first year teaching. I enjoyed listening to her experiences and was encouraged by her struggles which were not unlike my own. But 
towards the end of her talk, she said something jarring. Each week, this teacher set 
aside time to plan one or two ‘really good’ lessons. ‘Really good’ meant engaging, 
resource-heavy and, dare I say, fun. I was most troubled by the admission that the 
best she could do was one or two lessons a week that she considered ‘really good’. 
This teacher’s headteacher was sitting on the stage, having just given a talk herself, 
and I looked across to gauge the reaction there. The headteacher smiled and nodded through the talk, clearly proud. And I’m sure there was much to be proud of, 
but I was left baffled that we were about to enter a profession where ‘really good’ 
could only be expected a fraction of the time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the previous chapter, we explored the things we need to know about the curriculum to be ready to teach it well. You don’t need these things so that you can 
write your own curriculum from scratch. Rather, we know the curriculum so that 
our planning of lessons is effective; effectiveness is based on an understanding of 
how students learn and a faithfulness to the curriculum being taught.
In this chapter, we’ll look at planning. Usually, this starts with a focus on individual lessons and progresses to thinking about sequences of lessons. This is fine as long 
as an individual lesson’s place in the curriculum is considered as it is put together]]>
			</paragraph>
			<paragraph>
				<![CDATA[In some training schemes, lesson planning is adherence to a particular form, a 
Word document segmented into boxes which focus your attention on different 
aspects of the lesson. Structure can be helpful when you’re starting out. A scaffold 
to your thinking, the form gradually creates for you a mental model of lesson planning. Or it should.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unfortunately, lesson planning forms can construct faulty scaffolding where 
prominence is attached to a minor part of your thinking. Forms also tend to make 
new teachers a slave to timing, forcing them to predict timings before accuracy is 
likely and making them feel guilty when these timings aren’t followed. Possibly 
the greatest problem with lesson planning proformas is that they exist in a vacuum, with no apparent connection to the curriculum or to your subject knowledge. 
Thinking is funnelled towards activity rather than content and skill.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A set of principles or questions, outlined below, can direct our thinking both 
as we use a planning form and when we need to shed it. All too quickly, planning 
can move from a process constrained and restricted to one with too much freedom. 
Do you plan in your teacher planner, or in Word, or PowerPoint? There are benefits and drawbacks to each of these, some significant, but the principles providing 
foundation to your planning should remain the same.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Where does this lesson fit?
The narrative tendrils of the individual lesson stretch back into what has already 
been taught and they stretch forward, foreshadowing what is yet to come. If it is 
possible to completely disentangle the lesson from the curriculum then either the 
lesson or the curriculum must be at fault. A moment in the curriculum, the individual lesson is the culmination of much and the preparation for more. How then 
does the individual lesson look back? How does it look forward?]]>
			</paragraph>
			<paragraph>
				<![CDATA[What is the purpose of this lesson?
For a long time after I trained, it was the fashion to get students to write down 
a learning objective. They’d spend the first few minutes of each lesson jotting it
down, with those students who might struggle to write and copy quickly failing 
to keep up. The act of copying was largely an act of fear on the part of the teacher, 
fear that someone – a senior leader or inspector – would come into a room and ask 
students what they were learning or why they were learning it. If this happened, 
and it was rare, students dutifully recited the lesson objective. Job done – they 
knew what they were learning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thinking about purpose of the lesson buckled under the weight of this act of 
objective writing, where every kind of learning had to fit into a To understand…
or To be able to… sentence structure. If someone asks you to get students to write 
down the lesson objective into their book each lesson, politely ask if 2010 wants 
its bad idea back.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A lesson’s purpose or objective ties it to the curriculum. Substantive or disciplinary knowledge is introduced, revisited or connected to what has come before. A 
piece of the narrative jigsaw is added. Purpose must be tied to the place in the curriculum we find ourselves. Don’t look beyond the curriculum for another purpose. 
Trends come and go; one trend that won’t die is to give your lesson an aim outside 
of the content. Creativity, critical thinking as well as social and emotional ‘skills’ 
have all had their day in the sun as aims of individual lessons. All are worthy aims 
for education but when we make them central to our planning we step too far from 
the content of the curriculum.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Instead, we focus on the ultimate aims of the curriculum. To borrow Stephen 
Covey’s phrase, we ‘begin with the end in mind’. More than just a principle, this 
should direct the planning process. To get to ‘the end’, what will students need to 
draw on? What is ‘the end’ made up of and therefore what should be broken down 
and practised? What vocabulary, concepts, dates and ideas is ‘the end’ made up of? 
This is the content of your lesson.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plan thinking, not activities
Beginning with the end doesn’t work if we’re labouring under the illusion that 
students need to repeat the end product endlessly to reach mastery. We get them 
to write essays to get good at essays, play basketball matches to get good at basketball, draw scatter graphs to get good at scatter graphs. There’s a problem: you 
don’t get good at something by repeating the end result. The end result comprises 
a tangle of skill and knowledge. Planning involves untangling the content and 
sequencing it into manageable chunks, a process that gets easier with time and 
experience.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Remember Rob Coe’s encouragement to see learning as the result of thinking 
hard.
1 Remember too Daniel Willingham’s maxim that ‘memory is the residue of 
thought’.2 A series of resourced activities give the appearance of a well-planned 
lesson, and it’s true that lessons will need activities. The vital distinction for the 
lesson planner is between filling time with activities and planning activities that 
will prompt the right thought, about the right content, in the right order.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Beware of tasks that mask content rather than shine a light on it. If students have 
to juggle a complex process with new content, they’re likely to struggle with both. 
Once, when being observed, I was teaching students about imperative verbs, a 
rather dry topic that I felt needed to be made more engaging. I got the class to write 
a scripted argument between a downtrodden teacher and a recalcitrant student, a 
situation they were more than familiar with. Students enjoyed deciding what their 
badly behaved character would do but they loved writing the teacher, a draconian 
but ultimately ineffectual figure. A script posed problems though: how was it laid 
out, were speech marks necessary, what were stage directions for? More than this, 
imperative verbs were entirely absent from some of their work. My observer gently suggested that it was, unsurprisingly, quite hard to write a good script with no 
direct teaching on scriptwriting.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Students hadn’t thought about imperatives whilst they were writing but the same 
is true about so many activities. If memory is the residue of thought, then the teacher’s 
responsibility is to carefully direct thought, a difficult task when thinking is invisible. 
Directing thought becomes easier when we shed the complexity, the frills and the 
distraction from our lessons. Content should drive activity, not the other way around.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Learning happens in the extended periods we spend with a class, as we work our 
way through the narrative of the curriculum. Starting out, planning individual 
lessons and assessing success of individual lessons are all valid and useful activities. New teachers should celebrate individual lessons when they go right; new 
teachers should reflect on what’s happened when they don’t. You should dwell 
in the individual lesson until the slog of planning these wanes. As you reach the 
point that lesson planning uses less mental effort, a new horizon opens up, one 
focused on the sequences of lessons rather than the one-off. A sequence of lessons 
better reflects how learning happens. Learning accumulates, built from connection 
between past content and present study. A sequence allows us to tell the story of 
the curriculum rather than take an episodic cruise around the archipelago.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Beneath the lofty aim of doing the curriculum justice lies a more pragmatic goal: 
making your life easier. Planning in sequence reduces workload because tasks will 
fill the time we give them. As soon as a teacher can plan two or three lessons for
one class in one sitting, life becomes easier. Let’s turn to some questions we can ask 
as we plan our first sequences.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching a sequence requires understanding of the narrative. Even a small segment 
of the narrative has a point we’re working from and a point we’re working towards. 
As we do with individual lessons, we begin with the end in mind. The end, in this 
case, is just a more distant and complex goal.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Often the codified curriculum will dictate the sequence so responsibility for 
narrative design won’t be yours. As you start to plan sequences of lessons, select a 
series and a goal that makes sense to you. This might be two lessons to begin with, 
extending into longer chains with time. Whatever number of lessons you’re working with, work hard to make clear – to yourself first of all – what part this sequence 
plays in the wider curriculum.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A good way to see how well you understand this is to plan an explanation of the 
sequence for your students. Identify the way you want them to understand the target, 
learning or end result. Check the clarity of this explanation with a colleague. Use this 
explanation at the start of the sequence and return to it in every subsequent lesson.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Our aim is a memorable curriculum. To retain more, evidence from neuroscience 
and cognitive psychology makes clear that students need to activate relevant prior 
learning in advance of new, related content.3 A reasonable question is then What 
does activation look like?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Explanation. At its simplest, activation of prior learning is making explicit 
the implicit links between content. Last time, we learned how to use technical 
drawing to create three dimensional images. This lesson we’re going to use 
technical drawing to design packaging. Or In yesterday’s lesson we learned 
what the Blitz was and when it was. Today we’re going to read about what 
life was specifically like for children. Activation by way of explanation starts 
simply (like the examples above) but we can make it more effective by using 
retrieval practice.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Retrieval practice. The Forward-Testing Effect is the phenomenon where testing 
on previously learned content supports retention of new learning.4 A quiz or 
other retrieval activity requires students to bring back prior learning. Writing 
down everything they can remember about a topic or answering five questions 
on the screen or explaining a diagram relevant to today’s lesson – all these 
retrieval activities can do more than strengthen memory.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Activation prompts links and connections but so does the clear connection between 
past and present content. Findings from neuroscience research show that when 
new learning is ‘congruent’ with past learning, new memories and connections are 
more likely to form.5 Congruent shares some meaning with harmony: past learning 
reaches harmony with the present when we make links explicit.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What might students get wrong?
A study into teacher knowledge of student misconceptions revealed the power 
of knowing how students will approach a subject. Science teachers were tested 
on what they thought were the most likely student misconceptions in an upcoming topic. Teachers who accurately identified student misconceptions in advance 
achieved better results than those who couldn’t.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Effective teachers do three things with student misconceptions: know them, 
pre-empt them, adapt to them. One subject’s misconceptions will not look like 
another’s and, depending on your subject, misconceptions might be easy to 
spot but they might not. Get to know your misconceptions by talking to experienced staff about what students regularly struggle with. A trainee Maths teacher 
I worked with completed every assessment students had to, including GCSE 
papers. This kept her knowledge sharp but it also allowed her to view topics from 
the students’ perspective, searching for possible pitfalls. Your first time teaching 
a topic might throw up mistakes you hadn’t expected; make a note of them for 
the second teaching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Once we have an idea of some misconceptions of students, we plan accordingly. 
We don’t plan to react to mistakes when they arrive. Plans pre-empt mistakes, 
stopping them from happening in the first place. If we know students are likely to 
comma splice, we show them how and why this is wrong and give them practice 
getting it right. If students regularly miss a step in their calculation, we model that 
carefully in advance and tell them what others have missed.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A slew of inspirational posters tells us that making mistakes is how we learn. 
It’s true that we can learn from mistakes. Proactively trying to stop mistakes from 
happening might feel desperate and futile. Not every mistake can be caught. And 
aren’t we withholding a powerful learning experience from the students? Even if 
we do some serious preventative work, students are going to make mistakes and 
hopefully learn from them. Learning and retention are more likely when we minimise what it is possible to minimise.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How will pre-empting misconceptions manifest in your planning? Write down a 
topic or a process as your heading. List everything it is possible to get wrong under 
that heading. Planning for misconceptions is also difficult as a new teacher because 
you are an expert, or at least probably haven’t struggled as some students will. 
Examine resources. Ask other teachers. Use what you find to create that list. Plan 
your tasks and explanations to deter these errors by getting out in front of them. Your 
explanation includes, Students often get this wrong by… Your resources highlight 
steps in minute detail. You monitor purposefully during a task fraught with misconception and share correct answers early or intervene the moment you see someone 
going wrong. Planning in this way will be time consuming and is not necessarily 
possible for every lesson but you can think about it for each new topic you teach.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A well-planned sequence anticipates student error. In part, the sequence pre-empts the error through practising the specific processes and retrieving the knowledge necessary to avoid future error. Not every mistake can be caught. Sequences 
must be adjusted where students aren’t getting what we’re teaching. As with an 
individual lesson, a plan must be adapted when it becomes clear that more time or 
different activity is required for student learning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What retrieval practice or spaced practice do I need to embed?
As we saw in Chapter 10, retrieval and spaced practice both help students to retain 
the knowledge and skills you teach them. Across a sequence, the choice about what to 
retrieve and what to practice becomes about which parts of the whole do students need 
to automate. When writing that essay, playing that match, writing up that experiment, 
or doing any number of other activities, students call on a range of prior knowledge. 
As with the curriculum, retrieval and spacing should increase in complexity, working 
cumulatively the more students are taught and forging links between content.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As well as increasing in complexity, when planning a sequence we should consider how we slowly remove supports, prompts and aids from students. If students 
are using vocabulary or formula sheets or sentence starters at the start of a unit, 
that’s fine. But should they still be using them at the end? Students will need 
support to practice initially but they shouldn’t always. At the start of a term, your 
retrieval quiz might give students everything but the answer. By the end, more 
should be expected of them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A practical note about planning
How you plan – physical planner, PowerPoint, using booklets and textbooks – doesn’t 
really matter. Those things aren’t the curriculum; they are your intention for how to 
implement the curriculum. Your planning should work for you. Good planning isn’t 
leaving a big chunk of your job to a set of shared resources. Teachers lucky enough 
to have workbooks, PowerPoints or textbooks with ready-prepared lessons can plan 
lessons with those resources by annotating, editing or making notes on resources 
available. This is still far quicker than making those resources from scratch!]]>
			</paragraph>
			<paragraph>
				<![CDATA[When I was training, the schools I worked in used PowerPoint for lesson 
resources so my first sequences were single PowerPoints acting as one part of the 
curriculum narrative. I was able to pick up a PowerPoint where I’d left it and 
continue in the sequence. The problem with this approach is the possibility that 
the PowerPoint puts you on a track which is difficult to get off when you need to. 
A PowerPoint’s focus tends to be on activity and often the slides exist to remind 
the teacher of something as much as the students. PowerPoint is easy to overfill. 
Whilst I wouldn’t tell you to never use PowerPoint, it’s worth learning to teach on 
the board and under a visualiser as well. Overreliance on one medium is likely to 
embed the drawbacks of that medium into your teaching. As Head of Department, 
I introduced booklets we all used containing content and some ideas for activities. 
Perhaps less activity-driven, booklets still need careful thought from the teacher.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Remember that having resources isn’t the same as having a plan. Resources may 
be vital to your plan but they aren’t everything. Often resources provide an idea of 
what students will do but you still need to consider what you will do: How will you 
explain and introduce a task? Where will you stop and ask questions or get feedback? What questions will you ask? What answers are you expecting? How will 
you know if students have ‘got it’? The next chapters will examine some of the 
answers to these questions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
Planning a sequence means you don’t have to rush to finish everything in a single 
lesson. Links become overt and clear when they are natural: where we left off last 
time is where we begin this time. Be wary of never getting through the content, 
though, and ask where you’re unsure if you’re covering things in the expected time. 
Doing less but doing it better, doing it in more depth is good advice for planning. 
Each new activity adds to students’ cognitive load. So include fewer activities 
where students have more time to think about the content without being moved on.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When the lesson is elevated above learning, we send our thinking down a blind 
alley, one fixated on activity and engagement. If you find yourself scouring the 
internet for a more ‘engaging’ way of doing a task, stop it. If you spend your weekends cutting up pieces of paper into smaller pieces of paper or marshalling friends 
and family to help you organise resources, stop it. To return to the concept of progressive problem solving, planning the individual lesson is a problem we should 
aim to pass through. Our intended destination? Planning sequences of lessons with 
fidelity to the curriculum. A career can be spent facing the challenge of making 
content comprehensible for young people over the extended periods we spend 
with them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One weekend, I got my wife, sister-in-law and her husband to help me organise 
the most complicated card-sort I could have devised. They dutifully helped me 
organise paper into envelopes when we should have been enjoying each other’s 
company. I decided then I wouldn’t do that sort of activity anymore. I’d be lying if 
I said this prompted an immediate change to well-planned sequences of lessons. 
Gradually, my planning turned to activities that weren’t such a drain on time for so 
little reward. I became comfortable with teaching activities that were straightforward but focused on the right things. I stopped looking for the most engaging way 
of introducing a concept because engagement tended to mask rather than enhance 
the content. My planning sped up dramatically and, to my surprise, the quality of 
my teaching improved as well.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Shifting your perspective from teaching a lesson to teaching the curriculum 
reaps only benefits for you and your students. It may take some brave decisions 
when you start out as you ignore the short term in favour of the long, as you focus 
on thinking and not activity. Curriculum and sequence thinking reflects the nature 
of learning: it happens over time. Perhaps most importantly, students’ experience 
of our subjects is all the richer for it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For most of my career, I’ve not planned my explanations or models or questions. 
I’ve just talked. Even when I realised how important what I said could be, I just 
turned up and said some things to classes or did the task in front of them. I’d plan 
other parts of lessons in detail but leave explanations and models to chance, perhaps because they feel ephemeral or of the moment.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I realised, more recently, that I hadn’t really differentiated between planning 
and resourcing a lesson. With resourcing, we make sure students will have something to do, something to look at: there’s a PowerPoint slide or worksheet for every 
moment of the lesson. Planning, of course, cares about the resources but it cares far 
more about student thinking.
Thus far, the subject knowledge we’ve been talking about has been one step 
removed from what you do in the classroom. Subject knowledge development is 
not some abstract process though; it is about the classroom. If we’re unable to pull 
the subject knowledge developed in the background into the foreground, our work 
has been in vain.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teachers, like students, are susceptible to cognitive overload. Amongst other 
things, this means that new teachers need to prepare for lessons differently to their 
more experienced colleagues. An experienced teacher might have a set of polished 
explanations banked in long-term memory. These explanations may not have been 
scripted and practised diligently. Steady accumulation of remembered process is 
an effective but inefficient way of developing. And just because time and experience have led to automaticity for those teachers, there’s no guarantee that time and 
experience will develop effective explanations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Real planning should attend far more to those moments in between activities. Real 
development for new teachers, too, will focus on exercising the often ignored muscles 
used for teacher activity in lessons. This is true as we manage the practical transitions 
and routines of our classrooms. It is all the more true and all the more important, 
however, as we plan the way we will communicate the subject to our students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For Lee Shulman, pedagogical content knowledge, including analogy, explanation, illustration and demonstration, is the missing link between content and the 
            classroom.1 For many teachers, myself included, pedagogical content knowledge 
has been a muscle in constant atrophy. The activity of the student, perhaps rightly, 
is seen as more important than the activity of the teacher. Reality, however, is not 
either/or. Both are vital. Both require careful planning. The final step between content knowledge and classroom is not simply resource creation or activity planning. 
It is the careful craft of any in-lesson behaviour that brings your subject expertise 
into focus for the students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What counts as an explanation? A ten-minute scripted explanation about sweatshops in Geography? A 30 second reminder? A one-on-one conversation? So much, 
in fact, that it’s easy to just do these things without really thinking about how we 
do them or how to get better at them. When you think about it, this is bizarre. 
Explanation and modelling pose a particular problem to those with subject expertise because that expertise needs to be distilled, filtered through the curriculum or 
scheme of work and ultimately communicated effectively.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When I trained to teach, I was mainly told not to speak too much to my classes 
and praised when I didn’t. It’s true that spending half an hour to explain a concept you could have spent five minutes on (or broken down into more manageable 
segments) is not a good use of time. But there’s no escaping the fact that you have 
expertise, knowledge and skill that students don’t yet have. To sit in the same room 
with them and never communicate these things is a dereliction of duty. We need 
to be able to take what we know and make it accessible and understandable for our 
students. That’s what we’ll think about as we look at explanation and modelling.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Principles for explanation
As we examine the principles for explanation, we’re drawing on our knowledge of 
the curriculum. What we decide to explain and when is directed or driven by the 
curriculum, the sequence that best guides students through the content they are 
learning.
The principles outlined here demonstrate where your knowledge, and what 
type of knowledge, will be needed. Good grounding in a subject is important but 
understanding how to make that subject clear, how to join links to a great chain of 
understanding, that is the challenge. That is the essence of Lee Shulman’s pedagogical content knowledge, the combination of pedagogy and content.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Build on prior knowledge and skill
A simple but powerful question is What previously taught knowledge will students 
need to understand what they will learn in this lesson? As one study explains, 
‘reactivation of previous memories while learning new information [can help to]
integrate them with an existing schema’.2 Remember, schema are the interconnected webs of knowledge we’re trying to build in students’ minds.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We connect what we want them to know with what they know already. If you 
have the chance before your explanation, quizzing students on this knowledge 
primes them to cope with what’s coming. This could take the form of a few questions to answer, a whole class mini-whiteboard session or any other task where 
they must retrieve relevant details from what they already know.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Prior knowledge includes what you’ve taught them, what they learned last year, 
what they’ve been studying in other subjects, but it also includes what the students 
are likely to know from their lives and experiences. Finding the sweet spot between 
familiar and new, the research tells us, is the key to making learning stick.3 So, as 
well as linking new learning to prior knowledge, we should show students that 
what we’re teaching goes beyond, supplements or is significant in different ways to 
what has come before.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Start with the concrete
Michaela School in Wembley is a veritable emporium of quality explanation, with 
teachers who are expert in being the expert. Explicit teaching is on offer in every 
classroom but such teaching is often characterised as dry and detached. When I 
visited, nothing could be further from the truth. I saw teacher after teacher explain 
their subjects to gripped rooms full of children. Delivery was powerful but so was 
the thinking underlying each explanation. It’s worth considering how such teachers go about the business of explanation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pritesh Raichura, Head of Science at Michaela, describes how students need concrete, real world examples to understand complex definitions. Concrete examples 
‘give pupils something to think about when they encounter the generalised definition’.4 Pritesh teaches homeostasis, the maintenance of a constant internal environment (a definition sure to initially confuse) by using concrete examples: body sweat 
in response to exercise, urination in response to drinking a lot, thirst in response 
to heat. Each concrete example gives texture and layer to student understanding 
of definitions. In this way, concrete examples work better than definitions for the 
introduction of an idea. Even the most student friendly definition is likely to be 
harder to grasp unless it links to what students already know or have just seen.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Examples can’t always tap into a student’s lived experience. Some concepts are 
too abstract or unknown. The definition of a comma splice is joining two independent clauses with a comma. Sounds straightforward but leaves lots of students 
stumped. Real sentences, either projected or written in front of the students, can 
better illustrate the principle than a simple definition, at least initially.
At times, the use of prior knowledge will be enough, particularly when students 
have already been introduced to at least the fringes of the topic. At times, concrete 
examples will feel elusive. Their power is in their ability to add layers of complexity, 
a deep richness, to the definitions and concepts we want students to understand.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Use examples and non-examples
In the process of introducing a concept to students, we’ll often, intuitively, show 
them examples. Intuition doesn’t lead as directly to showing students non-examples. A non-example is an incorrect example. Often non-examples look right or 
exemplify another, different principle. At best, as one guide to teaching explains, 
examples and non-examples should be ‘similar to one another except in the critical 
feature and indicate that they are different’. We are then able to show the examples 
side by side, ‘making the similarities and differences most obvious’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In our comma splicing example, our aim is to show students examples of comma 
splicing and examples of comma usage which isn’t spliced.
Examples of comma splicing:
Jake ran after his friends, he couldn’t keep up.
I like English, I hate Maths.
Non-examples of comma splicing:
At break, Jake ran after his friends.
I like English, but I prefer Maths.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One example and one non-example won’t be sufficient. Examples and non-examples serve to tease out the edges of the concept, to delineate the boundaries between 
right and wrong. For this reason, the boundaries of a concept come into focus when 
non-examples highlight a range of errors or differences with the examples.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Make use of Cognitive Load Theory
In Chapter 9, we saw how an understanding of Cognitive Load Theory helps us 
to plan in a way that will support learning. Our understanding of what students 
already know, of concrete examples and of non-examples marries well with what 
we know about cognitive load:
- Content should be broken down into manageable chunks. At times, this will 
mean breaking down your explanation into parts with tasks and interaction 
planned between.
- Extraneous load should be minimised. Whizzy animations, unnecessary pictures or content that you’re not covering now can all be avoided. Student attention should be telescopic in its focus. Cut everything else away.
- Avoid splitting attention in your explanation. Diagrams with separate, and complex, annotations. Slides with text you’re talking through as well. All of these things 
split student attention, making it harder for them focus on what you are saying.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Principles are useful but they don’t necessarily prepare you for a tricky concept 
and a trickier class last lesson tomorrow. How do those principles marry with your 
planning? The process of planning and delivering these explanations puts your 
subject knowledge under a type of pressure it is unlikely to otherwise experience. 
You, an expert, don’t think like your students but to explain well you’re going to 
have to try. Experts don’t need help defining the boundaries of a concept. Experts 
don’t usually need to refresh their understanding of examples and non-examples. 
Experts don’t need concepts broken down for them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This is the curse of knowledge: a bias built from the assumption that others have 
the knowledge we possess. Stephen Pinker explains, ‘Anyone who wants to lift the 
curse of knowledge must first appreciate what a devilish curse it is. Like a drunk 
who is too impaired to realize that he is too impaired to drive, we do not notice 
the curse because the curse prevents us from noticing it’.6 Because the curse means 
we’re likely to underestimate the power of our own expertise, we need to put safeguards in place to mitigate the effects]]>
			</paragraph>
			<paragraph>
				<![CDATA[Tackle one thing at a time
Concepts we can swallow whole are likely to cause problems for our students. Our 
refrain in applying cognitive science must be to narrow our focus. For ourselves 
and our students, we focus on the component parts to create the whole in students’ 
minds. The problem, of course, is what constitutes one thing?
To begin with, the curriculum, or your planning, dictates the content of the lesson. Which concepts, in the curriculum or your plan, are new or, as yet, without 
strong foundation in student thinking? These require explanation. Some concepts, 
however, are unwieldy when dealt with in one go.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A Religious Studies teacher is planning a lesson on Christian beliefs surrounding 
the resurrection of Jesus. A two-page textbook spread provides the basis for the lesson, and the teacher’s planning. The two-page spread covers the biblical accounts 
of the resurrection, Christian views on the significance of the resurrection, and how
this affects the Christian calendar or a Christian lifestyle. At the end of the two-page 
spread, a set of questions helps students to think about this content.]]>
			</paragraph>
			<paragraph>
				<![CDATA[That different teachers would approach this content differently is not a problem. 
All lessons planned with this content in mind are aiming at a student understanding of the resurrection. A lesson where the teacher tries to explain everything about 
the resurrection in one go before students read and answer questions is likely to 
demand too much of their cognitive resources. We break tasks and content down; 
we should do the same with our explanations. The teacher decides to start by telling the story using a student-friendly account from the Bible.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Prepare them for your explanation
If learning works better after recap or retrieval of prior learning, you are responsible for the work of preparation. You are responsible for checking students are ready 
for your explanation. Firing off hands-down questions around the room doesn’t 
just retrieve what students need to know. Questions check if students are ready for 
your explanation. A multiple choice or mini-whiteboard task can check the understanding of all very quickly]]>
			</paragraph>
			<paragraph>
				<![CDATA[A Spanish teacher might be about to explain how students say what they plan 
to do at the weekend. To start the lesson, the students go through their embedded 
mini-whiteboards routine. Boards, pens and rubbers are passed down by students 
at the end of each row. The teacher says:
Today, we’re going to look at how we can share a plan of activities we plan
to do in the future and why we plan to do those things. This is different to 
what we’ve done before where we talked about and asked each other what 
we’d done at the weekend – in the past. Before I explain the difference, I want 
to check you still understand the vocabulary we’ll need and the way we’ve 
talked about activities in the past.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The teacher projects one sentence in Spanish at a time onto the screen. Each sentence is from an imagined Spanish student, explaining what they did at the weekend 
and why. The class have to write the activity the Spanish students did and the reason why they chose that activity. The teacher is checking students understand what 
they’ve studied previously; students are retrieving lots of vocabulary knowledge to 
complete the task. But the teacher is also preparing students for the next task, drawing 
on required memory and preparing to highlight a division between past and future
activities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The teacher must, of course, be awake and alert to the information received in 
this preparation phase. We’re both bringing our subject knowledge down to the 
level of our novice students and checking they are ready to make a small step (or 
at times, a giant leap) to the next bit of content. Deep understanding of curriculum, 
of subject and pedagogy, will be lost if we don’t adequately prepare and check.
If, at this stage, students reveal that they don’t understand the prerequisites for 
the current lesson, taking a step back is better than stumbling forward. Teaching, 
even the most teacher-led teaching, is never just delivery.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Script and practise the essentials
As a teacher of English, I have become used to students telling me that they don’t 
need to plan and the reasons for this. Whether it’s a story or an essay, students often 
tell me that they plan in their heads. Planning gets in the way of their process. And 
anyway, they’ve done okay so far without it (they haven’t). Teachers can reject 
scripting with similar romantic but misguided arguments: scripting undermines 
or even ignores the teacher’s expertise; scripting removes the spontaneity from my 
lessons; scripting isn’t my style.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Later in your career, you’ll reach the point where each year’s explanations are 
habitual repetitions of the explanations that have come before them. Be careful of 
assuming these future explanations will be any good without careful planning. 
And be careful, when they become automatic, of assuming automatic explanations 
are good explanations. A script can be on a post-it note, in your planner, annotations on a resource, or on a screen in front of you. It shouldn’t be, for reasons 
of split attention, something projected so that the students can see it. If you’re 
explaining it, they don’t need to read it too.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What could your notes include?
- A complete script of everything you want to say to students.
- A scripted definition or other key sentence with notes for the rest.
- A list of words, events, concepts or examples you will include.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A script without practice is likely to become an unhelpful crutch in lessons. 
Heavily relying upon, or simply trying to find, your notes in lessons can disrupt 
the flow of learning and activity. Practise with your notes, considering emphasis 
and delivery, pause and projection. Practise until you don’t need to look at your 
notes all the way through your explanation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Feedback is a core element of deliberate practice. Our aim should be to improve and 
refine before we need to deliver our explanation. A coach can help with each step of 
scripting and practice. Present it to them. Let them question and unpick it. Set your sights 
on improvement and excellence, on the best and clearest version of an explanation. When 
you do this, acknowledge that feedback from a coach is an important refining fire.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Enrich but don’t distract
When Christine Counsell talks about core and hinterland, she reminds us of the 
delicate balance we must strike in explaining anything to students. Students who 
walk a curriculum road lined with only the essential may come to see the journey 
as purely utilitarian, about getting from A to B. As teachers we need to be careful 
of rushing through the curriculum or the content. We need to be careful of turning 
the curriculum, or our explanations, into a list of things students must know for 
the test.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If hinterland becomes part of your explanation, it should be planned and scripted. 
Some of my favourite teachers at school seemed to revel in the tangential, the question asked, the illuminating diversion. Underlying the ability to pursue these threads 
on a whim is the unseen expertise, built from expansive knowledge and experience. 
Trying to be that teacher without the same knowledge and experience is likely to 
lead to disappointment and confusion. I’m also not sure that we should aim to be the 
teacher who jumps around from one thing to another, at least not as the norm.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hinterland should enrich and expand a student’s understanding more than 
it should divert. Counsell describes it as a ‘supporter or feeder’ of the core.7
Hinterland is the story that illustrates the scientific concept or moment in history, 
the biographical detail, the quirk in a case study.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plan to embed
I was going to say Plan your check but check might be the wrong word. Or, at least, 
it’s incomplete. We don’t explain, check and repeat ad nauseam until the end 
of the year. Once something has been explained, it’s right to check that students 
have understood the explanation. But the activity after the explanation is doing 
more than checking; it is embedding the content of that explanation, allowing students to enrich their understanding of if through deliberate and directed thought. 
Assuming that students have got it because we have explained it is a risky strategy. 
If we’ve chunked learning down into manageable pieces, the explanation is likely 
followed by practice or activity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Explanation is the realised power of your subject and curriculum knowledge to 
make content accessible for students. It is not an add-on or off-the-cuff possibility 
for your lessons. What’s more, explanation is going to happen whether you think 
about it or not. You will stand in front of classes and tell them things. Don’t, as I did 
for much of my career, stand in front of them with nothing planned but an overconfidence in your ability. Instead, grapple with the content, with its challenges 
and pitfalls. Grapple with the connections between what your students know and 
what you want them to.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Modelling
Is modelling simply about doing the tasks we expect of students before they are 
attempted by students? Or is modelling any demonstration? So much of what we 
do is modelling that we can be unaware of it. Equally, modelling feels at times 
like something we’ll just come up with in the moment. Modelling, in this way, 
occupies the blank spaces, the gaps, in our planning proformas between activities. 
Tasks are planned. Models less so. Thinking about modelling pushes our pedagogical content knowledge to its limits. This is both a planning and a development 
activity because we are confronted with our abilities and inabilities, our capacities or lack thereof, to adequately communicate how students should progress 
through a task.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Competent modelling rests on our knowledge of the subject matter but, more 
crucially, an ability to condense and communicate that subject matter in a way 
that makes sense for children. More than simply saying how something is done, 
modelling offers students the ways they may get things wrong (and the ways we 
get things wrong), the ways they can get unstuck and, ultimately, the ways they can 
apply their subject knowledge. Modelling is, therefore, our way of trying to solve 
the problem of young, undeveloped knowledge which must be applied.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Modelling is for anything (but not everything)
Our focus is the subject but modelling can and should be for anything. How the 
books are handed out, how students come into the room, how they answer questions can all be modelled. In the realm of the subject, we model anything that will 
help students to access, progress through and succeed in the curriculum. Modelling 
isn’t just reserved for the big tasks, the extended writes, the 12-mark questions, the 
complex problem solving. The steps of those processes and activities can be modelled and unpicked. Students often answer short questions following the introduction of content across subjects and across phases. Modelling how students take a 
question and form a sentence can be usefully modelled. Modelling how students 
answer questions verbally is an investment in your whole class discussion.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A counterpoint to the model anything argument is that we can’t model 
everything. Deep understanding of a well-planned curriculum should direct some 
of the subject-specific modelling we do but decisions in this area should be based 
on our classes. Ascertaining what students know and can do is vital for our ability 
to model the right things, the things they need direct guidance through.
After you’ve planned a lesson go back through the plan and consider the tasks 
with the most demanding subject content, processes or both. Cast your mind back 
to element interactivity – the number of components a student will have to juggle 
in a task. Tasks with high element interactivity will benefit from a model.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Narrate the steps and pitfalls
Cognitive Load Theory and element interactivity might prompt our modelling but 
subject knowledge and pedagogical content knowledge offer direction of what and 
how to model. The subject domain we inhabit may feel alive and colourful to us 
but gloomy and dark to our students. Our job is to find those items, those nuggets, 
tasks and processes, that brighten a student’s understanding of that domain and 
enable them to navigate it without our future support.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Ask, What do I, as an expert, know that my students won’t? Models both provide 
strategies for using subject content and strategies for coping when students aren’t 
sure how to use that content. Students should witness the struggle even if such 
struggle, for you, is a bit of a performance. Students are likely to be more successful 
when they see the struggle than when they see effortless perfection.8
As we model, we can narrate what we’re thinking (or possibly what students 
should be thinking) and where mistakes can be made and how to avoid them. We 
may find it easy, as experts, to model and talk at the same time but we should be 
cautious of seeing this multi-tasking as evidence of effective narration.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Script your narration with your planning, resources and curriculum in mind. 
Make it succinct and direct. As with the scripts for our explanations, key vocabulary or points you’re likely to forget are both worth including. Practise it in your 
classroom without any students. To reduce your cognitive load, decide early how 
you will model. Under a visualiser on a piece of paper or in an exercise book have 
the benefit of mirroring what the students will be doing and can feel natural. For 
models you’re concerned about or haven’t quite ‘got’ yet, practising with a coach 
or mentor first can be invaluable.
You can’t practise every model for every class. Content you don’t know well or 
are teaching for the first time is worth practising and scripting.]]>
			</paragraph>
			<paragraph>
				<![CDATA[But don’t overload them
A further reason to script a model, or at least the narrated thought process, is to 
check that it isn’t likely to overload your students. Talking too much over your 
model is likely just as ineffective as not talking at all. Students can’t hold onto 
every tangential thought your brain forces from your mouth. A script is a limiting 
safeguard on what you say. Of course, you can deviate from it where unforeseen 
problems arise; you can still respond to the needs in your class. A script is simply 
your attempt to distil the necessary knowledge of curriculum, content and process. 
We only tackle the curse of expertise when we distil that expertise into something 
manageable for students.
]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unpick the model with questions
Completing the model isn’t the end of the story. Questions allow us to tease out 
what makes the model successful. Go for hands-down questions and hold your 
perception of the model up against their understanding. Questions like these aren’t 
just to check if students understand; we also check which bits students understood 
or how well. If you’ve spoken about mistakes to avoid, ask about them. If you’ve 
offered two methods, compare them. If there’s something students must remember 
at a certain point, see if they remember it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Paradoxically, much of this chapter offers you generic advice in that you can 
apply it to some degree no matter what subject you’re teaching. A distinction 
between generic teaching and learning strategies and a deep knowledge of what 
you teach depends on what you bring to these activities and how you apply knowledge to classroom practice. Your schema or mental models for your subject merge 
with principles about explanation and modelling.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Clearly, a teacher’s activity isn’t limited to explanation and modelling. Questions 
and feedback, and much more besides, use our subject knowledge to shape student 
knowledge in a lesson. Explanation, modelling, questioning and feedback are often 
things we don’t plan; we just do. Pressing into these unexplored areas, areas often 
ignored in the planning process, forces us to confront the capacity of our subject 
knowledge to meet the needs of our classes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
Subject knowledge goes on quite a journey from what you have when you start out 
to what can be useful in facing the challenges of the classroom. Subject knowledge 
developed at distance from the classroom – reading a book about our subject or 
attending a talk, say – can definitely improve our knowledge but these things are 
long-term strategies, unlikely to improve our teaching this week or next.
More likely to improve our teaching now is a growing knowledge of the curriculum, both zoomed out in its understanding of the narrative our curriculum tells 
and zoomed in on the lessons we plan within it. Within those lessons, the subject 
knowledge that will really matter is the knowledge of how to make clear what is 
unclear, how to contain key elements of it in a task, how to chip away at it until 
something comprehensible is left.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
1. Prepare to explain. Consider the prior knowledge that will be useful or 
essential for students to grasp your explanation. With brand new topics, this 
might include students’ general or everyday knowledge. In most cases, there 
will be previous topics, lessons, content or skill to be reviewed in preparation for the explanation. Plan a short quiz that lays the groundwork for your 
explanation.
2. Script and rehearse an explanation or model. It’s not possible to script 
everything. No doubt there will be times when you find yourself explaining a 
complicated topic that would have benefited from a bit more careful planning. 
Don’t beat yourself up about it. When you do script an explanation or model 
make sure you rehearse it, if only briefly, as well.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Conspicuous in their absence thus far, students pose a difficult question when 
it comes to teacher knowledge. What do we need to know about our students 
to be effective teachers? Knowledge of subject and pedagogy would be incomplete without turning our attention to those who are meant to benefit from such 
knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I was presenting on knowledge of students at a training day for teachers. As I 
started, I made a flippant comment about how knowing students’ birthdays or pets’ 
names might make you a nice person but it wouldn’t make you a better a teacher. 
In the discussion that followed, various members of the audience (and other presenters) made it clear that this kind of unfeeling attitude was exactly what is wrong 
with the profession. And they were right: there is a sense that our knowledge of 
these human beings we teach will shape our time in the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Like every area of teacher knowledge we’ve examined, the sheer quantity of 
what we could know about our students is a daunting prospect. Child development, adolescence, youth culture could all be worthy of our time. As I was 
corrected on that training day, there’s much to learn about the students in front 
of us. Reducing this to trivia about them ignores the importance of knowing 
who we teach and how they respond to the classroom and our subjects. Whilst 
there is no chapter in this section on how to find out the name of a student’s pet, 
knowing students’ interests, being able to talk to them about what they like and 
dislike and understanding something about their temperament can all be incredibly helpful.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Especially important in knowing our students is knowing any special educational needs they have which we may need to consider in our teaching. This 
book has no section on teaching children with dyslexia or ADHD or Asperger’s 
syndrome. In each case, our teaching will take into account such needs but we 
shouldn’t confuse knowledge of a need with knowledge of a child. Each need presents differently in each child.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One type of knowledge requires daily updates: knowledge of what students can 
and can’t do, as well as what they do and don’t know. We must update this knowledge constantly because:
- It is difficult to ascertain in the first place.
- It’s easy to misdiagnose struggling or successful students.
- It will inform short, medium and long-term adaptations to our teaching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mary Kennedy describes this as the problem of ‘Exposing student thinking’, highlighting that ‘the most useful knowledge for teachers is the knowledge they have 
in the moment, for this knowledge can guide their actions in the moment’.1 This is 
the reason why we’ve left knowledge of students until last: it’s probably the most 
complex and challenging to develop]]>
			</paragraph>
			<paragraph>
				<![CDATA[If these are the problems, the solutions will take us into a set of strategies sometimes called assessment for learning or responsive teaching. Having examined 
these solutions, this section will then put a dampener on proceedings by warning 
against trusting too much in our intuitions, expectations or other types of knowledge of students.
With that in mind, Part 6 will answer the questions:
- How do we figure out what students know and what do we do about it?
- What are the limits of what we can know about students?]]>
			</paragraph>
			<paragraph>
				<![CDATA[In The Matrix, a film I imagine is depressingly out of date for the young new teacher, 
Neo, the main character, learns new things by having a plug shoved in the back of 
his neck and downloading information straight into his brain. Unfortunately, as 
Neo is the hero in a dystopian world, he can’t spend time learning the complete 
works of Shakespeare, a new language or classical piano (and in my mind the film 
is the lesser for it). No, he must learn how to fly a helicopter, how to shoot and, of 
course, kung-fu. In the world of The Matrix, there is no gap between being taught 
– having a wire shoved in your neck – and having learned. One naturally leads to 
the other. Real life and real teaching are more complicated.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Dylan Wiliam and Paul Black use a metaphor to sum up this complexity. In their 
seminal work on formative assessment from 1998, Inside the Black Box, Black and 
Wiliam describe the classroom as the black box. Pupils, resources and teachers 
provide inputs to the box. Outputs include knowledge, test results and feelings of 
satisfaction and achievement.1 The problem, as Wiliam and Black put it, is that we 
can’t really be clear on the specific inputs resulting in certain outputs.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Formative assessment, any type of assessment where evidence gathered is used 
to ‘adapt the teaching to meet student needs’,2 is offered by Black and Wiliam as 
a method of breaking open the black box, the classroom, and understanding what 
is going on inside. Not only is there evidence that formative assessment strategies 
raise achievement of our students, but these strategies are also more likely to raise 
achievement for the lowest achievers in our classrooms. So, we must introduce a 
new type of knowledge the new teacher will require: the knowledge of what students know and can do]]>
			</paragraph>
			<paragraph>
				<![CDATA[When we question, when we examine work, when we look over their shoulders, we see and know in part. Where our knowledge of subject and pedagogy 
are brightening expansive landscapes, knowledge of student learning is viewed 
through a glass, darkly. Variously labelled formative assessment, assessment for 
learning, responsive teaching or checking for understanding, activities that seek
to understand what students have learned are vitally important. The names here 
reveal the dual function of such activities: assessing what students know and 
responding to this information. The concept of assessment, however, can confuse teachers and students. Assessment for learning happens whilst students are 
learning and whilst you are teaching. Assessment here is not after the fact nor is 
it assessment to grade, mark or define ability. Dylan Wiliam reportedly regrets the 
naming of assessment for learning, wishing he had used the phrase responsive 
teaching instead.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hang on a minute, you cry. Learning takes place over time. Can we really assess 
learning in the lesson? Or are we just assessing performance? There is, admittedly, 
a paradox at the heart of assessment for learning. Learning as assessed in a single 
lesson is not guaranteed. If a student shows us perfection in one lesson, don’t 
assume it’s permanent. If they know in the lesson, great – there’s at least a small 
likelihood they’ll know next lesson. If, however, they don’t understand in this
lesson, there’s a strong to certain chance they won’t know in the next. Anyway, we 
assess not just to know but to respond.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teachers, particularly new teachers, struggle with the response. A student gives 
a wrong answer but the rest of the class seem fine – do we just leave it? Some students are using the sentence construction we’ve taught incorrectly but it’ll come 
up again soon – do we just wait and see? Even if we want to act, what should we 
do? Options abound, from seeing an individual later right through to stopping the 
whole class and re-teaching something.
To gather and respond to this knowledge, use a three-part process: plan the 
assessment in advance, do it in lesson, respond to the information.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Before we ask What do they know? we need to be clear about what we want students to know. Teachers who get into the minutiae of their curriculum plan better assessment. Clear objectives and success criteria provide something to work 
towards and therefore to check.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Objectives are too often nebulous. Explaining to a class that We’re learning to 
synthesise our ideas or construct a powerful argument doesn’t give students or you 
a clear idea of what you’ll need to check at the end of the lesson. As with planning 
though, thinking about purpose and here more deeply about success criteria is 
an important part of being able to check that students have learned. Checks don’t 
work in general. An in-lesson check isn’t a review of everything students know. 
Precision about what we’re looking for is important because we can’t look for 
everything. That said, checks struggle to work in binary too. It’s not that students 
do understand or don’t. They understand in part. They misunderstand slightly]]>
			</paragraph>
			<paragraph>
				<![CDATA[A success criteria might precisely define the vocabulary students should know 
and use; it might set a goal of competence in a process. More broadly, it should 
define what students should know or be able to do. Expecting students to Write 
184 Knowledge of students
a range of sentences or Simplify fractions can mean wildly different things. Such 
objectives don’t meet the demands for clarity and precision that we’re after. Write 
a range of sentences can become a sequence of lessons including, Use a comma 
to separate adverbs, subordinate clauses or prepositional phrases from the main 
clause and Use semi-colons accurately before students are expected to combine 
these different techniques.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A success criteria like this can exist in your mind – but there is a danger you 
forget it – in your planner, in a planning proforma, or in the resources you share 
with students. Some curricula will provide this for you. Others will leave at least 
some of that decision to you.
Even when we’ve fixed on what we’re checking, learning happens over time. In 
a single lesson, there’s no watertight task you can plan that will guarantee long-term learning. All you can do is check that students understand what you explain 
and practice in lessons.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Planning then needs to make two things clear in your mind:
- What do students need to know?
- How will you check?
We’ll turn to that second question now.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Do
What we do is a result of what we’ve planned. Most of the activities listed require 
careful preparation so that questions or resources are ready. Every activity is about 
gathering knowledge you can act on. None of them are about catching students out.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hands down
Picture the scene: you’re being observed with a tricky Year 9 class. You’ve just finished an explanation of a challenging concept. Students need to do some independent practice but first you want to check they understand and, looking round the room, 
it’s not clear they understand. One student is picking crumbs from his tie; two girls 
are having a whispered conversation at the back of the room. Tentatively, hopefully, 
you ask a question of the class. One hand shoots up. Another far less confident hand 
follows. The rest of the class sit in sullen disengagement. What’s your next step?]]>
			</paragraph>
			<paragraph>
				<![CDATA[The easy way out is to ask the confident hand. This student knows. You know 
they know. They know they know. The class knows too. Perhaps, at times, there’s 
wisdom in students hearing the right answer from a peer rather than their teacher. 
But as a method of figuring out what the class know, confirming the most able child 
in the room understands you is useless information. They understood before you 
started speaking.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If questions like this exist to check student understanding, the questions themselves are incredibly important. Most teachers don’t tend to plan questions. 
Questions just emerge, unformed and unplanned, from our mouths. Planning all
questions you ask in a day is impossible. Planning some is essential.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some practical points about hands down:
- Tell the students you’re going to do it: ‘Keep your hands down for these questions. I want to see what you know’.
- Put the name at the end of the question, after a pause. That way, students will 
have to listen and get used to thinking about each question asked.
- If you have a particularly reluctant class, get them to talk to their partner or write 
down their thoughts first, just for 30 seconds. Your first question can then be, 
‘What did you talk about?’ or ‘What have you written?’
- Don’t accept ‘I don’t know’. Ask a follow up. Ask another student or students 
and then return to ‘I don’t know’. Ask them to sum up what they’ve heard or 
choose the most likely answer or, at a minimum, just repeat the correct answer 
they’ve heard.
- Allow students to demonstrate enthusiasm and interest by offering answers with 
hands up. Be clear with yourself and your students about when it is important to 
gather responses with hands down.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We ask hands-down questions to elicit responses from students who may not have 
got it. In fact, we go directly to those students. Which student can act as a barometer of class understanding? If they know, most will. This approach may cause 
some awkwardness in that observation, producing situations where the class haven’t understood and you have to return to the board, to your examples, to your 
resources to try again. We’ll look at what to do in these situations when we get to 
Respond, but for now, it’s vital you’re able to stare down the awkward revelation 
that you’ve taught something the class haven’t understood.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Before we become too enamoured with, too proud of, our new hands-down habit, 
we need to realise something. Trying to ask your way to understanding, one child 
at a time, is an incredibly inefficient way of grasping for knowledge. Particularly 
if the kind of questions we’re asking can be directed towards the whole class. The 
ephemeral nature of the whiteboard also means students sometimes feel more able 
to have a go because answers will always be rubbed away.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I’ve heard history and English teachers say that mini-whiteboards don’t work 
in their subjects. It’s true that certain tasks don’t transfer well: writing an essay or 
even an introduction won’t fit into a relatively small white rectangle. But lots will.
Some practical points about whiteboard use:
- As with hands-down questions, planning the questions you ask is essential.
- Ask a question, give a clear time limit and train students not to lift their board 
until you want. Students can hold them horizontally so you know they’re ready 
but others can’t see their answer. Countdown and give a clear ‘Go’.
- Don’t worry too much about seeing every answer. Unless students are writing a 
single letter or number, checking every board will be very difficult. First check 
the students who you’re worried have got it wrong. Arrange a seating plan so 
their boards are easy to see.
- Logistically, there’s a lot to think about. Storage should enable ease of distribution and tidying away. Each row could have its own box with the necessary 
boards, pens and rubbers. You need to train students to unpack and pack them 
away but also just to use them well. Devote time to this when first working with 
a class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Not every task can be done on mini-whiteboards but do consider the challenge 
when not using them: Could I gather this information from more children, more 
quickly? Other strategies can similarly be used to get information from the whole 
class. Students can offer answers to multiple choice questions on their fingers. 
Principles of mini-whiteboards apply. Call and response can be used to check 
confidence on definitions, vocabulary and factual recall. Shallow though this 
information might be, observing the room can show you who knows and who 
doesn’t.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lessons are full of transitions. Students enter and start working. One task draws 
to a close and another begins. The process is repeated until the lesson ends. We’ll 
do the same again tomorrow. Chapter 6 talked about the practicalities of managing 
transitions to maximise positive behaviour.
Teachers are not on a track, inescapably moving through the curriculum. Even 
when we feel that we must get through the content – the curriculum is so full – we 
must remember our responsibility for learning, not coverage. Teachers build learning by ensuring students understand, by getting students to practice well and by 
linking that understanding and practice to what comes next. As well as attending 
to the practicalities of transitioning from one part of the lesson to the next, we must 
consider how we move from one part of the learning to the next.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Dylan Wiliam describes a lesson’s hinge as the ‘point at which the teacher checks 
whether the class is ready to move on by asking a diagnostic question’. This is a 
hinge because ‘how the lesson proceeds depends on the understanding shown by 
the students’.3 A hinge question, therefore, is one we ask at the point where the 
lesson should move on if, and only if, enough students demonstrate the required 
understanding. Diagnostic insofar as they offer right answers and reveal misconceptions, hinge questions check student understanding]]>
			</paragraph>
			<paragraph>
				<![CDATA[A teacher writes a hinge question for finding the area of a rectangle. The teacher 
shows a diagram where the sides of the rectangle are 5 centimetres and 3 centimetres. The teacher offers the following multiple choice answers:
a. 8 cm2
b. 15 cm2
c. 16 cm2
Sarah, a Year 4 teacher in her first year, describes responding to 
student misconceptions.
It’s important to give children a chance to show you what they don’t understand in the 
lesson. I don’t just ask them to tell me they if they understand. I test their knowledge by 
asking diagnostic questioning. I use whiteboards a lot for this. The children might know 
the answer but struggle to articulate it. The whiteboard gives them an opportunity to 
think and get it down before they’re asked about it.
When we’re teaching, we can assume that children understand things but they don’t. 
I’ve got to know them. I’ve got to know them in that way but also through looking at their 
work. We have a feedback book where we write everything they’re doing, everything 
they’re struggling with. I like writing specific names of children and what they’re not 
getting in the lesson. That helps me when I’m planning the next lesson.
188 Knowledge of students
Students who choose a. see two numbers on the diagram and add them together. 
Students who choose c. have added all the sides, confusing area with perimeter. b. 
is the correct answer; it is the area. Note, this question works well when finding the 
area has recently been explained and modelled. A teacher uses such a question to 
check if students are ready to practice independently. Students who have already 
practised finding the area of a rectangle will not find this a challenge. The principles described above apply: this question can be answered on mini-whiteboards or 
by holding up the fingers on one hand. Alternatively, hands-down can be used for 
the initial answer with lots of further questions to unpick why each answer is right 
or wrong. Two or three questions like this confirm that students are or aren’t ready 
for some independent practice.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Multiple choice questions are effective where misconceptions are obvious, easily selected as possible right answers. Poor multiple choice questions include one 
right answer and several others which students immediately know aren’t correct. 
These don’t check understanding; they prompt and remind. Because planning time 
is important and you don’t want to waste it, spending hours conjuring close but 
wrong answers isn’t worth it. When you can’t think of important misconceptions 
you want to check, plan some questions that all students can answer or a task that 
helps you to check understanding before moving on.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Monitoring
James is in his first year as a secondary science teacher. He’s doing well but his 
classes can be a bit chatty, particularly in the freer, more independent sections of 
his lessons. He models and explains concepts clearly and concisely. He explains 
tasks with precision. He starts his tasks with a clear ‘Off you go’. Students start 
putting their hands up. First one and then a small flurry. James dutifully begins 
his way around the room. He does what most teachers do as they move around the 
room: he helps where he needs to, he manages behaviour, and he cajoles students 
into working when they lose the energy for it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are two problems. Firstly, something has gone wrong in the set-up of the 
task. Either students weren’t ready for it – and a hinge question or mini-whiteboards 
could have been used to better effect – or students have learned helplessness: they 
know they can switch off in the explanation because James will be there to help 
them. Secondly, James isn’t checking for understanding. He isn’t really gathering 
knowledge. He’s firefighting.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What does James need to do? Firstly, he needs to sort out that transition into the 
task, embedding a clear check that shows students are ready. Secondly, he needs 
to make clear what support is available to students. Can they use a textbook or 
another resource in the room? Can they turn back in their exercise books or talk to 
their partners? Can they move on if they’re stuck? Students need to be told. Finally, 
James can tell the class that no student will put their hand up in the first five minutes of the task (other timeframes are available). This might feel brave or cruel or 
How do we figure out what students know and what do we do about it? 189
stupid but hands up after you’ve explained a task, checked they’ve understood and 
signposted support suggest overreliance on the teacher.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To start with, James will stand at a point in the room where he can see the class 
well.4 He’ll stay there, watching, gently gesturing for students to lower their hands 
when raised. If students aren’t following instructions, James will stop the class and 
bring their attention back to the front. For example, if he’s asked for silence but 
isn’t getting it, that’s an opportunity to reiterate expectations to the whole class. 
Alternatively, if students aren’t using one of the sources of help available and this 
is causing lots to struggle, it might be time to bring the class back together to remind 
them how to get unstuck.]]>
			</paragraph>
			<paragraph>
				<![CDATA[After the class are clearly settled – possibly after a long couple of minutes – 
James can set out into the room. No longer is James setting off to sort out all the 
issues that need to be resolved. Now, he’s looking for something precise – whatever content or skill the students are using, practising or describing. Doug Lemov 
describes this as naming the lap5 because we’re doing a lap of the classroom with 
a specific purpose.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I find it helpful to step out into the classroom saying, ‘When I come round, I’m 
looking for…’ and sticking rigidly to this. End the sentence with something specific. I’m looking for finger spaces. I want to see answers to one decimal place. I 
want to see the word ‘significant’ in your first two sentences. Getting sucked into a 
conversation about something else, correcting every mistake you see, or debating 
how soon after break it is acceptable to need the toilet will stop you gathering any 
useful information]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some practical tips about monitoring:
- When planning, note down specifically what you’re monitoring for in all independent activities. Link these moments in your lesson to the success criteria, 
curriculum content or objective of your lesson.
- If you see a mistake across several students’ work, don’t make a note to deal 
with it later. Deal with it now. Stop the class and explain, A few students are… 
I’ve seen the answer… There’s some confusion over what to do when… Or take a 
book to the visualiser and ask what’s wrong. Go through the example.
- Take round a notebook or a mini-whiteboard (or something to take photos with) 
and capture the best answers, the common errors, and the students who are 
struggling. This process can work without recording what you see but you’re 
liable to forget something important or someone who needs your help.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Exit tickets
An exit ticket is what it sounds like: a task students do before they go to show you 
they understand. Generally, these are done on paper so that they can be gathered, 
checked and organised efficiently. In Responsive Teaching, Harry Fletcher-Wood 
190 Knowledge of students
describes three qualities of effective exit tickets. Firstly, they should ‘permit valid 
inferences about students’ learning’. Whatever the task, it should allow you to ‘differentiate accurately between levels of understanding’ whilst throwing out potential misconceptions. Secondly, exit tickets ‘provide useful data’. Too often, teachers 
design tasks that provide too much data. If a lesson should have a clear focus so 
should the exit ticket. Thirdly, exit tickets should ‘be focused’. They should be 
‘swift to answer’ and ‘swift to mark’.6 Paragraphs and 12-mark questions don’t 
make good exit tickets because they don’t quickly demonstrate knowledge that can 
be quickly assessed.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Exit tickets then should act like hinge questions at the end of the lesson. We 
might have been adding fractions, describing circuits, explaining why sweatshops 
exist or evaluating the key features of a particular artwork. For any topic, the exit 
ticket should bring to the fore the key element we hope students will take away. 
They can be open – list three features of… – or multiple choice – which of the 
following is correct… – or complete the sentence activities. Have the students – as 
best as you can tell from a short task or small number of questions – got what you 
wanted them to? Remember, that students can do something at the end of the lesson is not a guarantee that they will be able to do it next lesson. However, when 
students can’t do (or don’t understand) something at the end of the lesson, we can 
be fairly sure that they won’t be able to do it next time. As we pore over the exit 
tickets, we see the success and the failure of our teaching. Just knowing about this 
is not enough. We have to do something. We have to respond.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Adaptation is hard because it requires in the moment changes to what we had 
planned. Teaching, as we’ve seen, is a chain of intentionally or unintentionally 
automated processes. To check students’ understanding, the chain must be broken. 
Intending to respond is unlikely to yield results. Instead, to begin with at least, we 
should have some additional tasks, questions or activities ready where we think 
students might need them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Knowing that some haven’t got it, knowing that we need to respond – even these 
aren’t enough. What form should our response take? Options include everything 
from stopping the class and re-teaching a topic right down to catching up with a 
single student when you have a moment.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If only a handful of students haven’t got it in the hinge question or exit ticket, 
stopping the class or re-teaching may not be an option. Instead offer individual or 
small group support.
Think about the practicalities:
- If there are students you regularly need to support, seat them somewhere that 
is accessible for you. This may not be the front. At the back, you can watch the 
class and work quietly with a small group.
- Keep checking, even with one or two students. Ask them questions or use the 
mini-whiteboard. Don’t just re-explain and ask if they’ve got it. Eventually, 
they’ll say they’ve got it just to get rid of you.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At times, students will need to see the same principles borne out across different 
tasks. In Maths, do the same task but change the numbers. In English or History, 
change the question but focus on the same content. In Art or Design, keep the same 
process but change the purpose.]]>
			</paragraph>
			<paragraph>
				<![CDATA[More questions or tasks
Perhaps just one more. In your planning, it’s worth having a spare activity or set 
of questions at the hinge points in the lesson. If lots of the class don’t get it when 
Sarah describes how she works with small groups of students to 
support them.
I tend to work with the lowest of ability and EAL children at the front of my classroom. 
I know everyone doesn’t do that but I like to do that because I was a TA and I know 
those children need the most support from the teacher. I’ll be honest, it can be draining 
sometimes. With a lot of the children, it’s vocabulary that they’re really struggling with. 
It’s not that they don’t understand all the work. It’s that they don’t understand the words 
and the sentences and the phrases we use – the language that goes with the work.
All students start off on the same thing but some children might need scaffolds, some 
need extra discussion. The four children sat with me just need some extra support, 
sometimes re-teaching or covering content in more detail.
192 Knowledge of students
you’re monitoring, explain or model once more and set the class off on some additional questions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Future planning
Some errors can’t be picked up in the moment because it’s the end of the lesson, 
because they’re big, or because they’re subtle and need a bit more work to unpick. 
Use your planner, notebook or resources to track larger errors you want to come 
back to. It may be that the term is ending and you know they’ll need this in the next 
topic. Figure out what this is; narrow it down.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A note on whole class feedback
There’s no section on marking in this book. I don’t want you to waste your time. 
When I was a new teacher, I basically ignored the school marking policy. I did mark 
my books but not with the frequency and detail that were expected. The policy 
itself felt cruel and unusual to me and I was surprised to watch teachers, all with 
more experience than me, bow thoughtlessly to its demands. No one checked the 
policy was being implemented and I was frequently told I was doing a good job. 
This is a cautionary tale rather than a recommendation. I could have got in trouble but perhaps we all knew – particularly leaders – that deep down the policy 
expected too much.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Marking in its most excessive forms seems to be on the way out. To be clear, it’s 
not that marking doesn’t have an impact. Of course, it can. When done effectively, 
you’re basically offering one to one tuition. But this is far from an effective use of 
your time. Save it for assessments and mock exams or questions. Many schools 
have adapted marking policies into feedback policies, calling on whole class feedback to save them from workload and, hopefully, support learning. Because whole 
class feedback means different things to different schools, it’s impossible to say 
what your school will mean when they say whole class feedback.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It might involve filling in a particular sheet or booklet. It might not. It might 
involve feedback lessons, where students write the title ‘Feedback Lesson’ so that 
everyone – particularly visitors – knows that ‘Feedback Lessons’ have taken place. 
It might not. It might involve purple polishers (other colours are available) or other 
editing. It might not.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What then are the common principles or activities of whole class feedback?
First, we examine and record:
- Look at all or a sample of the books. Ask students to hand in books open on the 
page they’ve been working on. That way you can quickly go through them.
- Focus on and make note of the general – spelling errors, previously taught material – and the specific – what was being taught in that lesson.
How do we figure out what students know and what do we do about it? 193
- Make a note of specific students who need support.
- Make a note of students whose work can exemplify success.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Second, we plan:
- If lessons start regularly with review, plan questions that will practice weaker 
areas. Add words to spelling tests.
- If the task allows, plan some prompted editing of the work. You can set this up 
in different ways, telling students what to look for or modelling with a piece of 
work from the class.
- Plan your links to future content, making sure you properly prepare students by 
reviewing what they’ve got wrong or misunderstood.
- Plan in the medium and long term. Make a note or set a reminder to revisit a 
topic or aspects of it where your checks reveal potentially shaky understanding.
- Alternatively, you can plan a single lesson where students practise spelling, 
complete editing and any additional tasks. You can use these times to repeat 
models and explanations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
All that subject knowledge, all that knowledge of pedagogy and how students learn 
will have been for nothing unless we respond to what we’re seeing in the moment, 
from our students. A well-planned curriculum, however we come by it, will be 
meaningless if we just plough on regardless of what students are understanding or 
misunderstanding, regardless of the mistakes and misconceptions they’re embedding. Knowledge of students is dynamic because it requires constant attention. We 
don’t just update this knowledge as we teach new students; we update it as our 
current students grow and develop.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
1. Develop a checking habit. To start with, you’re just making sure not to breeze 
through a lesson without pausing to glance at the students and see if they’re 
keeping up. Creating a hands-down habit here is positive and can be used in 
pretty much every lesson, phase or subject. If you arrive in a department where 
mini-whiteboards are embedded, using them yourself won’t feel difficult. 
Write ‘hands-down’ on the whiteboard, on a wall at the back of your room. Or 
remove the friction from using mini-whiteboards by setting up boxes for each 
row with rubbers, pens and boards. The habit is not simply borne of a section 
in your plan or a check in your resources. It emerges from the way you force it 
into your practice with unmissable reminders and quick wins.
2. Plan to respond in advance. It’s no good in the moment realising that students 
don’t understand but having nothing for them to do. Have some extra tasks in 
the textbook ready. Change the numbers in a Maths question but keep the process. When you make this part of your plan, you make it more likely that you 
will actually respond to knowledge gained about student understanding. When 
good intentions are left to drive your in-lesson adaptation, it’s more likely such 
adaptation won’t happen.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In Greek mythology, Pygmalion was a sculptor who fell in love with one of his 
statues and was blessed by Aphrodite when she brought the sculpture to life. 
Pygmalion’s expectation became reality. There’s more to the story: as you’d expect 
from Greek mythology Pygmalion’s behaviour is strange to say the least. Pygmalion 
is important to us because of the effect named after him. The Pygmalion Effect is 
the phenomenon where high expectations of an individual lead to high performance. Its opposite is the Golem Effect where low expectations lead to low performance. Like a self-fulfilling prophecy, we get what we expect to get.]]>
			</paragraph>
			<paragraph>
				<![CDATA[So far, we’ve looked at ways we can build knowledge to solve the problems of the 
classroom. That is still our aim. Now, however, we need to admit that our knowledge 
can be flawed; we need to admit that sometimes our assumptions, biases and expectations masquerade as knowledge. The knowledgeable teacher needs to be able to step 
back from what they believe to be true about their teaching and assess it dispassionately. Nowhere is this more true than in the judgements we make of our students.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the mid-sixties, Rosenthal and Jacobson conducted a now famous experiment. 
They told a group of teachers that certain students in their classes were ‘growth 
spurters’, children who looked average now but would make rapid progress through 
Kate, a secondary Maths trainee, describes how she started to put her 
students into categories.
I fell into a little bit of a habit of thinking, The class (or the student) didn’t really understand 
that so they won’t really understand this. It’s really easy to categorise a student when you’re 
meeting 80 of them in a day. I had to really check myself on the thought process that said, 
they won’t understand this because they didn’t understand that.
16 What are the limits 
of what we can know 
about students?
196
the year. Those identified as ‘growth spurters’ were chosen at random. Rosenthal 
and Jacobson came back at the end of the year, keen to know whether a change in 
teacher expectation would lead to a change in outcome for students. They found 
that ‘When teachers expected that certain children would show greater intellectual 
development, those children did show greater intellectual development’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There were caveats. The Pygmalion Effect was more pronounced with younger 
children. All students made progress and the gap between ‘growth spurters’ and 
‘non-growth spurters’ was relatively small. These caveats have led one modern 
summary of the Pygmalion research to conclude that ‘self-fulfilling prophecies 
in the classroom do exist, but they are generally small, fragile, and fleeting’.2 Is 
Pygmalion or similar research worth our time if the effects of such expectations 
may be small or fragile?]]>
			</paragraph>
			<paragraph>
				<![CDATA[To answer that question, we need to acknowledge Pygmalion isn’t the end of the 
story, or the research. Various studies inform us that teachers form biases of students based on prior attainment, personal characteristics, socio-economic status, 
ethnicity, gender, SEND and even physical appearance.3 When we read research 
like this, we mustn’t think This doesn’t apply to me because I’m a good person. We 
don’t hold biased views because we’re bad people. We hold biased views because 
we’re human. I don’t know which biases you’ll hold and you might not either. 
The point of this chapter is not to fixate on the fact that we are biased individuals. 
The point is to ensure that neither bias nor assumption affect how we teach the different students we find in front of us.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The point is to interrupt a cycle of behaviour 
that goes something like this:
1. We form or hold inaccurate judgements of students.
2. We behave differently with different students, for example in interactions in 
lessons.
3. Students notice different treatment of themselves or others. Students were able 
to observe the differing treatment of different students in a videotaped lesson, 
even when the lesson they were watching was in a different language.]]>
			</paragraph>
			<paragraph>
				<![CDATA[How do schools treat different groups of students?
None of this is ground-breaking. We’re talking about research harking back to a study 
almost 70 years old. In England, schools have had to shoulder more and more of the 
responsibility for the gaps between groups of students – boys and the disadvantaged 
are perennially behind. Gap hysteria follows, a frenzied search for action to do something about the gap. In one meeting, a deputy head asked us to list our disadvantaged 
students from memory. We chanted the names to our partner like some hopeless 
prayer, all the more hopeless when we realised we’d forgotten several.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Schools try all manner of strange and desperate things to improve the outcomes 
for problem groups. Unthinkingly defining or perceiving a whole group of students 
What are the limits of what we can know about students? 197
to be a problem is a problem. Unfortunately, lots of schools have no idea what to 
do and a bizarre raft of unhelpful suggestions refuses to die. In the past – hopefully 
well into the past – teachers have been told to mark the books of their disadvantaged students first (or whoever’s doing badly that term). Consider two assumptions this suggestion makes about marking: number one, marking potentially can 
have a huge impact on learning and number two, quality of marking dramatically 
deteriorates the longer you spend doing it. Unless you’re playing some kind of 
marking drinking game neither assumption is true.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Other strategies for dealing with disadvantaged students in the classroom reach 
at something equally ephemeral: sit them at the front, ask them lots of questions, 
spend more time with them in lessons. Some of these might help but often they 
are suggested before we’ve assessed the particular challenges of this disadvantaged 
student, or whether the student is actually struggling to begin with.]]>
			</paragraph>
			<paragraph>
				<![CDATA[By paying lip-service to groups of students and gaps between them, teachers 
and schools can entrench biases instead of toppling them. The teachers of the 
‘growth spurters’ in the Pygmalion experiment didn’t do things differently with 
their students, at least not consciously. They believed different things about these 
students and this shaped their actions. Until we can understand and alter our 
beliefs about students, we need to make sure that we treat students equitably in 
the classroom.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For the rest of this chapter, we’ll examine three research-based areas5 where 
these biases can affect our teaching and what we can practically do about it. As 
has been mentioned already, our aim is to build knowledge to improve our mental 
models of classroom practice. Our three areas are:
1. Explanations.
2. Questions.
3. Feedback.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Our three areas are:
1. Explanations.
2. Questions.
3. Feedback.
For each area, we’ll look at ways we can diagnose potential biases and what we can 
do to improve. We’ll first diagnose whether this is an area to focus on for you before 
turning to how we can improve. As you work through the diagnosis activities, you 
might find these things are not a problem for you. If so, great – move on to the next 
section. Every child deserves equal treatment in our classrooms. Good intentions 
are not enough. We need to better understand our own classroom behaviour. Only 
then can we transform what we offer to all children.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Explanation
Recent research has shown that where teachers have low expectations of students, 
explanations of concepts are watered down or key ideas are ignored entirely.6
If, over time, explanations are weakened because we believe – implicitly or 
198 Knowledge of students
explicitly – that students can’t handle them, those students pass through a dogeared curriculum, hollowed out by low expectation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Diagnosis
How would we know if our explanations to students have been altered by our 
expectation of them? Unless you teach the same thing twice to two different 
groups, it’s difficult to tell if your explanations change for different students. You 
might just know that with a particular group – because of your perception of their 
ability or behaviour or something else – you aren’t giving them a full taste of the 
curriculum. Even then it can be hard to tell what changes about your explanation 
or why those changes take place. This is complicated further by the variety of 
explanations you give in a single lesson: from the front, to individuals, to groups 
or tables. Where you’re unsure whether this is a problem, the following strategies 
give you a way of checking.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Measure your explanations against clear criteria
Often, these criteria will be found in the curriculum you’re teaching from. It might 
be co-constructed across subject or phase teams. When introducing a new or significant concept, create a checklist or script for an explanation. A checklist is your 
way of ensuring your expertise and the expertise of those who’ve planned the 
curriculum makes its way into the classroom. You won’t always need it but don’t 
shirk it to begin with.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Record your explanations
If you have a checklist, you’ll want to know it’s working. You can do this yourself 
by recording an explanation and comparing. An observation works just as well as 
long as the observer knows what you’re trying to achieve. A recording won’t lie or 
misremember so, however awkward it might feel to listen back, you can check you 
cover what you intend to.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A problem with recording is that we’ll often capture from-the-front explanations, potentially missing the one-to-one interactions around the room. You don’t 
just explain to the whole class. So many of your interactions are you explaining 
and re-explaining content to children around the class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Improvement
Improving our explanations is something we should be trying to do anyway. With 
our knowledge of subject, with our knowledge of pedagogy and cognitive load, we 
fine tune, clarify and foreground helpful detail in our explanations. To think back 
What are the limits of what we can know about students? 199
to progressive problem solving, the first problem we face is managing the class 
whilst we explain simple concepts. Once mastered, we can move on to explaining 
more complex material or we can more expertly use a combination of explanation 
and visual aid.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plan together
Simply making explanations more complex it not our aim. Pitching an explanation 
at the right level is a vital part. Colleagues you work with have valuable knowledge 
of subject but also how students misunderstand the subject. Asking What are these 
children capable of understanding? is a difficult question for a new teacher (and 
one that takes us back to our knowledge of curriculum). Work with others – mentor, trainer, colleague – to seek to understand what should go into your explanation. At times, working with a teacher just ahead of you is more helpful as they will 
be able to point to recent mistakes and lessons learned.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Check they’re ready for the explanation
When we activate prior knowledge we make it more likely that students will 
learn the new content. Knowledge connects; understanding builds. Skills link 
together, cumulatively creating a chain of processes that make up competent activity. Assumptions encourage us into ineffective practices, where we believe they 
know it because we taught it. Before you explain, check. Check they remember and 
understand what the explanation will build upon.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hands-down questioning – Who is the barometer for knowing if the majority of 
the class will have got something? Ask them. You don’t need to ask the student 
who struggles and will need additional support. Give that afterwards. Ask the 
student who might get it but might not. Preferably, ask a few students like that. 
Hands-down questioning is a must for any classroom. You don’t find out what 
they know without asking them. This can be done sensitively and carefully, never 
being used to catch a student who wasn’t listening. But there is a problem with 
it: even at your most efficient, you can only ask a handful of students at a time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mini-whiteboards – Remember, you’re checking the students are ready for an 
explanation; this isn’t practice or extended independence. You might check 
vocabulary, answers to quick questions, solutions to problems but you could 
also ask for sentences or Completion Problems. The point of mini-whiteboards 
is not that you must study every single answer. You can’t. Mini-whiteboards 
force everyone to think and give you an opportunity to check a sample. Where 
students sit becomes important, with those you want to read necessarily sitting 
where you can see their board.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Quiz – Like a set of mini-whiteboards a quiz involves everyone. It just takes a 
bit longer. To get to the kind of information we need, we have to step out into 
the class and observe students answering the questions. Pre-empt the slow process of back and forth feedback by looking at the answers as they appear. Visit 
students you expect much of and students you think might struggle. Observe 
without judgement. An alternative to quizzing for a practical subject is a process 
check. Set up the shooting drill the class seemed to master last time and see if 
they’ve retained what you taught.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Assess the success of your explanations
It’s no good thinking, I’ve explained it so they must understand it. Immediate 
checks through task design help us to see whether students have got it. Returning 
to the content of the explanation a week or month later helps us to see whether 
they’ve retained it. Cultivate a set of strategies that can, as accurately as is possible, 
let you know what students have understood. The same strategies that helped you 
check students were ready for the explanation will help to check if they’ve understood it: hands-down questions, whiteboards, quizzes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Practise
If we want an explanation to be pitch-perfect every time, it’s worth practising it 
until it becomes automatic. Other steps have been taken: careful planning, collaboration with others, a prepped pre-explanation check. Practice ensures the 
explanation lands well. Say it out loud to your empty classroom, record and listen back on your phone or, perhaps preferably, share what you’re planning with a 
mentor or colleague. Don’t panic when it’s not perfect but consider how to make 
it clearer.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Questions
Teachers are likely to ask more and more complex questions of those of whom 
they have high expectations.7 This is unsurprising but troubling. If the students 
you expect great things for are the students who answer the bulk of the questions in your classroom, who benefits? Those we have lower expectations of won’t 
become more knowledgeable by osmosis. They need to be held accountable for 
their learning, asked to articulate it and expected to retrieve what they’ve learned 
regularly]]>
			</paragraph>
			<paragraph>
				<![CDATA[Diagnosis
Diagnosing your questioning habits is about looking at who you ask questions to 
and what kinds of questions you ask. Legitimate choices can be made about using 
certain questions to challenge whilst using others to check knowledge. Closed 
questions aren’t bad. Open questions aren’t good. Both are needed and we will 
vary them depending on need. The purpose of this section is not to make sure you 
ask everyone similar questions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teachers are likely to ask more questions of the students they expect to know 
the answers. This is unlikely to be a conscious decision: sometimes we go to the 
ones with hands up or go to the student we know will answer correctly. We can’t 
observe something, particularly one of our own behaviours, without changing it, 
perhaps subconsciously. If we decide to tally up who we ask questions to in a lesson and who gets to respond – either hands up or hands down – it’s likely we’ll 
start to change who we ask, no bad thing. The tally will poke holes in regular 
patterns of behaviour. A visitor to your lesson will spot these patterns too. A tally 
is just the start. Drill down into the process and a rich seam of self-knowledge 
emerges. Who is asked a follow up question? Who has to justify their ideas? Who is 
held to account when they say ‘I don’t know’ and who is let off the hook?
Whilst you might start by getting someone in to tell you this, their job and yours 
is to bring this hidden knowledge into the light. Self-awareness is the aim.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Improvement
A tally doesn’t tell you what kinds of questions you ask and to which students. 
Who is asked and how much are only the first questions in our reflective investigation. Teachers tend to ask more challenging questions to those students they expect 
more of, meaning certain students or types of students are in danger of being asked 
basic questions or none at all. Our questions communicate our expectations and 
students are likely to understand this better than us.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Now, there isn’t a good type of question. Teachers are unhelpfully funnelled 
towards ‘higher order’8 thinking and ‘higher order’ questions, largely because 
‘higher order’ sounds good. Lower order questions express what feels like a low 
expectation: to remember. Higher order questions demand students evaluate and 
synthesise and create. Reality, unfortunately, doesn’t work like this. Students need 
the remembering to do the evaluating. They need the lower to achieve the higher. 
Furthermore, certain What questions are more challenging than some How questions. It’s sometimes more difficult to explain than it is to create because it depends 
what you’re explaining and what you’re creating. Content should dictate questions. Questions, or types of questions, shouldn’t be decided upon before we’ve 
pinned down what it is we’re teaching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[My point, laboured thought it may be, is not that you have to ask a certain type 
of question to all students or even to the students you neglect to ask questions as 
202 Knowledge of students
revealed by a tally. Improving the questions you ask is not just about making them 
more difficult. Improving your questions is about asking the right people the right 
questions, an incredibly complex task you can spend a career perfecting.
Let’s turn to three ways we can improve on what we’ve found in our questions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plan your questions
If you know you don’t ask particular students, or if you know you don’t ask particular students a particular kind of question, plan to ask them. Write it in your 
planner. Scribble their name on a post-it. Or take a seating plan and tick names as 
you ask the questions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Improve pace by avoiding repetition
New teachers quickly stumble into a habit with questions that can linger for entire 
careers. A student answers a question; the teacher repeats and rephrases the answer. 
Sometimes we do this to praise what we’re hearing. Often, we do what Doug Lemov 
calls ‘rounding-up’: adding what is lacking to a student’s response in our summary 
of it. A student gives a partial answer. We complete it and praise them for giving it. 
They go away embedding incomplete or misconceived knowledge.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When a student answers a question, you have several options:
- Say Thank you and move on or ask someone else.
- Ask a follow up question: Why do you say that? How do you know that?
- Give specific praise: Your use of our subject terminology was really strong in that 
answer. You explained that concept really clearly.
Each of these options is dependent on the context and the question but each of 
them is better than repeating and rephrasing the answer.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Ask questions everyone answers
Mini-whiteboards are equitable because everyone answers every question. A powerful message is sent to the class: You all can, and you all must engage with this. 
Use them regularly and you stop seeing certain students as ‘difficult to ask’ or 
‘unlikely to know’. They both reveal who doesn’t yet know whilst expecting everyone to attempt.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Mini-whiteboards might not always be available (ask your school if you want to 
use them but don’t have any). Even without mini-whiteboards, you can and should 
plan questions that all students answer. These might be your hinge questions or a 
call and response involving everyone. Students can indicate an answer by holding 
up a number of fingers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Feedback
Teachers tend to be quicker to reprimand low expectation students. Levels of detail 
in feedback also contrast between high and low expectation students. When a 
teacher has high expectations of a student, that student is more likely to be on the 
receiving end of feedback tied to content and skill. Where belief in the student’s 
abilities is low, the student can expect minimal feedback on performance, with 
the attention focused on attitude and behaviour. This can be true even when a 
teacher is impressed with a low-expectation student. When we’re surprised by a 
student’s performance, there’s a danger we hold back further developmental feedback because we’re amazed they made it this far. They deserve some recognition… 
and then a break. In doing so, we put on hold the progress which is possible for 
them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Diagnosis
If you’ve recently marked some assessments or books, you’ve got an insight into 
the kinds of comments students in your class get. Having done this with my classes 
in the past, I noticed some patterns emerging. Some students would get insight 
and questions, paragraphs and partial editing. Others got, ‘WHY HAVE YOU LEFT 
THIS SPACE IN YOUR BOOK?’ and ‘Not enough completed’. The comments did 
nothing to help them and at times weren’t remotely concerned with what they 
were learning. Consider the experience of those two sets of children. The former 
get a rich diet of correction, suggestion and encouragement. The latter get the message they aren’t doing anything right.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Now, I do very little marking of books but the dangers are exactly the same. 
One student will have a totally different experience of your classroom to a different student sitting on the same row. You’re walking around the room. At one student, Rheanne, you stop, congratulate her on recent homework or test result and 
check their work. Every question has been answered correctly. For the next questions, you give Rheanne the challenge to answer using some key words from the 
glossary that haven’t been introduced yet. You keep walking and stop at another 
student, Ben. Ben finds your lessons challenging. He’s picking at the corner of 
his page. The title has been scruffily written. The first task has been attempted 
but the ones after that are left blank. Ben gets a gentle lecture about making an 
effort and asking if he’s stuck. You add some comments about presentation and 
move on.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As above, filming or recording your teaching might give you some insight into 
the differences in the way you talk to different students around the room. A supportive observer could create a record of this as well. The point is not so much to 
identify the students you only ever berate or chide. The point is to notice the differences in how you give feedback and what feedback is given. Of course, noticing 
is not enough. We have to do something about it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Improvement
Set them up for success
If a student or several students regularly get reprimanded for effort or presentation 
rather than on the content they are learning, what have you done to pre-empt it? 
We set students up for success when there is absolute clarity of expectation and 
certainty of challenge when these expectations are not met. We set them up for 
success when support is readily available in whatever form will ensure students 
are on task and learning in our lessons.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Make expectations clear by:
- Explaining where a task will be completed. Everyone will write on the sheet… 
This should be completed right underneath your title…
- Explain the task as clearly and concisely as possible. You will complete questions 1 to 5 and then close your book to show you’re finished. Write the paragraph we’ve just planned on the board. Complete your technical drawing.
- Explain the behaviour expectation for the task. This will be completed in silence. 
You can talk only to the person next to you.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Challenge students early by:
- Standing at the front and scanning the room. An observer once told me I 
was missing things around the room. As I got drawn into conversations and 
tried to help, even as I spoke to the whole class, I missed the fact students 
weren’t listening or had drifted off task. Now, to check I’ve got attention or 
everyone on a task, I try to look at every face in the room deliberately. It’s 
easy to wave your head around the room and believe students are on task. It’s 
harder to look at every face and fail to notice when students are off-task. You 
shouldn’t be surprised when a student isn’t completing a task. You should 
know already.
- Where students clearly aren’t engaging with the task, prompt either non-verbally – 
preferable at first – or verbally with a name. Point to the work, the task or the 
support available. The aim is that all students are working. We know this doesn’t 
guarantee learning but it makes it more likely. Widespread work also means 
you’ll have more to feedback on than a failure to engage with the task.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Offer support by:
- Signposting help given. Remember you have the example we created in your 
books/on the board. Use the sentence starters as you work through your plan. 
Talk quietly to the person next to you if you have a question.
What are the limits of what we can know about students? 205
- Remind them of these sources of help. Some people look like they’re stuck but I 
can’t hear conversations about the questions we’re stuck on. Some people have 
struggled on the third question but I’m not seeing students go back through their 
books to look at our notes on this topic.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What’s this got to do with feedback? Thinking about how you introduce a task can 
feel removed from feeding back. But setting it up right ensures feedback opportunities during the task. When you move around the room, you want to be able to give 
specific, learning-focused feedback. You can’t do this if students have, wilfully or 
otherwise, misunderstood the task or aren’t engaged with it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Multiply learning feedback
We can’t always escape telling Ben he needs to improve presentation or get on 
with things. What we must remember is that Ben also needs feedback on what 
he is learning. Stay with him and give two bits of feedback on what he has done 
already that are specifically and only about what he is learning. Or, particularly 
when the work is missing entirely, ask Ben questions to reveal what he does know. 
Give feedback specifically on the answers. The feedback should outweigh the 
reprimand.
At times, where students just don’t want to engage, the behaviour system must 
be used to communicate that disengagement or opting out are not acceptable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Blind marking
Ultimately, eliminating bias may require us to give feedback without knowing who 
the feedback is going to. Clearly, this is only possible for work we’re taking in. For 
exams, essays and assessments, blind marking gives you insight into your expectations. Many students will live up or down to these expectations. The student you 
thought might struggle does. The student who knew it all before you taught them 
gets full marks.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Blind marking forces you to confront these assumptions and expectations. It 
works both ways: pleasant surprises from those you thought would struggle and 
reality checks for those you thought had already mastered it. Get students to write 
names on the back of assessments or exit tickets when you take them in. Or ask for 
books to be handed in to you open on the page you’re looking at.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I expected nothing of one of the hardest GCSE classes I’d ever taught. I had been 
appointed Head of English and gave myself a class I knew would be difficult. There 
were only ten students. Half were impossible; the other half I just felt sorry for. I 
thought I could handle them but every lesson descended into chaos. A student 
took phone calls and texted from the back of the room. Others made crude jokes, 
watching eagerly for my reaction. They arrived late and left whenever they felt like 
it. To look at some students’ books, you’d think we’d not had any lessons all year. 
206 Knowledge of students
I gave so many lectures about how they would fail without work, without change. 
It’s not that they disagreed; they just shrugged it off.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unsurprisingly, on results day, those students who hadn’t worked all year did 
quite badly. At least one of them expressed regret at how he’d behaved. The rest 
weren’t bothered. But with the half of the class who turned up each lesson and 
quietly persevered, something strange had happened. They’d done quite well. Or, 
rather, they had done much better than I’d expected. And that’s when I realised 
my expectations of the class had been so incredibly low. I’d checked out when I 
realised every lesson would be a struggle. Rather than seeing the positive results 
for some as a success, I wonder what they could have achieved if I’d expected anything of them. One quiet, diligent student who’d passed – a minor miracle – came 
up to me. He said ‘thank you’ which made me feel terrible and then offered the 
cliché, ‘Hard work, sir. It pays off’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Often students will meet your expectations. Although it does happen, it’s rare to 
be totally surprised by what a student achieves. It’s also hard to change your mind 
about a student. It takes time. Our aim should be high expectations of all. Until we 
change those expectations, the least we can do is to ensure the same behaviour 
towards all. This is an ongoing battle to achieve parity of curriculum and treatment 
for all students in every lesson.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In summary
Knowledge of what students know and can do is essential for the adaptation of our 
teaching. We adapt so that our lessons meet students where they actually are, rather 
than where we assume they are. Checks for understanding force information out 
of students. Learning happens over time; we can’t guarantee students who demonstrate competence now will do so in the future. But students who don’t understand 
now are unlikely to magic understanding from somewhere just in time for our next 
lesson. Checks are important but so is our response. Free yourself from limiting a 
check to a specific timeframe in the lesson. Prepare to spend time embedding or 
re-teaching what students haven’t got.]]>
			</paragraph>
			<paragraph>
				<![CDATA[All this talk of knowledge of students comes with a caveat. We know in part. 
Sometimes we don’t know at all. We assume. Humans are biased by nature. I’ve not 
offered a way to get around that quirk in our humanity. Instead, I’ve offered some 
areas of your teaching that may be affected by bias and what we can do about it. 
We go upstream of a problem – perhaps watered down explanations – and make 
sure that doesn’t happen in our lesson for any student. When we do that, we start 
to communicate that we have high expectations for all even when we don’t.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Possible next steps
1. Diagnose potential problems. We aren’t really diagnosing our biases. We’re 
diagnosing the potential effects of those biases. Search out the knowledge 
of how your behaviour changes for different students or groups of students. 
Record a lesson. Ask an observer to look directly at your questions or the feedback you give to students. Accept the findings of these observations and consider the problems they throw up and the solutions at your disposal.
2. Put one safeguard in place. Mini-whiteboard tasks or questions to all communicate 
an expectation: everyone has to engage here. A tally of questions makes obvious 
that which we can ignore: I don’t ask questions of everyone. A scripted and practised explanation makes it more likely all students have access to the same content. 
Do one thing that helps you to avoid the potential problems of your biases.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Each section of this book ends with possible next steps. Let’s be honest, it’s not 
possible to implement all of those in one go. Even successfully implementing a 
handful will take time, effort and energy. One of the weird things about reading a 
teaching book is that the book doesn’t make you a better teacher. Reading a book 
can give you a growing confidence that you know your stuff, that you understand 
what others don’t, that you’re grappling with the big ideas in your profession. It’s 
not that those things aren’t true. They probably are. But if nothing changes, no 
improvement has been made. Books which feel transformative often aren’t because 
the feeling dissipates and leaves nothing tangible.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Teaching is a battle ranging between two sides. On one, your accumulated knowledge – your mental models of teaching and learning and subject and students. On 
the other, the challenges and problems of the classroom – students need to understand and behave, we need to elicit and evaluate thinking and learning. The battle 
exists for every teacher but how you approach it will be shaped by your personality 
and preferences. Mary Kennedy’s final persistent challenge of teaching is to address 
all the challenges of teaching whilst accommodating your ‘personal needs’.]]>
			</paragraph>
			<paragraph>
				<![CDATA[I don’t know you. I don’t know how you’ve arrived in teaching or what you’ve 
found difficult or straightforward. I don’t know the challenges or problems that 
you currently face. There is a gap, then, between the book you’re reading and the 
solutions you need, a gap that can only be filled by your thinking with the support 
of those around you.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this final chapter, we’ll look at some principles and strategies that can make 
sure that thinking takes place. These are:
1. Manage your time to make room for progressive problem solving.
2. Work with your coach to create mental models.
3. Develop a habit of identifying problems and searching for solutions.
4. But perfection is not possible.
17 What do I do now?
210
Everything listed here is attempting to marry the thinking we’ve done in the sections of this book with where you are right now.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Time management strategies are a bit of a con, particularly for teachers. You can’t 
manage time you don’t control, but we need to use the time we have to focus on 
those high-leverage activities that will help us to improve:
- Growing knowledge.
- Automating process.
- And out of these activities, creating mental models that help us to think, act and 
solve problems.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To manage your time, the following activities may help.
1. Make a list. Put everything on it.
Everything means everything. Lessons to plan. Resource making. Phone calls 
home. Prep for meetings. Replying to emails. Practice of classroom behaviours. 
This could be a real, physical list or tech-based equivalent. Mine is a Word 
document with everything I need to do on it.
Tasks on the To Do list should be as narrow as possible.
If you write:
- Plan Science lessons for new unit.
Your list won’t work. It isn’t specific enough.
What you really need is:
- Read up on the KS2 science curriculum.
- Read an email from Lauren.
- Talk to Becky about expectations about the curriculum.
- Adapt resources for lesson one from shared drive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Often you realise a task is too broad when you put in work towards that thing 
and realise you still can’t cross it off your list. Crucially, the next steps that you – 
or you and a mentor or coach – have decided will help you go onto the list. Or 
they get broken down further and go onto the list.]]>
			</paragraph>
			<paragraph>
				<![CDATA[You can then prioritise tasks by importance and by deadline. If the science 
planning needs to be done by the end of the week, the preparation tasks you’ve 
listed are going to have to fit into the time you have on Monday and Tuesday. 
Larger, more intellectually challenging tasks work better in extended bits of 
time, your PPA or a day you know you’re staying a bit later, whereas quick 
email tasks can fit in a short gap between lessons or the half-hour you have 
before a class arrives. With this in mind, it’s worth reviewing your list at the 
start or end of each week. Slot those big tasks in the time you have for them 
and prioritise developmental, learning-focused activity over admin and email.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A list can give the illusion of productivity as you diligently work your way 
through it. The substance of your list needs to be right to enable the development and application of knowledge. If your list is all email and printing – 
things you have to do but aren’t going to help you improve – then it will likely 
fix your attention on altogether the wrong things.]]>
			</paragraph>
			<paragraph>
				<![CDATA[2. Ask How?
In one of my early teaching placements, I was unlucky enough to be present 
the afternoon of a circus skills workshop. Circus performers came in, wowed 
the children and taught them how to spin plates and juggle. Fire-breathing 
and sword-swallowing were sadly missing from their repertoire. I spent the 
day half watching and half frantically planning the next week’s lessons. I was 
pulled from this productive reverie by the unexpected request for teachers to 
come to the front: ‘Now, let’s see what your teachers have learned this afternoon’. I may have learned to better attend to the expectations of me when 
visitors run a workshop but I hadn’t learned to spin plates. Students watched 
pityingly as I repeatedly picked up and dropped a plastic plate after trying and 
failing to balance it on a long plastic stick.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Plate-spinning is the go-to metaphor to describe what you have to do as a 
new teacher. Or maybe it’s juggling. Anyway, you’re going to have to juggle a 
lot of plates. Lots of ideas from books and suggestions from colleagues sound 
nice but feel impractical. Where are they going to fit? A crucial conversation 
you have with those supporting you is about how you balance competing ‘priorities’. If you have decided scripting and practising your explanation is your 
next step but can’t figure out where it will fit, ask how. If you want to observe 
212 Knowledge of students
some teachers or book in some co-planning but can’t find a time, ask for support to make it happen]]>
			</paragraph>
			<paragraph>
				<![CDATA[Work with your coach or mentor
A coach’s job can feel like it is to critique your practice or tell you what to do, at 
least early on. These aren’t really a coach’s job. When we drill down into what a 
coach is doing, their role is to help you to create a set of mental models, rich in 
knowledge. Just shifting your view of those supporting you in this way can repurpose the relationship into something all the more powerfu]]>
			</paragraph>
			<paragraph>
				<![CDATA[At times, you might be working with your coach on behaviour management, 
scripting and rehearsing phrases to use in the classroom. Or studying curriculum 
materials together to plan for the coming term. Specific actions are decided upon 
in these meetings: narrate the countdown, define relevant prior knowledge from 
previous lessons, practise the language of the behaviour policy, script and rehearse 
your initial explanation. Your job from these actions is to develop knowledge and 
automate process in order to master those things. But those things aren’t ends in 
themselves.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To understand where current actions fit into a broader picture or your current 
model, ask questions as you talk through your next steps:
- How does what we’re working on now build on what I’ve done before?
- What alternatives are we ignoring or leaving to focus on this?
- What problems will automating this/knowing this solve?
- What will success here enable me to do next?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Our aim is not the narrow view of an individual technique, another slice of granular development. Our aim is to understand how this task, here and now, fits into 
the broader mental model we’re building, how this focus enables another step 
towards expertise to be taken.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Identify problems and solutions
Working with a coach early in your career should help you to identify where problems exist and possible ways to approach them. The types of knowledge we’ve 
looked at, the lenses we’ve been developing, should also help you to look at yourself or your classroom and consider the challenges you face and solutions available.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As you consider how to progressively problem solve, note down your thoughts 
in a notebook or document. You’ll do a better job of capturing your thoughts and 
will make it easier to see how you’ve done. This process of capture and identification of problems and solutions is a process of using and expanding knowledge. 
You begin with knowledge of your current practice. You identify gaps or weaknesses and seek out solutions. Your knowledge of how different solutions work in 
practice helps you to identify future problems and solutions]]>
			</paragraph>
			<paragraph>
				<![CDATA[1. Decide the area you want to work in.
Write down every problem on your mind, every problem you can think of that 
you’re facing currently. Think about every aspect of your current practice – 
planning, subject knowledge, teaching, assignments, behaviour management, 
other jobs.
Your list might look something like this:
- 7x are really slow to settle at the start of lessons.
- Jo is really slow to do what I ask/influencing others.
- Planning for 8 feels hard work/takes ages.
- I don’t know enough about Anglo Saxons.
- My explanations all feel like a rambling mess, even the short ones.
- End of lesson routines across classes don’t feel tight/controlled.
- Not sure where to start with reading for the next assignment.
- I forget to give clear expectations for task (time/noise level) lots.
- Not sure how to do whole class feedback for Year 10 – so many different 
errors in their books.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Notice we’re not trying to find solutions yet and we’ve not decided on an area 
to work on. Please don’t stop here; that would leave you incredibly frustrated.
How you decide what problem to prioritise from that list is up to you. 
Decisions about priorities should be driven by what is most time-sensitive but 
also what you can work on to save yourself time in the future. In our example 
above, the teacher might choose to focus on planning for Year 8 because that is 
draining their capacity to deal with anything else – it just takes too much time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[2. Define the problem specifically.
Knowing there’s a problem in the huge area of Year 8 planning doesn’t help 
much. Break it down further:
- Resources on the shared drive take a lot of work to adapt into something I can 
use.
- I don’t have good background knowledge of X.
- I’m overthinking/spending too much time on my planning because behaviour 
can be difficult, particularly at the start of lessons.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Narrowing it down, the teacher decides they need to make planning more 
efficient by focusing on background knowledge. It seems to be taking too long 
because so much of the content needs to be revisited by the teacher before they 
feel happy to teach it.
Alternatively, behaviour is settled on as the problem, as it often is, but to 
narrow that down you need to break it into its component parts. There are difficult students in the class but they aren’t currently the main issue. The class 
come into the room boisterously making the starts of lessons a constant battle. 
The problem is the entry routine.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At times you will proactively search out specificity from your problems; at 
others you might wait for the narrowest version of a problem to present itself 
to you as you plan and teach. Whatever you’re doing, your job is to be on the 
lookout for what the actual problem is, not a broad category you’re unhappy 
with.]]>
			</paragraph>
			<paragraph>
				<![CDATA[3. Identify possible solutions.
To begin with solutions might mainly come from colleagues, from observation 
and conversation. Ask as many people as you can, How would you deal with X?
Remember experts commonly suffer from a blind spot where they struggle to 
articulate embedded knowledge and automated process. Some answers might 
frustrate you. Other new teachers, a year or two ahead, often provide a more 
tangible solution than those with experience. The path you’re taking is fresher 
in their minds.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Alternative questions to ask these colleagues are, Why do you think I’m struggling with X? What could make X easier for me? Note down the solutions you 
come across even if they feel counter-intuitive or unworkable to you. If there 
are several possible ways of dealing with a problem, order them in terms of 
how easy they are to implement right now.]]>
			</paragraph>
			<paragraph>
				<![CDATA[4. Test the solutions.
You’ve selected the solution that seems fastest and easiest to implement. If 
time spent planning was the problem, perhaps background reading on the 
topic could help to make quicker decisions about what to leave in (and out) of 
lesson plans. Your mentor suggests some reading and diligently you go away 
and do it. Background reading helps you feel confident about the topic but 
planning still takes an age. Don’t panic. Each solution that fails, or only partially succeeds, provides useful knowledge of what actually tackles the problems we’re facing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[What’s next? You decide to look at how other teachers use the shared 
resources. You arrange to meet with a colleague and ask them forensically 
about how these resources translate into their lessons. You watch this happen 
in their classroom. Slowly, the thinking behind the resources begins to make 
more sense to you. You trial their approach with a couple of adjustments of 
your own. Planning time reduces dramatically.]]>
			</paragraph>
			<paragraph>
				<![CDATA[5. Identify what’s next.
Planning time reduces. Great. You have a little more time and feel less stressed 
about those lessons. Progressive problem solving isn’t about constantly trying to solve everything at once or about filling up every minute available to 
you. Instead, it’s a recognition that there’s another level to what you’re facing. 
If planning has become easier, you could focus on tightening those rambling 
explanations. Alternatively, you might feel those can wait until you’ve sorted 
the classroom routines for this class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[6. Leave some problems for later.
As you develop a list of problems you want to solve, a parallel list of problems you are leaving until later is an important reminder that you can’t do 
everything at once. Talk with your mentor about what can be left and what 
should be a priority.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perfection is not possible
I’ve never been a perfectionist. The problem with problems and lists, with not 
settling for where you are, is that these things can drive you to feel that nothing is 
ever good enough. Problems don’t exist in your classroom because you’re a terrible 
teacher. Problems exist in every complex environment. The job of the teacher, then, 
is to cultivate a perspective of the problem solver whilst avoiding a negative fixation on everything that makes you uncomfortable or unhappy about your practice]]>
			</paragraph>
			<paragraph>
				<![CDATA[Although it’s a cliché, it’s true that teaching is a marathon not a sprint. You can’t 
rush into the classroom and solve every problem you find there. The list you make to 
manage your time can and should include those next steps that are going to help you 
to improve. As you review that list each week, keep checking that you’re still focused 
on finding solutions for problems rather than just clearing a bunch of tasks from a 
list. Don’t settle for doing the jobs. Focus on improvement that will make a difference 
for your students. Manage your frustration with the slow rate of progress at times by 
keeping track of how you have improved and the problems you have solved.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perfectionist teachers are often very good at what they do. They may teach 
thoughtful, engaging lessons. Perfectionists probably also manage behaviour well. 
They certainly know their students. Eager for the next thing, they devour reading 
on subject and pedagogy. Workdays for perfectionists extend beyond what might 
be considered reasonable or healthy. Burnout is a danger but so is a loss of enjoyment or lingering feelings of disappointment. Any road to expertise needs to be 
lined with stops to rest and refuel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It’s all very well saying that perfectionism can take us in the wrong direction but 
what can we actually do about it?
1. Stop.
Teaching is a job that we can never complete. Every aspect of it can be refined 
further. Knowing when we’re ready – when a lesson is ready to teach, or when 
216 Knowledge of students
you’re ready to give that explanation, or when we’re ready to adapt in-lesson 
to student responses – knowing that with certainty is not always possible. We 
can feel uncomfortable with finishing a task when we know possible improvements are left. But something has to be left for another day, for another lesson, 
for another assignment, possibly for good.
Learning to say to yourself, I could have probably done that better but I’ve 
done what I need to is not easy but it is essential. Initially, it can be difficult to know if you’re ready, and it’s difficult to let go of preparation activities 
that make you feel safe and confident. And it is vital to prepare properly so, 
initially, a mentor, coach or colleague might have to work with you to discuss whether you’re ready to stop and move on. Ask, Am I done with this? Do 
you think I can stop there? Is this good enough for now? Whilst others might 
help you consider these questions initially, their answers should push you on 
towards independence.]]>
			</paragraph>
			<paragraph>
				<![CDATA[2. Switch off.
I am quite strange in that I will happily read about teaching or my subject in my 
spare time or on holiday. You don’t need to be like this to be a good teacher. In fact, 
if long-term and sustained improvement is the goal, we need to be able to switch 
off at least for a time. Constant fixation on the next thing, the next problem, what 
we need to prep, what we need to get done – all of this is a recipe for burnout.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When I first started teaching, I didn’t realise how stressed I was. I emptied Sunday 
of activity, trying – unconsciously, I think – to slow time before the start of another 
week. I didn’t want to go out. I didn’t really want to see or speak to anyone. Monday 
was this all-encompassing thing. Don’t worry, that fear of Monday wasn’t permanent. I’ve realised, over time, that I am happier when I have something, however 
insignificant, to do or someone to see. Distraction is the wrong way to see these 
activities that take you away from the worries and the problems and the unknown 
and the absence of solutions. It isn’t distraction; it’s just life. Don’t give up the stuff 
of life just because you have to go to work on Monday. This may not be advice you 
need to hear but don’t wait to feel settled before you switch off and do the activities 
you love and see the people you love. The problems will wait until Monday.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If the final challenge of teaching is accommodating your personal needs, we 
need to acknowledge, as Mary Kennedy does, that there are a ‘variety of ways these 
challenges can be met’. Of greatest importance is ‘finding strategies’ to solve them 
which ‘are consistent with [your] personal needs’.2 How you solve the problems in 
front of you is up to you.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When you’re writing a book about what teachers should know, everyone has something they think should be included. Generally, these titbits are about not microwaving fish in the staffroom as well as basic photocopier maintenance. Whilst not 
unimportant topics, these set the bar a little low for new teachers. You’re embarking 
on a challenging, rewarding intellectual journey. Your knowledge will span from 
the debates within your subject area or phase to the minutiae of your students. 
Your understanding will reach back into the recesses of study as well, latching on 
to your rapid accumulation of current experience.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It would, however, be flippant or dishonest to claim this is a definitive guide 
of all new teachers should know. Specific areas of teaching have been dealt with 
briefly. Large policy areas have been ignored. This is not because they are unimportant. What to keep in and leave out has kept me up at night as I’ve chaired internal 
debates about the most vital knowledge for new teachers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When such debates between me and myself reached unsatisfactory conclusions, 
I was comforted by a rather obvious realisation. It’s not possible for a book to give
you or tell you everything you need to know. Each part of this book, and each area 
of knowledge, is setting you off on a path that I can’t join you on. These paths start 
out as little more than dirt tracks. Initial curriculum knowledge is a confusing 
wilderness that becomes a road that becomes a highway. Where the journey was 
arduous to begin with, increasing knowledge makes it effortless. Initial steps in 
behaviour management are steps in a city we don’t recognise, one where we don’t 
speak the language. Options overwhelm us. Knowledge gives direction until we 
can navigate with ease.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Any work we do to develop this knowledge is work to develop our understanding of the landscape of teaching. My hope is that, in reading this book, you have 
a rough guide, an initial blueprint or roadmap to overcome the first difficulties in 
navigation in your development as a new teacher. And that, as a new teacher, you 
recognise that you aren’t powerless to find your own way because your development is a personal journey. Others will help, to be sure, but they can’t do it for you. 
They can’t turn your subject knowledge into pedagogical content knowledge for 
218
you. They can’t automate processes. They can’t get to know your students. But they 
will help. Their advice shouldn’t be ignored.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The threads of our knowledge, if you’ll forgive one more metaphor, start frayed 
and fragile but intertwine and, with time, become a single thick and powerful rope. 
Initially, like the parts of this book, the knowledge feels disconnected. With time, 
we see the connections between subject and pedagogy and student until the point 
where we can’t unravel them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Too often, teachers have to look back at a process they didn’t quite understand 
and realise that progress has been made. I have got better at this. Development 
feels implicit and unconscious. My aim has been to clear the fog surrounding this 
development even just a little bit. I hope that, as you set out on your career-long 
journey towards expertise, your awareness of the route is a little clearer. I hope 
that, as it has been for me, teaching is an intellectually rewarding pursuit, enriched 
by knowledge.]]>
			</paragraph>
		</content>
	</book>
	<book name="Bioinformatics and Human Genomics Research">
		<content>
			<paragraph>
				<![CDATA[After performing the genome assembly with adaptor trimming and filtering for good quality 
sequences, the next step is assigning biological information to the raw sequence data called 
genome annotation such as function, pathway, location, size, molecular weight and other attributes. 
This includes providing annotation to protein-coding genes as well as non-coding genes. This 
allows us to identify ORFs, introns, exons, repetitive regions, regulatory elements, etc. 
Development of genome annotation strategies started in the year 1990, with the launch 
of the Human Genome Project with an aim to sequence and decipher all 3 billion letters of a 
human genome. The first draft of the human genome was published in 2001 with computational 
annotation for 30,000 to 40,000 protein-coding genes from 22 pairs of autosomes and the X 
and Y sex chromosomes in a genome of 2.9 billion bases. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Since then, there has been significant progress in the efficiency of genome annotation 
pipelines, with the development of better bioinformatics algorithms, tools and availability of 
reference genomes. The growth of NCBI sequence data for eukaryotic organisms clearly shows 
that annotation and also re-annotation of organisms is exponentially increasing with time. 
Despite the availability of more efficient and faster automated annotation pipelines, manual 
annotation is still performed to improve the quality of automatic annotations. 
The annotation pipelines and its quality are also dependent on the sequencing technologies 
used. An overview of new sequencing technologies is presented in the following section (Fig. 1).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Genome sequencing and genomics research are developing gradually after the report of the first 
human genome sequence in 2001 (International Human Genome Sequencing Consortium, 2001, 
2004). Small genomes like bacteria and fungi can be easily assembled and annotated with fewer 
amounts of resources within a limited time whereas performing such tasks for higher genomes 
require more computation resources and longer periods. Assembly and annotation of eukaryotic 
genomes take few months up to years where no reference genomes can be used (Dominguez 
Del Angel et al., 2018). However, with the progress in bioinformatics tools and emerging new 
sequencing techniques, it is now more expedient and convenient to assemble and annotate large 
eukaryotic genomes (Jansen et al., 2017). Nonetheless, obtaining high-quality genome assembly 
and annotation is still a major challenge. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Since 2001, NGS technology has been developed which allows us to sequence genome, 
exomes, proteomes and gene panels much faster with less error rate in a cost-effective way 
(Goodwin, et al., 2016). Several sequencing platforms can be classified into three main types 
based on the coverage and genome sequencing technique. The first type, e.g., Illumina and Ion 
Torrent platforms, are based on finding the clonally amplified target and Pac Bio and Oxford 
Nanopore technology that uses single-molecule detection per reaction. The second type consists 
of sequencing by synthesis and direct measurement of DNA which is used by Illumina, Ion 
Torrent, and Pacific Biosciences platforms. The third type is based on either optical detection, 
for example, Illumina and Pacific Biosciences platforms, or non-optical detection for base read 
call. Currently, a hybrid genome sequencing platform is also available which relies on taking 
advantage of the different types of sequencing platforms (Koren et al., 2012).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Current sequencing methods to perform whole-genome sequencing are categorized mainly 
into two categories based on the length of the nucleotide sequence produced or sequence ‘read 
with their advantages and disadvantages, shown in Table 1. Choosing the appropriate technology 
according to the query plays a crucial role in downstream analysis. 
Table 1 Currently used sequencing strategies with their advantages and disadvantages 
Parameters Illumina-MiSeq 
NextSeq 
500 
HiSeq 
2500 PacBioRSII GSFLX]]>
			</paragraph>
			<paragraph>
				<![CDATA[Genome assembly refers to the process of arranging the raw reads into the correct order as 
they are naturally packed. Accurate assembly of the genome is the key step before beginning 
successful annotation. Genome assembly can be performed in two ways: reference-based 
assembly, or de novo assembly. For reference-based assembly, a known pre-sequenced and the 
assembled genome is used for mapping with a newly sequenced genome (Fig. 2). Bowtie and 
bwa are widely used tools for mapping and genome assembly (Langmead and Salzberg, 2012).]]>
			</paragraph>
			<paragraph>
				<![CDATA[For example: Bowtie is an ultra-fast short-read aligner too, which helps in mapping and 
assembly. It aligns a large number of short sequencing reads to a reference sequence. For the 
longer read with the above, Bowtie 2 is used, which is generally faster, more sensitive, and uses 
less memory than Bowtie. There are various popular assemblers developed for the assembly of 
small and large genomes from the sequencing data from various platforms, as shown in Table 2. 
Table 2 Widely used genome assemblers (El-Metwally et al., 2013a; Schlebusch and Illing, 2012) 
Software Algorithm Operating 
systems Sequencer Genome 
specificity Input]]>
			</paragraph>
			<paragraph>
				<![CDATA[Accurate assembly with good assembly statistics is the initial step that should be considered 
before annotation of genomes. N50 is a measurement parameter often used to evaluate the 
genome assembly characteristics (Dominguez Del Angel et al., 2018). 
Various bioinformatics tools that are available to evaluate the quality of genome assemblies 
such as Quast (Gurevich et al., 2013), Reapr (Hunt et al., 2013), FRCBam (Vezzi et al., 2012) 
and BUSCO (Simão et al., 2015) are very useful.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Repetitive DNA consists of hundreds to thousands of repeated sequence motifs and estimated 
to contribute around ~30% of the total genome. Repetitive regions are mainly classified into 
microsatellites, i.e., short tandem repeats (STRs) or simple sequence repeats (SSRs), minisatellites 
and satellite DNAs (Richard et al., 2008) according to their size. Most of them are found to 
be located in centromeres, telomeres, and dispersed throughout the genome (Pelley, 2012). 
Eukaryotic genomes such as human are highly repetitive, constituted of around 47% of repeats 
throughout the genome (International Human Genome Sequencing Consortium, 2001). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Complete fragments of repeats are found occasionally because the borders of repeats are 
not well defined and often inserted within other repeats, also found to be poorly conserved 
across genomes. This leads to complication in genome assembly and annotation. For accurate 
annotation, repeats need to be correctly identified and annotated. Several tools till now have been 
developed to do such analysis which is based on two methods, i.e., homology-based (Buisine 
et al., 2008; Han and Wessler, 2010) and de novo-based repeats identification approach (Flynn 
et al., 2020). Repeat identification, its masking and annotation thus plays a significant role 
in accurate genome annotation projects. Some of the recent tools are RepeatModeler2 (Flynn 
et al., 2020), RepeatAnalyzer (Catanese et al., 2016), Red (Girgis, 2015) and TAREAN (Novák 
et al., 2017).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Computational gene prediction is the essential stage for the functional annotation of genes and 
genomes. Through decades significant progress has been made to predict prokaryotic genes. 
But eukaryotic gene prediction is still more challenging due to the presence of coding as well 
as non-coding regions (Xiong, 2006). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Discovery of gene includes identification of ORFs, introns and exons in the case of eukaryotic 
genomes. In prokaryotes, DNA is transcribed into mRNA by the process of transcription as 
translated into proteins known as translation without being modified. Whereas, in eukaryotic 
organisms, the transcription process involves the removal of introns by the process of splicing 
followed by other necessary modifications. This complexity in the eukaryotic genomes cause 
gene prediction much more difficult as compared to prokaryotic genomes (Wang et al., 2004). 
Gene prediction can be broadly characterized into homology-based and ab initio-based gene 
prediction (Fig. 2). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Sequence similarity or homology-based approach is where the given genome is used to discover 
similarity in gene sequences between ESTs (expressed sequence tags), proteins, and spliced 
variants. This approach is based on the fact that coding regions are more evolutionarily conserved 
than non-coding regions. After similarity-based EST and protein identification, the next step is to infer the function of the region. EST-based similarity search led to the identification of 
only small portions of gene sequence which often led to difficulties in complete gene prediction 
for the given region (Wang et al., 2004). NCBI, Ensembl and Celera have recently added EST 
sequences for exon and transcript predictions. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are various similarity-based gene prediction tools that have been developed, BLAST 
is a widely-used tool for homology-based gene finders. It performs a similarity search by first 
cutting down the input sequence into a series of DNA or protein sequences and then searches 
against a local or NCBI database. Local and global alignment of BLAST are the two main 
algorithms used for gene prediction. genBlast program, which is based on local alignments, has 
two programs, viz. genBlastA and genBlastG (She et al., 2009; Medema and Breitling, 2013). 
genBlastA performs local alignments by BLAST and WU-BLAST and identifies groups of 
high-scoring segment pairs (HSPs). However, genBlastG is a fast homology-based gene finding 
tool that uses the output from geneBlastA as the input to define gene models. Homology-based 
prediction is a fast and accurate way of identifying gene models. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(i) Ab initio Methods 
Ab initio gene predictions do not rely on external evidences such as EST and protein alignments. 
They extract gene information by generating mathematical models and identifying their intron-exon pattern. This method can help in the identification of gene regions that are located in 
boundaries which include promoters and start and stop codons. But only relying on the ab initio 
method for gene prediction generates more false positives and cannot predict splice variants 
and 5¢ – 3¢ UTRs. To improve the accuracy of ab initio method tools like TwinScan (Korf 
et al., 2001), FGENESH (Solovyev et al., 2006), Augustus (Nachtweide and Stanke, 2019), GAZE 
and SNAP use external evidences (Korf, 2004).]]>
			</paragraph>
			<paragraph>
				<![CDATA[To better predict the genes with stronger evidence RNA-seq data can be used. RNA-seq data 
plays a significant role in improvising the accuracy of gene annotations, as this generates 
evidence for the presence of exons, splice sites and alternative splicing events. This can be 
performed in two ways: first by assembling the reads by de novo assemblers such as ABySS, 
SOAPdenovo and Trinity and then align it to the reference genome using TopHat, GSNAP 
followed by generation of transcripts using Cufflinks (Yandell and Ence, 2012). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Genome annotation is typically divided into two types: structural annotation and functional 
annotations. Structural annotation describes the sequence features of genes such as CDS, ORFs, 
TSS, exons and introns whereas functional annotation describes as attaching functional information 
to the genes such as domains, pathways and others which is described in the forthcoming sections. 
Genes are the heritable material transferred from parents to offspring. Genes are further 
distinguished into coding and non-coding genes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are only ~1.5% of the genes that code for proteins while the remaining 98.5% constitutes 
the non-coding genes for the human genome and was previously referred to as junk DNA with 
no biological role (Lander, 2011). The amount of coding and non-coding DNA varies significantly 
across species. As mentioned, non-coding genes were earlier referred to as junk DNA with no biological role but now they are reported to have a significant role in regulating gene expression 
(Rinn and Chang, 2012). 
ncRNAs are broadly classified into two groups that include small RNAs (sRNAs) and 
lncRNAs. Further, sRNAs consist of microRNA (miRNAs), Pi-interacting RNAs (piRNAs), short 
interfering RNAs (siRNAs), small nucleolar RNAs (snoRNAs) and other short RNAs (sRNAs) 
(Gomes et al., 2013). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Annotation techniques for non-coding RNA (ncRNA) are less efficient as techniques for 
protein-coding genes. But with the advancement in technology and software, several programs 
have been developed for the identification and annotation of non-coding RNAs. Recently, various 
studies have shown the process of how miRNAs interact with their targets, i.e., mRNAs, long 
non-coding RNAs (lncRNAs), pseudogenes and circular RNAs (circRNAs) (Tay et al., 2014). 
Among the other classes of sRNAs, microRNAs (or miRNAs) have been well studied that are 
known to regulate essential biological processes such as gene expression regulation and gene 
silencing in animals as well as plants. The biogenesis of miRNA in animals and plants is 
different and involve different types of proteins (Chen et al., 2018). There are a variety of tools 
and databases that have been developed specifically for plant and animal miRNAs and are used 
for the identification of miRNAs, their targets and cleavage sites (Table 3). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Genes that code for proteins are called coding genes, also known as CDS. With the accumulation 
of protein sequences from proteomics and genomics data, the functional annotation of protein 
sequences with high accuracy is a daunting task. Functional annotation includes identification 
of domains, pathways, gene ontology analysis, protein-protein interaction, structural analysis 
and domain identification. Till now there are many tools and bioinformatics databases that have 
been developed for the functional analysis of proteins as shown in Fig. 3. Some of them are 
discussed below in Table 4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The basic function of a protein can only be extracted from its primary sequence. The correct 
protein sequence is the prior and foremost thing that is required for the functional annotation 
of proteins. Many protein sequences are incorrect due to errors in sequencing and/or false ORF 
prediction. Incorrect protein sequence leads to false annotation like domains, sites of interactions, 
pathways and other downstream analysis (Bridge et al., 2008). There are a large number of 
databases that are available for protein sequence retrieval. These databases mainly consist of two 
types of information: protein sequences repositories and protein sequence annotation databases. 
Protein sequence repositories contain protein sequence information and also provide sequences 
that are newly sequenced but with no annotations of sequences. These databases also have a 
large number of redundant sequences with different record id’s. There are other freely available 
repositories of protein sequences and annotation (discussed below).]]>
			</paragraph>
			<paragraph>
				<![CDATA[(i) UniProtKB 
UniProt Knowledgebase (UniProtKB) is a manually curated protein database divided into two 
sections: Swiss-Prot, where each entry is reviewed and manually annotated, and TrEMBL, which 
consists of unreviewed and automatically annotated entries, as shown in Fig. 4. UniProt is the 
central open access repository of protein sequences with its annotation. 
The UniProt Consortium was formed as a collaboration between the European Bioinformatics 
Institute (EBI), the Protein Information Resource (PIR) and the Swiss Institute of Bioinformatics 
(SIB). The main aim behind UniProt’s development was to provide a high-quality functional 
annotation of proteins which are curated and reviewed manually. Every three weeks UniProt is 
updated with new annotations and entries (http://www.uniprot.org). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(ii) SwissProt 
SwissProt was established by the Department of Medical Biochemistry at the University of 
Geneva in collaboration with the European Molecular Biology Laboratory (EMBL). Since 1987, 
it is maintained by the Swiss Institute of bioinformatics (SIB) and the European Bioinformatics 
Institute (EBI). SWISS-PROT is a repository of protein sequences with their complete annotations. 
SWISS-PROT consists of protein sequence entries in its format. 
With every sequence entry, three main data are available, i.e., the sequence information, 
bibliographical and the taxonomic data.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Also, annotation with every protein sequence is present, which includes post-translational 
modifications, variant information, protein domain, protein structure, functions of protein and 
associated disease with protein are available. This database has protein annotations with less 
redundancy and is also integrated with other useful sequence-related databases (nucleic acid 
sequences, protein sequences and protein tertiary structures) that make SWISS-PROT highly 
useful and unique as compared to other protein sequence database. Since 2002, this database 
is maintained by the UniProt consortium which can be accessed using the UniProt website. 
Currently, there are 562,253 protein sequence entries available at SWISS-PROT. Among them, 
there are 20,365 entries for humans, 17, 038 for mouse, 15, 952 for A. thaliana, 8,094 for rat 
and 6721 for S. cerevisiae available in the current version of the database (Release 2020_2). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(iii) TREMBL (EMBL) 
TREMBL, (Translated EMBL) is a very large protein database which is a supplement 
of SwissProt that consists of computer translations of EMBL nucleotide sequence entries. 
Translation of sequences is not always correct, therefore the proteins predicted in the TrEMBL 
database are poorly annotated. In the year 1999 Release 11 of TrEMBL was developed to 
translate all 3,79,000 CDSs in the EMBL Nucleotide Sequence Database release 58. Amongst 
these, around 119 000 of CDS were reported in SWISS-PROT, and therefore, removed from 
TrEMBL. To remove redundancy the remaining sequences were merged automatically with the 
new method. There are 4, 180, 690, 447 of TrEMBL entries in the current release, i.e., 2020_2 
of UniProtKB]]>
			</paragraph>
			<paragraph>
				<![CDATA[(iv) RefSeq 
NCBI Reference Sequence (RefSeq) database is another popular and freely accessible protein 
sequence and annotation. This database consists of a large number of curated, non-redundant 
and annotated sequences of DNA, RNA, and protein. 
Refseq includes species from taxonomically different groups which include viruses, 
archaea, bacteria, and eukaryotes. Every entry in Refseq was retrieved and dependent on 
the sequence submitted to the International Nucleotide Sequence Database Collaboration 
(INSDC). Refseq entries can be retrieved by searching other interlinked NCBI resources such 
as PubMed, Nucleotide, Protein, Gene, and Map Viewer. RefSeq was developed to provide high 
standard functional annotation consisting of locations of SNPs found in medical records with 
no redundancy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A domain can be defined as a conserved portion in protein sequence which can form a semi-independent 3D structure in proteins. 
They play a role in specific protein functions genetically 
evolving and transfer them within different organisms. Several protein families are known to 
have arisen from a common ancestor by attaining a different combination of domains (George 
and Heringa, 2002). The identification of these conserved domains is crucial to understand the 
function of proteins. Popular methods like the Hidden Markov Model (HMM) and PSI-BLAST 
are widely used to detect domains in a protein sequence. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(i) Pfam 
Pfam is a database comprising curated protein families, defined by conserved Hidden Markov 
Model (HMM) profiles. The database can be accessed via servers in the UK (http://pfam.sanger. 
ac.uk/) and the USA (http://pfam.janelia.org/). 
HMM is defined as a class of probabilistic models that are used to predict homology. Profiles 
can be generated by aligning a predefined set of defined family-representative sequences. In the 
Pfam database the profile HMM is searched against a large number of sequences obtained from 
UniProt Knowledgebase (UniProtKB) to identify all instances of the protein family. These HMM 
profiles are searched using the HMMER software. There is a predefined threshold that is set for 
each family to remove false positives called a ‘gathering threshold’. Sequences that are above 
these thresholds are aligned to the defined HMM profiles to generate the full alignment. The 
entries which are curated by Pfam called Pfam-A entries. The main aim of the Pfam database is 
to include the maximum number of protein sequences representing the fewest number of models.]]>
			</paragraph>
			<paragraph>
				<![CDATA[ include the maximum number of protein sequences representing the fewest number of models. 
Each Pfam entry is tagged with one of the types—family, domain, motif, repeat, coiled-coil, 
or disordered—representing the functional unit class. In the current release (release 32) there 
are 17,929 protein families. With comparison to the structural classification database, namely, 
the Evolutionary Classification of Protein Domains (ECOD) by Pfam community, there is an 
addition of 825 new families. Moreover, each Pfam entry is now connected to the Sequence 
Ontology (SO). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(ii) Conserved Domain Database (CDD) 
Conserved Domains can be described as regions in the sequence that have a similar or exact 
pattern of amino acid in a variety of organisms. These conserved domain patterns in the 
sequence can be achieved by performing multiple sequence alignment. CDD is a part of the 
NCBI Entrez search and retrieval system and is cross-linked with NCBI databases such as 
Entrez/protein, Entrez/Gene, 3D-structure (MMDB), NCBI BioSystems, PubMed and PubChem. 
For improving bacterial genome annotations there exists NCBIfam in CDD which is a set of 
HMM models (PMC6943070). In CDD, high-confidence (specific) domain annotation is used to 
differentiate between superfamily and subfamily domain architectures. CDD collects data from 
five other major resources including Pfam, SMART, COGs, TIGRFAMs and PRKs. Data can 
be downloaded in ASN Text, XML, JSON, BLAST Text format.]]>
			</paragraph>
			<paragraph>
				<![CDATA[(iii) InterPro 
InterPro is a freely accessible software available at https://www.ebi.ac.uk/interpro/. This database 
is useful in the classification of proteins into families by predicting their domains and other 
important sites. For the classification of proteins, Interpro relies on different predictive models 
referred to as ‘signature’ collected from several different databases that are the part of Interpro 
consortium. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[KEGG: Kyoto Encyclopedia of Genes and Genomes was developed by Minoru Kanehisa in the 
year 1995 in Japan by the Ministry of Education, Science, Sports and Culture in Japan and is 
available at https://www.genome.jp/kegg/pathway.html. KEGG database consists of high-level 
annotation, i.e., genomic to the molecular level annotation of biological systems. It provides 
detailed diagrammatic representation in the form of a network for genes, proteins, drugs and 
chemicals involved in various biological processes and diseases depicted in Fig. 5. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is important to identify the interaction between proteins because proteins do not work alone, 
instead, they interact with other proteins and form a network of interactions. There are various 
experimental techniques which have been developed to identify such interactions. 
Yeast two-hybrid (Y2H) system and affinity purification followed by mass spectrometry 
(AP-MS) are two popular methods used to identify physical interactions. Various bioinformatics 
databases have been developed which consists of protein-protein interactions (PPI) information 
from various experimental as well as prediction methods. PPI information is visualized in the 
form of graphs with nodes as proteins and edges as an interaction between nodes. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(i) STRING 
STRING database is a widely used database comprising known and predicted protein-protein 
interactions from experimental data, text mining, predictions and literature. The current version 
of the STRING database (11.0) has PPI of 24,584,628 proteins from 5090 organisms. Among 
them, 4445 are from bacteria, 477 from eukaryotes and 168 from Archae. The STRING resource 
is available online at https://string-db.org. STRING databases include physical (direct) as well 
as functional (indirect) interactions. STRING 11.0 has one important new feature where users 
can upload complete genome-wide datasets and perform gene-set enrichment analysis such as 
Gene Ontology and KEGG pathway analysis. 
IntAct: This database was developed by EMBL-EBI. IntAct is freely available at http://www. 
ebi.ac.uk/intact and is an open-source database consisisting of molecular interactions extracted 
from either the literature or from direct data depositions. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(ii) DIP 
The Database of Interacting Proteins consists of experimentally verified PPI and is available at 
http://dip.doe-mbi.ucla.edu. The DIP database provides three-level details: protein information 
protein-protein connections information and details of experiments detecting the protein-protein 
interactions. The protein information table contains protein identification codes from the SWISS-PROT, PIR and GenBank sequence databases, as well as each protein’s gene name, description, 
enzyme code and cellular localization if known.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Gene Ontology was developed in the year 1998 by a consortium of researchers working 
on the genomes of three model organisms: Drosophila melanogaster (fruit fly), Mus 
musculus (mouse), and Saccharomyces cerevisiae (brewer’s or baker’s yeast). Gene Ontology (GO) 
provides functional details associated with gene or gene products in detail. GO mainly consists 
of three types of information that include molecular function, biological process and cellular 
localization associated with every gene. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Molecular Function: Molecular function involves different activities that are present at 
the molecular level such as “catalysis” or “transport”. Activities that can be accomplished 
by individual gene products such as proteins or RNA or activities that are performed by 
molecular complexes consist of multiple gene products fall under the molecular function 
category. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biological Process: The larger processes or ‘biological functions’ performed by 
multiple molecular activities fall under this category, such as DNA repair or signal 
transduction. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Cellular Component: Unlike the other two classes this class does not provide information 
on processes but rather cellular anatomy. It provides information on the cellular location 
of gene products where it performed various functions (e.g., mitochondrion, ribosome, 
Golgi and others)]]>
			</paragraph>
			<paragraph>
				<![CDATA[Till now a number of databases have been developed for GO identification. 
One of the main important repositories is the Gene Ontology Consortium and is available at 
http://geneontology.org/ number of model organisms GOs. The Gene Ontology (GO) consortium 
is the world’s largest knowledgeable database of information with regard to the functions of 
genes. As of June 2020, the GO consortium contains 44,441 GO terms, 7,975,639 annotations, 
and 1,558,956 gene products from 4,611 species. 
The main aim behind GO Consortium is to develop an up-to-date, comprehensive, database 
of biological systems functional annotations from the molecular level, biological pathways and 
cellular level.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Genome-specific databases are biological databases developed to provide in-depth annotations 
for a specific model organism. These databases allow scientists to perform various integrated 
analysis in a single platform. These kinds of databases also provide a platform to scientists from
the same fields for discussions and updates related to a specific organism’s information such as 
latest publications, new achievements, related conferences and meetings, etc. 
Some of the widely used organism-specific databases that have been developed until now, 
are discussed below in Table 6. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[We should check various genome properties which have an effect on quality of the downstream 
analysis. These include: 
(i) Quality and purity check of extracted DNA 
Contaminants such as phenol, salt and ethanol should be removed before proceeding with DNA 
sequencing. The introduction of such salts and alcohols may produce nicks in the DNA which 
makes it fragile. DNA becomes fragile due to other factors too such as using inappropriate 
storage such asstorage of DNA at above –20 degrees Celsius which causes DNA degradation. 
RNA contamination in the DNA samples also plays a major role in DNA structural integrity. 
This results in an overabundance of nucleic acid molecules concentration.]]>
			</paragraph>
			<paragraph>
				<![CDATA[(ii) Choosing an appropriate sequencing technology 
If you are working for new genome assembly and annotation, longer reads producing sequencing 
should be used such as Nanopore, PacBio, etc. For improving the assembly and annotation of 
specific genomes, sequencing platforms generating shorts reads need to be considered. This will 
help in filling the gaps which were not yet properly assembled. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(iii) DNA repeats 
DNA repeats occur in multiple copies in different locations in the genome and influences 
significantly the genome assembly statistics. The main reasons behind it is similar reads are 
produced from different repeats which ultimately confuses the assembly tools which are not able 
to distinguish them properly (Phillippy et al., 2008). This can further result in a poor assembly 
where repeats from different regions assemble incorrectly. It can be avoided by choosing 
sequencing technology which produces longer reads. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(iv) GC-content 
Bias in the GC content of various organisms have been observed. This bias in GC content has 
shown to cause low or no coverage in that region of the genome (Chen et al., 2013). Therefore, 
it is suggested that while working with these types of genome, sequencing technology which 
does not consider that bias (i.e., Pac-Bio or Nanopore) is recommended. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The file after complete genome annotation is called the GFF format. GFF file format is used 
for representing genes with their structural and functional features such as protein domains, 
pathways, interactions, structure and localization. It is in a tab-delimited format with 9 fields. 
Other file formats are gene transfer format (GTF), BED, GenBank, and EMBL. GFF file format 
does not include gene sequence information whereas BED and GenBank contain sequence 
information with annotations. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(i) RAST 
In 2008, the RAST (Rapid Annotation using Subsystem Technology) annotation software was 
developed to annotate bacterial and archaeal genomes (Aziz et al., 2008). It uses manually 
curated gene annotations from the SEED database onto newly submitted genomes and then 
helps in identifying genomic features (i.e., protein-encoding genes and RNA) and annotating 
their functions. RAST has become popular for consistent and accurate annotation of microbial 
genomes. The RAST community currently consists of ~10,000 active users who contributed an 
average of 1,170 microbial genomes per week in the year 2014. To make RAST a more useful 
tool with advancements in bioinformatics that is both customizable and extensible to the RAST 
tool kit (RASTtk), a modular version of RAST was developed (Brettin et al. 2015). This pipeline 
enables researchers to customize their annotation pipelines. RASTtk offers many softwares for 
the identification and annotation of genomic features. RASTtk also allows batch submission of 
genomes with the ability to customize the annotation pipeline during batch submissions. This 
is the first major software restructuring of RAST since its inception. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(ii) MAKER 
MAKER is an open access easy-to-use genome annotation pipeline. The main purpose behind 
this pipeline is to independently annotate and create databases for smaller eukaryotic and 
prokaryotic genomes. MAKER performs gene annotation by identification of repeats, ESTs 
alignment and proteins to the query genome which generates ab initio gene predictions and 
automatically converts these data into gene annotation database. The input for MAKER is 
minimum and its output can be directly uploaded into a GMOD database. MAKER has proven 
to be extremely useful for scientists with minimal bioinformatics experience during handling 
sequencing projects from a model organism. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[With the advent of second-generation sequencing technologies, more genomes have been 
sequenced and it is becoming difficult to handle such an enormous amount of data. Techniques 
like mRNA-seq generate large amounts of data but they have a great advantage in improving 
genome annotation quality. Therefore, it is necessary to convert the output of the annotation 
pipeline into a genome database. To address the challenges faced while annotating second-generation sequencing data, 
MAKER 2 was developed. This is an updated version of MAKER 
which is an improvement upon the de novo annotation capacities of MAKER and integrates 
multiple ab initio prediction tools. The main feature of MAKER2 is the integration of the 
Annotation Edit Distance (AED) metric to improve annotation quality and database management. 
To scale up and handle the data of any size this pipeline supports distributed parallelization on 
computer clusters via MPI. 
]]>
			</paragraph>
			<paragraph>
				<![CDATA[(iii) Galaxy 
Galaxy is a widely used, web-based genomic workbench that enables users to perform 
computational analyses of genomic data. The Galaxy Genome Annotation (GGA) Project is 
focused on supporting genome annotation inside the Galaxy workspace. GGA project consists 
of various teams, tool and suites that are working together to bring extensive and easy to use 
Genome Annotation experience for galaxy users. 
Galaxy developed a docker available at https://github.com/galaxy-genome-annotation/docker-galaxy-genome-annotation which provides end-to-end solutions for genome annotation. The 
docker has tools for Assembly (Spades, Mira), Structural Prediction (Glimmer, Augustus), 
Functional Prediction (BLAST+, InterProScan, BLAST, Diamond, Blast2GO), various Utilities 
(FASTA manipulation tools, EMBOSS), tools for Comparative Genomics (CD-Hit, ClustalW, 
AntiSmash, mummer), and Visualization tools (Apollo Tools, JBrowse-in-Galaxy, JBrowse-in-Galaxy Extras, Triple Admin tools, Circos).]]>
			</paragraph>
			<paragraph>
				<![CDATA[(iv) G-OnRamp 
G-OnRamp is a user-friendly system web-based platform for collaborative, end-to-end 
annotation of eukaryotic genomes using UCSC Assembly Hubs and JBrowse/Apollo genome 
browsers (Liu et al., 2019). It integrates the Galaxy platform, over 25 community and custom 
bioinformatics tools and the UCSC and JBrowse/Apollo genome browsers to create a single 
platform for annotation. The data used in this browser is from evidence collected from sequence 
alignments, ab initio gene predictors, RNA-Seq data and repeats identification.]]>
			</paragraph>
			<paragraph>
				<![CDATA[(v) Ensembl 
The Ensembl gene annotation system is a very popular annotation system that has been used 
to annotate over 70 different vertebrate species from a large number of genome projects. 
Moreover, it produces automatic annotation for the human and mouse GENCODE gene sets. 
The annotation in this pipeline is based on the alignment of protein sequences, ESTs, cDNAs, 
and RNA-seq reads, to the reference genome to generate transcript models (Aken et al., 2016). 
By filtering and assessing and of the selected transcripts ultimately forms the final gene set.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Genious Prime is acommercial software developed by biomatters that provides easy to 
use solution for genome annotation for users with less bioinformatics expertise. Geneious 
Prime includes genome annotation features and reports that include the structure and 
function of various genomic regions, such as genes, CDS’s (coding sequences), exons, introns, 
5¢- 3¢ UTR’s, tRNA’s, rRNA’s, ORF’s (open-reading frames), etc. Geneious can be easily installed 
on a local workstation. It does not require high RAM for example, for Illumina data roughly 
1 GB of RAM will be required to assemble a data set of 1 million reads. Raw FastQ files and 
the reference genome can be directly imported into Geneious, used for sequence alignment of 
the sample against the reference genome and downstream annotation analysis. It provides a 
graphical visualization of genome analysis such as genome assembly statistics, domain and 
pathway information. 
]]>
			</paragraph>
			<paragraph>
				<![CDATA[Data visualization is one of the important steps during genomic data analysis. High throughput 
sequencing and array-based profiling generate a large quantity of diverse genomic data. Such 
diversity in the data types created major challenges to the visualization tools. Several genomics 
data visualization tools have been developed such as Tablet, IGV and Artemis. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(i) Integrative Genomics Viewer (IGV) 
Integrative Genomics Viewer (IGV) is a commonly used visualization tool developed by Broad 
Institute that enables visualization of diverse, large-scale genomic data sets (Thorvaldsdottir 
et al., 2013). It is a web-based application and can be easily installed on the desktop. IGV 
advancement began in 2007 in consequence of a need by the Cancer Genome Atlas (TCGA) 
undertaking to visualize copy number variations, gene expression and clinical information. It 
supports the integration of a wide range of genomic data types that includes aligned sequences, 
variant data, gene expression, methylation and genomic annotations. The current version of IGV 
supports several file formats such as GFF, BAM, BED, FASTA, GTF, MAF, VCF and others. 
It consumes minimal desktop requirements. Besides, IGV also allows collaborators to load and 
share data locally or remotely using the internet. IGV is available at www.broadinstitute.org/igv]]>
			</paragraph>
			<paragraph>
				<![CDATA[(ii) Artemis 
Artemis is a very popular DNA sequence visualization and annotation tool developed by Sangers 
Institute for analyzing the genomes of bacteria, archaea and lower eukaryote (Rutherford et al., 
2000). It allows the visualization of results from any sequence analysis such as NGS data and 
its six-frame translation. It is implemented in Java and can be run on any suitable platform. 
It uses sequences and annotations in EMBL, GenBank and GFF format. Another version of 
Artemis is ACT (Artemis comparison tool), which is a Java-based application for displaying 
pairwise comparisons between two or more DNA sequences. 
ACT can be utilized to identify and analyze regions of similarity and distinction among 
genomes and to investigate conservation of synteny with regard to the complete sequences and 
their annotation (Carver et al., 2005). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[(iii) Apollo 
Apollo is an application for annotating genome sequences and provides an interactive tool to 
allow biological experts to edit and create gene models (Lewis et al., 2002). The first version 
of Apollo was a standalone desktop application. 
But with the advancement in software development, Apollo was also upgraded and took 
advantage of new technologies to provide a better experience to users. In the year 2010 Apollo 
changed to running inside a web browser. The latest version of Apollo provides new interface 
features that include support for real-time collaboration, allowing users to edit the same encoded 
features at the same time also show the updates made by other researchers in the same region 
(Dunn et al., 2019). This feature of Apollo allows users to integrate it into multiple genomic 
analysis pipelines and diverse laboratory workflow platforms. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[making data publicly available 
Annotating and publishing research paper is not just the end of the solution. Submitting the 
annotation data to publicly available repositories is also necessary, which will probably be used 
to help annotate other genomes. 
Some popular databases for submitting your annotations are GenBank, EMBL-EBI and 
Ensembl; or users can also submit to organism-specific genome databases. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[With the sequencing of newer genomes there is an increasing gap between documented and 
annotated protein sequences. This gap is increasing at a much fast rate everyday. There is an 
urgent need for accurate functional annotation of proteins. This need is much more important 
for human-related proteins. In the beginning, a few decades ago, functional annotations were 
mainly done with multiple sequence alignment techniques. These methods, however, have 
certain pitfalls, including excessive computational requirements due to the heuristic nature 
of the algorithms and performance degradation for low similarity sequences. With the advent 
of machine learning methodologies, various tools and techniques have been developed by 
various researchers for alignment-free annotation of protein functions. These techniques include 
extraction of a plethora of domain-dependent features and descriptors differing in size and 
information content. A need arose for selecting a subset of features with the highest information 
content and discarding noisy features. Several feature selection techniques have been developed to achieve this purpose. Appropriate classification and regression techniques with different levels 
of complexity and rigors have been developed alongside for robust prediction tasks. Several 
data bases and webservers have also been developed for ready use by practitioners, researchers 
and academicians. In this review we have discussed about all the above aspects in detail with 
special reference to human-related proteins, wherever necessary we have given illustrations and 
tables for ready reference. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Support Vector Machines (SVMs) can be used for both supervised and unsupervised learning 
tasks. SVM classifiers are formulated from statistical learning theory by Vapnik (Mika et al., 
1999). For binary classification problems, SVM builds a linear maximum margin hyperplane 
defined by the following equation: 
w ● xi + b = 0 (1) 
where xi
 represents the vector of input attributes, b the bias and w represents the weight vectors. 
For linearly separable examples, SVM creates such a linear hyperplane which maximizes the 
margin and is defined as the distance between the hyperplane and the nearest examples belonging 
to both classes. This can be formulated as a weight vector norm minimization problem with 
suitable constraints. The convex quadratic optimization problem obtains the optimized vectors 
solely defined by the examples falling on the margins. These examples are known as support 
vectors and hence the name Support Vector Machines. This quadratic convex optimization 
problem is highly desirable because it provides a unique solution as opposed to several classifiers 
which get trapped into the local minima. This aspect has driven researchers in various fields 
to employ support vector machines. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[For non-, linearly separable problems, SVM converts the data into a higher dimension. 
Thereafter it employs a linear hyperplane. Such a transformation can create intractability 
difficulties. SVM overcomes this by defining the kernel functions. These functions connect 
the dot products in higher dimensional space to functions of dot products in the input space. 
Kernel functions facilitate all computations to be carried out in the original space itself. Kernel 
functions must be positive, definite along with Hilbert Space axioms to be satisfactory. The usual 
kernel functions employed in typical classification tasks are: Polynomial, Gaussian Radial Basis 
Function (RBF), and Multi-layer Perceptron kernel functions. We have in bioinformatics domain 
several domain-dependent kernels. For increasing generalization capabilities, a soft margin 
formulation is used. In this formulation a modified convex quadratic optimization problem is 
formulated. This formulation incorporates a cost parameter to handle trade-off between margin 
maximization and misclassification error. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Decision trees are tree-structured algorithms which progressively divide the original data set into 
smaller and smaller subsets. They start with a head node in which the most informative feature 
and most optimal split point of that attribute are used to divide the data into smaller subsets. 
The split can be binary or multiway. This splitting is continued at the intermediate nodes in the 
same way. At every stage the most informative attribute is used for node splitting. This process
continues until the leaf nodes are reached. The splitting is done from a node to the children 
nodes so that the children nodes are purer than the original parent. Several criteria like Gini 
Index, entropy and misclassification error are used by different authors to evaluate the quality 
of splits. Splitting is stopped when 
(i) the attributes of examples in that node do not differ much in their values and 
(ii) when the number of examples are less than a previously defined threshold. The fully 
grown trees are finally subjected to pruning which helps in avoiding overfitting and 
increasing generalization capabilities ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Protein function prediction plays a vital role in the sensitive procedure of drug development. 
Though drug discovery process is labour intensive and expensive, efficient computational 
methods can be used to reduce the labour, time as well as cost associated with this process. In 
such a flow, decision trees can be used as classifiers, which can learn classification rules from 
the given training data which are used to predict functions of unknown proteins. Sandhu et al., 
worked on enhancing the use of decision trees as classifiers for Human Protein Function (HPF) 
prediction based on sequence derived features (Singh et al., 2007). In the work, decision tree is 
created by HPF predictor by using training data by processing sequence derived features and 
known functional classes of protein sequences. The test data is used to compute percentage 
accuracy of the decision tree created. The highlighted advantage of usingsequence derived 
features in this study is that new prediction technique creates decision trees with depth of thirteen 
nodes, as compared to decision trees with depths of two nodes using existing techniques. The 
large depth of the tree has led to the consideration of more number of tests before functional 
class assignment and has thus resulted in more accurate predictions. For the same test data, 
the percentage accuracy of the new HPF predictor is 72% and that of the existing prediction 
technique is 44%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Random Forest (RF) is an ensemble of decision tree classifiers (Breiman, 2001; Liaw and 
Weiner, 2002; Cutler et al., 2007). With an improved bagging version, RF employs two kinds 
of randomness while growing trees. In the first, bootstrap sampling with replacement is used 
in each decision tree. The other randomness is in selection of a subset of attributes in node 
splitting; it employs a predetermined random subset of trees in each tree. Every split and split 
point is optimized by various measures like Gini Index, Entropy and misclassification error. 
Each tree is grown to full size and pruning is not carried out . The overall prediction is made 
averaging the prediction of individual trees. As bootstrap sampling is employed, roughly one-third of the examples are left unused by each tree; these instances are known as Out of Bag 
(OOB) examples. RF performance can be estimated by CV measures as well as by estimating 
OOB error measures. Accuracy of prediction depends upon the performance of each tree and 
the correlation between the trees. An optimal value of subset of features used in each tree will 
provide the best trade-off. Such a trade-off will enhance generalization capabilities of RF. RF 
has several advantages: (a) two different feature rankings can be embedded in the algorithm, 
(b) the algorithm can be used to remove outliers, (c) the algorithm can be effectively used for 
missing values imputation. RF has been found to be a very robust classification algorithm and 
has found uses in different function prediction tasks. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Random Forests can also be used to find out the most informative attributes embedded in the 
algorithm itself. This is done by permuting each feature column and finding the mean decrease in 
performance of the OOB estimates. This raw importance score is an estimate of the contribution of this feature in the data set performance. The other measures the grand weighted average of the 
mean decrease in Gini measure, an attribute during split into different trees. This mean decrease 
in Gini importance score has also been found to be an effective way of scoring attributes. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Artificial Neural Networks mimic this cooperative functioning of the neurons in the brain by 
connecting the inputs of a given data (input neurons) to the required outputs of a specific task 
through a series of layers of neurons (Zurada, 1992). A typical ANN architecture consists of an 
input layer, 1 or 2 hidden layers and an output layer. The inputs are weighted and then passed 
to each of the neurons in the first hidden layer. These are summed, squashed (non-linearly 
mapped), weighted and then passed to the next hidden layer of neurons. These are summed 
and further squashed by activation function, summed up and sent to the output layer. Every 
input example is fed through the layers, following the same procedure. The network output is 
compared with the actual output and overall error is computed. The weights are revised using 
back propagation algorithm. The procedure is repeated until the total error in minimized. The 
working of ANN is schematically shown in Fig. 1. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The conventional neural networks are shallow and at most contain two hidden layers. Depth 
differentiate deepneural networks are different from conventional neural networks. They have 
many hidden layers in their configurations. Additionally, deep neural networks train on distinct 
levels of features in each layer. With increasing depth, the levels of features learned are higher. 
With increase in depth, the complexity of features learnt increases due to a built-in aggregation 
and recombination mechanism. Convolutional neural network (CNN) is a very popular class of 
deep neural networks and finds applications in analysis of visual imagery. CNNs, like shallow, 
consists of layers of receive input data, aggregates a weighted sum and propagates through an 
appropriately selected activation function. The output received from the last layer of hidden 
neurons is compared with the actual output and the weights are corrected using back propagation algorithm.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In deep neural networks, the input is a multi-channelled image. For an RGB image, say of 
size 32×32×3, is input to CNN. This input is subjected to a series of convolution operations in 
CNN with several filters each having random weights. These convolve over the image, as shown 
in Fig. 2. Assume that a 5×5×3 filter slides over the complete image covering all possible unique 
5×5×3 subsets of the image; on every convolution operation the resulting output (WT.X + B) 
is a scalar (one number), where W.X represents the dot product between weights and inputs. 
Similarly, for every other dot product taken we obtain a scalar output. It is easy to compute that 
28×28 unique image subsets are to be convolved. A single filter after a complete convolution 
operation provides an output of size 28×28×1, shown in Fig. 3. The convolution layer consisting 
of six filters will provide six feature maps. Hence, we will obtain a combined size of 28×28×6. 
Each filter is independently convolved with the image with the shape of the filter map obtained 
being 28×28×1. This is diagrammatically represented in Fig. 4. The architecture consisting of 
several convolution layers in sequence will look like as shown in Fig. 5. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Not all attributes are informative in data sets. Features which are non-informative will act as 
noise, do not have discriminative power, and interfere with the classification process. Hence the 
model will have very little predictive accuracy. In protein function identification in viral biology, 
several sequence and structural features can be extracted (Ma and Huang, 2008; Saey et al., 
2007). For example the AA, dipeptide and tri-peptide compositional features put together amount 
to 8400 in number and not all of them will be important in a particular function annotation task. 
To select a subset of informative features by brute force, we need to evaluate a huge number of 
subsets of features which becomes computationally time consuming. Various feature/attribute selection methods are available to simplify the process of subset selection. Feature selection 
techniques help us to avoid overfitting and improve model performance to provide faster and 
more cost-effective models; they also provide invaluable domain information. However, feature 
selection techniques have to employ appropriate search techniques, and additional level of 
complexity and computational cost. Feature selection techniques differ from one another in the 
way they incorporate this search in the added space of feature subsets in the model selection. 
These methods can be broadly classified as filter, wrapper and embedded methods. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Filter ranking methods employ appropriate heuristics to rank the features of a given data 
set. Once the features are ranked, top ranking feature subsets can be used for classification, 
discarding the rest of the redundant features. Mutual information, student t-test, correlation-based 
feature selection (CFS), Minimum Redundancy-Maximum Relevance (mRmR) and Uncorrelated 
Shrunken Centroid (USC) algorithm are some of the popular filter ranking methods. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Filter methods are very fast and can provide an estimate of accuracies of informative features. 
As they are not accurate, different wrapper methods are used in literature. Conventional wrapper 
methods include forward selection and backward selection algorithms. In forward selection, in 
the first iteration, the most relevant attribute providing maximum classification performance 
is selected. In the next iteration, along with the selected feature in the first iteration, the next 
most relevant feature is identified by combining each of the remaining features with the already 
selected feature. In the rest of the iterations this procedure is repeated with an additional feature 
added progressively. In backward selection the reverse operation is conducted removing the 
least relevant feature at every iteration and employing a procedure similar to forward selection. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Genetic algorithms, Ant Colony Optimization or other swarm intelligent methods are also 
used profusely for attribute selection. These methods mimic natural processes for any given 
optimization problem. The idea is to use only a fraction of all possible combinations to arrive 
close to the optimal feature set. Biogeography-based optimization (BBO) is another method 
which mimics the natural processes of immigration and emigration of populations. Srivatsava 
et al. employed BBO based feature selection for MHC Class I Peptide Binding Prediction with 
Support Vector Machines and Random Forests as classifiers (Srivatsava et al., 2013). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[In embedded class of feature selection techniques, feature ranking facility is embedded in the 
algorithm itself. SVM recursive feature elimination (SVMRFE) and the two random forest 
ranking methodologies are examples of embedded methods. In SVMRFE the hyperplane is 
built with all features initially and the attribute having lowest absolute weight is removed. This 
attribute has the lowest relevance with the classification. This process is repeated until only one 
attribute is left. Thus, for a data with ‘n’ features ‘n’ different experiments are needed to rank 
the features which is much less than that required for conventional recursive methods.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Domain information can be given to the machine learning algorithm in a variety of ways. 
These may includesequence derived and structure based. Several sequence-based features can be extracted. These include amino acid, dipeptide and k-mer features, evolutionary information in 
the form of position-specific scoring matrix (PSSM), predicted structural information, sequence 
conservation score, annotations of functional sites, network properties, etc. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Sequence features represent domain knowledge extracted from protein sequences. The simplest, 
most popular and most widely used features are composition-based features. These descriptors 
convert unequal length protein alphabet from various sequences in the data set into equal length 
attributes. Such equal length attribute extraction is very convenient because they can be readily 
employed as input to various classifiers. Figure 6 enlists broad categorization of feature based on 
composition of peptides and amino acid properties in protein sequences.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Amino acid composition (AAC) can be defined as a fraction of each amino acid present in a 
peptide sequence. AAC can be computed using the following equation: 
AAC(i) = Frequency of amino acid(i) / Length of the peptide (2)
where i can be any natural amino acid. The AAC is very attractive as it represents the domain 
information in the form of a very small number of constant length input vectors. In the same 
way dipeptide and higher k-mer compositions can be extracted from the primary sequences. 
Once these descriptors are extracted, they can be represented in a stacked form vector as 
input features. As all of them may not have predictive power, an attribute selection method is 
commonly employed to identify the most informative features. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Atomic Composition (ATC) is the frequency count of each atom (C, H, N, O, and S) present 
in the given peptide sequence. Kumar et al. (Kumar et al., 2015) discusses details about the 
number and types of atoms in naturally occurring amino acids. In this study ATC features were 
employed for antihypertensive peptides design. The ATC has a fixed length of five features. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Pseudo amino acid composition (PseAAC) was first employed by Kuo-Chen Chou in 2001 
(Chou, 2001). Similar to conventional amino acid composition method, this characterizes proteins 
using a matrix of frequencies to represent protein sequences. Additionally, the method provides 
additional information to include local features, such as correlation between residues of different 
distances. As a consequence, the amino acid frequencies contain a set containing more than 
20 discrete descriptors. The additional components incorporate sequence-order information and 
various pseudo-components. Over the period of time, different variants of iPseAA composition 
were developed to address several kinds of problems in proteins and protein-related systems. 
Inspired by PseAAC, Lin et al. (2015) have incorporated g-gap dipeptide composition. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Cancer is a pathological condition which is characterized by uncontrolled cell division, invading 
or spreading to other parts of the body. Due to the complex and heterogenous nature of this 
dreadful disease, development of effectual anticancer therapies havebecome one of the most 
prevalent area of research (Basith et al., 2017). Cancer treatment by conventional methods like 
radiotherapy and chemotherapy are expensive. Moreover, the side effects of these methods 
are deleterious to normal cells. Peptide-based cancer therapy has emerged as one of the most 
promising approaches dues to its several advantages. These peptides can have high specificity, 
increased capability for tumor penetration, and minimal toxicity under normal physiological 
conditions (Harris et al., 2013). Anticancer peptides (ACPs) are peptides capable of use as 
therapeutic agents to treat various cancers. Recent studies demonstrated the selectivity of ACPs 
toward cancer cells without affecting normal physiological functions, making them a potentially 
valuable therapeutic strategy (Thundimadathil, 2012; Vlieghe et al., 2010). ACPs have cationic 
amphipathic structures composed of 5–30 amino acid residues. These structures are capable of 
interacting with the anionic lipid membrane of cancer cells, thereby enabling selective targeting 
(Gaspar et al., 2013; Yan and Liu, 2016). Manavalan et al. demonstrated prediction of anticancer 
peptides based on machine learning algorithm (Manavalan et al., 2017). SVM and Random 
Forest-based machine learning methods were employed for the prediction of ACPs using the 
features calculated from the amino acid sequence. In this study, features were extracted based 
on amino acid composition, dipeptide composition, atomic composition, and physicochemical 
properties. These features have been thoroughly discussed in Section 4.1. The training dataset 
contained 450 ACPs and 450 non-ACPs sequences. SVM yielded an average accuracy of 88.2% 
with Matthews correlation coefficient of 0.750, while Random Forest outperformed with 94.6% 
of accuracy and Matthews correlation coefficient of 0.885.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Similar work was done by Hasan et al. in identifying an uncharacterized protein sequence as 
phosphorylated protein. It has been already established that protein phosphorylation potentially 
plays a vital role in regulating protein function and conformation (Hasan et al., 2017). For 
prediction of multi-label phosphorylated proteins, the authors developed a novel computational 
tool termed ‘iMulti-HumPhos’. The first step was to extract three different sets of features 
from protein sequences. Individual kernel functions were defined for each set of features. 
Using multiple kernel learning, these kernels were later combined into a single kernel. Lastly, 
a combination of SVMs was employed where each SVM was trained with a combined kernel 
to achieve construction of a multi-label predictor. The features considered in this study includes 
Amino Acid Composition (AAC), Dipeptide Composition (DC), and Sequential evolution feature 
representation: data of the Position-Specific Scoring Matrix (PSSM). In this study, the authors 
observed that the iMulti-HumPhos predictor performed significantly better than the existing
predictor, i.e., Multi-iPPseEvo (Qiu et al., 2017). iMulti-HumPhos achieves 0.5855 for multi-label accuracy, which is also higher than other predictors.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The most abundant form of genetic variation is single nucleotide polymorphism (SNP). 
Eventually they can lead to change in amino acid due to missense mutation, and is thus often 
referred to as single amino acid polymorphism (SAP) (Yip et al., 2004). Not all the amino acid 
substitutions lead to deleterious diseases, most of them are neutral substitutions and they are 
believed to cause phenotypic differences between individuals. There are studies interrelating 
protein structure with its function and revealing evidences that single amino acid substitutions 
are responsible for certain disease types (Gong and Blundell, 2010; Sunyaev et al., 2000). Some 
studies also suggested that about 60% of Mendelian disease is caused by amino acid substitutions 
(Botstein and Risch, 2003; Gong and Blundell, 2010). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The structural properties of macromolecules provide a more accurate description of the actual 
environment or the neighborhood of a residue. These properties are extracted from the 3D 
structures of the molecules. Most common features in this category are: 
 (i) Secondary structure types, viz. a-helix, b-strand and coil 
 (ii) dihedral angle 
(iii) hydrogen bond 
(iv) 3D distance of a mutation residue position to other functional sites. 
One limitation of this approach is that the feasibility of feature extraction is dependent on 
the availability of a 3D structure of the protein. DSSP (Kabsch and Sander, 1983) is often used 
to determine structural annotation from PDB file (Berman et al., 2000).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Amino acids in the core of proteins are relatively conserved compared with those in the solvent 
accessible regions (Hubbard and Blundell, 1987; Worth and Blundell, 2009). Catalytic amino 
acids responsible for enzymatic reactions are also well conserved throughout evolution. Thus, 
mutations tend to occur in amino acid residues where evolutionary pressure is relatively relaxed 
and where they can remain in the population. Gong et al. catalogued structural and functional 
features of proteins that restrain genetic variations leading to single amino acid substitutions 
(Gong and Blundell, 2010). In this work, the authors used the features from the structural 
environments of amino acid variants, namely, side-chain solvent accessibility, main-chain 
secondary structure, and hydrogen bonds from a side chain to a main chain or other side chains. 
These features were extracted from three categories of datasets, viz. Mendelian disease-related 
variants, neutral polymorphisms and cancer somatic mutations. In this detailed study, the focus 
was on the amino acid substitutions located at functionally important sites involved in protein-protein interactions, protein-ligand interactions or catalytic activity of enzymes. With supporting 
evidences, the authors concluded that the occurrence of amino acid variants is affected by the 
structural and functional restraints. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solvent accessibility property has been shown to be the one of the most powerful attributes 
in predicting function, which is evident from the work done by Dobson et al. (Dobson 
et al., 2006) and Saunders and Baker (Saunders and Baker, 2002). In one, study by Ye et al. 
investigated different definitions of solvent accessibilities (Ye et al., 2007). For determination of 
protein function, they considered solvent accessibilities by calculating the absolute and relative 
solvent accessibilities of all atoms, total side chain, main chain, non-polar side chain and all-polar side chain. A novel approach was developed by defining a new attribute, the structural 
neighbor profile, to comprise a 20-D vector of the counts of different types of residues found 
in the 3D vicinity of a site: a count was obtained for each of the 20 residue types; a residue 
was considered as a ‘Neighbor’ if one or more of its heavy atoms fell within a specific radius 
around the C atom of the residue at the center. We calculated the structural neighbor profiles 
for both the wild-type and variant residue positions. Further, authors built a support vector 
machines (SVMs) classifier employing a carefully selected set of new and previously published 
attributes. Through a strict protein-level 5-fold cross-validation, an overall accuracy of 82.61%, 
and an MCC of 0.60 was attained. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first step in computational biology approach is to generate the data from a biological 
source, which can further be used to deduce meaningful evidences to support a study. Post-genomic era has an abundance of a large volume of sequencing data, but tasks like providing 
accurate function annotation, has always been a challenge. The most common approaches 
are based on looking for similarity in orthologous sequences. A major disadvantage of such 
methods is that it cannot address problems like annotating distant protein sequences or orphan 
protein sequences. Jones et al. used feature characteristics of protein sequence to address the 
problem of performing function prediction (Cozzetto et al., 2016). For this machine-learning 
based approach, the authors also developed a user-friendly web server using Gene Ontology 
Annotations (Ashburner et al., 2000). This server processes query amino acid sequences as 
input for generating gene ontology termed predictions. The most important part of the pipeline 
flow is to start with generating descriptors for query sequence. This feature set is based 
on predicted properties like transmembrane regions, post-translational modification patterns, 
cellular localization and secondary structures (Lobley et al., 2008). Though prediction algorithms 
can process single amino acids, but in this, for transmembrane and disorder features, secondary 
structures, algorithms processing PSI-BLAST profiles, can make more accurate predictions. 
Thus, in this case, data from uniref90 (Apweiler et al., 2004) was used to generate three separate 
PSI-BLAST profiles. The resulting output was translated in feature descriptors defining all the 
attributes in the query sequence. SVMs were employed to screen the normalized feature matrix, 
whose classifier output was a binary decision value. Finally, the probability and decision values 
were used to deduce gene ontology terms and reported annotations along with the confidence 
scores. In this exciting study, the authors demonstrated an alternative to homology inference-based methods by integrating data from many different sources. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The functional sites on the structure of proteins execute their biological functions. Most likely 
a protein loses its function if the neighboring residues of the functional sites alter the structure. 
Therefore, annotation of functional sites is an important feature. There are various studies 
demonstrating use of sequence and 3D distances between mutation position and functional 
sites as features to predict the functional impact on protein due to sequence mutation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In many of the examples, we saw that predicting the unknown protein function protein interaction 
information is an emerging area of research in the field of bioinformatics. It is a well accepted 
fact that proteins execute critical functions in essentially all biological processes. Computational 
methods like gene neighborhood, sequence and structure, protein-protein interactions (PPI), 
etc. have naturally created a larger impact in the field of protein function prediction than the 
biological-based experimental methods. On the same lines Nasipuri et al. demonstrated an 
approach to determine the functions of unannotated proteins, by utilizing their neighborhood 
properties in PPI network on the basis of the fact that neighbors of a particular protein have
similar functions (Saha et al., 2017). Here the authors used Gene Ontology (GO) dataset of 
humans obtained from UniProt database. GO system used involved three categories, namely: 
(i) Cellular-component, (ii) Molecular-function and (iii) Biological process. There is a possibility 
that each protein may be annotated by several GO terms in each category, thus every GO 
term of three categories were ranked based on the maximum number of occurrences in each 
of them. Then 10% of proteins belonging to the top 15 GO terms in each of three categories 
were selected as unannotated while the remaining 90% proteins were chosen as training samples 
using random sub-sampling technique. In this comparative study, performance of the proposed 
approach relatively performed much better than the other existing methods in unannotated 
protein function prediction.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Sequencing of phenotyped clinical subjects will soon become a method of choice in studies 
of the genetic causes of Mendelian and other complex diseases. Adzhubei et al. presented a new 
method and the corresponding software tool, PolyPhen-2 
for predicting the damaging effects of missense mutations (Adzhubei et al., 2010). PolyPhen-2 is 
different from the earlier tool PolyPhen-1 in the set of predictive features, the alignment pipeline 
and the method of classification. PolyPhen-2 uses eight sequence-based and three structure-based predictive features, which were selected automatically by an iterative greedy algorithm. 
The majority of these features involve comparison of a property of the wild-type (ancestral, 
normal) allele and the corresponding property of the mutant (derived, disease-causing) allele. 
The alignment pipeline selects a set of homologous sequences using a clustering algorithm and 
then constructs and refines its multiple alignment. The most informative predictive features 
characterize how likely the two human alleles are to occupy the site, given the pattern of 
amino-acid replacements in the multiple-sequence alignment; how distant the protein harboring 
the first deviation from the human wildtype allele is from the human protein; and whether the 
mutant allele originated at a hypermutable. The functional importance of an allele replacement 
is predicted from its individual features by a naive Bayes classifier. For a false positive rate 
of 20%, PolyPhen-2 achieved true positive prediction rates of 92% and 73% on two different 
datasets used. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Many non-synonymous single nucleotide polymorphisms (nsSNPs) in humans are suspected 
to impact protein function. Bromberg et al. proposed the method SNAP (screening for non-acceptable polymorphisms) that predicts the functional effects of single amino acid substitutions. 
SNAP was developed using annotations extracted from PMD, the Protein Mutant Database 
(Kawabata et al., 1999; Nishikawa et al., 1994). SNAP identifies over 80% of the non-neutral 
mutations at 77% accuracy and over 76% of the neutral mutations at 80% accuracy at its default 
threshold. Each prediction is associated with a reliability index that correlates with accuracy 
and thereby enables the observererto zoom into the most promising predictions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Physicochemical Properties 
Physicochemical properties include a number of features, such as introduction of an inflexible 
proline into a beta strand, replacement of a hydrophobic side-chain by hydrophilic substances 
or vice versa, a charged residue into a buried position, and over-packing of a pocket in the 
protein by changing the size of the residue, the hydrophobicity difference between original and 
mutated residues (Saha et al., 2017). Several approaches for predicting the functional impact of 
SAP were developed based on physicochemical properties (Apweiler et al., 2004; Bromberg and 
Rost, 2007; Care et al., 2007; Hubbard and Blundell, 1987). It represents the physicochemical 
class of residues present in a given peptide sequence. The percentage compositions for amino 
acids can be calculated based on different categories.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Along with these categories, one more important property is peptide mass (Chen et al., 
2016; Sanders et al., 2011; Tyagi et al., 2013). 
Many of these properties have been encoded as descriptors individually and with various 
combinations for different function identification problems. Gromiha et al. have considered the 
AAIndex1 currently containing 544 amino acid indices (Rawat et al., 2020). Each entry consists 
of an accession number, a short description of the index, the reference information and the 
numerical values for the properties of 20 amino acids. 
We have provided a link to the corresponding PubMed entries of each AAIndex entry, 
instead of a link to the LitDB literature database that we originally used. In addition, each entry 
contains cross-links to other entries with an absolute value for the correlation coefficient of 
0.8 or larger. The links enable the users to identify a set of entries describing similar properties. 
In some instances, the values are not reported for all 20 amino acids. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The AAIndex2 currently contains 94 amino acid substitution matrices: 67 symmetric 
matrices and 27 non-symmetric matrices. The format of the entry is almost the same as that of 
AAIndex1 except that it contains 210 numerical values (20 diagonal and 20 × 19/2 off-diagonal 
elements) for a symmetric matrix and 400 or more numerical values for a non-symmetric matrix 
(some matrices include a gap or distinguish two states of cysteine). In the previous release, 
each symmetric matrix, which is triangular in shape, was folded into a 10 × 21 table for the 
purpose of saving space, and columns were separated by space characters. In the present release, 
symmetric matrices are not folded, and the delimiter of the columns has been changed into a 
tab character for making easier parsing of the entry. 
The AAIndex3 section currently contains 47 amino acid contact potential matrices: 
44 symmetric matrices and 3 non-symmetric matrices. The format of the entry is almost the 
same as that of AAIndex2. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[An important genetic process in molecular biology is ‘translation’, which is directly linked 
with synthesis of peptide sequences. In the process, the messenger RNA (mRNA) carries 
information which is decoded by the ribosome complex. Rules of genetic code are followed 
to produce a specific protein (or peptide) chain (Jackson et al., 2010). The initiation of peptide 
chain synthesis starts with identification of proper start position on mRNA. This site where the 
translation is initiated is called the Translation Initiation Site (TIS). In-depth genome analysis 
includes identification of the TIS. Computational methods proposed in this regard do not consider 
the global or long-range sequence-order effects of DNA, and hence their prediction quality is 
limited. To address this inadequacy, Chen et al. developed a new predictor called ‘iTIS-PseTNC’ 
incorporating the physicochemical properties into the pseudo trinucleotide composition (Chen 
et al., 2014). The physicochemical properties considered by the authors included: (i) numerical 
values of hydrophobicity, (ii) hydrophilicity, and (iii) sidechain mass of the peptide chains. The 
genome coordinates of the annotated translation initiation sites in the human genome were 
obtained from the TIS database (TISdb); iTIS-PseTNC predictor correctly identified 195 TIS and 194 non-TIS and yielded accuracy of 97%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Structure-function relationship of a protein can also be studied by representing the protein 
structures as networks. In this type of depiction, the vertices of the network are the amino 
acid residues, while the edges are their interactions. Network representation has proved to be a 
prevailing tool to study complex networks of interacting amino acid residues since it provides 
noteworthy insights into the organization of protein structure and the regulation of protein 
function (Li et al., 2011a). ]]>
			</paragraph>
			<paragraph>
				<![CDATA[With the advent of cutting edge sequencing technologies, the sequence data is readily available 
to the research community. The most studied sequence variations are the Non-synonymous SNPs 
(nsSNPs), which are often referred to as Single Amino acid Polymorphisms (SAPs). This type of 
sequence variation accounts for the majority of human inherited diseases. Though SAPs are often 
associated with certain diseases, they are not always deleterious. Thus, distinguishing neutral 
SAPs with deleterious one, is extremely important. As discussed in the above sections, sequence-based or structure-based features are the most popular approaches. Huang et al. believe that: the 
likelihood of relating a SAP to a disease is based on the fact that the presence of SAP within 
a region of protein, alters the sequence and eventually its structure (Huang et al., 2010). They 
consider this as a better rationale for deleterious SAP prediction. So they developed a prediction 
method to find out the deleterious SAPs-based hybrid properties and protein interaction network. 
In this interesting study, the authors used 472 features such as network, sequential and structural 
features to characterize SAPs. Optimal feature set was obtained using Incremental Feature 
Selection (IFS) and mRmR methods, while the prediction model employed in this study was 
the Nearest Neighbor Algorithm (NNA). Prediction model using 263 optimized features yielded 
jackknife cross-validation of 83.27% accuracy. This prediction model with optimized features 
performed well even when tested with independent datasets with an accuracy of 80%. Moreover, 
this work demonstrated that accurate prediction can also be achieved using network features. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[For analyzing association of nsSNPs with diseases, the most accepted approach is to explore 
sequence and evolutionary information. To evaluate the potential of structural information, Cheng 
et al. developed a structure-based approach called Bongo (Bonds ON Graph) for prediction of 
structural effects of nsSNPs (Cheng et al., 2008). In this work, authors used graph measures 
of residue–residue interaction networks to identify the residues that are critical for maintaining 
the structural stability of proteins. This approach showed considerably good performance with 
an outstandingly low false positive rate by identifying the mutations that cause both local and 
global structural effects. The highlight of this study was a positive predictive value of 78.5%, 
which was attained for prediction of 506 disease-associated nsSNPs. This study is an important 
example of association between structural changes resulting from nsSNPs and their pathological 
consequences.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are several notable instances where complex networks were used effectively in 
proteomics. One of the most important reasons for the success of this approach is that the 
topology network representation of the protein structures provides innovative perception of 
protein folding mechanism. Yizhou et al. developed a new feature to reveal the correlations
between residues using a protein structure network (Li et al., 2011b). In this attempt to quantify 
the effects of several key residues on catalytic residues, a power function was used to model 
interactions between the residues. The results indicated that focusing on a few residues is a 
feasible approach to identifying the catalytic residues. The spatial environment surrounding 
a catalytic residue was analyzed in a layered manner. Feature analysis revealed satisfactory 
performance for the features used, which were combined with several conventional features in 
a prediction model for catalytic residues using a comprehensive data set from the Catalytic Site 
Atlas. Values of 88.6% for sensitivity and 88.4% for specificity were obtained by 10-fold cross-validation. These results suggest that these features reveal the mutual dependence of residues 
and are promising for further study of structure–function relationship. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Researchers have developed local tools or online servers to provide the service of predicting the 
function of proteins in various different applications. All these tools have been implemented on 
the basis of acceptable predictive capabilities of the method employed. Table 3 shows some of 
the web servers based on SVM models that are used in protein function prediction. 
As in many other areas, decisions play an important role also in medicine, especially in 
medical diagnostic processes. Decision support systems helping physicians are becoming a 
very important part in medical decision making, particularly in those situations where decision 
must be made effectively and reliably. Since conceptual simple decision-making models with 
the possibility of automatic learning should be considered for performing such tasks, decision 
trees are a very suitable candidate. They have been already successfully used for many decision-making purposes. As in many other areas, decisions play an important role also in medicine, 
especially in medical diagnostic processes. Decision support systems helping physicians are 
becoming a very important part in medical decision making, particularly in those situations 
where decision must be made effectively and reliably. Since conceptual simple decision-making 
models with the possibility of automatic learning should be considered for performing such 
tasks, decision trees are a very suitable candidate. They have been already successfully used 
for many decision-making purposes. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Protein being such a complex macromolecule, offers numerous aspects of feature definition, 
ranging from sequence to structure. In this review, we elucidated numerous sequences-based 
characteristic features of proteins which are explored in several studies for constructing 
protein function prediction models. We have enlisted several examples where some set features 
outperformed over others, indicating that use of feature set depends on the problem statement. 
We have also listed some of the important methods employed in feature selection. We have 
tried to cover different examples which are mostly focused critical research areas that directly 
or indirectly touch human life. Out of all the studies, the area which has been explored the most 
by the research community is predicting the effect of mutations in humans, which ultimately 
alters the function of protein. We have also listed large number of case studies and examples 
of protein function predictions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Genomics is the branch of biology that studies genomes, i.e. any organism’s full set of genes 
and how these interact with one another in order to produce observable phenomena at the 
phenotype level. This is in contrast to genetics, which generally studies genes and their effects 
individually. Considerable advances in sequencing technology in the recent years have resulted in 
an explosive growth of genomic and proteomic data. Given the massive amounts of information 
stored in a single genome, genomics generally relies on the use of computational tools in order 
to analyse these data and subsequently derive biological interpretations from them. Bioinformatics 
is the interdisciplinary field devoted to the design, use and application of mathematical and 
computational techniques for the analysis of biological data in order to identify patterns, derive 
generalizations, or make predictions from these, e.g., prediction of disease risk from gene 
expression (Huet et al., 2018; Shedden et al., 2008; Zhou et al., 2018) as well as prediction of 
genes or protein structures (Lomsadze et al., 2018; Senior et al., 2020), among others applications. 
Thus scientific interest in this field has seen an increase in recent years with the growing 
availability of genomic and proteomic data. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Bioinformatics makes extensive use of various subdisciplines of computer science, namely, 
data mining, machine learning, deep learning, data science, and big data, which are all closely 
related to each other with each exhibiting significant overlap with the rest. Data mining 
refers to the application of statistical methods and algorithms in order to discover interesting
patterns in large amounts of data, which would otherwise be a very difficult and impractical 
task to achieve through manual means. Machine learning, as the name implies, refers to the 
study and design of computer algorithms that are able to learn from data in order to make 
generalizations or predictions from them, with deep learning being a more advanced form of this 
discipline. On the other hand, data science and big data are loosely and frequently defined as 
the extraction of useful knowledge through the analysis of large, complex, raw and unstructured 
data, as well as the storage and manipulation of these in such large amounts that would not 
be manageable through traditional, computational means. There is no universal agreement on 
the difference between these disciplines, or where the boundary should be placed between any 
two of them, given that all refer to the study of data, generally in large volumes. Nevertheless, 
some distinctions can be unequivocally made.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Data mining focuses more on the discovery of 
previously unknown patterns, relationships and anomalies that are present in large amounts of 
data. Classical textbook examples of data mining applications include the discovery of unknown 
patterns found in historical purchase data and the identification of new associations that could 
indicate customer purchasing behaviour in the future that could be exploited commercially. 
For this reason, data mining is frequently used in retail in order to identify buying patterns 
and trends. On the other hand, machine learning and deep learning focus on the design of 
algorithms that can learn from and make predictions about the data, without human intervention 
and in more general contexts than data mining, while keeping the emphasis on the optimization 
of the learning task. Data science and big data can be defined as umbrella terms encompassing 
methods for the elucidation of insights in data of such volumes that make them not manageable 
through traditional computational means. Furthermore, all these disciplines are related to, and 
widely considered to overlap significantly with, statistical inference. However, while the main 
purpose of statistical inference is to discover underlying properties from a data population, along 
with an estimate of the uncertainty of these, data mining, machine learning and deep learning 
are more concerned about learning how to make predictions from raw pieces of information, 
which necessarily implies some degree of inference as well, whereas data science and big data 
are more concerned with the descriptive and manoeuvrability aspects of the data. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Applications of data mining, machine learning, deep learning, data science and big data 
are varied and dependent on the area. In the particular case of bioinformatics, the purpose 
frequently is to build a mathematical model from biological data, e.g., from gene expression 
measurements, in order to make predictions about similar observations, previously unseen by 
the model. Common applications include the prediction of an organism’s risk of developing a 
given disease based on gene expression levels (Bashiri et al., 2017; Park et al., 2020; Salem 
et al., 2017), differential gene expression analysis (Blanco et al., 2019; Spies et al., 2019), 
gene clustering (Gulisija and Plotkin, 2017; Zareizadeh et al., 2018) and identification of gene 
regulatory networks (Carey et al., 2018; Mochida et al., 2018; Ni et al., 2016). Other ambitious 
applications include, but are not limited to, the prediction of gene expression (Dong et al., 2012), 
gene locations (Mathé et al., 2002), regulatory regions (Fernandez and Miranda-Saavedra, 2012), 
an organism’s genetic response to a medication (Wang et al., 2011) or health status (Kourou et al., 
2015), the effect of single-nucleotide polymorphisms on gene regulation (Zhou and Troyanskaya, 
2015), among many others. The application of machine learning and related areas, such as data 
mining and deep learning, on biological data accelerates the understanding of complex diseases, 
for instance, cancer or diabetes, and eventually lead to automated diagnostic tools in medicine, 
among other advancements.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One of the greatest scientific achievements of the late twentieth century is the Human Genome 
Project (HGP), an international and multidisciplinary endeavour aimed at mapping the full 
sequence of genes found in the human DNA, documenting the functional roles of each. Since 
strictly speaking there is no single human genome, with each person having their unique genome, 
HGP aimed to provide a single sequence from the integrated mappings obtained from a limited 
number of individuals. The results obtained from the HGP included, among many others, the 
revelation that the human genome comprises more than 20,000 genes, which is a significantly 
lower figure than thought earlier by the academic community at the time, with prior estimates 
ranging between 50,000 and 100,000. This collaborative effort had its origins in 1984, followed 
by several years of planning after which the project officially started in 1990 and ended on 
14 April 2003. Various sources of funding from around the world contributed to the completion 
of the project, particularly the National Institutes of Health1 in the United States. One of the main 
legacies of the HGP and related initiatives is the massive and growing amount of genomic data 
that can be used to expand the current understanding of the inner workings of an organism, e.g., an 
individual’s response to a pathogen, such as a virus, or to a novel medication. This has numerous 
applications in a variety of fields, such as molecular medicine and biology. Specialized databases 
exist to store these data, many of which are freely accessible to the general public on the internet. 
Examples of these include the Gene Expression Omnibus2 (GEO), the DNA Data Bank of Japan3, 
the European Molecular Biology Laboratory’s repository4, and GenBank5, Other publicly-available repositories exist, albeit less specialized on biological data that nonetheless store some genomics-related datasets, including Kaggle and the UCI Machine Learning Repository]]>
			</paragraph>
			<paragraph>
				<![CDATA[Currently there are various technologies that allow the sequencing, either partial or in full, 
and posterior analysis of genomic data. This mass quantification of gene transcripts is frequently 
used, for instance, in intervention studies, e.g., experiments where one tissue sample has been 
exposed to a stimulus, such as a pathogen or medication, and its expression levels are assessed 
and contrasted against those of a similar sample, referred to as the control, where no stimulus 
has been applied. These experiments are conducted in order to quantify the effectiveness of 
novel medications or the host organism’s response to an infection, for instance. Experiments 
based on the sequencing of a single individual are referred to as personal genomics. The most 
common methods used nowadays for the collection of genomic data include DNA microarrays 
and next-generation sequencing (NGS). DNA microarrays are laboratory devices designed for the 
simultaneous quantification of up to thousands of gene transcripts from a single tissue sample 
and have for years been the standard tool in experiments designed for the characterization of 
gene expression variation across different biological conditions. Each consists of a grid where 
each probe allows the hybridization of a known DNA or oligonucleotide target with a matching 
RNA sample that has previously been reverse-transcribed and labeled. On the other hand, next 
generation sequencing (NGS), also known as high–throughput sequencing, is a set of related 
technologies that allow the examination of the entire genome without requiring a predefined set 
of targets. This is achieved through the process of synthesis, where DNA polymerase incorporates 
a set of nucleotides. All this, in contrast to microarrays, which are based on hybridization and 
return results only for those regions of the transcriptome for which their probes have been 
specifically designed. Even though NGS technologies offer a more modern and powerful option 
than microarrays, the choice between the two depends mainly on the experiment to be conducted, 
especially considering that the latter is generally more affordable than the former. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Given the large amount of data collected by microarray experiments, systematic errors 
frequently occur which may lead to variation in the reported values even for identically replicated 
experiments. These errors, which may originate from dye intensity effects among other, may 
make two different arrays or samples not directly comparable. These problems are corrected 
through a process known as normalization, which is a pre-processing step normally required 
in order to make microarray data comparable and thus plays an important role in the earlier 
stages of the analysis of these. The main purpose of this procedure is to remove the sources 
of noise in the gene expression levels measurements. However, no single, universally-accepted 
method or algorithm exists for achieving this, and the results of the subsequent analysis may 
depend remarkably on the method used (Park et al., 2003; Quackenbush, 2002; Yang et al., 
2001). Normalization may consist of statistical methods (Kerr et al., 2000; Wolfinger et al., 2001), 
locally weighted scatterplot smoothing (LOWESS) (Cleveland, 1979), subset normalization (Chen 
et al., 2003), local regression (Kepler et al., 2002), iterative estimation of coefficients (Wang et al., 
2002), non-linear methods (Workman et al., 2002), data scaling as well as averaging duplicated 
values. and can be visualized through an MA plot, which can be produced using software tools 
such as Excel, R, or MATLAB. Genomic data is typically stored in plain text using standardized 
formats and submitted to publicly-available repositories such as the Gene Expression Omnibus, 
GenBank or Kaggle for reference and posterior re-use by the academic community. Section 
5 presents an example of data mining analysis for the prediction of cancer types from gene 
expression data stored in text format and available on Kaggle.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In data mining, as well as in machine learning, classification analysis refers to the task of 
automatically identifying the appropriate category, alternatively called the label or class, for each 
one of a set of items, also called observations or data points, from a predefined set of known 
categories. One example of a classification problem is the task of learning how to automatically 
categorize emails either as “spam” or “not spam”. In this example, each email is an observation 
and the possible categories are “spam” or “not spam”. The purpose of classification analysis is to 
design a computer program, referred to as the model or the classifier, that can be trained in order 
to later be able to identify automatically the class of any new observation. During training, the 
model is presented with as many observations as possible whose correct categories are already 
known, with these known observations being called training data or labelled data, and then 
learns to identify the characteristics of each observation that are predictors of its class. In the 
case of the emails, for instance, the model would be fed many example emails, some known to 
be spam and some others known to not be spam, and would be expected to learn to identify 
the characteristics present in those messages that are known to be spam, e.g., their structure, 
writing style or their use of certain words, and to differentiate them from those messages whose 
characteristics indicate they are not spam. Another common application of classification analysis 
consists of facial recognition, i.e., learning to identify a specific person’s face from a set of 
photos (the observations) who are already known to belong to one of two categories, namely, 
“person is present in the photo” and “person is not present in the photo”, where the process 
of training is analogous. Another example is the automated classification of cancer types based 
on gene expression read through DNA microarrays, such as in the study conducted by Golub 
et al. (1999), who train a model in order to automatically categorize patients’ acute leukemias as 
either acute lymphoblastic leukemia (ALL) or acute myeloid leukemia (AML)]]>
			</paragraph>
			<paragraph>
				<![CDATA[During training, 
the model is given the gene expression data of various patients (the observations) with known 
diagnoses, either ALL or AML (the categories), and learns to identify the common characteristics 
present in these data that serve as predictors of either condition. Regardless of the problem being 
addressed through classification analysis, these characteristics refer to quantifiable properties 
observed in each item and are often referred to as features, attributes, independent variables or 
explanatory variables. The model learns to use these to predict automatically any item’s correct 
category, which is normally referred to as class, as explained earlier, or, less frequently, as 
dependent variable. These features may be variables of different types and domains, including 
real-valued (e.g., a real number to refer to a patient’s blood pressure), integer valued (e.g., an 
integer to represent a patient’s age), ordinal (e.g., “low” or “normal” or “high”, another way to 
refer to a patient’s blood pressure), or categorical (e.g., “masculine” or “feminine” to refer to a 
patient’s gender). Once the model has been sufficiently trained it is able to use these features in 
order to predict, with reasonable accuracy, the category of a message not seen before, as in the 
example of the email classification, or the patient’s cancer typein the leukemia example. From 
the perspective of machine learning, classification analysis is called supervised learning because 
training of the model is conducted over data for which the labels or categories are already 
known, e.g., the emails which are known to be either spam or not. There are other types of 
machine learning techniques, also used frequently in data mining, where these labels are neither 
available nor needed during training, such as clustering analysis (described in Section 6), that 
are thus referred to as unsupervised learning. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The supreme goal of any classification model, regardless of the specifics of the underlying 
problem, is to be able to make reliable predictions for future or previously unseen data, e.g., 
future emails or photos or patients not seen before. Frequently, more than one model is trained 
over the same labelled data using different learning algorithms and parameterizations of these
and then the reliability of each candidate model is evaluated after training in order to assess its 
predictive performance and choose the best. Various training-evaluation strategies exist, with the 
choice for each particular problem being normally made prior to training the model and with the 
most basic of these consisting of randomly partitioning the labelled data into two independent 
subsets, namely, the training dataset and the testing dataset. As their names suggest, the former 
is used first to train the model while the latter is subsequently used to evaluate how accurate 
the predictions of the resulting model are.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Various evaluation metrics are normally used during 
and after the training of the candidate models in order to determine which of these performs 
best at the task of generalizing from the training data and identifying the relationships between 
the items’ features and their corresponding categories, which would ultimately lead to better 
predictions and insights on future previously unseen data. Each one of these evaluation metrics, 
all computed from the number of correct and mistaken predictions made by the model during 
testing, has its own meaning and interpretation and can be summarized briefly as follows, for 
the sake of simplicity, assuming that there are only two classes, which can be referred to as the 
positive class and the negative class. It should be noted first, however, that the use of the words 
“positive” and “negative” in this context is neutral and should not be interpreted as the value 
or desirability of the class being referred to. In the context of predicting the risk of developing 
a disease, for instance, the “positive” class could refer to those patients who are “diseased” or 
in high risk whereas the “negative” class might refer instead to those who are “healthy” or that 
exhibit a low risk. The number of true positives (TP) is the number of observations from the 
testing dataset where the model correctly predicts the positive class, i.e., the situation where the 
model predicts the positive class for an observation that is already known to truly belong to this 
category, whereas the number of false positives (FP), also known as type I error in statistics, 
is the number of observations from the testing dataset where the model incorrectly predicts 
the positive class, i.e., the situation where the model mistakenly predicts the positive class for 
an observation that is known to actually belong to the negative category. The number of true 
negatives (TN) and false negatives (FN), the latter being known also as type II error in statistics, 
are defined analogously. These four metrics are generally tabulated in a confusion matrix in 
order to visualize the performance of the model, as illustrated in Fig. 1. The confusion matrix is 
simply a table that outlines the predictions made by the model during testing and contrasts them 
with the corresponding actual values. Also known as the error matrix, it is used in data mining 
as well as in machine learning and statistics. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The matrix shows that the 
model correctly predicted the class of all the negative observations (i.e., TN = 4 and FP = 0) as 
well as the correct class of 8 of the positive observations (TP = 8), having mistakenly identified 
the 6 remaining as negative (FN = 6). Figure 1(b), on the other hand, depicts a performance 
evaluation where the test data was composed of 100 examples, 99 of which truly belonged to 
the positive class whereas the remaining one example belonged to the negative class, and where 
the classifier under examination commits one false negative and zero false positives.]]>
			</paragraph>
			<paragraph>
				<![CDATA[More descriptive metrics, computed from the false and negative predictions, can be briefly 
described as follows. A classifier’s accuracy is defined as the ratio of correct predictions made by 
the model as shown in Eqn. (1). Accuracy can often be used as an easily interpretable heuristic 
or rule of thumb to assess immediately whether a model is being trained correctly and how 
it may perform generally on unseen data. Even though this metric may seem as an intuitive 
evaluation method it has some limitations, especially when the training data is imbalanced, e.g., 
when there are only two possible classes in the data with a significant discrepancy between 
the number of observations belonging to one class and the number of observations belonging 
to the other, such as in the example depicted in Fig. 1(b). The main drawback from this is that 
with a training strategy that emphasizes the maximization of this metric, the model may simply 
‘learn’ to indiscriminately predict always the class with the most observations, without actually 
generalising from the features of these, since this blind approach would still result mostly in 
correct predictions and hence result in high accuracy scores. For instance, a classifier that 
ignores the features of all observations and predicts always the positive class would obtain an 
almost perfect accuracy score in the training data used in the example depicted in Fig. 1(b). 
Therefore, accuracy is reliable as a performance metric mainly when there are approximately 
equal number of samples belonging to each class. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Precision and recall are robust metrics of predictive performance to be used when the 
classes in the input data are imbalanced, in contrast to accuracy. Both are based on the concept 
of relevance, from the information retrieval theory (Maron and Kuhns, 1960; Schamber, 1994). 
Algebraically, precision is calculated by dividing the number of true positives by the total number 
of positive results predicted by the classifier, as shown in Eqn. (2), while recall is calculated by 
dividing the number of true positives by the number of all relevant samples, i.e., all samples 
that should have indeed been identified as positive, including those the model failed to retrieve, 
as shown in Eqn. (3). Precision can be interpreted as the ratio of observations classified by the 
model as positive that are actually positive whereas recall is indicative of the number of all 
truly positive observations that are correctly predicted as such by the classifier. A related metric 
is the specificity, shown in Eqn. (4), which is the ratio of true negatives, i.e., the proportion of 
true negatives that are correctly predicted as such by the classifier]]>
			</paragraph>
			<paragraph>
				<![CDATA[Two metrics that provide a balanced view of the classifier’s predictive performance are 
F1 and the receiver operating characteristic curve, also known simply as ROC curve or ROC 
AUC. F1, whose algebraic definition is depicted in Eqn. (5), provides an integrated measure 
of the evaluated classifier’s precision and recall, consisting of the harmonic average of these 
two metrics. In this manner a model with perfect precision and recall is identified with an F1 
score of 1, the maximum possible value. This metric can be interpreted then as a balanced 
measure of how precise the classifier is, in terms of the number truly positive observations it 
classifies correctly, as well as how robust it is, in terms of not failing to identify a significant 
number of truly positive observations as such. The ROC curve, on the other hand, is a plot of 
the false positive rate, also known as sensitivity, against the true positive rate, also known as 
fallout and shown in Eqn. (6), for various candidate discrimination thresholds between 0 and 1 
and the area under, referred to as ROC AUC, provides an assessment of model’s the predictive 
power as these thresholds are varied. Therefore, ROC AUC can be interpreted as a measure of 
the trade-off between the true positive rate and false positive rate for a predictive model using 
different probability thresholds. There are other metrics that are used less frequently, such as 
mean absolute error, mean squared error, logarithmic loss, among others. The choice of the 
evaluation metrics to use normally depends on the problem being solved and is a critical step 
in the design of the prediction model, since these influence how the performance of machine 
learning algorithms is measured and compared as well as the importance given by the model 
to the different characteristics in the data and ultimately the best learning algorithm to apply. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Numerous learning techniques and algorithms exist for the implementation of classifiers, 
including support vector machines (SVMs) (Anaissi et al., 2016; Huang et al., 2018; Theera-Ampornpunt et al., 2016), artificial neural networks (ANNs) (Mobadersany et al., 2018; 
Zou et al., 2019), logistic regression (Staley et al., 2017; Wienbrandt et al., 2018), decision 
trees (Kretowski, 2019; Ludwig et al., 2018) and random forests (Acharjee et al., 2016; Ram 
et al., 2017), among others. However, this chapter will focus on SVMs, which can be briefly 
described as follows. When addressing a classification problem using this learning technique, 
each observation is represented as a point in an N-dimensional space, N being the number 
of features, and the objective of the support vector machine algorithm is to find the best 
hyperplane that distinctly separates points belonging to one class from the points belonging to 
the other. Hyperplanes are boundaries in the N-dimensional space can be used to discriminate 
the data points, which inevitably fall on either side of the hyperplane depending on the class 
each belongs to. The dimension of the hyperplane is also dependent on the dimensionality of 
the space, and hence on the number of features found in the training data. The hyperplane has 
a dimension of 1, i.e., it is a line, when the number of features is 2, i.e., when the space has 
two dimensions. If, instead, the space is 3-dimensional then the resulting hyperplane has two 
dimensions. Given any two classes of data points, there may be many possible hyperplanes that 
separate the two groups and the training algorithm is designed to find the best, namely, the one 
that imposes the greatest margin or the greatest possible distance between points in different 
groups.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This is illustrated in Fig. 2, which depicts a hypothetical scenario with two-dimensional 
data points, each with attributes x and y (i.e., the coordinate points in space), all distributed 
into two classes, namely red and blue. The solution support vector machine is the one that 
produces the hyperplane (i.e., a line, in this case) that best separates the members of the two 
categories and that thus serves as the decision boundary, i.e., anything on one side of the 
hyperplane is predicted to belong to one class while anything on the other side is predicted to 
belong to the other class. Analogous solutions are found through SVM training in problems 
with more than two dimensions or features. 
Section 5 presents a simple programming example for the training and evaluation of a 
support vector machine designed to predict the type of cancer to be developed by a patient 
based on this person’s gene expression. This is achieved using the programming tools introduced in Section 4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Python8 is a programming language used frequently for machine learning, data mining, data 
science, statistical computing, data analytics and scientific research, including in the biological 
sciences. It is arguably the most commonly used language in projects requiring data mining 
or machine learning and has become in recent years increasingly popular not only among 
computer and data scientists but also among biologists and statisticians, given its ease of use 
and smooth integration with other programming languages, such as C++, Java, SQL. For this 
reason, it is widely used by financial institutions and organizations for their internal research. In 
addition to these advantages, both Python and its most commonly used libraries and packages 
are free to download and use. What this means in practice is that both the language and most 
of its accompanying tools can be used for both academic and commercial purposes without 
having to pay for a usage licence. All these advantages make Python an attractive choice 
for any beginner wishing to get started with a powerful programming language regardless of 
the operating system used. Apart from its default features, Python allows the installation of 
packages. Each one of these is a structured collection of code, sometimes written in other 
programming languages, such as R and C, as well as documentation, and/or data for performing 
specific types of analyses. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[Section 5 presents an example of the training and evaluation of a classifier for cancer type 
prediction from the patient’s gene expression using Python and related libraries. In order to run 
the example, it is necessary to establish first a workstation in Python on a computer, which 
can be running any of the most common operating systems, such as Microsoft Windows, Mac 
OSX, or Ubuntu Linux. The easiest way to install both Python and the required libraries is by 
first installing Anaconda9
, which is a software package management system that facilitates the 
download and deployment of both Python and Python related packages and whose installer must 
be downloaded from its official website, which always contains the updated documentation and 
installation instructions for each operating system. The installer for Windows computers, for 
instance, is a .exe downloadable from the website and that must be executed on the destination 
machine using administrator rights. Generally, the default settings offered by the installer 
wizard should be used unless specific user requirements stipulate otherwise. For computers 
running other operating systems, similar instructions must be followed, and the up-to-date 
detail of these can be found in Anaconda’s official website. Once the installation of Anaconda 
is complete, this package manager must then be used in order to install the required libraries, 
namely scikit-learn, numPy and pandas, using the instructions provided in each package’s 
official website.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This section presents an example of a classification problem showing how a classifier can be 
trained to predict the category of various cancer types from several patients’ genomic data. The 
dataset was collected and published in the study conducted by Golub et al. (1999), which aimed 
at showing that cancer can be classified from a patient’s gene expression measured through a 
DNA microarray, i.e., the experiment shows that tumors can be categorized into previously 
established classes using an appropriately trained classifier. The investigation conducted by 
these authors consisted of the implementation and training of various models for the automatic 
classification of cancer tumors as either acute myeloid leukemia (AML) or acute lymphoblastic 
leukemia (ALL) and their full dataset is freely available to the general public from Kaggle13 and 
distributed in three files: data_set_ALL_AML_train.csv, containing the data for model training, 
data_set_ALL_AML_train.csv, containing the data for model testing, and actual.csv, containing 
the known labels of all patients in the study. Each line in the dataset consists of the expression 
of a single gene and each numbered column refers to a patient while the first two columns are 
the descriptors of the gene. The training data file contains the gene expression of patients 1 
through 38 whereas the test data file contains the gene expression of patients 39 through 72, 
with each of all these having containing 7,129 values. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[The classification example presented in this section is written in Python (version 2.7.16) 
using library scikit-learn (version 0.23) in order to implement a support vector machine (SVM) 
to make the cancer type prediction and the full code can be found in supplementary source 
file classification.py. 
14 The purpose of this example is to classify patients as either diagnosed 
with AML or ALL from their genomic data using the SVM model. The code is divided into 
six steps that can be briefly described as follows.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first line of the source file allow the import of the libraries required for the classification 
analysis to be performed. These are pandas and numPy, for data analysis and manipulation and 
sklearn (i.e., scikit-learn), which includes the machine learning algorithms to be used, namely, 
support vector machines. The lines of code that follow after this load the data from the input files, 
which, as described above, are divided into a training data file (data_set_ALL_AML_train.csv) 
and a test data file (data_set_ALL_AML_test.csv). As the names of these files clearly indicate, 
the former includes the data for the training of the classification model, a support vector 
machine in this case, whereas the latter contains the data to be used for evaluating the predictive 
performance of the models once the training of these has been completed. After this, the data 
found in these two files are merged into a single data structure, i.e., variable preprocessed_ 
dataset, after which feature scaling is applied in order to reduce the large variability observable 
in the gene expression data so that these are in a comparable range. Furthermore, principal 
component analysis is applied in order to reduce the dimensionality of the data from their original 
7,129 features to 38, with the latter figure having been identified as the one that explains around 
90% of the variance in the dataset.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Once the dataset has been fully pre-processed, the support 
vector machine model is trained. Prior to this, however, the best possible hyperparameters for
this model over these data are found first in order to optimize the learning process. This is 
what in machine learning and data mining is referred to as hyperparameter optimization or 
hyperparameter tuning and is fundamental for the training since these hyperparameters control 
the ability of the model to learn from the given data and there is no universal parameterisation 
that serves optimally in all situations. In classification.py this hyperparameter optimisation is 
achieved using function GridSearchCV from library scikit-learn. The optimal hyperparameters 
found are used to train the SVM model, which is subsequently evaluated in terms of various 
predictive performance metrics, namely, ROC AUC, F1, accuracy, precision, and recall. The 
confusion matrix is displayed, showing that the number of true positives is 8 and the number of 
true negatives is 4, whereas the number of false positives and negatives is 0 and 6, respectively. 
The performance metrics show that the model achieves a ROC AUC score of 0.7, which indicates 
that the SVM trained is effectively able to differentiate between cancer types, albeit within an 
error margin.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While classification analysis is one of the most common applications of data mining and data 
analysis in genomics there are others that can be briefly described as follows. One of the most 
recurrent of these is differential gene expression analysis (DGEA), i.e., the identification of genes 
that are significantly down- or up-regulated in an intervention, experimental condition when 
contrasted to a control, since this is fundamental for the understanding of phenotypic variation 
(Costa-Silva et al., 2017; Finotello and Di Camillo, 2015; Salentijn et al., 2003; Sulkava et al., 
2017; Zambonelli et al., 2016). More specifically, the purpose of a DGEA is the discovery of 
differentially expressed genes (DEG), i.e., those genes that exhibit statistically significant changes 
in their expression levels within a time period between two and more experimental conditions. 
Broadly speaking, DGEA consists of taking the normalized read count data and applying 
statistical tests in order to discover quantitative changes in expression levels between two or 
more experimental groups. Generally, DGEA consists of comparing the distribution of the gene 
expression data in one experimental condition, e.g., the samples taken from the control subject, 
against the distribution of corresponding data in the other condition, e.g., the intervention subject. 
No universal convention exists for determining when a gene is differentially expressed or not 
and this categorization may depend on the statistical methods used or the thresholds chosen, 
all of which there are many. In the same manner as in the methods used for the normalization 
of microarray data, different techniques for the detection of differentially expressed genes may 
lead to rather different results (Jaakkola et al., 2017; Rapaport et al., 2013).]]>
			</paragraph>
			<paragraph>
				<![CDATA[DGEA is commonly 
involved in investigations for the identification of a gene regulatory network (GRN), which 
refers to the set of regulatory interactions occurring between groups of genes and transcription 
factors (Davidson, 2010; Karlebach and Shamir, 2008). These interactions are fundamental for 
the activities played by the cell within the organism, including the morphogenesis. A GRN is 
frequently represented graphically as a directed graph indicating and quantifying how some 
groups of genes are up- or down-regulated by others. These networks can be inferred through 
the use of ordinary differential equations from the expression levels of individual genes or 
groups of genes (Carey et al., 2018). Investigations on this topic frequently attempt to identify 
GRNs in a variety of experimental settings, including those reflecting the organism’s response to 
a pathogen, such as the human immunodeficiency virus (HIV) (Song et al., 2018), the influenza 
virus (Carey et al., 2018) or the Bordetella pertussis bacterium (Deng et al., 2019).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Gene clustering analysis is another method crucially employed when analyzing genomic 
data. Its purpose, broadly speaking, is to identify clusters (i.e., groups) of genes with similar 
expression patterns over time under the same experimental condition (Carey et al., 2018). In 
other words, the main purpose is to divide the genes in such a way that those with similar 
expression patterns fall into the same cluster whereas those with different patterns fall into 
different clusters. It can be considered an exploratory or descriptive method since it does not 
depend on previously established hypotheses nor on predefined categories or labels, in contrast 
to classification analysis (Section 3). Nevertheless, the general assumption is that genes exhibiting 
similar expression behaviour over time are likely to be either co-functional, i.e., share a biological 
function, or are co-regulated, i.e., promote the regulation of others in the same group. Therefore, 
clustering is useful to initialize data-driven hypotheses that can be examined more closely with 
complementary methods, such as functional gene annotation. ]]>
			</paragraph>
			<paragraph>
				<![CDATA[These complementary methods are 
needed for a variety of reasons, one of which is the fact that there are many genes with unknown 
functions that may be discovered to be co-regulated with others with known functional roles. 
Clustering belongs to the type of techniques known as unsupervised learning, because it does 
not rely on previous knowledge about the data under examination, such as known categories 
or labels. Results obtained from clustering analysis may allow, for instance, to discover groups 
of genes that undergo similar a similar perturbation in response to a specific experimental 
condition. Some or many of the differentially expressed genes discovered during the DGEA may 
exhibit similar expression patterns over time and clustering analysis allows them to be grouped 
for further examination since they are likely to be playing related biological roles. Furthermore, 
this clustering reduces the dimensionality of the data for the subsequent identification of the 
gene regulatory network. The resulting clusters, however, may vary greatly in size. This is 
because while many co-expressed DEGs may effectively show the same temporal expressions 
patterns, others may be unique in their patterns (Carey et al., 2018). There are various types of 
clustering available in data mining, possibly the most commonly known of which is k-means, 
variants of which have been proposed for genomics analysis (Lam and Tsang, 2012). However, 
in genomics analysis it is common to use hierarchical clustering analysis (HCA) which, as the 
name clearly suggests, aims to identify gene clusters in an ordering or ranking, where the most 
similar of these are nested together from the bottom to the top (Qin et al., 2003). Examples 
of statistical analysis procedures in genomics that use hierarchical clustering include the one 
proposed by Carey et al. (2018) and others.]]>
			</paragraph>
			<paragraph>
				<![CDATA[After identification of the DEGs and GRNs, a gene set enrichment analysis (GSEA) or gene 
functional analysis is normally conducted. GSEA is the process of determining the functional 
roles of a group of genes by looking up their known annotations. This is frequently done in 
order to derive biological interpretations from genomics data, e.g., in order to determine the 
biological processes involving a set of genes that were previously identified as differentially 
expressed (Subramanian et al., 2005). GSEA comprises the use of various statistical methods in 
order to identify classes or groups of genes that are significantly over-represented in the given 
set and that are likely to be participating together in the same biological processes. Various tools 
have been proposed for GSEA (Chen et al., 2013, 2009), the most common of which include the 
Database for Annotation, Visualization and Integrated Discovery (DAVID).]]>
			</paragraph>
			<paragraph>
				<![CDATA[summary 
- Bioinformatics is the discipline that aims to devise methods of storage and analysis of 
biological data, such as sequences and gene expression. Given the large amounts of 
data that can be collected in a single study as well as the individual volumes of these, 
computational methods are required, mainly from branches of computer science, such 
as data mining as well as related areas, including machine learning, deep learning, data 
science and big data. 
- One of the most common types of data mining is classification analysis, whose 
applications in genomics include, among others, disease type prediction from gene 
expression data. An example of this is provided in Section 5. 
- Support vector machines are among the most powerful techniques for solving 
classification analysis problems and scikit-learn, for programming language Python, 
is one of the most widely used libraries for their implementation. Other classification 
techniques exist, including neural networks, random forests, decision trees, among many 
others. ]]>
			</paragraph>
		</content>
	</book>
	<book name="Practical Swarm Intelligence in Python">
		<content>
			<paragraph>
				<![CDATA[What is Swarm Optimization?
We have a problem. To solve the problem, we need to find the best set of something. The
something might be the parameters of a function best fitting a set of data, an arrangement of
cell towers, products on store shelves, circles in a square, the weights and biases of a small
neural network, or even the notes and durations of a melody. All of these are examples
found in this book. In general, we have a problem where we need to find the “best” set of
something from a broader set of possible somethings. How should we proceed?
If we can make the following two statements true:
- We have a problem where the set of possible solutions can be mapped in some way to
a position in a multidimensional space.
- We have a function defined everywhere in that multidimensional space such that the
value of the function is a proxy for the quality of the solution represented by that
position.
then we might hope to solve our problem by somehow searching through this space of
solutions seeking the best position as that will, we hope, map to the best solution to our
actual problem.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This is good; we have now reduced our problem to moving through a multidimensional
space looking for a particular position, the position with the best, usually minimum, function
value. How should we search this space?
One fairly obvious way to search the space is to pick points at equal intervals along each
axis and evaluate our function at those positions. The position leading to the best function
value will be our solution. We’ll note here that we can always define our function so smaller
is better; we’ll seek the global minimum of the function. We lose nothing by insisting on
this, so, henceforth, please assume all functions are to be minimized.
]]>
			</paragraph>
			<paragraph>
				<![CDATA[Searching by picking points at equal intervals is known as a grid search. It’s quite
systematic, and if the space to search is small in terms of the number of dimensions and
size in each dimension, then grid search might be a sensible thing to do. For example,
grid search is often used to optimize the C and γ parameters of a support vector machine
classifier.
However, with a bit of thought, we see a problem with this approach. As the dimensionality of the search space increases, the number of points to consider to claim a reasonable
search was performed increases far faster. This effect is quite similar to the curse of dimensionality that so plagued early machine learning mod]]>
			</paragraph>
			<paragraph>
				<![CDATA[Can we do better? Here’s a thought: let’s pick a point in the space at random and
evaluate the function there. Then, using some rule, choose a nearby point, and evaluate the
function at that point. If the nearby point has a smaller function value, we move to that
point. If not, pick another nearby point and try again.
If we do this repeatedly, we might hope to move, eventually, to a good position in the
space and thereby find a suitable solution to our problem. Indeed, if the space is convex
with only one minimum position, think of a bowl, then this process will succeed as we only
ever move closer and closer to the global minimum. Of course, it might be highly inefficient
compared to more sensible approaches, but it will work.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Excellent! We have an approach to solving our problem. Unfortunately, we don’t. Our
approach could fail because our space might have multiple minima. If it does, and we fall
into one, we’ll never move back up to get out and find a still better minimum.
Therefore, instead of picking just one point at a time, let’s scatter many points throughout the space and apply the same rule to each of them to search for the best position.
Scattering points throughout the search space and moving them according to some rule to
find the best position in the space is the essence of swarm optimization.
]]>
			</paragraph>
			<paragraph>
				<![CDATA[Swarm optimization falls under the larger heading of metaheuristics:
A metaheuristic is a high-level problem-independent algorithmic framework that provides a set of guidelines or strategies to develop heuristic
optimization algorithms. The term is also used to refer to a problem-specific implementation of a heuristic optimization algorithm according to the guidelines expressed in such a framework. ([1])
From this definition, it is clear we are using metaheuristics in the second sense, as a particular implementation of an algorithm to solve a specific problem. To be even more precise,
our goal is global optimization, we seek a single, best solution for a single function]]>
			</paragraph>
			<paragraph>
				<![CDATA[ When we use the word “swarm”, we
mean the following:
A swarm is a collection of agents, usually identical, moving through
a space guided by a set of rules governing their overall motion.
Here the set of rules means any set, from the empty set (no rules, pure randomness) to
highly ordered motion (like a grid search, no randomness).
This definition tells us what a swarm is, but not precisely what we mean by agents,
space, and rules.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This definition tells us what a swarm is, but not precisely what we mean by agents,
space, and rules. Nor does it tell us anything more about the mysterious function referred
to above. We need additional definitions, so let’s start with agent:
An agent is a conceptual entity representing a position in a multi-dimensional space. The agent may also possess other characteristics
and information, such as knowledge of its previous motion through
the space. In practice, our agents are vectors of continuous numbers representing a position in the
space.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s continue defining terms:
Space is multidimensional and (frequently) continuous. It is also often
bounded. The positions in space are representations of solutions to a
problem.
This definition assumes the mapping from a position in space to a problem solution exists.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Sometimes this mapping is obvious. If we want to find the minimum of a function over some
range, the space is the argument to the function itself. For example, if we have z = f(x, y)
and we want the minimum value, zmin, our space has two dimensions, x and y. However, in
most interesting cases, the number of dimensions is higher and the mapping less obvious.
Chapter 9 has us using a swarm to train a traditional neural network. In that case, the
space encompasses all the weights and biases of the network, so each position in the space
represents, quite literally, the actual weight and bias values of the network. The space here
is large, with nearly 2000 dimensions. Most of our experiments are less ambitious. For
example, our melody experiments use a space of 40 dimensions to represent a melody of 20
notes where dimensions are note and duration pairs.
]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s recap: we’ve taken our original problem, cast it as a space in which agents can
move, knowing that each position represents a potential solution to our problem. We know
now what we mean by a swarm, an agent, and a space. We’ve also hinted that we’ll use
a swarm of agents to search this space for an acceptable solution. This leaves two terms
to define: the set of rules governing the motion of the swarm of agents and the function
defined at every point in the space telling us how good a solution that point represents.]]>
			</paragraph>
			<paragraph>
				<![CDATA[First, the set of rules:
A set of rules governing the motion of a swarm of agents is a swarm
optimization algorithm. The algorithm uses the positions of the
swarm agents and other related information to decide how to move
each agent to a new position.
]]>
			</paragraph>
			<paragraph>
				<![CDATA[Algorithm 1 A swarm optimization search.
Input: An objective function, bounds, and initialization type
Output: The best position found by the swarm
Initialize the swarm
while not done do
Update the particle positions
Evaluate the new positions
if new global best position found then
Store the new global best
end if
Increment the iteration counter
end while]]>
			</paragraph>
			<paragraph>
				<![CDATA[We developed a simple swarm optimization algorithm above: scatter a set of agents
throughout the space and move them to new positions whenever an agent finds someplace
better than the place it currently is. This simple approach has a name, random optimization,
and we’ll develop it in detail in Chapter 3. Naturally, there are many other ways to move
the swarm through the search space; otherwise, there would be no point in this book. We’ll
get to these other ways for a select set of algorithms throughout all of Part I.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We have one definition remaining, the mysterious function defined everywhere in space
that tells us how good a solution a position represents:
The objective function is defined everywhere in space. The value
returned by the objective function, almost always a scalar, represents
the quality of the solution that position represents.
The word “optimization” implies something is being optimized, being refined, or made
better. The objective function, the formal name for the function we’ve been referring to
so far, is the thing we intend to optimize by finding its minimum value. Since we define
the mapping between the points of the multidimensional space and possible solutions to
the problem, we likewise need to define a suitable objective function so that the minimum
of the objective function truly represents the best solution to the problem. The objective
function needs to capture the problem’s essence, so the values returned correspond to better
or worse solutions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If we seek the minimum of a mathematical function over some range, then the function’s
value is the objective function. However, objective functions in swarm optimization can be
more complicated. The objective function for the experiment of Chapter 9 where we are
training a neural network is the network’s performance on the held-out test dataset. In
Chapter 11, our objective function is a set of measurements we intend to represent how
“nice” a melody sounds in terms of fidelity to the desired musical mode and the types of
intervals and note durations it contains. The objective function is the key to success in a
swarm search, so we need to develop or chose it carefully. There are many cases, however,
where the choice of the objective function is rather apparent. For example, we’ll use the
mean squared error between two values or sets of values more than once.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Algorithm 1 presents, in generic terms, the method employed by virtually all swarm optimization algorithms. It will fit on a t-shirt. What distinguishes one algorithm from another
is implementing the phrase Update the particle positions. Note, Algorithm 1 refers to the
agents as “particles.” This is common, and we’ll continue to use the word particle throughout the book, even for algorithms more traditionally associated with a genetic metaphor,
i.e., genetic algorithms. This handy abuse of terminology simplifies the presentation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If you peruse the literature on swarm optimization, you’ll run into two terms repeatedly:
exploration and exploitation. The former refers to the swarm’s wandering through the space
of possible solutions to the problem. The latter refers to the swarm’s deciding a particular
location in the solution space is worth a closer look. Different swarm algorithms seem to
favor one over the other or switch during the search from one mode to another, usually
from exploration to exploitation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When the search starts, the swarm as a whole knows nothing about the search space.
As the swarm evolves from iteration to iteration, one pass through the while loop of
Algorithm 1, it discovers more about the search space by its explorations. When a particularly promising location is found, many algorithms switch, usually implicitly, to a mode
of exploitation – the swarm focuses on that region of the search space to narrow down the
search and locate the best solution. For many algorithms, it is not unusual for the swarm
to collapse on itself in the vicinity of the best solution found. At that point, barring some
additional feature of the algorithm allowing for renewed exploration, the swarm is done; it
won’t find any place in the search space that might be better. We’ll see examples of this
collapse several times during our experiments.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As with most things in life, balance matters. One, often serious, issue with swarm
optimization is switching to exploitation mode too quickly – the swarm’s collapse. If not
identical, this is akin to becoming trapped in a local minimum of the objective function.
Many algorithms have some ability to avoid this effect. Perhaps part of the swarm focuses
on the promising region, but other parts of the swarm continue to explore if the grass is
greener somewhere else.
For example, we’ll work extensively with particle swarm optimization (PSO) and differential evolution (DE). PSO addresses the exploration versus exploitation balance by
adjusting parameters: c1, c2, and ω. Typically, c1 and c2 are fixed for the problem, and ω
is decreased as the search proceeds under the (hopeful) assumption that later iterations of
the swarm will have found a good place in the search space, and it makes sense to begin
exploitation. Still, parts of the swarm will continue to explore, and there are variations of
PSO that explicitly add a “repulsive” term to prevent premature exploitation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[By contrast, though useful and powerful, differential evolution is known for giving up
on exploration too quickly. If the objective function has a strong minimum that is easily
found or lacks many local minima, this tendency to converge quickly is a benefit – fewer
iterations are needed to find a good solution. However, balance again shows itself the better
approach, so there are variants of DE that balance exploration and exploitation by using
random components of the swarm regardless of the quality of their current location in the
search space.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In [2], global optimization algorithms are grouped into five categories where the category
description is my summary:
- “Mountaineer” The mountaineer’s goal is to climb a mountain, to reach the highest peak. Algorithms in this class only move towards the goal; they only move up,
never down – exploitation over exploration.
- “Sightseer” The sightseers traverse the search space looking for interesting places
to exploit by maintaining knowledge gained by the history of objective function evaluations.
- “Team” The team is a population algorithm, a group of individuals (agents) work
together to explore and exploit the search space.
- “Surveyor” The surveyor builds an approximate map of the search space, then
uses the map to select new regions to explore and map in finer detail.
- “Chimera” The chimera is a hybrid algorithm built from components of existing
algorithms.
]]>
			</paragraph>
			<paragraph>
				<![CDATA[The six algorithms we’ll work with in this book are random optimization (RO), particle
swarm optimization (PSO), Jaya, Grey Wolf Optimizer (GWO), genetic algorithm (GA),
and differential evolution (DE). The details of each algorithm are given in their respective
chapters, but we can group them according to the taxonomy above as follows:
Algorithm Category
Random Optimization “Mountaineer”
Particle Swarm Optimization “Team”
Jaya “Team”
Grey Wolf Optimizer “Team”
Genetic Algorithm “Team”
Differential Evolution “Team”]]>
			</paragraph>
			<paragraph>
				<![CDATA[I suspect you’ve noticed a theme in the assignments. Except for random optimization,
all of our algorithms fall into the “Team” category – a population of agents working together
in some manner to locate the best position in the search space.
Our implementation of random optimization uses a population of agents as well, but,
unlike all the other algorithms, the agents are blissfully unaware of each other. Individually,
they are all mountaineers, each moving only when a better position is located relative to
their current position.3 A supreme overlord watches the population to pick who is doing
the best. Still, that knowledge is cached; it is not used to influence the motion of the
individuals.
The taxonomy of [2] is useful, but less so to us, because it’s likely almost all swarm-based
algorithms fall into the “Team” category – that’s what makes them swarm-based.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Within swarm optimization, we can create a trivial categorization, not worthy of the
name “taxonomy”, by separating algorithms into two groups: those inspired by something in
nature and those that aren’t. Many such partitions use an animal or plant-based criterion, as
opposed to something based on physics. We’ll save the animal and plant-inspired algorithms
for Section 1.3.
Table 1.1, based on [3] with additions from [4], lists optimization algorithms involving
some process in physics. Building an optimization algorithm by simulating some aspect of
the physical world is a reasonable thing to do. After all, physics is the foundational science,
and physicists have spent centuries working to understand the basic principles involved.
Table 1.1 is for your reference, should you care to explore these algorithms. For this book,
we’ll declare these algorithms not to be nature-inspired.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Artificial physics algorithm Particle collision algorithm
Big bang-big crunch Rain water algorithm
Black hole Rain-fall optimization algorithm
Central force optimization Ray optimization
Charged system search River formation dynamics
Chemotherapy Science algorithm Self-driven particles
Colliding bodies optimization Simulated annealing
Electro-magnetism optimization Simulated raindrop algorithm
Fractal-based algorithm Sine cosine algorithm
Galaxy-based search algorithm Sonar inspired optimization
Gravitational search Space gravitational algorithm
Harmony search Spiral optimization
Hydrological cycle algorithm Stochastic difusion search
Intelligent water drop Thermal exchange optimization
Ions motion algorithm Vision correction algorithm
Integrated radiation algorithm Vortex search algorithm
Light ray optimization Water cycle algorithm
Mass and energy balances algorithm Water wave optimization
Optics inspired optimization Weighted attraction method]]>
			</paragraph>
			<paragraph>
				<![CDATA[Since we are restricting the term “nature-inspired” to algorithms based on the behavior
of animals and plants, let’s see what label to use for our algorithms:
Algorithm Nature-inspired?
Random Optimization No
Particle Swarm Optimization Yes
Jaya No
Grey Wolf Optimizer Yes
Genetic Algorithm Yes
Differential Evolution Yes
Something called Grey Wolf Optimizer is likely to be nature-inspired, and it is. Particle
swarm optimization is often said to be inspired by the flocking of birds. Both the genetic
algorithm and differential evolution are, not surprisingly, based on evolution. As stated
above, random optimization is a hill-climbing algorithm, not at all nature-inspired. Jaya,
likewise, does not claim inspiration from nature. Therefore, we have a healthy mix of
algorithms for our experiments.]]>
			</paragraph>
			<paragraph>
				<![CDATA[teBefore we wander too far along the path, I feel a need to pause and make a few observations
on the plethora of new “nature-inspired” algorithms now on the market. To be clear, if an
algorithm offers something substantive, something that previous algorithms lack in some
way, then I’m all for it. However, that does not seem to be the case with what we’ve decided
to label as “nature-inspired” algorithms, meaning those distinct from algorithms seeking to
emulate some physical process (e.g., simulated annealing).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Wait, you say, Section 1.2 makes it clear that you, dear author, are willing to use nature-inspired algorithms. True, this book discusses several, starting with one of the first, particle
swarm optimization. However, except for the Grey Wolf Optimizer and Jaya, the algorithms
selected for inclusion are well-proven and battle-hardened. Also, Jaya makes no claims to
be inspired by nature. In that sense, the only new, nature-inspired algorithm we’ll explore
is the Grey Wolf Optimizer.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The following is my take on the current state of affairs; your mileage will, of course,
vary. However, I’m not alone. I point you to [5]. The introduction frames the situation
precisely and plainly. I recommend you spend a little time with that paper.
Figure 1.1 shows the number of Google Scholar results by year to the search: nature-inspired optimization metaheuristic. These results include five books published since 2016:
[6], [7], [8], [9], and [10]. Of these five books, three were published as recently as 2020.
Interest in nature-inspired algorithms is indeed high and growing. This interest is a good
thing, but that doesn’t mean the ever-growing list of nature-inspired algorithms necessarilyis.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is difficult to believe all of these algorithms are unique or sufficiently distinct
from existing algorithms that their publication is warranted.
That the algorithms of Table 1.2 “work” I have no doubt. The usual formula for a
new swarm algorithm paper includes testing the algorithm against standard test functions
(Section 8.1) and a select handful of existing algorithms, usually including at least canonical
PSO. The typical result is something along the lines of “our new <insert-nature-inspired-name-here> algorithm is <better|competitive|comprable> in these limited test cases to
existing algorithms.” While no doubt true, I find such a conclusion unsatisfying and, to be
frank, unhelpful in the long run. Are we really to accept that there is something about the
Salp Swarm algorithm that is fundamentally better than PSO or one of its variants? Given
that no algorithm will always be best at highly diverse tasks, why even report a competitive
result? I have nothing against the salp swarm algorithm; it was selected randomly as the first
name I noticed when I glanced at the list in Table 1.2. But, even if the nature-inspiration is
valid, why would we expect salp motion to be better at searching a space than the flocking
of birds when the same rules of evolution produced both?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Thoughtful comments from [11] are in order here:
Are recent nature-inspired algorithms novel? Yes and no. On the one hand,
most (but certainly not all) of the algorithms reviewed in this paper ([11]) are
distinct from existing optimisation algorithms, and given a particular search
space, they would likely follow different trajectories to existing algorithms. On
the other hand, many of these algorithms use variants of well-established meta-heuristic concepts that are also found in existing metaheuristic frameworks such
as PSO, EAs and local search. Furthermore, the analysis of PSO-style algorithms shows that many of their underlying ideas have also been explored by
the more mainstream PSO community.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Should this trend of inventing and publishing nature-inspired algorithm after nature-inspired algorithm continue, we might run out of obvious names. I recommend researchers
wishing to continue the trend go prehistoric. Indeed, there’s a certain pleasant ring to
“Trilobite Swarm Optimization,” a name I offer to the community as a candidate. Indeed,
given their long successful run and vast numbers of fossils, swarms of trilobites effectively
plied the Cambrian and Ordovician seas, locating food sources while avoiding hostile predators like Anomalocaris. Might an algorithm combining the near-mindless wanderings of a
trilobite with primitive evasion strategies to avoid fast swimming Anomalocaris’ overhead be
a useful analogy for searching a space of solutions? I leave the implementation to interested
readers, but please, don’t publish your results.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Tongue-in-cheek names and mild ranting aside, I believe there is yet much to discover
in this field, perhaps less in seeking inspiration from nature, at least at the level of animal
behavior, and perhaps more from enhancing the intelligence of the agents combined with
existing search strategies. Hybrid algorithms, which we do not discuss in this book because
of space constraints, also seem promising, at least from my testing, to say nothing of the
mountain of literature references. A hybrid algorithm combines aspects of one or more
swarm algorithms and is categorized as a chimera according to [2].
Let’s get started with the rest of the book. You’ll learn key swarm optimization algorithms, how they work, and how to create reference implementations you can use or modify
for your own tasks. With the intuition gained through the experiments of Part II, I believe you’ll be in a good position to effectively evaluate and assess the utility of existing
algorithms and the many new ones that will inevitably appear in the years to come.]]>
			</paragraph>
			<paragraph>
				<![CDATA[SWARM ALGORITHMS
African buffalo optimization Grasshopper optmisation algorithm
Alienated ant algorithm Great salmon run
Ant colony optimizer Grey wolf optimizer
Ant lion optimizer Harris hawks optimization
Artificial bee colony Invasive weed optimization
Artificial root foraging algorithm Jaguar algorithm
Bacterial foraging Japanese tree frogs calling
Bacterial-GA foraging Keshtel algorithm
Bat algorithm Killer whale optimization
Bee colony optimization Krill herd
Bee hive Lion optimization algorithm
Bees algorithm Locust swarm algorithm
Bees swarm optimization Marriage in honey bees
Bee system Monkey search
Bottlenose dolphin optimization Moth-Flame optimization algorithm
Bumblebees Opt bees
Cat swarm Owl search algorithm
Chicken swarm optimization Paddy field algorithm
Coral reefs optimization algorithm Queen-bee evolution
Coyote optimization algorithm Red deer algorithm
Cricket algorithm Roach infestation algorithm
Crow search algorithm Salp swarm algorithm
Cuckoo search Shark smell optimization
Cuttlefish algorithm Sheep shepherding algorithm
Dolphin echolocation Shuffled frog leaping algorithm
Dragonfly algorithm Sperm whale algorithm
Dynamic virtual bats algorithm Spotted hyena optimizer
Eagle strategy Squirrel search algorithm
Egyptian vulture algorithm Swine flow optimization algorithm
Elephant search algorithm Termite colony optimization
Emperor penguins colony Tree growth algorithm
Fast bacterial swarming algorithm Virtual ant algorithm
Firefly algorithm Virtual bees
Fish swarm school Virus colony search
Flower pollination algorithm Weightless swarm algorithm
Fruit fly optimization Whale optimization algorithm
Glowworm swarm optimization Wolf search]]>
			</paragraph>
			<paragraph>
				<![CDATA[A swarm optimization search follows a set algorithm as we saw in Chapter 1. In that
chapter, we introduced our general algorithm, here reproduced as Algorithm 2 for easy
reference.
The inputs to the search are,
- The objective function, the thing that lets us know how well we are doing.
- The bounds for the search, including the number of dimensions our objective function
is expecting along with the ranges allowed for each of those dimensions.
- How we’ll initialize our swarm, which includes the type of initialization and the number
of particles,
- And how we’ll know when we are done searching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Algorithm 2 A swarm optimization search.
Input: An objective function, bounds, and initialization type
Output: The best position found by the swarm
Initialize the swarm
while not done do
Update the particle positions
Evaluate the new positions
if new global best position found then
Store the new global best
end if
Increment the iteration counter
end while]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s pick an example to make things less abstract. We have a function of two variables,
z = f(x, y), and we’re looking for the minimum value of this function in the range [0.01, 1] for
both x and y. In other words, we seek two numbers, xm and ym, such that zm = f(xm, ym)
is as small as possible for 0.01 ≤ xm, ym ≤ 1.
We already know two things: we have a two-dimensional search space, and we know the
bounds on the dimensions, [0.01, 1]. What we have left to select, aside from the type of
search algorithm – the Update the particle positions part of Algorithm 2 – is the objective
function, the number of particles in the swarm, the type of swarm initialization, and what
it means for the search to be done.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The objective function measures the quality of each particle, i.e., how good of a solution
it represents. For our example, the value of f(x, y) for a given x and y is the objective
function since we seek the minimum value of this function. Notice that we are making no
assumptions as to what f(x, y) is. It could be a simple algebraic function, or it could be
a complex multistep algorithm in its own right. All we know or care about at this point
is that f(x, y) accepts two numeric inputs and returns a numeric output. This is already
a step better than traditional optimization as we don’t need to know of nor require the
existence of any derivatives of f(x, y).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The larger the swarm, the more computation needs to be done, but we might also expect
to find the solution more quickly. However, this is not always the case; sometimes, it is
better to use a smaller swarm and search more by using more iterations. In general, a
modest-sized swarm is a good place to start, say in the range of 10 to 100 particles. The
computational overhead of the objective function plays into selecting the swarm size. If the
objective function can be evaluated quickly, we might choose a slightly larger swarm or more
iterations of the swarm. However, if the objective function is computationally expensive,
we might go with a smaller swarm and hope for quicker convergence to the solution or use
tighter bounds to reduce the size of the search space.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We’ll use N particles for the swarm. This means that our swarm consists of N two-dimensional vectors represented in Python as a Nx2 NumPy matrix of positions where each
row of the matrix is a particle, and each column of the matrix is the position of the particle
along that dimension. As we’ve stated before, the goal of the search is to find the best
position within this space where best means we can use the particle position to construct a
solution that best solves our problem.
For our running example, the swarm represents candidate positions, candidate values of
x and y. The search moves the swarm through the two-dimensional search space evaluating
candidate positions as it goes to try and find the best x and y possible to minimize f(x, y)
subject to the boundary condition of 0.01 < x, y < 1. The difference between types of
swarm algorithms is how they move the particles through the search space.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We have the objective function, the swarm size and dimensionality, and the search
bounds. The next step is to initialize the swarm. Think of this as scattering the particles
throughout the search space in some manner. The most obvious, and often wholly satisfactory, approach, is to scatter the particles randomly within the bounds of the search space.
So, for our example here, our initialization step is to assign each particle to randomly selected values in the range 0.01 < xi
, yi < 1 where i refers to the i-th particle in the swarm.
We’ll encounter two other initialization approaches later in the chapter.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The final choice we have to make is when to stop searching. Typically, we use two
criteria. The first is to set an upper limit on the number of iterations. Here an iteration
is a pass through the while loop of Algorithm 2. The second is to test the best position
the swarm currently knows of, and if it is within some tolerance, we call it a day and
stop searching. For our minimization of f(x, y), we set an iteration maximum of M and a
tolerance of θ. Therefore, after each swarm position update and evaluation of the objective
function for each new particle position, we check to see if we’ve performed the maximum
number of steps or if our f(xbest, ybest) is below our threshold, θ. Recall, the framework
always minimizes the objective function. This is not a difficulty in cases where we desire to
maximize. In those cases, we negate the objective function value so maximization is now
minimization]]>
			</paragraph>
			<paragraph>
				<![CDATA[Algorithm 2 has the step: Update the particle positions. This is the part where the
particular swarm optimization algorithm comes into play. Each does this in its own way, as
we will see in later chapters where we develop the optimization classes. However, the general
form of a search does not change: we still initialize a swarm and step through updating
and evaluating it until we’re done returning the best position found by the swarm as our
solution.
The framework is a collection of classes we’ll pass to the individual swarm algorithms.
The classes encapsulate concepts like initialization, boundaries, and objective functions.
We’ll also define ancillary classes used by particular swarm optimizations, like the inertia
classes used by the particle swarm optimization (PSO) class of Chapter 4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[To set up a swarm optimization problem, then, we’ll use the framework classes to create
objects passed to an instance of the desired swarm algorithm. We run the search by calling
the Optimize method of the swarm object.
Briefly, here are the framework components (classes) developed in this chapter,
- Objective - the objective function used by the swarm. We’ll typically implement this
class from scratch.
- Bounds - the boundary conditions used by the swarm and by the Initializer.
- Initializer - the particular initialization method used by the swarm. We use this
functionality to explore the effect of different initialization approaches.
- Done - the concept of “done”. If not used, each swarm algorithm counts iterations
and, optionally, looks for a tolerance to be met.
- Inertia - the inertial update method used by PSO. This is particular to particle swarm
optimization. It can be used to explore the effect of different inertia schedules.
Let’s develop the components of the framework beginning with the objective function.]]>
			</paragraph>
			<paragraph>
				<![CDATA[class Objective:
def __init__(self):
pass
def Evaluate(self, pos):
pass
Figure 2.1: The most basic Objective class.
]]>
			</paragraph>
			<paragraph>
				<![CDATA[The objective function is key to successfully applying swarm optimization. It is also the
thing most tailored to the particular task at hand. We saw above how the objective function
might be as simple as evaluating an algebraic function. However, we’ll see complex objective
functions later in the book that bear no resemblance to an algebraic function.
The goal of the objective function is to measure the quality of each particle position, each
possible solution to the problem. The objective function is assumed to return a floating-point value where smaller is better.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In code, the objective function class is quite simple. A skeleton of one is presented in
Figure 2.1. There are only two methods we need to fill in. The constructor (__init__)
can be used to pass any extra information to the objective function when initialized. For
example, in Chapter 8, we’ll experiment with nonlinear curve fitting. In that case, we’ll
pass the sampled data points we want to fit to the objective function via the constructor.
The most important method is Evaluate. This method is called for each particle on
each pass through Algorithm 2. The single argument to Evaluate is a particle position
vector. In the example of the previous section, where we sought to minimize f(x, y), pos
would be a two-element NumPy vector where pos[0] is x and pos[1] is y. The Evaluate
method’s job is to return a single number indicating how good of a solution pos represents.
Let’s define f(x, y) = xy going forward. The Objective class becomes,
class Objective:
def Evaluate(self, pos):
return pos[0] * pos[1]
where we need not define the constructor explicitly as we are passing no information used by Evaluate.]]>
			</paragraph>
			<paragraph>
				<![CDATA[class Bounds:
def __init__(self, lower, upper, enforce="clip"):
self.lower = np.array(lower)
self.upper = np.array(upper)
self.enforce = enforce.lower()
def Upper(self):
return self.upper
def Lower(self):
return self.lower
def Limits(self, pos):
1: npart, ndim = pos.shape
2: for i in range(npart):
if (self.enforce == "resample"):
3: for j in range(ndim):
4: if (pos[i,j] <= self.lower[j]) or
(pos[i,j] >= self.upper[j]):
pos[i,j] = self.lower[j] +
5: (self.upper[j]-self.lower[j])*
np.random.random()
else:
6: for j in range(ndim):
if (pos[i,j] <= self.lower[j]):
pos[i,j] = self.lower[j]
if (pos[i,j] >= self.upper[j]):
pos[i,j] = self.upper[j]
pos[i] = self.Validate(pos[i])
return pos
def Validate(self, pos):
return pos
Figure 2.2: The Bounds class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We said above we’ll reimplement the Objective class to tailor it to each problem. The
Bounds class will most often be used as-is or subclassed. The purpose of the Bounds class
is to ensure swarm positions are kept within a set range.
The Bounds class sets the upper and lower values for each dimension of the problem.
It also handles checking of these limits and allows, in subclasses, for an extra step we’ll use
to ensure that not only are values for a particular dimension within the set bounds, they
are also valid in terms of what we expect that dimension to represent. For example, we
may want to enforce integer values for particular dimensions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Bounds class is shown in Figure 2.2. The constructor expects two arguments,
lower and upper. These are lists or NumPy vectors with the lower and upper limits for
each dimension, respectively. The optional third argument decides what to do if a particular
particle’s position along a dimension is out of bounds. The default is to clip, to set the
dimension to the lower or upper limit. If, however, it is set to “resample”, a randomly
selected value within the lower and upper limit replaces the offending value. All of this
checking is done by the Limits method. Note, these methods are called by the swarm
algorithm classes; we need not call them ourselves. The Upper and Lower methods are
used to return the upper and lower limits passed in via the constructor. The Validate
method by default does nothing beyond returning its argument. This is the method we’ll
sometimes override in a subclass to enforce special requirements on particle values.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s look more closely at the Limits method in Figure 2.2. Its argument, pos, is a
matrix representing the current positions for each particle in the swarm. First, we return the
number of particles (rows of pos) and the number of dimensions for each particle (columns
of pos) (1). We then loop over the particles to process them individually (2). How
each particle is processed depends on whether we’re clipping or resampling. In either case,
we loop over the dimensions of the particle (3) (6). If clipping, we ask if the current
particle’s current dimension (pos[i,j]) is less than or greater than the limits for that
dimension. If so, we set the value to the respective limit. If we’re resampling, and we find
a particle dimension that exceeds its bounds (4), we replace that value by a randomly
selected one that is within limits (5). Regardless of whether we’re resampling or clipping,
after processing the limits on the current particle’s position, we call Validate, which, by
default, does nothing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Returning to our running example for this chapter, if we want to minimize f(x, y) over
x and y in the range [0.01, 1], we create a Bounds object like so,
bounds = Bounds([0.01,0.01],[1,1])
where we set the lower and upper limits on x and y to be 0.01 and 1.0, respectively. Note,
we are implicitly taking the default action to clip particle positions to the lower or upper
limit if they go out of bounds.
The Bounds class is used by the swarm algorithms and by the initializers, to which we
now turn.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first step in Algorithm 2 says Initialize the swarm. This is the part where we set up our
initial particle positions within the search space. We already alluded above to one possible
way to do this: by randomly scattering the particles within the bounds of the search space.
As far as our framework is concerned, whatever object we pass to the swarm algorithms
as an initializer needs to support a constructor that accepts the number of particles in the
swarm, the dimensionality of the particles, and an optional Bounds object to set the limits
on the initial positions. It also needs to have an InitializeSwarm method that takes no
arguments, but returns a NumPy matrix representing initial particle positions. This matrix
has as many rows as there are particles in the swarm and as many columns as there are
dimensions in the search space. This is the matrix that the algorithms will evolve to search
for the best solution to our problem.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We’ll define three initializers here and illustrate how they work graphically for a two-dimensional search space. The three are RandomInitializer, QuasirandomInitializer,
and SphereInitializer. We’ll experiment with these in later chapters. The first does
what you might expect; it scatters the swarm particles randomly within the bounds of the
search space. The second is similar, but instead of a pseudorandom generator, it uses a
quasirandom generator. Quasirandom generators are space-filling, meaning they distribute
the particles more uniformly throughout the search space. The SphereInitializer
places the particles on the edge of a hypersphere bounded by the search space. The idea
here is to put the particles near the edges so that the likely solution is within the hyper-sphere.]]>
			</paragraph>
			<paragraph>
				<![CDATA[class RandomInitializer:
def __init__(self, npart=10, ndim=3, bounds=None):
self.npart = npart
self.ndim = ndim
self.bounds = bounds
def InitializeSwarm(self):
if (self.bounds == None):
self.swarm = np.random.random((self.npart, self.ndim))
else:
self.swarm = np.zeros((self.npart, self.ndim))
lo = self.bounds.Lower()
hi = self.bounds.Upper()
for i in range(self.npart):
for j in range(self.ndim):
self.swarm[i,j] = lo[j] +
(hi[j]-lo[j])*np.random.random()
self.swarm = self.bounds.Limits(self.swarm)
return self.swarm
Figure 2.3: The RandomInitializer class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The code for RandomInitializer is in Figure 2.3. The constructor (__init__)
accepts the number of particles in the swarm (npart), the dimensionality of the search
space (ndim), and, optionally, a Bounds object to define the limits of the search space.
These values are stored in the instance for use by InitializeSwarm.
Initialization happens when InitializeSwarm is called. If no Bounds object was
given, initialization is particularly simple; we just return a NumPy matrix of random values
in the range [0, 1). This matrix has npart rows and ndim columns.
If a Bounds object was given, we first define the matrix representing the swarm, extract
the per dimension lower (lo) and upper (hi) bounds from the Bounds object and then
loop over particles and the dimensions of the particles selecting random values in the bounds
for each.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The call to Limits seems superfluous at first since we just finished selecting values
for the swarm particles that we know are within the limits given by the Bounds object.
However, this is not the whole story. If we look back at Figure 2.2, we see that the Limits
method, after checking that the entire swarm is in bounds, calls Validate. If the object
passed to RandomInitializer is a subclass of Bounds, it is possible that the subclass
implements this method. Hence, the call to Limits ensures that Validate will be called
on the newly initialized swarm.
The two other initializer classes, QuasirandomInitializer and SphereInitializer,
implement the same methods as RandomInitializer but tailor the assignment of values.
Let’s start with QuasirandomInitializer as shown in Figure 2.4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A Halton process can generate a quasirandom sequence. This process is implemented in
Figure 2.4 in the Halton method. This method returns the i-th Halton number for the given
base, b.
1 Halton sequences work best when the bases are primes, so in __init__ we set
up a table to have a different prime for each dimension (self.primes). The constructor
accepts the number of particles (npart), dimensionality (ndim), and a Bounds object as
before. It also optionally accepts an initial index into the Halton sequence (k) and a jitter
value (jitter). The jitter value is used to slightly adjust the actual value returned by the
Halton sequence to prevent each initialization from being the same. By default, we don’t
use jitter and start the Halton sequence from one.]]>
			</paragraph>
			<paragraph>
				<![CDATA[class QuasirandomInitializer:
def Halton(self, i,b):
f = 1.0
r = 0
while (i > 0):
f = f/b
r = r + f*(i % b)
i = floor(i/float(b))
return r
def __init__(self, npart=10,ndim=3,bounds=None,k=1,jitter=0.0):
self.npart = npart
self.ndim = ndim
self.bounds = bounds
self.k = k
self.jitter = jitter
self.primes = [
2, 3, 5, 7, 11, 13, 17, 19, 23, 29,
31, 37, 41, 43, 47, 53, 59, 61, 67, 71,
73, 79, 83, 89, 97,101,103,107,109,113,
127,131,137,139,149,151,157,163,167,173,
179,181,191,193,197,199,211,223,227,229,
233,239,241,251,257,263,269,271,277,281,
283,293,307,311,313,317,331,337,347,349,
353,359,367,373,379,383,389,397,401,409,
419,421,431,433,439,443,449,457,461,463,
467,479,487,491,499,503,509,521,523,541,
547,557,563,569,571,577,587,593,599,601,
607,613,617,619,631,641,643,647,653,659]
def InitializeSwarm(self):
self.swarm = np.zeros((self.npart, self.ndim))
if (self.bounds == None):
lo = np.zeros(self.ndim)
hi = np.ones(self.ndim)
else:
lo = self.bounds.Lower()
hi = self.bounds.Upper()
1: for i in range(self.npart):
for j in range(self.ndim):
h = self.Halton(i+self.k,self.primes[j %
len(self.primes)])
q = self.jitter*(np.random.random()-0.5)
2: self.swarm[i,j] = lo[j] + (hi[j]-lo[j]) * h + q
if (self.bounds != None):
self.swarm = self.bounds.Limits(self.swarm)
return self.swarm
Figure 2.4: The QuasirandomInitializer class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[class SphereInitializer:
def __init__(self, npart=10, ndim=3, bounds=None):
self.npart = npart
self.ndim = ndim
self.bounds = bounds
def InitializeSwarm(self):
self.swarm = np.zeros((self.npart, self.ndim))
if (self.bounds == None):
lo = np.zeros(self.ndim)
hi = np.ones(self.ndim)
else:
lo = self.bounds.Lower()
hi = self.bounds.Upper()
radius = 0.5
for i in range(self.npart):
p = np.random.normal(size=self.ndim)
self.swarm[i] = radius + radius* p / np.sqrt(np.dot(p,p))
self.swarm = np.abs(hi-lo)*self.swarm + lo
if (self.bounds != None):
self.swarm = self.bounds.Limits(self.swarm)
return self.swarm
Figure 2.5: The SphereInitializer class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The InitializeSwarm method creates the swarm matrix and gets the bounds from
the Bounds object or uses [0, 1) if no Bounds object is given. All the action happens in
the dual loops (1). We loop over the particles (i) and the dimensions (j) where for each
particle and dimension, we get the next value of the Halton sequence and any jitter (q).
The actual swarm position is set as a random value in the range for that dimension (2).
Note, like a pseudorandom generator, the Halton sequence is bounded to [0, 1), so we can
use it directly as we did for the random case (see Figure 2.3). Finally, if we have a Bounds
object, we call Limits to let the Validate method run.
The SphereInitializer selects random points on an ndim-dimensional hypersphere.
To select a p-dimensional vector on a hypersphere, we need to select p values from a Gaussian distribution and scale them by the norm of the resulting vector.2 The code for the
SphereInitializer class is in Figure 2.5.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The constructor is identical to that of the RandomInitializer. All the fun happens
in the InitializeSwarm method. After selecting the bounds, we set radius = 0.5 to
construct the set of points in [0, 1). After all swarm points have been sampled, we’ll scale
them from [0, 1) to the bounds limits in lo and hi. This sets the size of the hypersphere
so that it will fit within the bounds of all dimensions. We build the swarm, one particle
position vector at a time using the loop over npart. First, we select the vector, p, of ndim
dimensions, from a Gaussian with zero mean and standard deviation of one. We then apply
the hypersphere transformation scaling by the radius. Note, we add in the middle point,
0.5, to shift the hypersphere to center on 0.5. Finally, we get the actual particle positions
by scaling the [0, 1) positions by the per dimension range (np.abs(hi-lo)) and add in
the lower bounds. As before, if we have a Bounds object, we call Limits before returning
the newly initialized swarm matrix.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s trade code for plots to see what these initializers are doing. We’ll initialize swarms
of 100 particles in two dimensions so we can plot them. The way to get the initialized
swarms is straightforward,
>>> import numpy as np
>>> from RandomInitializer import *
>>> from QuasirandomInitializer import *
>>> from SphereInitializer import *
>>> r = RandomInitializer(npart=100, ndim=2).InitializeSwarm()
>>> q = QuasirandomInitializer(npart=100,ndim=2).InitializeSwarm()
>>> s = SphereInitializer(npart=100, ndim=2).InitializeSwarm()
Each of the swarms (r, q, and s) is a 100x2 matrix. We plot the particle positions using
the first column as the x-coordinate and the second as the y-coordinate to show us how the
initial particles are scattered throughout the two-dimensional search space. This leads to
Figure 2.6.
On the left, we see the randomly initialized swarm. It looks pretty random, as it should.
We also see that it is not uniform over the two-dimensional search space. There are areas
of concentrated particles and voids. In the middle of Figure 2.6, we see the quasirandomly
initialized swarm. Here the distribution of particles is much more uniform. Finally, on
the right, we see the spherically initialized swarm. All of the particles are on a 2D sphere
(circle) within the default [0, 1) bounds.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Each swarm algorithm knows how to stop the search when the maximum number of iterations through Algorithm 2 has been reached or when the objective function value for
the best particle position known by the swarm is below a given threshold. However, if the
swarm algorithm was passed an instance of a class supporting a Done method, it calls the
method passing in information about the state of the search. The Done method returns a
boolean value. If it returns True, the search is completed. If it returns False, the search
continues. The information passed to the Done call includes the list of current swarm best
positions found and their associated objective function values. It also includes the current
position of each swarm particle, the maximum number of iterations set, and the current iteration value.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For the majority of our experiments, we’ll simply use the intrinsic checks inside the
swarm algorithms to decide if the search is done or not. An exception is when we explicitly
iterate through the swarm algorithms by calling the Step method. In those cases, we’ll
call the Done method directly. We’ll see exactly what that means when we discuss the
individual swarm algorithms in later chapters.
We’ll end this chapter with an example showing how to use the framework to set up
and execute an optimization search, but, before we do that, let’s take a quick look at the
last part of the framework, the inertia class used by PSO.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While we do not yet know the details of the PSO class, we know it includes a parameter, the inertia, which is generally changed during the search. Historically, this value is
changed linearly with increasing iteration number so it gets smaller the longer the search
has been run. We’ll define two inertia classes for PSO, though it is straightforward after
seeing how they are implemented to design your own. These are the LinearInertia and
RandomInertia classes.
The purpose of the inertia objects is to return a single floating-point number in the
range [0, 1], which is typically labeled w or ω in the PSO literature. We’ll see in Chapter 4
precisely what this ω represents, but for now, we’ll content ourselves with the interface the
PSO class expects from any inertia object passed to it.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The LinearInertia class is,
class LinearInertia:
def __init__(self, hi=0.9, lo=0.6):
if (hi > lo):
self.hi = hi
self.lo = lo
else:
self.hi = lo
self.lo = hi
def CalculateW(self, w0, iterations, max_iter):
return self.hi - (iterations/max_iter)*(self.hi-self.lo)]]>
			</paragraph>
			<paragraph>
				<![CDATA[The constructor accepts upper (hi) and lower (lo) limits for ω, both scalar values. A
bit of code ensures that hi ≥ lo. The main method of the class is CalculateW, which
the PSO class calls. The arguments are w0, an initial ω value defined when the PSO object
is created, the current number of swarm iterations completed (iterations), and the
maximum number of iterations (max_iter). For LinearInertia, we want ω to decrease
linearly from hi to lo over max_iter iterations. The code in CalculateW does this
by subtracting an ever increasing fraction of the difference between hi and lo from hi so
when the search starts, ω is hi and when the search ends, ω is lo. Typical literature values
for hi and lo are 0.9 and 0.5, respectively. If you are familiar with neural networks, ω acts
in much the same way as momentum during gradient descent.
If, instead of a constantly decreasing ω, we want a random value, we can use the
RandomInertia class. Structurally, it is identical to LinearInertia except for the
actual equation used to return ω in CalculateW. In this case, the return value is,
which returns a random value in the range [0.5, 1.0).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Throughout this chapter we’ve made use of a running example, that of finding the minimum
of f(x, y) = xy for x and y in [0.01, 1]. Now that we have a developed framework to support
the swarm optimization algorithms, let’s set up and run a search using the yet-to-be-defined
PSO class. Of course, given our bounds on x and y, we already know the answer we should
get: x = 0.01 and y = 0.01. Let’s see if the PSO algorithm agrees.
To set up the problem, we need an objective function class, an instance of the Bounds
class, an initializer (we’ll use RandomInitializer), and, since we’re using PSO, an inertia
object. We’ll use the LinearInertia class. We’ll list the code in pieces. The actual code
is in the file fxy.py in the source code available with this book.]]>
			</paragraph>
			<paragraph>
				<![CDATA[First, we import NumPy and the framework components,
import numpy as np
from PSO import *
from LinearInertia import *
from Bounds import *
from RandomInitializer import *
Next, we define our objective function class and make an instance of it,
class Objective:
def Evaluate(self, pos):
return pos[0]*pos[1]
obj = Objective()
where we see how simple an objective function can be.
We now define the parameters of the search, including the bounds, initializer, and inertia,
npart = 10
ndim = 2
m = 100
tol = 1e-4
b = Bounds([0.01,0.01], [1,1])
i = RandomInitializer(npart, ndim, bounds=b)
t = LinearInertia()
where npart is the number of particles in the swarm, ndim is the number of dimensions for
each particle, here two because we are looking for x and y to minimize f(x, y).]]>
			</paragraph>
			<paragraph>
				<![CDATA[We set the
maximum number of iterations to 100 (m) and the tolerance to 0.0001 (tol). Recall, this
means that the search will run for at most 100 swarm updates or stop if the best position
found by the swarm returns an objective function value less than 0.0001.
We then define the bounds for the problem, b, to keep both x and y inside [0.01, 1].
The first argument to Bounds is a list of the lower limit for each dimension, the second
is for the upper limit. Next, we set up the random initializer (i). We tell it how many
particles (npart), how many dimensions for each (ndim), and pass in the bounds instance
so it knows the range of values it can use. Finally, t defines the default linear inertia
object which will start the PSO ω parameter at 0.9 and decrease it linearly to 0.5 over the
iterations of the search.
We are now ready to define the actual PSO swarm and run the optimization. The code
required is particularly straightforward given the framework,
swarm = PSO(obj=obj, npart=npart, ndim=ndim, init=i, tol=tol,
max_iter=m, bounds=b, inertia=t)
swarm.Optimize()]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first line constructs the swarm by passing in the objective function (obj), number
of particles (npart), their dimension (ndim), the initializer (i), tolerance (tol), maximum
number of iterations (m), the bounds (b), and the inertia (t). The actual search becomes
a single call to the Optimize method. This method, defined virtually identically for each
of the swarm algorithms we’ll discuss in later chapters is,
def Optimize(self):
self.Initialize()
while (not self.Done()):
self.Step()
return self.gbest[-1], self.gpos[-1]
which follows Algorithm 2 quite closely.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The swarm is initialized using the initializer object
passed to it (Initialize), then a loop searches until Done returns True. The Step
method performs the required swarm position updates and evaluations of the objective
function. When the search is over, the best objective function value (gbest[-1]) and
associated position in the search space (gpos[-1]) are returned. These are lists tracking
the evolution of the search so the last element of the list is the best position found. We’ll use
these lists later to show how the search evolved. In our example call to Optimize above,
we ignored these return values. That’s because we can get more information by calling the
Results method,
res = swarm.Results()
x,y = res["gpos"][-1]
g = res["gbest"][-1]
print("f(%0.8f, %0.8f) = %0.8f" % (x,y,g))
print("(%d swarm best updates, %d iterations)" %
(len(res["gbest"]), res["iterations"]))]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Results method returns a Python dictionary containing the gbest and gpos
values along with other information like the number of iterations performed. The number
of elements in the gbest list is the number of times the swarm improved its best solution
during the search.
If we run this code, we’ll get slightly different answers each time because of the stochastic nature of the initialization process, but, because our search space is straightforward,
we’ll always find the true minimum position of (0.01, 0.01), though the number of swarm
improvements needed will vary.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For example, a single run produced this output,
f(0.01000000, 0.01000000) = 0.00010000
(8 swarm best updates, 100 iterations)
showing that the minimum position was found, that all 100 iterations were used, and there
were eight times that the swarm improved its initial best position. By displaying the
positions (res["gpos"]) and objective function values (res["gbest"]), we can see
how the swarm moved to the best solution,
f(0.96153452, 0.01689004) = 0.01624035
f(1.00000000, 0.01000000) = 0.01000000
f(0.72279699, 0.01000000) = 0.00722797
f(0.66630552, 0.01000000) = 0.00666306
f(0.39462535, 0.01000000) = 0.00394625
f(0.37345245, 0.01000000) = 0.00373452
f(0.15094400, 0.01000000) = 0.00150944
f(0.01000000, 0.01000000) = 0.00010000
where the first line is the best position found by the initial swarm. The PSO algorithm
then improved the initial swarm to progressively find better and better locations until the
true minimum was found. Notice that in this particular run, y converged to is limit very quickly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[You may have a bit of an uneasy feeling about this example. Yes, it was contrived, but
especially so because we told the Bounds object to use clipping when a particle dimension
went out of bounds, and it so happens that the proper value for our minimum is on the
boundary in this case. This is why the y value so quickly found the edge. What if we change
the boundary conditions to use resampling instead? The change to the code is,
b = Bounds([0.01,0.01], [1,1], enforce="resample")
with everything else the same. If we now run the search, we see a different result,
f(0.07176122, 0.01060304) = 0.00076089
(4 swarm best updates, 100 iterations)
where the quick convergence we had before has disappeared. If we increase the number of
swarm iterations from m = 100 to m = 1000, we get closer,
f(0.01084622, 0.02919965) = 0.00031671
(8 swarm best updates, 1000 iterations)]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here, we’ve told the algorithm to use a small swarm size (10 particles) and search for
a long time (1,000 iterations). As we work with the algorithms, we’ll build intuition and
see that 10 particles are probably too few, so let’s increase the number of particles to 100
(npart=100) while still using resampling on the Bounds object. We’ll leave the number
of iterations at m = 100, as before. A search with this larger swarm gives us,
f(0.02181587, 0.01145074) = 0.00024981
(3 swarm best updates, 100 iterations)
which is a better result. Notice, the smaller swarm for more iterations and the larger swarm
for fewer iterations both performed the same amount of work: 10,000 objective function
evaluations after initialization since 10 ×1000 = 100×100 = 10, 000. The stochastic nature
of the random initializer means that multiple runs of each of these swarms will oscillate
between which one returns a better result.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the following chapters, we’ll define multiple swarm search algorithms. Some, like
PSO, are swarm intelligence algorithms, while others, like differential evolution (DE), are
really evolutionary algorithms in disguise. The point of the framework introduced in this
chapter is to make switching and experimenting with these algorithms straightforward.
Let’s replace the PSO object with DE for the exercise above where we are using 10 particles
for 100 iterations. After importing the DE module, we need only replace the call to PSO
with,
swarm = DE(obj=Objective(), npart=npart, ndim=ndim, init=i,
tol=tol, max_iter=m, bounds=b)
and run the code again. Notice that the DE class does not use an inertia object. This run
produces,
f(0.01000140, 0.01000028) = 0.00010002
(31 swarm best updates, 100 iterations)
which is a still better result, about as good as we could hope to get.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This illustrates a general
observation about differential evolution: it seems particularly good at rapidly converging
when the search space is relatively simple. However, as we’ll see later in the book, more
sophisticated search spaces are sometimes best searched with an algorithm like PSO, where
an algorithm like DE is likely to get stuck in a poor position in the search space. We need
to be aware of multiple algorithms and their relative strengths and weaknesses. As you
work through the book, you’ll develop the intuition you need to know when to try which
algorithm, though experimentation is vital. Always indulge your “what if?” questions.
In this chapter, we defined our framework and we demonstrated its use on a simple
example. Let’s move ahead to investigate our first swarm algorithm, one so simplest it isn’t
really a swarm intelligence algorithm at all: random optimization.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Recalling our mental picture of optimization as a search by the swarm through a space
of possible solutions, random optimization is akin to a set of prospectors, each searching
for gold by doing his or her thing in separate parts of the search space. In this sense,
random optimization is a purely local search approach. The individual particles search their
immediate region, but they never communicate with each other to see if there is a better
place to search somewhere else in the search space, hence, there is no global intelligence to
the algorithm.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As before, we need framework parts for the objective function, the bounds, and the type
of initialization we want to use. Then, the for loop initializes the swarm by scattering the
particles throughout the search space according to the initializer we decided to use.
The swarm searches individually by letting each particle perform an independent search
while checking after each particle move whether or not the new position is the best the
entire swarm as a whole has seen. The critical point in this algorithm that separates it from
all the others we’ll investigate in later chapters is the particles do not communicate with
each other. There is no sharing of information that might cause one particle to move to a
different region of the search space based on what another particle has learned. Regardless,
with a proper swarm size and initialization, this approach can be surprisingly effective.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Algorithm 4 The random optimization algorithm.
Input: An objective function, bounds, and initialization type
Output: The best position found by the swarm
for each particle do
Select an initial position within the bounds of the search space
Evaluate the objective function at this position
Mark this position as the best found by the particle so far
end for
Store the best initial particle position as the swarm global best position
while not done do
for each particle do
Select a new position some random distance away from the current position
Evaluate the new position
if fitness of new position < fitness of current position then
Move to the new position
if new position fitness < swarm best position fitness then
Store the new global best
end if
end if
end for
Increment the iteration counter
end while]]>
			</paragraph>
			<paragraph>
				<![CDATA[The concept behind RO is straightforward enough, but there is that one line in Algorithm 4,
Select a new position some random distance away from the current position
What should we make of this? What does “at random” and “some distance away” actually
mean?
By “at random” we mean just that, we select a new position relative to the current
position by selecting a random offset vector to add to the current position vector. There
are multiple ways to do this, but ultimately, we need a random offset vector to add to
the current particle’s position to move to a new position in the search space. As we lack
any outside information to tell us that moving from the current particle position to a new
position in a particular direction is any better than moving in any other direction, we can
pick the offset vector freely. At the same time, we lack any information to tell us how far
we should move in these steps, so we might as well make most steps relatively small, but
allow for the possibility of selecting a large step from time to time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Selecting a set of random values that are usually small but sometimes larger speaks to
a Gaussian or normal distribution. For example, Figure 3.1 shows a histogram of samples
from a standard normal distribution, one with a mean of zero and a standard deviation of
one. The data for the figure is easy to generate in Python,
import numpy as np
import matplotlib.pylab as plt
d = np.random.normal(size=(200000,))
h,x = np.histogram(d, bins=100)
h = h / h.sum()
plt.plot(x[:-1],h,color=’b’)
plt.show()]]>
			</paragraph>
			<paragraph>
				<![CDATA[Here, we select 200,000 random values from N(0, 1) (d) and then plot a histogram of those
values. Each time the code is run, we’ll get a slightly different plot, but it will always have
much the same appearance and be like the plot shown in Figure 3.1.
In Figure 3.1, we see that the vast majority of samples are very close to zero. This
satisfies the desire to select new candidate positions for our swarm that aren’t too far from
where the particle currently is. However, with decreasing likelihood, we’ll sometimes get
output that is much further from zero. This satisfies the desire to occasionally make a larger
jump to explore elsewhere.
We see that the output is, to a high degree, contained within the range [−5, 5]. So, if
we divide our selected samples by five, we’ll be close to selecting in the range [−1, 1]. We
want both positive and negative values to make the offset vector point in all directions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We still haven’t addressed what we mean by “some distance away”. We’re close, though.
We know we want an offset vector with specific characteristics, and we see how to get it,
but we’d like to control the relative distance or size of the vectors, too. To do that, we
introduce a parameter, η, which we’ll use as a scale factor. We pass this scale factor in at
swarm creation to adjust for a particular problem, but we’ll see that often we don’t need
to adjust η at all.
Let’s get specific. If the current particle’s position is p, where we know that p is an n-dimensional vector 
(here n is ndim from Chapter 2), we can select a new candidate position like so, p0 = p + ηpq/5
where q is the offset vector made up of random draws from N(0, 1) for each of the n
dimensions in p. The size of the jump is controlled by η as a fraction of the current position
of the particle, p. Of course, we must ensure that the new candidate position, p0, is within
the bounds of the search space. We’ll see how in Section 3.2 when we develop the code for
the RO class]]>
			</paragraph>
			<paragraph>
				<![CDATA[Next, we evaluate the objective function at the new candidate position, p0. If the
objective function value at the candidate position is less than the objective function value
of the particle’s current position, we found a better place to be, so we update the particle’s
position, p ← p0 and store the associated objective function value. However, if the candidate
position is not better, we stay where we are until we find someplace better to go.
In Algorithm 4, we evaluate candidate positions for each of the particles to make up
one iteration of the swarm. Whenever we find a new position, we also check to see if this
position is better than the global best position known by the swarm. If so, we update that
position as well. It is the global best position which decides whether the tolerance has been
met and is returned when the search concludes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The elegance of RO is its simplicity and the implicit invitation to expand the algorithm
to make it more sophisticated. With RO as a base, it is no wonder so many researchers have
developed their different approaches, either to RO updates or to entirely new algorithms
where the swarm does pass information among itself to converge more rapidly or reliably.
For example, we’re using a normal curve to select new candidate locations. It isn’t too hard
to imagine replacing the normal curve with something else, something with parameters that
could be adjusted for the task at hand. A beta distribution, perhaps?]]>
			</paragraph>
			<paragraph>
				<![CDATA[Let’s generate a two-dimensional walk for a particle using our update equation to see
how it might explore the search space. We’ll limit the space to [0, 1] and set η = 0.2. The
result is Figure 3.2, where the path shows how the particle moved through space. In terms
of the RO algorithm, this path shows cases where the particle found a new, better position
and moved to it. Most jumps are small, corresponding to the small offset vector we’d expect
from the distribution of Figure 3.1. However, some are large, so the particle does have the
opportunity to move through the search space to explore new regions while still spending
time looking closely around its current position. The path starts near the center of the
search space so we can track the motion more easily.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The code to generate the random walk
is,
def candidate(p, eta):
return p + eta*p*np.random.normal(size=2)/5.0
eta = 0.2
d = []
p = [0.5,0.5] + np.random.random(2)/8
d.append(p)
for i in range(300):
p = candidate(p, eta)
p = np.clip(p,0,1)
d.append(p)
d = np.array(d)
where candidate returns a new “candidate” position according to our update equation.
For the walk, we always move to the candidate position. The current particle position is
in p, and the path is tracked in d, which is turned into a 2D NumPy array for plotting in
the final line. The np.clip function acts like a call to the Limits method of a Bounds
object.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The RO algorithm has only one adjustable parameter, η. Figure 3.3 shows the effect
of different values of η where on the left η = 0.05, in the middle η = 0.5, and on the right
η = 0.8. When η is large, the particle jumps around quickly but does not closely inspect
its local region before moving on. When η is small, the particle explores the local region
without jumping to more distant regions of the search space. As we’ll see in Section 3.2,
the RO class uses a default value of η = 0.1 as a compromise between local and regional
searching.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Look again at Figure 3.3 and picture the overlapping paths from a swarm of particles,
each one taking a similar random walk. This is what the RO algorithm will do, so we
have some reason to hope that this approach, as unguided as it is, isn’t entirely doomed to
failure. The overlapping paths will explore the space, eventually. With the right compromise
between local exploitation and regional exploration, we might expect the swarm, or at least
parts of it, to collapse into the best region of the search space. This implies a possible
strategy is to set η to some value like 0.1 or 0.2 initially, and as the swarm evolves to make
η smaller to force more local searching to fine-tune the optimization. In effect, this is what
the ω parameter of PSO, which we saw in Chapter 2, does. We’ll see explicitly how in
Chapter 4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[class RO:
def __init__(self, obj,
npart=10,
ndim=3,
max_iter=200,
eta=0.1,
tol=None,
init=None,
done=None,
bounds=None):
def Results(self):
def Initialize(self):
def Done(self):
def Evaluate(self, pos):
def CandidatePositions(self):
def Step(self):
def Optimize(self):
Figure 3.4: Skeleton of the RO class]]>
			</paragraph>
			<paragraph>
				<![CDATA[Parameter Description
obj Objective function object
npart Number of particles in the swarm
ndim Number of dimensions in the search space
max_iter Maximum number of swarm iterations
eta Step size parameter
tol Tolerance value
init Initializer object
done Done object
bounds Bounds object
Table 3.1: The arguments to the RO class constructor.
Let’s move on from algorithm description to implementation and build the RO class.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The RO class is an archetype for the swarm algorithms in following chapters. In each case,
we’ll present the skeleton of the class, the methods, and then fill in the source code for
the methods with description. Most of the algorithms we implement in code have at least
the methods we see in the RO class, though how the methods work varies somewhat from
algorithm to algorithm. All, however, use the framework objects we defined in Chapter 2.
Note, the code listings have been made more compact for presentation purposes. The actual
source code file, RO.py, contains appropriate comments and spacing.
Figure 3.4 shows the skeleton of the RO class. There are only a handful of methods.
One point of this book is to help you see what these approaches have in common and how
many are just variations on a theme.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The constructor (__init__) accepts nine possible arguments. The constructor is the
interface to the framework parts. The arguments are as shown in Table 3.1.
Except for η, all of these parameters are present in the constructor argument lists for
each of the swarm algorithms. We already know what these parameters do. If the default
value is None, the RO class supplies default functionality. For example, the done argument
will typically be None to use the simple case of counting swarm iterations or meeting
the tolerance value. If no tolerance value is given, the search stops after completing all
iterations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The code for the constructor is,
def __init__(self, ...):
self.obj = obj
self.npart = npart
self.ndim = ndim
self.max_iter = max_iter
self.init = init
self.done = done
self.bounds = bounds
self.tol = tol
self.eta = eta
self.initialized = False
where the arguments to the constructor, or their default values, are stored as member
variables. The final line sets initialized to False. This is used to prevent calling
methods before actually initializing the swarm.
Let’s now implement each of the remaining methods of the RO class. We’ll do so in a
top-down fashion beginning with Optimize.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Optimize method is typically how we’ll use the RO class. It is a literal implementation
of Algorithm 2,
def Optimize(self):
self.Initialize()
while (not self.Done()):
self.Step()
return self.gbest[-1], self.gpos[-1]
Before we can use the swarm, we need to set up the initial conditions, so, Optimize
calls Initialize (see Section 3.2.2). With the swarm initialized, we start iterating where
each iteration step selects possible candidate positions, moves to them if they are better
than the current positions, and updates the global best found by the swarm. All of this is
inside the Step method. The while loop runs until Done returns True.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When the search ends, the list representing the sequence of global best objective function
values is in gbest, and the corresponding particle positions in gpos. Therefore, Optimize
returns these values to the caller, though, as we saw in Chapter 2, we can get this information
and more by calling the Results method.
Note, the methods of the class are public, meaning we can call any of them from external
code. While using Optimize is easy to do, there will be times when we call Initialize
and loop calling Step so we can interrogate the swarm as it evolves.
Let’s now look at how the swarm is initialized.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The RO constructor was passed an initializer object to set up the initial particle positions (see
Section 2.4). The Initialize method uses this object and sets up additional housekeeping
to track the evolution of the swarm. In code,
def Initialize(self):
self.initialized = True
self.iterations = 0
self.pos = self.init.InitializeSwarm()
self.vpos= self.Evaluate(self.pos)
self.gidx = []
self.gbest = []
self.gpos = []
self.giter = []
self.gidx.append(np.argmin(self.vpos))
self.gbest.append(self.vpos[self.gidx[-1]])
self.gpos.append(self.pos[self.gidx[-1]])
self.giter.append(0)
where we set initialized to True. We also set the iteration counter (iterations) to
zero.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The particle swarm is stored in pos, a NumPy matrix with npart rows and ndim
columns. Therefore, each row of pos represents a single swarm particle’s current position in the ndim-dimensional search space. We get the initial positions by calling the
InitializeSwarm method of the initializer passed to the RO constructor.
Each particle is located somewhere in the search space. The position in the search space
translates into a particular value of the objective function. So, we need to know not only
the current position of the particle, but also the current value of the objective function for
that position. We’ll store the objective function values in vpos. To initialize the search,
we need to evaluate the initial positions and keep their objective function values. This
is accomplished by the call to Evaluate, which we’ll define below in Section 3.2.6. As
the objective function values are scalars, vpos is a NumPy vector of npart elements.
Therefore, if we want to know the status of particle i, we get its position by asking for
pos[i] and the objective function value at that position by asking for vpos[i].]]>
			</paragraph>
			<paragraph>
				<![CDATA[The next four lines set up lists to track the evolution of the swarm search. These values
are updated together each time a new global best is located. We track the particle number
of the new best (gidx), the objective function value (gbest), the position (gpos), and
the iteration number when it was found (giter). With this approach, we get the current
best objective function value of the swarm by asking for gbest[-1] and the position of
the best with gpos[-1]. Or, if we’d rather track the evolution, we can walk through the
list and see how the swarm moved to its final best position.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A single swarm iteration is captured in the Step method,
def Step(self):
new_pos = self.CandidatePositions()
p = self.Evaluate(new_pos)
for i in range(self.npart):
if (p[i] < self.vpos[i]):
self.vpos[i] = p[i]
self.pos[i] = new_pos[i]
if (p[i] < self.gbest[-1]):
self.gbest.append(p[i])
self.gpos.append(new_pos[i])
self.gidx.append(i)
self.giter.append(self.iterations)
self.iterations += 1
]]>
			</paragraph>
			<paragraph>
				<![CDATA[First, we get a set of new candidate positions for each particle in the swarm, CandidatePositions.
This sets new_pos to a matrix the same size as pos. There is a one-to-one correspondence
between new_pos and pos. This means that the current position of particle 17 is in
pos[17] while the new candidate position is in new_pos[17]. We next evaluate the objective function for each of the new candidate positions and put these values in the vector
p.
Then, we loop over each particle in the swarm. We first ask whether the candidate
objective function value for the current particle, i, is less the current objective function
value, vpos[i]. If it is, we move to the new position by updating both vpos[i] and
pos[i].
Next, we ask the same question about the current particle’s candidate position and the
best position the swarm has current knowledge of, gbest[-1]. If the new position is
better, we append the new objective function value to gbest and append the position to
gpos. Similarly, we update gidx with the current particle number and add the current
iteration number to giter. When all particles have been processed, the swarm update
step is completed, so we increment iterations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Checking whether or not the search is complete means asking a few questions about what
has been handed to the RO class when the instance was constructed. Specifically, the code
is,
def Done(self):
if (self.done == None):
if (self.tol == None):
return (self.iterations == self.max_iter)
else:
return (self.gbest[-1] < self.tol) or
(self.iterations == self.max_iter)
else:
return self.done.Done(self.gbest,
gpos=self.gpos,
pos=self.pos,
max_iter=self.max_iter,
iteration=self.iterations)]]>
			</paragraph>
			<paragraph>
				<![CDATA[First, we ask whether or not an object supporting a Done method was given to RO. If so,
we drop down and call the Done method of that object returning whatever boolean value
it returns. If False, then the search continues; otherwise, the search terminates. Note, the
Done method is solely responsible for making this decision. The RO class will happily loop
forever if Done consistently returns False. The arguments to Done reflect the current
state of the search. This is to help the code in Done decide whether to continue or not.
If no object was given when the RO instance was constructed, we fall back to checking
whether or not a tolerance value was set. If not, we return whether we’ve run out of
iterations. If a tolerance value was set, we check for maximum iterations and whether the
current best objective function value, gbest[-1], is less than the tolerance value.
The Initialize, Step, and Done methods implement the calls in Optimize. Now,
let’s develop the code for selecting and evaluating candidate positions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We worked through the candidate position method we’re using in Section 3.1. Let’s put
that into code here. The implementation works with NumPy arrays, so we can select new
candidate positions for the entire swarm with a minimum of code,
def CandidatePositions(self):
n = np.random.normal(size=(self.npart, self.ndim))/5.0
pos = self.pos + self.eta*self.pos*n
if (self.bounds != None):
pos = self.bounds.Limits(pos)
return pos
We first set n to an npart by ndim matrix of random values drawn from a normal
distribution with zero mean and a standard deviation of one. We divide these values by
five to map virtually all of them to the range [−1, 1]. This is the offset vector we’ll scale
by η and use as a fraction of the current position, both positive and negative, to add as an
offset. This is the new candidate position.
If we passed a bounds object to the RO instance, we make sure to call its Limits
method to give it a chance to clip or resample out of bounds particle positions and to run
any custom Validate method the Bounds object may have. We then return the set of
candidate positions so we can evaluate them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When we create the RO instance, we must pass it an objective function object. This object,
as we saw in Chapter 2, is problem-specific and initialized outside of the RO class. At that
time, any ancillary information needed to evaluate a single particle position must be passed
in via the constructor. Here, the RO class Evaluate method calls the objective function
object’s Evaluate method for each particle,
def Evaluate(self, pos):
p = np.zeros(self.npart)
for i in range(self.npart):
p[i] = self.obj.Evaluate(pos[i])
return p
For the RO class, the argument to Evaluate is a set of candidate positions, one for
each particle in the swarm. Therefore, we need an npart sized vector to hold the objective
function values (p). Then, we loop over each particle and store the objective function value
returned by calling obj.Evaluate.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this book, we do not concern ourselves with performance considerations. However,
since the evaluation of the objective function for a single particle position is independent
of all other particles, we could at this point parallelize the algorithm and evaluate multiple
positions at one time. This might increase the performance of the search considerably as
calling obj.Evaluate will happen thousands and thousands of times for a typical search.
For our experiments starting in Chapter 8, we’ll instrument the objective function object
so it tracks how often it is called, and we’ll see just how many evaluations are (sometimes)
necessary for the algorithm to converge.
We now have all the methods we need to run a random optimization task. The final
method is a convenience one to return information about the swarm and the search.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We get information about completed searches by calling Results. The definition is,
def Results(self):
if (not self.initialized):
return None
return {
"npart": self.npart,
"ndim": self.ndim,
"max_iter": self.max_iter,
"iterations": self.iterations,
"tol": self.tol,
"eta": self.eta,
"gbest": self.gbest,
"giter": self.giter,
"gpos": self.gpos,
"gidx": self.gidx,
"pos": self.pos,
"vpos": self.vpos,
}]]>
			</paragraph>
			<paragraph>
				<![CDATA[The Results method packages swarm results and returns them as a dictionary. We
used this method in Chapter 2 to demonstrate how to use the framework objects. We’ll
use this method going forward. We already know what each of the items returned are, so
we won’t elaborate on them here. Every swarm optimization algorithm we implement has
a Results method. What is in common between the methods, like gbest and gpos, will
always be in the dictionary. Additionally, any algorithm-specific parameters will be present
as well. Therefore, for the RO class, we see that η is present.
Our implementation of the RO class is complete. Again, the source code is in the file
RO.py. Let’s take the RO class for a test drive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The example of Chapter 2 was a simple optimization problem for which we had every reason
to expect the algorithm to, in time, find a good solution. To test the performance of the RO
class, and to gain insight into not only how it operates but how all swarm-based algorithms
operate, let’s pick an example we’ll use going forward in subsequent chapters. This lets us
compare algorithm performance while testing.
Our test example will be in the same vein as the example of Chapter 2. We’ll find the
minimum of a 2D function, but one that isn’t as simple as f(x, y) = xy. Instead, we’ll look
for the minimum of *f(x,y) equation here*
which is a pair of two-dimensional Gaussians with minima at (2.2, −4.3) and (−2.2, 4.3),
but the minima are not of equal depth so the global minimum is at (−2.2, 4.3). Figure 3.5
shows two views of this function.
The set up for the RO class requires initializer and bounds objects, and to define an
objective function class. As in Chapter 2, the objective function is simply the value of
f(x, y) for a given x, y. We have a two-dimensional problem we’ll bound to −6 ≤ x, y ≤ 6.
The code presented below is found in the fxy_gaussian.py file of the source code distribution. Note, the fxy_gaussian.py file is configured to use all the swarm algorithms
we’ll develop. The listing in this chapter is specific to the RO class only.]]>
			</paragraph>
			<paragraph>
				<![CDATA[ll develop. The listing in this chapter is specific to the RO class only.
First, let’s import our modules, define the objective function class, make an instance of
it, and the Bounds object,
from RO import *
from Bounds import *
from RandomInitializer import *
from QuasirandomInitializer import *
from SphereInitializer import *
class Objective:
def Evaluate(self, p):
return -5.0*np.exp(-0.5*((p[0]+2.2)**2/0.4+(p[1]-4.3)**2/0.4)) +
-2.0*np.exp(-0.5*((p[0]-2.2)**2/0.4+(p[1]+4.3)**2/0.4))
obj = Objective()
b = Bounds([-6,-6], [6,6], enforce="resample")]]>
			</paragraph>
			<paragraph>
				<![CDATA[We still need to create an initializer object. We’ll show the random case here, but trust
you’ll experiment with the quasirandom and sphere initializers as well. All we need is a
single line,
i = RandomInitializer(npart=npart, ndim=2, bounds=b)
where we pass in the Bounds object and set the number of dimensions to two. We assume
npart is set to the number of particles we want in the swarm. Below, we’ll experiment
with the number of particles and the maximum number of iterations of the swarm, which
we’ll call miter.
Let’s create our random optimization swarm object and do the search,
swarm = RO(obj=obj, npart=npart, ndim=2, max_iter=miter, init=i, bounds=b)
swarm.Optimize()
Now, let’s get the results and see how we did,
res = swarm.Results()
x,y = res["gpos"][-1]
v = res["gbest"][-1]
print("f(%0.8f, %0.8f) = %0.10f" % (x,y,v))
print("(%d swarm best updates)" % (len(res["gbest"]),))]]>
			</paragraph>
			<paragraph>
				<![CDATA[Notice, we call Results to get the dictionary of results and then extract the best
position, x and y, along with the function value at that position, v. The length of gbest
gives us the number of swarm updates.
Let’s run the search using npart=10 and miter=100. Each time we run, we’ll get a
different output, but one run returned,
f(-2.24570451, 3.82387180) = -3.7563742785
(19 swarm best updates)
To use the fxy_gaussian.py file to run this search, use a command line like,
> python3 fxy_gaussian.py 10 100 RO RI]]>
			</paragraph>
			<paragraph>
				<![CDATA[We know the global minimum of f(x, y) is at (−2.2, 4.3) and has a value of −5. Our
search above was heading in the right direction, but it wasn’t able to find a good approximation of the minimum.
If we repeat the search ten times, we’ll get some statistics on how well we do on average.
Of course, when you run the code ten times, you’ll get ten different results. My run showed
that for eight of the ten runs, the swarm selected an endpoint near the global minimum of
(−2.2, 4.3). However, twice it was moving in the wrong direction, towards (2.2, −4.3). The
mean and standard error of the minimum found, and the number of swarm updates needed
was,
Minimum found −3.7851 ± 0.8534
Swarm updates 38.9 ± 6.2
Recall, this is for a swarm of ten particles and 100 iterations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We have a small swarm, only ten particles. Let’s increase the number of iterations by
a factor of ten and see if letting the swarm explore more helps. So, change miter=100 to
miter=1000. Ten runs gives us a new set of means,
Minimum found −4.9999777 ± 0.0000098
Swarm updates 49.0 ± 11.8
which is significantly better. For this run, the swarm headed for the proper global minimum
in all cases. A small swarm with many iterations was able to do a good job, on average.
What if we use a larger swarm but return to using miter=100?]]>
			</paragraph>
			<paragraph>
				<![CDATA[teLet’s move from npart=10
to npart=100. In that case, we get,
Minimum found −4.9997805 ± 0.0001287
Swarm updates 21.4 ± 2.9
where the results are nearly as good as for ten times as many iterations of a swarm with
only one-tenth the number of particles. Note, also, that the number of swarm updates is
lower, on average, than for the case with only ten particles. This observation makes sense:
a smaller swarm is more likely to need to search around to find good places, and that leads
to a large number of swarm updates.xt]]>
			</paragraph>
			<paragraph>
				<![CDATA[All of the above validates our intuition that small swarms that get to explore are good,
as are larger swarms with fewer iterations. So, shouldn’t we always use large swarms, then?
Not really. If the objective function is computationally expensive, and we have good reason
to believe our bounds are tight, meaning the solution is likely within them, then we might
opt for a smaller swarm to save on doing too many calls to the objective function. If we run
the search to miter iterations every time, the number of calls to the objective function is
npart×miter plus another npart calls to initialize the swarm.
Also, in the example above, the swarm of ten particles run for 1000 iterations, 10,000
objective function calls, was, on average, better at finding the global minimum than the
swarm of 100 particles run for 100 iterations, also 10,000 objective function calls. We’ll do
similar analyses for the other swarm algorithms we develop.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If we don’t call Optimize of RO, but instead manually call Initialize followed by
miter calls to Step, we can interrogate the swarm during the search to see where it is in
the search space. This opens up the possibility of creating a movie of the search since we
are in two dimensions. Naturally, we can’t show such a movie in a book, but we can show
frames from it. Building the movie and showing specific frames from it gives us Figure 3.6.
Figure 3.6 shows the swarm positions at different points in the search. Moving clockwise
from the upper left, we see the initial swarm configuration. The particle positions are circles,
the best position of the swarm is a star, and the known global minimum of f(x, y) is shown
as a square. Note, for this figure, and similar ones in subsequent chapters, we’re fixing the
NumPy pseudorandom number seed so each swarm initializes in exactly the same way. We
set the NumPy seed by adding,
np.random.seed(8675309)
before the Bounds object is created.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Fixing the seed means we can compare how the swarms evolve from the same starting
point. Notice that the initial swarm best position is actually quite close to the second
minima of the function at (2.2, −4.3) yet the swarm evolves to find the true global minima
at the end of the search.
Figure 3.6: Frames from a search with a swarm of twenty particles and 100 iterations.
Clockwise from the upper left: initial swarm positions, frame 33, frame 66, and final swarm
positions. Swarm particles are circles. The swarm best position is a star and the known
global best position is a square.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Next in Figure 3.6, we see frame 33, after 33 iterations of the swarm. The best particle
has now moved quite close to the global minimum. The next frame is frame 66. We see
that the swarm best is virtually on top of the global minimum. We also see that other
particles near the global minimum have moved in that direction. The final frame shows
the configuration of the swarm at the end of the search. The global best position has been
found with reasonable quality, and we see that other nearby particles are also closing in on
the minimum. Given the plot of Figure 3.5, the particles near the minimum are “falling
into” the hole]]>
			</paragraph>
			<paragraph>
				<![CDATA[Random optimization, as mentioned at the beginning of this chapter, is individualistic.
There is no communication between members of the swarm. We see this in Figure 3.6.
The swarm best particle has moved to the global minimum, but the majority of the swarm
is still wandering about in the search space. Those particles are unaware of how well the
swarm best particle is doing. We’ll repeat this sort of analysis when we test the other swarm
algorithms in later chapters. In some of those cases, we’ll see a different kind of behavior
from the swarm precisely because the particles share information about their current state.
This concludes our implementation and testing of the RO class. Now that we know how
a specific swarm algorithm works and interfaces with the framework created in Chapter 2,
we can move on to our next swarm intelligence algorithm: particle swarm optimization.]]>
			</paragraph>
		</content>
	</book>
	<book name="First Ever Synthetic Data for Deep Learning">
		<content>
			<paragraph>
				<![CDATA[In this chapter, we proceed from datasets of static synthetic images, either prerendered or procedurally generated, to entire simulated environments that can be used
either to generate synthetic datasets on the fly or provide learning environments for
reinforcement learning agents. We discuss datasets and simulations for outdoor environments (mostly for autonomous driving), indoor environments, and physics-based
simulations for robotics. We also make a special case study of datasets for unmanned
aerial vehicles and the use of computer games as simulated environments.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While collecting synthetic datasets is a challenging task by itself, it is insufficient
to train, e.g., an autonomous vehicle such as a self-driving car or a drone, or an
industrial robot. Learning to control a vehicle or robot often requires reinforcement
learning [831], where an agent has to learn from interacting with the environment, and
real-world experiments to train a self-driving car or a robotic arm by reinforcement
learning are completely impractical. Fortunately, this is another field where synthetic
data shines: once one has a fully developed 3D environment that can produce datasets
for computer vision or other sensory readings, it is only one more step to active
interaction with this environment. Therefore, in most domains considered below we
can see the shift from static synthetic datasets to interactive simulation environments.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Reinforcement learning (RL) agents are commonly trained on simulations because
the interactive nature of reinforcement learning makes training in the real world
extremely expensive. We discuss synthetic-to-real domain adaptation in this context
in Section 10.6. However, in many works, there is no explicit domain adaptation:
robots are trained on simulators and later fine-tuned on real data or simply transferred
to the real world.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 7.1 shows a brief summary of datasets and simulators that we review in this
section. To make the exposition more clear, we group together both environments
and “static” synthetic datasets for outdoor (Section 7.2) and indoor (Section 7.3)
scenes, including some works that use them to improve RL agents and other models.
Next, we consider synthetic robotic simulators (Section 7.4) and vision-based simulators for autonomous flying (Section 7.5), finishing with an idea of using computer
games as simulation environments in Section 7.6. Reinforcement learning in virtual
environments remains a common thread throughout this section]]>
			</paragraph>
			<paragraph>
				<![CDATA[An important direction of applications for synthetic data is related to navigation,
localization and mapping (SLAM), or similar problems intended to improve the
motion of autonomous robots. Possible applications include SLAM, motion planning,
and motion for control for self-driving cars (urban navigation) [239, 498, 601, 649],
unmanned aerial vehicles [11, 170, 428], and more; see also general surveys of
computer vision for mobile robot navigation [83, 190] and perception and control
for autonomous driving [662].]]>
			</paragraph>
			<paragraph>
				<![CDATA[Before proceeding to current state of the art, let me remind an interesting historical
fact that we discussed in detail in Section 5.3: one of the first autonomous driving
attempts based on neural networks, ALVINN [680], which used as input 30 × 32
videos supplemented with 8 × 32 range finder data, was already training on synthetic
data. One of the first widely adopted full-scale visual simulation environments for
robotics, Gazebo [464] (see Section 7.4), also provided both indoor and outdoor
environments for robotic control training.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In a much more recent effort, Xerox researchers Gaidon et al. [254] presented
a photorealistic synthetic video dataset Virtual KITTI1 intended for object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation. The dataset contains five different virtual
outdoor environments created with the Unity game engine and 50 photorealistic synthetic videos. Gaidon et al. studied existing multi-object trackers, e.g., based on an
improved min-cost flow algorithm [676] and on Markov decision processes [950];
they found minimal real-to-virtual gap. Note, however, that experiments in [254]
were done on trackers trained on real data and evaluated on synthetic videos (and
that’s where they worked well), not the other way around. In general, the Virtual
KITTI dataset is much too small to train a model on it, it is intended for evaluation,
which also explains the experimental setup.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Johnson-Robertson et al. [412], on the other hand, presented a method to train on
synthetic data. They collected a large dataset by capturing scene information from
the Grand Theft Auto V video game that provides sufficiently realistic graphics and
at the same time stores scene information such as depth maps and rough bounding
boxes in the GPU stencil buffer, which can also be captured; the authors developed an
automated pipeline to obtain tight bounding boxes. Three datasets were generated,
with 10K, 50K, and 200K images, respectively.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The main positive result of [412] is
            that a standard Faster R-CNN architecture [719] trained on 50K and 200K images
outperformed on a real validation set (KITTI) the same architecture trained on a real
dataset. The real training set was Cityscapes [167] that contains 2,975 images, so
while the authors used more synthetic data than real, the difference is only 1-2 orders
of magnitude. The VIPER and GTAV datasets by Richter et al. [724, 725] were also
captured from Grand Theft Auto V; the latter provides more than 250K 1920 × 1080
images fully annotated with optical flow, instance segmentation masks, 3D scene
layout, and visual odometry.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The SYNTHIA dataset presented by Ros et al. [731] provides synthetic images of
urban scenes labeled for semantic segmentation. It consists of renderings of a virtual
New York City constructed by the authors with the Unity platform and includes segmentation annotations for 13 classes such as pedestrians, cyclists, buildings, roads,
and so on. The dataset contains more than 213,000 synthetic images covering a wide
variety of scenes and environmental conditions; experiments in [731] show that
augmenting real datasets with SYNTHIA leads to improved segmentation. Later,
Hernandez-Juarez et al. [338] presented SYNTHIA-SF, the San Francisco version of
SYNTHIA. We illustrate SYNTHIA with a sample frame (that is, two frames since
the dataset contains two cameras) from the SYNTHIA-SF dataset in Figure 7.1.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Saleh et al. [754] presented a Unity3D framework called virtual environment for
instance segmentation (VEIS); while not very realistic, it worked well with their
detection-based pipeline (see Section 6.5). Li et al. [511] present a synthetic dataset
with foggy images to simulate difficult driving conditions. We note the work of
Lopez et al. [548] whose experiments suggest that the level of realism achieved in
SYNTHIA and GTAV is already sufficient for successful transfer of object detection
methods (Fig. 7.2).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Tian et al. [857] present the ParallelEye synthetic dataset for urban outdoor scenes.
Their approach is rather flexible and relies on previously developed Esri CityEngine
framework [954] that provides capabilities for batch generation of 3D city scenes
based on terrain data. In [857], this data was automatically extracted from the Open-StreetMap platform2. The 3D scene is then imported into the Unity3D game engine,
which helped add urban vehicles on the roads, set up traffic rules, and add support
for different weather and lighting conditions. Tian et al. showed improvements in
object detection quality for state-of-the-art architectures trained on ParallelEye and
tested on the real KITTI test set as compared to training on the real KITTI training set.]]>
			</paragraph>
			<paragraph>
				<![CDATA[et.
Li et al. [513] develop the Augmented Autonomous Driving Simulation (AADS)
environment that is able to insert synthetic traffic on real-life RGB images. Starting
from the real-life ApolloScape dataset for autonomous driving [372] that contains
LIDAR point clouds, the authors remove moving objects, restore backgrounds by
inpainting, estimate illumination conditions, simulate traffic conditions and trajectories of synthetic cars, preprocess the textures of the models according to lighting and
other conditions, and add synthetic cars in realistic places on the road. In this way, a
single real image can be reused many times in different synthetic traffic situations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This is similar to the approach of Abu Alhaija et al. [5] (recall Section 6.3) but due to
available 3D information AADS can also change the observation viewpoint and even
be used in a closed-loop simulator such as CARLA or AirSim (see below). We do
not go into details on the already large and diverse field of virtual traffic simulation
and refer to a recent survey [123].
Wrenninge and Unger [935] present the Synscapes dataset that continues the work
of Tsirikoglou et al. [876] (see Section 6.5) and contains accurate photorealistic
renderings of urban scenes (Fig. 7.3e-f), with unbiased path tracing for rendering,
special models for light scattering effects in camera optics, motion blur, and more.
They find that their additional efforts for photorealism do indeed result in significant
improvements in object detection over GTA-based datasets, even though the latter
has a wider variety of scenes and pedestrian and car models.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Khan et al. [449] introduce ProcSy, a procedurally generated synthetic dataset
aimed at semantic segmentation (we showed a sample frame on Fig. 1.7c-d). It is
modeling a real-world urban environment, and its main emphasis is on simulating
various weather and lighting conditions for the same scenes. The authors show that,
e.g., adding a mere 3% of rainy images in the training set improves the mIoU of a
state-of-the-art segmentation network (in this case, Deeplab v3+ [135]) by as much
as 10% on rainy test images. This again supports the benefits from using synthetic
data to augment real datasets and cover rare cases; for a discussion of the procedural
side of this work see Section 12.1.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Synthetic datasets with explicit 3D data (with simulated sensors) for outdoor
environments are less common, although such sensors seem to be straightforward
to include into self-driving car hardware. In their development of the SqueezeSeg
architecture,Wu et al. [936, 937] added a LiDAR simulator to Grand Theft Auto V and
collected a synthetic dataset from the game. SynthCity by Griffiths and Boehm [298]
is a large-scale open synthetic dataset which is basically a huge point cloud of an
urban/suburban environment. It simulates Mobile Laser Scanner (MLS) readings
with a Blender plugin [300] and is specifically intended for pretraining deep neural
networks.
Yogamani et al. [975] present WoodScape, a multi-camera fisheye dataset for
autonomous driving that concentrates on getting 360° sensing around a vehicle
through panoramic fisheye images with a large field of view. They record 4 fisheye
cameras with 190° horizontal field of view, a rotating LiDAR, GNSS and IMU sensors, and odometry signals with 400K frames with depth labeling and 10K frames
with semantic segmentation labeling. Importantly for us, together with their real
dataset they also released a synthetic part (10K frames) that matches their fisheye
cameras, with the explicit purpose of helping synthetic-to-real transfer learning. This
further validates the importance of synthetic data in autonomous driving]]>
			</paragraph>
			<paragraph>
				<![CDATA[Simulated environments rather than datasets are, naturally, also an important part
of the outdoor navigation scene; below we describe the main players in the field and
also refer to surveys [430, 824] for a more in-depth analysis of some of them. There
is also a separate line of work related to developing more accurate modeling in such
simulators, e.g., sensor noise models [605], that falls outside the scope of this book.]]>
			</paragraph>
			<paragraph>
				<![CDATA[TORCS3 (The Open Racing Car Simulator) [946] is an open-source 3D car racing
simulator that started as a game for Linux in the late 1990s but became increasingly
popular as a virtual simulation platform for driving agents and intelligent control
systems for various car components. TORCS provides a sufficiently involved simulation of racing physics, including accurate basic properties (mass, rotational inertia),
mechanical details (suspension types, etc.), friction profiles of tyres, and a realistic aerodynamic model, so it is widely accepted as a useful source of synthetic
data. TORCS has become the basis for the annual Simulated Car Racing Championship [544] and has been used in hundreds of works on autonomous driving and
control systems (see Section 7.4); TORCS and other outdoor driving simulators are
illustrated in Fig. 7.4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[CARLA (CAR Learning to Act) [203] is an open simulator for urban driving,
developed as an open-source layer over Unreal Engine 4 [434]. Technically, it operates similarly to [412], extending Unreal Engine 4 by providing sensors in the form
of RGB cameras (with customizable positions), ground truth depth maps, ground
truth semantic segmentation maps with 12 semantic classes designed for driving
(road, lane marking, traffic sign, sidewalk, and so on), bounding boxes for dynamic
objects in the environment, and measurements of the agent itself (vehicle location
and orientation). DeepDrive [692] is a simulator designed for training self-driving
AI models, also developed as an Unreal Engine plugin; it provides 8 RGB cameras
with 512 × 512 resolution at close to real-time rates (20Hz), as well as a generated
8.2-hour video dataset.]]>
			</paragraph>
			<paragraph>
				<![CDATA[VIVID (VIrtual environment for VIsual Deep learning), developed by Lai et
al. [492], tackles a more ambitious problem: adding people interacting in various
ways and a much wider variety of synthetic environments, they present a universal
dataset and simulator of outdoor scenes such as outdoor shooting, forest fires, drones
patrolling a warehouse, pedestrian detection on the roads, and more. VIVID is also
based on the Unreal Engine and uses the wide variety of assets available for it; for
example, NPCs acting in the scenes are programmed using Blueprint, an Unreal
scripting engine, and the human models are animated by the Unreal animation editor. VIVID provides the ability to record video simulations and can communicate
with deep learning libraries via the TCP/IP protocol, through the Microsoft Remote
Procedure Call (RPC) library originally developed for AirSim (see Section 7.4)]]>
			</paragraph>
			<paragraph>
				<![CDATA[
Many works on autonomous driving use TORCS [946] as a testbed, both in virtual
autonomous driving competitions and simply as a well-established research platform.
I do not aim to provide a full in-depth survey of the entire field and only note that
despite its long history TORCS is being actively used for research purposes up to this
day. In particular, Sallab et al. [756, 757] use it in their deep reinforcement learning
frameworks for lane keeping assist and autonomous driving, Xiong et al. [960] add
safety-based control on top of deep RL, Wang et al. [908] train a deep RL agent for
autonomous driving in TORCS, Barati et al. [41] use it to add multi-view inputs for
deep RL agents, Li et al. [509] develop Visual TORCS, a deep RL environment based
on TORCS, Ando, Lubashevsky et al. [21, 560] use TORCS to study the statistical
properties of human driving, Glassner et al. [279] shift the emphasis to trajectory
learning, Luo et al. [564] use TORCS as the main test environment for a new variation
of the policy gradient algorithm, Liu et al. [534] make use of the multimodal sensors
available in TORCS for end-to-end learning, Xu et al. [844] train a segmentation
network and feed segmentation results to the RL agent in order to unify synthetic
imagery from TORCS and real data, and so on.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As for reinforcement learning (RL) in autonomous driving, the original paper on
the CARLA simulator [203] also provides a comparison on synthetic data between
conditional imitation learning, deep reinforcement learning, and a modular pipeline
with separated perception, local planning, and continuous control, with limited success but generally best results obtained by the modular pipeline.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Classical vision-related problems for UAVs include three major tasks that
UAVs often solve with computer vision:
- localization and pose estimation, i.e., estimating the UAV position and orientation
in both 2D (on the map) and in the 3D space; for this problem, real datasets collected
by real-world UAVs are available [82, 727, 779, 820], and the field is advanced
enough to use actual field tests, so synthetic-only results are viewed with suspicion,
but existing research still often employs synthetic simulators, either handmade for
a specific problem [921] or based on professional flight simulators [26];
- obstacle detection and avoidance, where real-world experiments are often too
expensive even for testing [106];
- visual servoing, i.e., using feedback from visual sensors in order to maintain position, stability, and perform maneuvers; here synthetic data has usually been used in
the form of hardware-in-the-loop simulators for the developed controllers [503],
sometimes augmented with full-scale flight simulators [485] or specially designed
“virtual reality” environments [763].]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are plenty of extensions and projects that use AirSim and provide interesting
synthetic simulated environments (see also the survey [575]), in particular:
- Chen et al. [134] add realistic forests to use UAVs for forest visual perception;
- Bondi et al. [81] concentrate on wildlife preservation, extending the engine with
realistic renderings of, e.g., the African savanna;
- in two different projects, Smyth et al. [807, 808] and Ullah et al. [885] simulate
critical incidents (such as chemical, biological, or nuclear incidents or attacks)
with an explicit goal of training autonomous drones to collect data in these virtual
environments;
- Huang et al. [364] extend AirSim with a natural language understanding model to
simulate natural language commands for drones, and so on.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Computer games and game engines have been a very important source of problems
and virtual environments for deep RL and AI in general [113, 417, 766]. Much of
the foundational work in modern deep RL has been done in the environments of 2D
arcade games, usually classical Atari games [55, 607]. First, many new ideas for
RL architectures or training procedures for robotic navigation were introduced as
direct applications of deep RL to navigation in complex synthetic environments, in
particular to navigating mazes in video games such as Doom [69] and Minecraft [638]
or specially constructed 3D mazes, e.g., from the DeepMind Lab environment [49,
780]. Doom became something of an industry standard, with the VizDoom framework
developed by Kempka et al. [443] and later used in many important works [15, 495,
705, 942]; it is illustrated in Fig. 7.10.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Games of other genres, in particular real-time strategy games [19, 851], also
represent a rich source of synthetic environments for deep RL; we note the TorchCraft
library for machine learning research in StarCraft [834], synthetic datasets extracted
from StarCraft [526, 599, 939], the ELF (Extensive, Lightweight, and Flexible)
library platform with three simplified real-time strategy games [856], and the SC2LE
(StarCraft II Learning Environment) library for Starcraft II [896]. Racing games have
been used as environments for training end-to-end RL driving agents [672].]]>
			</paragraph>
			<paragraph>
				<![CDATA[All of the above does not look too much in line with the general topic of synthetic
data: while game environments are certainly “synthetic”, there is usually no goal to
transfer, say, an RL agent playing StarCraft to a real armed conflict (thankfully).
However, recent works suggest that there is potential in this direction. For example,
while navigation in a first-person shooter is very different from real robotic navigation, successful attempts at transfer learning from computer games to the real world
are already starting to appear. Karttunen et al. [442] present an RL agent for navigation trained on the Doom environment and transferred to a real-life Turtlebot by
freezing most of the weights in the DQN network and only fine-tuning a small subset
of them. As computer games get more realistic, we expect such transfer to become
easier]]>
			</paragraph>
			<paragraph>
				<![CDATA[While computer vision remains the main focus of synthetic data applications, other
fields also begin to use synthetic datasets, with some directions entirely dependent on
synthetic data. In this chapter, we survey some of these fields. Specifically, Section 8.1
discusses how structured synthetic data is used for fraud and intrusion detection and
other applications in the form of network and/or system logs; in Section 8.2, we
consider neural programming; Section 8.3 discusses synthetic data generation and
use in bioinformatics, and Section 8.4 reviews the (admittedly limited) applications
of synthetic data in natural language processing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Therefore, since the early 2000s researchers in this field started paying closer
attention to the process of generating synthetic data. Let us highlight the work by
Lundin et al. [563] who developed a methodology for synthetic fraud data generation.
Noting the deficiencies of the ad-hoc approach, they design a general framework that
highlights the necessary design choices; we show it in Figure 8.1. The framework
identifies the following steps:
- data collection that should yield real data after which synthetic data is supposed
to be modeled;
- data analysis that identifies important properties of collected data, including user
statistics, a classification of users and attackers, and system profiles;
- profile generation that identifies various user profiles to be used in synthetic data
generation;
- user and attack modeling that produces (usually simplified) models of user and
attacker behaviour, e.g., by a finite state machine as in [186], with user profiles
serving as parameters for these simulators;
- system modeling that produces a simulator for the system’s reactions to user
actions; one can use the real system if it’s available in software, but in general
it is sufficient to limit the modeling to the aspects relevant for fraud or intrusion
detection.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As a result of going through these steps, we obtain a set of user profiles that can be
fed to user and attacker simulators, and the behaviour produced by these simulators
is fed into the system simulator. Note that while this methodology may look trivial
at first glance, the point is actually to highlight which design choices one has to
make in constructing a synthetic data generation system; from this point of view, this
methodology remains relevant to this day.
Apart from network fraud/intrusion detection, this methodology was put to work
in, for instance, fraud detection in mobile transfer services [252] and generation of
synthetic electronic health records [592].]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another similar field where synthetic data in the form of structured system logs is
needed for fraud detection is financial security [552]. Here, real-life financial records
usually represent sensitive information, and cases of fraud in real data are often hard to
label, which also leads to the need for a synthetic generation. Here, we can highlight
a sequence of works by two researchers from Blekinge Institute of Technology,
E.A. Lopez-Rojas and S. Axelsson. In [550], they constructed a multiagent-based
simulator (MABS) for synthetic data designed to train fraud detection systems. In
subsequent works, they have applied this agent-based approach to several different
domains where financial fraud needs to be detected:
- the original work [550] applied to money laundering detection, and later the same
simulator was applied by the authors to detecting suspicious financial transactions
in a mobile money system [551];
220 8 Synthetic Data Outside Computer Vision
- in [553], Lopez-Rojas and Axelsson developed RetSim, an agent-based simulator
of a shoe store based on real data from a Swedish retailer; RetSim was used for
fraud detection in retail transactions in [554, 555];
- in [549], they extended the original simulator to PaySim, an agent-based simulator
that simulates mobile money transactions based on an original dataset.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One interesting domain where synthetic data is paramount is neural program synthesis and neural program induction. The basic idea of teaching a machine learning
model to program can be broadly divided into two subfields:
• program induction aims to train an end-to-end differentiable model to capture an
algorithm [191];
• program synthesis tries to teach a model the semantics of a domain-specific language (DSL), so that the model is able to generate programs according to given
specifications [432].
Basically, in program induction the network is the program, while in program synthesis the network writes the program.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Naturally, both tasks require large datasets of programs together with their input–
output pairs. Since no such large datasets exist, and generating synthetic programs
and running them in this case is relatively easy (arguably even easier than generating
synthetic data for computer vision), all modern works use synthetic data to train the
“neural computers”.
In program induction, the tasks are so far relatively simple, and synthetic data generation does not present too many difficulties. For example, Joulin and Mikolov [415]
present a new architecture (stack-augmented recurrent networks) to learn regularities in algorithmically generated sequences of symbols; the training data, as in previous such works [273, 350, 354, 926], is synthetically generated by handcrafted
simple algorithms, including generating sequences from a pattern such as anb2n or
anbmcn+m, binary addition (supervised problem asking to continue a string such as
110 + 10 =), and similar formalizations of simple algorithmic tasks.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As a separate thread, we mention works on program synthesis for visual reasoning
and, generally speaking, question answering [362, 409, 759]. To do visual question
answering [8], models are trained to compose a short program based on the natural
language question that will lead to the answer, usually in the form of an execution
graph or a network architecture. This line of work is based on neural module networks [22, 120] and similar constructions [23, 363], where the network learns to
create a composition of modules (in QA, based on parsing the question) that are
also neural networks, all learned jointly (see, however, the critique in [776]). Latest
works use architectures based on self-attention mechanisms that have proven their
worth across a wide variety of NLP tasks [507]. Naturally, most of these works use
the CLEVR synthetic dataset for evaluation (see Section 6.7).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Generation of synthetic data itself has not been given much attention in this field
until very recently, but the interest is rising. The work by Shin et al. [790] presents a
study of contemporary synthetic data generation techniques for neural program synthesis and concludes that the resulting models do not capture the full semantics of a
programming language, even if they do well on the test set. Shin et al. show that synthetic data generation algorithms, including the one from the popular tensor2tensor
library [890], have biases that fail to cover important parts of the program space and
deteriorate the final result. To fix this problem, they propose a novel methodology
for generating distributions over the space of datasets for program induction and
synthesis, showing significant improvements for two important domains: Calculator
(which computes the results of arithmetic expressions) and Karel (which achieves a
given objective with a virtual robot moving on a two-dimensional grid with walls and
markers). We expect more research into synthetic data generation for neural program
induction and synthesis to follow in the near future.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We use examples from the heathcare and biomedical domains throughout this book;
see, e.g., Sections 10.7 and 11.5. In this section, we concentrate on applications of
synthetic data in bioinformatics that fall outside either producing synthetic medical
images (usually done with the help of generative models; see Section 10.7) or providing privacy guarantees for sensitive data through synthetic datasets (Section 11.5). It
turns out that there are still plenty, and synthetic data is routinely used and generated
throughout bioinformatics; see also a survey in [146]. Note that in this section, we
will present some sample applications of generative models, and a full treatment of
what deep generative models are will follow in Chapter 4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For many of such methods in bioinformatics and medicine, generated synthetic
data is the end goal rather than a tool to improve machine learning models. In particular, de novo drug design [324, 769] is a field that searches for molecules with
desirable properties in a search space of about 1060 synthesizable molecules [267,
722]. The goal is to find (which in a space of this size rather means to generate)
candidate molecules that would later have to be explored further in lab studies and
then clinical trials.]]>
			</paragraph>
			<paragraph>
				<![CDATA[First modern attempts at de novo drug design used rule-based methods that simulate chemical reactions [769], but the field soon turned to generative models, in
particular based on deep learning [130, 268]. In this context, molecules are usually
represented in the SMILES format [923] that encodes molecular graphs as strings in
a certain formal grammar, which makes it possible to use sequence learning models
to generate new SMILES strings. Segler et al. [773] used LSTM-based RNNs to learn
a chemical language model, while Gómez-Bombarelli et al. [286] trained a variational autoencoder (VAE) which is already a generative model capable of generating
new candidate molecules. To further improve generation, Kusner et al. [487] developed a novel extension of VAEs called Grammar Variational Autoencoders that can
take into account the rules of the SMILES formal grammar and make VAE outputs
conform to the grammar. To then use the models to obtain molecules with desired
properties, researchers used either a small set of labeled positive examples [773]
or augment the RNN training procedure with reinforcement learning [397]. In particular, Olivecrona et al. [640] use a recurrent neural network trained to generate
SMILES representations: first, a prior network (3 layers of 1024 GRU) is trained in a
supervised way on the RDKit subset [496] of ChEMBL database [267], then an RL
agent (with the same structure, initialized from the prior network) is fine-tuned with
the REINFORCE algorithm to improve the resulting SMILES encoding. A similar
architecture, but with a stack-augmented RNN [416] as the basis, which enables
more long-term dependencies, was presented by Popova et al. [681].]]>
			</paragraph>
			<paragraph>
				<![CDATA[We especially note a series of works by Insilico researchers Kadurin, Polykovskiy,
and others who applied different generative models to this problem:
- Kadurin et al. [419] train a supervised adversarial autoencoder [576] with the
condition (in this case, growth inhibition percentage for tumor cells after treatment)
added as a separate neuron to the latent layer;
 in [420], Kadurin et al. compared adversarial autoencoders (AAE) with variational autoencoders (VAE) for the same problem, with new modifications to the
architecture that resulted in improved generation;
- in [679], Polykovskiy et al. introduced a new AAE modification, the entangled
conditional adversarial autoencoder, to ensure the disentanglement of latent features; in this case, which is still quite rare for deep learning in drug discovery, a
newly discovered molecule (a new inhibitor of Janus kinase 3) was actually tested
in the lab and showed good activity and selectivity in vitro;
- Kuzmynikh et al. [488] presented a novel 3D molecular representation based on
the wave transform that led to improved performance for CNN-based autoencoders
and improved MACCS fingerprint prediction;
- Polykovskiy et al. [678] presented Molecular Sets (MOSES), a benchmarking
platform for molecular generation models, which implemented and compared various generative models for molecular generation including CharRNN [774], VAE,
AAE, and Junction Tree VAE [403], together with a variety of evaluation metrics
for generation results.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The above works can be thought of as generation of synthetic data (in this case,
molecular structures) that could be of direct use for practical applications.
Johnson et al. [406] undertake an ambitious project: they learn a generative model
of the variation in cell and nuclear morphology based on fluorescence microscopy
images. Their model is based on two adversarial autoencoders [576], one learning a
probabilistic model of cell and nuclear shape and the other learning the interrelations
between subcellular structures conditional on an encoding of the cell and nuclear
shape from the first autoencoder. The resulting model produces plausible synthetic
images of the cell with known localizations of subcellular structures.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One interesting variation of “synthetic data” in bioinformatics concerns learning
from live experiments on synthetically generated biological material. For example,
Rosenberg et al. [734] study alternative RNA splicing, in particular the functional
effects of genetic variation on the molecular phenotypes through alternative splicing. To do that, they created a large-scale gene library with more than two million
randomly generated synthetic DNA sequences, then used massively parallel reporter
assays (MPRA) to measure the isoform ratio for all mini-genes in the experiment,
and then used it to learn a (simple linear) machine learning model for alternative
splicing. It turned out that this approach significantly improved prediction quality,
outperforming state-of-the-art deep learning models for alternative splicing trained
on the actual human genome [959] in predicting the results of in vivo experiments.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We also note a related field of imitational modeling for bioinformatics data that
often results in realistic synthetic generators. For example, van den Bulcke et al. [99]
provide a generator for synthetic gene expression data, able to produce synthetic
transcriptional regulatory networks and simulated gene expression data while closely
matching real statistics of biological networks.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Synthetic data has not been widely used in natural language processing (NLP), at
least by far not as widely as in computer vision. In our opinion, there is a conceptual
reason for this. Compare with computer vision: there, the process of synthetic data
generation can be done separately from learning the models, and the essence of what
the models are learning is, in a way, “orthogonal” to the difference between real and
synthetic data. If I show you a cartoon ad featuring a new bottle of soda, you will
be able to find it in a supermarket even though you would never confuse the cartoon
with a real photo.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In natural language processing, on the other hand, text generation is the hard
problem itself. The problem of generating meaningful synthetic text with predefined
target variables such as topic or sentiment is the subject of many studies in NLP;
we are still a long way to go before it is solved entirely, and it is quite probable that
when text generation finally reaches near-human levels, discriminative models for
the target variables will easily follow from it, rendering synthetic data useless. In fact,
recent developments in natural language processing, especially the GPT family of
language models and BERT family of models for learning language representations
based on self-attention, are already proving this: recent works show that these models
can learn to perform complex NLP-related tasks with very little or no supervision [94,
698].]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nevertheless, there have been works that use data augmentation for NLP in a
fashion that borders on using synthetic data. There have been simple augmentation
approaches such as to simply drop out certain words [778]. A development of this
idea shown in [913] switches out certain words, replacing them with random words
from the vocabulary. The work [958] develops methods of data noising for language
models, adding noise to word counts in a way reminiscent of smoothing in language
models based on n-grams.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A more developed approach is to do data augmentation with synonyms: to expand
a dataset, one can replace words with their synonyms, getting “synthetic sentences”
that can still preserve target variables such as the topic of the text, its sentiment,
and so on. Zhang et al. [1004] used this method directly to train a character-level
network for text classification, while Galinsky et al. [256] tested augmentation with
synonyms for morphology-rich languages such as Russian. In [230], augmentation
with synonyms was used for low-resource machine translation, with an auxiliary
LSTM-based language model used to recognize whether the synonym substitution
is correct. Wang et al. [912], who concentrated on studying tweets, proposed to
use embedding-based data augmentation, using neighboring words in the word vector space as synonyms. Kobayashi [461] extends augmentation with synonyms by
replacing words in sentences with other words in paradigmatic relations with the
original words, as predicted by a bidirectional language model at the word positions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Techniques for generating synthetic text are constantly evolving. First, modern
language models based on multi-head self-attention from the Transformer family,
starting from the Transformer itself [891] and then further developed by BERT [192],
OpenAI GPT [697], Transformer-XL [179], OpenAI GPT-2 [698], GROVER [990],
and GPT-3 [94] generate increasingly coherent text. Actually, Zellers et al. [990]
showed that their GROVER model for conditional generation (e.g., generating the
text of a news article given its title, domain, and author), based on GPT-2, outperforms
human-generated text in the “fake news”/“propaganda” category in terms of style
and content (evaluated by humans), and GPT-3 is even better]]>
			</paragraph>
			<paragraph>
				<![CDATA[Moreover, recently developed models allow to generate text with GANs. This is
a challenging problem because unlike, say, images, text is discrete and hence the
generator output is not differentiable. There are several approaches to solving this
problem:
- training with the REINFORCE algorithm and other techniques from reinforcement
learning that are able to handle discrete outputs; this path has been taken in the
pioneering model named SeqGAN [982], LeakGAN for generating long text fragments [305], and MaskGAN that learns to fill in missing text with an actor-critic
conditional GAN [233], among others;
- approximating discrete sampling with a continuous function; these approaches
include the Gumbel Softmax trick [486], TextGAN that approximates the arg max
function [1007], TextKD-GAN that uses an autoencoder to smooth the one-hot
representation into a softmax output [313], and more;
- generating elements of the latent space for an autoencoder instead of directly
generating text; this field started with adversarially regularized autoencoders by
Zhao et al. [1018] and has been extended into text style transfer by disentangling
style and content in the latent space [405], disentangling syntax and semantics [40],
DialogWAE for dialog modeling [301], Bilingual-GAN able to generate parallel
sentences in two languages [704], and other works.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This abundance of generative models for text has not, however, led to any significant
use of synthetic data for training NLP models; this has been done only in very
restricted domains such as electronic medical records [302] (we discuss this field in
detail in Section 11.5). We have suggested the reasons for this in the beginning of
this section, and so far the development of natural language processing supports our
view.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Overall, in this chapter we have seen a bird’s eye overview of using synthetic data
in several different domains that do not usually spring to mind in discussions of
synthetic data: tabular data such as system logs, data for neural programming, and
synthetic data for natural language processing and for bioinformatics other than
medical imaging. Sometimes this kind of synthetic data generation is still restricted
(as in NLP), and sometimes it has different purposes (as in, e.g., de novo drug design),
but all of these examples show how synthetic data is starting to gain traction even in
less-than-obvious applications.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first direction that we highlight as important for further study in the field of
synthetic data is procedural generation. Take, for instance, synthetic indoor scenes
that we discussed in Section 7.3. Note that in the main synthetic dataset for indoor
scenes, SUNCG, the 3D scenes, and their layouts were created manually. While we
have seen that this approach has an advantage of several orders of magnitude in
terms of labeled images over real datasets, it still cannot scale up to millions of 3D
scenes. The only way to achieve that would be to learn a model that can generate
the contents of a 3D scene (in this case, an indoor environment with furniture and
supported objects), varying it stochastically according to some random input seed.
This is a much harder problem than it might seem: e.g., a bedroom is much more
than just a predefined set of objects placed at random positions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is a large related field of procedural content generation for video games [336,
786, 863], but we highlight a recent work by Qi et al. [687] as representative for state
of the art and more directly related to synthetic data. They propose a human-centric
approach to modeling indoor scene layout, learning a stochastic scene grammar
based on an attributed spatial AND-OR graph [1026] that relates scene components,
objects, and corresponding human activities. A scene configuration is represented by
a parse graph whose probability is modeled with potential functions corresponding to
functional grouping relations between furniture, relations between a supported object
and supporting furniture, and human-centric relations between the furniture based
on the map of sampled human trajectories in the scene. After learning the weights of
potential functions corresponding to the relations between objects, MCMC sampling
can be used to generate new indoor environments. Qi et al. train their model on the
very same SUNCG dataset and show that the resulting layouts are hard to distinguish
(by an automated state-of-the-art classifier trained on layout segmentation maps)
from the original SUNCG data. The sample pictures—some of them are reproduced
in Fig. 12.1—also look quite convincing.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In Section 7.2, we have already discussed ProcSy by Khan et al. [449]. In addition
to randomizing weather and lighting conditions, another interesting part of their work
is the procedural generation of cities and outdoor scenes. They base this procedural
generation on the method of Parish and Müller [656], which is, in turn, based on the
notion of Lindenmayer systems (L-systems) [686] and embodied in the CityEngine
tool [954] (see Section 7.2). They use a part of real OpenStreetMaps data for the
road network and buildings, but we hope that future work based on the same ideas
can offer fully procedural modeling of cities and road networks.]]>
			</paragraph>
			<paragraph>
				<![CDATA[We believe that procedural generation can lead to an even larger scale of synthetic
data adoption in the future, covering cases when simply placing the objects at random
is not enough. The nature of the model used for procedural generation may differ
depending on the specific field of application, but, in general, for procedural generation, one needs to train probabilistic generative models that allow for both learning
from real or manually prepared synthetic data and then generating new samples]]>
			</paragraph>
			<paragraph>
				<![CDATA[The next step was taken by Google Brain researchers Cubuk et al. [172], who
continued this work by presenting a framework called AutoAugment for learning
augmentation strategies from data. Their approach is modeled after recent advances
in neural architecture search [35, 57, 1031, 1032] that have recently yielded new
families of neural architectures for feature extraction from images and image classification (EfficientNet, see Section 3.2 for more details), object detection (the EfficientDet family, see Section 3.3), and more. Cubuk et al. use reinforcement learning
to find the best augmentation policy composed of a number of parameterized operations. As a result, they significantly improve state-of-the-art results on such classical
datasets as CIFAR-10, CIFAR-100, and ImageNet.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Zakharov et al. [986] look at a similar idea from the point of view of domain
randomization (see also Section 9.1). Their framework consists of a recognition
network that does the basic task (say, object detection and pose estimation) and
a deception network that transforms the synthetic input with an encoder-decoder
architecture. Training is done in an adversarial way, alternating between two phases:
- fixing the weights of the deception network, perform updates of the recognition
network as usual, serving synthetic images transformed by the deception network
as inputs;
- fixing the weights of the recognition network, perform updates of the deception
network with the same objective but with reversed gradients, updating the deception network so as to make the inputs hard for the recognition network]]>
			</paragraph>
			<paragraph>
				<![CDATA[The deception network is organized and constrained in such a way that its transformations do not change the ground truth labels or change them in predictable
ways. This adversarial framework exhibits performance comparable to state-of-the-art domain adaptation frameworks and shows superior generalization capabilities
(better avoiding overfitting).]]>
			</paragraph>
			<paragraph>
				<![CDATA[We believe that this meta-approach to automatically learning the best way to
generate synthetic data, both high-level and low-level, is an important new direction
that might work well for other applications too. In my opinion, this idea might be
further improved by methods such as the SPIRAL framework by Ganin et al. [258]
or neural painter models [375, 617, 1021] that train adversarial architectures to
generate images in the form of sequences of brushstrokes or higher-level image
synthesis programs with instructions such as “place object X at location Y ”; these
or other kinds of high-level descriptions for images might be more convenient for
the generation feedback loop. Whether with these ideas or others, I expect further
developments in this direction in the nearest future.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The second part of the pipeline generates the actual images with a conditional
BiGAN (bidirectional GAN) architecture [201]. Bidirectional GAN is an architecture
that learns to transform data in both directions, from latent representations to the
objects and back, while regular GANs learn to generate only the objects from latent
representations. The conditional BiGAN (c-BiGAN) modification developed in [905]
does the same with a condition, which in this case are the eye shape parameters
produced by HGSM. As a result, the model byWang et al. can work in both directions:
- generate eye images by sampling the gaze from a prior distribution, sampling 2D
eye shape parameters from HGSM, and then using the generator G of c-BiGAN
to generate a refined image;
- infer gaze parameters from an eye image by first estimating the eye shape through
the encoder E of c-BIGAN and then performing Bayesian inference in the HGSM
to find the posterior distribution of gaze parameters.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In direct applications of synthetic data, we have discussed many different domains
and use cases, from basic computer vision tasks such as stereo disparity estimation or
semantic segmentation to full-scale simulated environments for autonomous driving,
unmanned aerial vehicles, and robotics. In the domain adaptation part, we have
surveyed a wide variety of generative models for synthetic-to-real refinement and
for feature-level domain adaptation. As another important field of synthetic data
applications, we have considered data generation with differential privacy guarantees.
We have also reviewed the works dedicated to improving synthetic data generation
and outlined potential promising directions for further research.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In general, throughout this book, we have seen synthetic data work well across
a wide variety of tasks and domains. I believe that synthetic data is essential for
further development of deep learning: many applications require labeling which is
expensive or impossible to do by hand, other applications have a wide underlying data
distribution that real datasets do not or cannot fully cover, yet other applications may
benefit from additional modalities unavailable in real datasets, and so on. Moreover, I
believe that synthetic data will find new applications in the near future. For example,
while this book does not yet have a section devoted to sound and speech processing,
works that use synthetic data in this domain are already beginning to appear [510,
745]. As synthetic data becomes more realistic (where necessary) and encompasses
more use cases and modalities, I expect it to play an increasingly important role in
deep learning.
I strongly believe that synthetic data will be indispensable in the future of deep
learning. Let us build this future together!]]>
			</paragraph>
			<paragraph>
				<![CDATA[Still, despite all of these advances, the basic pipeline of using machine learning
for a given problem remains mostly the same, as shown in Figure 1.1:
- first, one has to collect raw data related to the specific problem and domain at
hand;
- second, the data has to be labeled according to the problem setting;
- third, machine learning models train on the resulting labeled datasets (and often
also validate their performance on subsets of the datasets set aside for testing);
- fourth, after one has trained the model, it needs to be deployed for inference
in the real world; this part often involves deploying the models in low-resource
environments or trying to minimize latency.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Still, any machine learning practitioner will tell you that it is exactly the “Data”
and (for some problems especially) “Annotation” phases that take upwards of 80%
of any real data science project where standard open datasets are not enough. Will
these 80% turn into 99% and become a real bottleneck? Or have they already done
so? Let us find out.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Machine learning is hot, and it has been for quite some time. The field is growing
exponentially fast, with new models and new papers appearing every week, if not
every day. Since the deep learning revolution, for about a decade, deep learning has
been far outpacing other fields of computer science and arguably even science in
general. Analysis of the submissions from arXiv1, the most popular preprint repository in mathematics, physics, computer science, and several other fields, shows how
deep learning is taking up more and more of the papers.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Machine learning has been growing in scale, breadth of applications, and the amounts
of required data. This presents an important problem, as the requirements of state-of-the-art machine learning models, especially data-hungry deep neural networks,
are pushing the boundaries of what is economically feasible and physically possible.
In this introductory chapter, we show and illustrate this phenomenon, discuss several approaches to solving the data problem, introduce the main topic of this book,
synthetic data, and outline a plan for the rest of the book.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The vast majority of the thousands of papers published in machine learning deal
with the “Training” phase: how can we change the network architecture to squeeze
out better results on standard problems or solve completely new ones? Some deal
with the “Deployment” phase, aiming to fit the model and run inference on smaller
edge devices or monitor model performance in the wild]]>
			</paragraph>
			<paragraph>
				<![CDATA[Imagine how much work it is to label a photo like this by hand. Naturally, people
have developed tools to help partially automate the process. For example, a state-of-the-art labeling tool (see, e.g., [829]) will suggest a segmentation candidate produced
by some general-purpose model, and the human annotator is only supposed to fix
the mistakes of this model by clicking on individual pixels that are segmented incorrectly. But it might still take minutes per photo, and the training set for a standard
segmentation or object detection model should have thousands or even tens of thousands of such photos. This adds up to human-years and hundreds of thousands, if not
millions, of dollars spent on labeling only.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There exist large open datasets for many different problems, segmentation and
object detection included. But as soon as you need something beyond the classes,
settings, and conditions that are already well covered in these datasets, you are out of
luck; for instance, ImageNet does have cows, but not shot from above with a drone.
And even if the dataset appears to be tailor-made for the problem at hand, it
may contain dangerous biases. For example, suppose that the basic problem is face
recognition, a classic computer vision problem with large-scale datasets freely available [37, 38, 366, 542]; there also exist synthetic datasets of people, and we will
discuss them in Section 6.6. But if you want to recognize faces “in the wild”, you need
a dataset that covers all sorts of rotations for the faces, while standard datasets mostly
consist of frontal photos. For example, the work [1017] shows that the distribution of
face rotations in IJB-A [460], a standard open large-scale face recognition dataset, is
extremely unbalanced; the work discusses how to fill in the gaps in this distribution
by producing synthetic images of faces from the IJB-A dataset (see Section 10.3,
where we discuss this work in detail).]]>
			</paragraph>
			<paragraph>
				<![CDATA[To sum up: current systems are data-intensive, data is expensive, and we are
hitting the ceiling of where we can go with already available or easily collectible
datasets, especially with complex labeling. So what’s next? How can we solve the
data problem? Is machine learning heading towards a brick wall? Hopefully not, but
it will definitely take additional effort. Over the next sections, we discuss what can
be done to alleviate the data problem. We will give a very high-level overview of
several possible approaches currently explored by machine learning researchers and
then make an introduction to the main topic of this book: synthetic data.]]>
			</paragraph>
			<paragraph>
				<![CDATA[First, one can use unlabeled data to produce new labeled data; although the new
“pseudolabels” are not guaranteed to be correct, they will still help, and one can
revisit and correct them in later iterations. A striking illustration of this approach has
appeared in a recent work by Xie et al. [956], where researchers from Google Brain
and Carnegie Mellon University applied the following algorithm:
- start from a “teacher” model trained as usual, on a (smaller) labeled dataset;
- use the “teacher” model on the (larger) unlabeled dataset, producing pseudolabels;
- train a “student” model on the resulting large labeled dataset;
- use the trained student model as the teacher for the next iteration, and then repeat
the process iteratively.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This is a rather standard approach, used many times before in semi-supervised
learning and also known as knowledge distillation [129, 293, 345, 541, 603]. But
by adding noise to the student model, Xie et al. managed to improve the state-of-the-art results on ImageNet, the most standard and beaten-down large-scale image
classification dataset. For this, however, they needed a separate dataset with 300
million unlabeled images and a lot of computational power: 3.5 days on a 2048-core
Google TPU, on the same scale as AlphaZero needed to outperform every other
engine in Go and chess [800]; we will shortly see that this kind of computation does
not come for free.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another interesting example of replacing labeled data with (lots of) unlabeled
data comes from the problem we already discussed: segmentation. It is indeed very
hard to produce labeled data for training segmentation models... but do we have
to? Segmentation models from classical computer vision, before the advent of deep
learning, do not require any labeled data: they cluster pixels according to their features
(color and perhaps features of neighboring pixels) or run an algorithm to cut the graph
of pixels into segments with minimal possible cost [657, 839]. Modern deep learning
models work better, of course, but it looks like recent advances make it possible to
train deep learning models to do segmentation without labeled data as well.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Approaches such as W-Net [948] use unsupervised autoencoder-style training
to extract features from pixels and then segment them with a classical algorithm.
Approaches such as invariant information clustering [399] develop image clustering
approaches and then apply them to each pixel in a convolution-based way, thus
transforming image clustering into segmentation. One of the most intriguing lines
of work that results in unsupervised clustering uses GANs for image manipulation]]>
			</paragraph>
			<paragraph>
				<![CDATA[The “cut-and-paste” approach [716] works in an adversarial way:
- one network, the mask generator, constructs a segmentation mask from the features
of pixels in a detected object;
- then the object is cut out according to this mask and transferred to a different,
object-free location on the image;
- another network, the discriminator, now has to distinguish whether an image patch
has been produced by this cut-and-paste pipeline or is just a patch of a real image.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The idea is that good segmentation masks will make realistic pasted images, and in
order to convince the discriminator, the mask generator will have to learn to produce
high-quality segmentation masks. We will discuss this approach and its ramifications
for synthetic data in Section 9.3.
Semi-supervised teacher–student training and unsupervised segmentation via cut-and-paste are just two directions out of many that are currently being developed. In
these works, researchers are exploring various ways to trade the need for labeled
datasets for extra unlabeled data, extra unrelated data, or extra computation, all of
which are becoming more and more readily available. Still, this does not completely
solve the data problem, and the computational challenges might prove to be insurmountable.]]>
			</paragraph>
		</content>
	</book>
	<book name="The Cost of Electricty, An Overview">
		<content>
			<paragraph>
				<![CDATA[Electricity is arguably the most important source of energy in the world.
Without it, the features that make our societies modern cannot operate. From
electric light bulbs to the most advanced mobile phone, modern technologies
cease to function without electricity to power them.
Electricity is environmentally important too. The electricity industry is the
single largest global producer of greenhouse gases through the extensive use of
fossil fuel power stations. In 2018, 42% of all energy-related carbon dioxide
emissions were produced by the power sector.1 Reducing the emissions from
these plants is a key to managing global warming. At the same time, electricity
is a major part of the solution to this problem. Electricity generated without the
production of greenhouse gases, from renewable or nuclear power plants,2 can
be used to supplant the use of fossil fuels not only for powering advanced
technological equipment but also in helping to meet vital human needs for
heating (or cooling) and cooking. If we want to eliminate combustion technologies that burn coal, oil, gas or wood but still maintain or improve living
standards across the globe, electricity is the only viable substitute capable of
meeting all these needs.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 1.1 shows figures for the total production of electricity across the
world each year between 1999 and 2019. Over this 20-year period, total global
production increased from 14,918 TWh to 27,005 TWh, an increase of 81%.
Much of this increase has been fuelled by growth in Asia. The figures also
show that electricity production has increased year upon year except between
2008 and 2009 when production decreased slightly. The fall coincided with the
global financial crisis which affected the economic performance of most
nations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 1.2 shows how electricity production was broken down regionally in
2019. The largest regional production was in the Asia Pacific region with 47%
of the aggregate total. North America, with 20% of annual production, had the
second largest output followed by Europe with 15%. At the bottom of the table
was Africa, which accounted for only 3% of the total. This figure suggests that
compared with other regions, in Africa access to electricity is limited.
Total energy consumption from all sources across the globe was 583.9 EJ
in 2019, or 162,194 TWh. Total electricity production of 27,005 TWh
represents around 17% of this, not allowing for losses between production and
consumption.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The pivotal role of electric power in the modern world makes the cost of
electricity an important indicator and determinant for societies across the
globe, both economically and environmentally. The cost of electricity, where it
is available, determines who has access to the energy source. The higher the
cost of a unit of electricity, the more difficult it becomes for those on lower
incomes to use it freely. And, from an environmental perspective, electricity
will only be able to replace other, more polluting energy sources if it is cheaper
than those other sources. Otherwise, combustion fuels will continue to be
employed. Our future environmental safety requires electricity to be affordable
for all.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At an industry level, cost is equally important. As economies expand and
require more electricity, cost will determine either wholly or in large part the
type of new power plant that is built even if this overrides environmental
considerations. This effect can be clearly seen in action today. Fossil fuels,
especially coal, are cheap and so in spite of the environmental cost of building
new coal-fired power stations, new coal-fired power stations continue to be
built.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The lifetime of a power plant
It is not only expanding economies that require new power stations but also
established economies. Like all modern industrial products, power stations
have a finite lifetime. For some, hydropower stations, for example, the lifetime
may be as much as 100 years. However most of the technologies that are used
to generate electric power have much shorter lives than this, typically 20e
30 years. This means that every 30 years or so, old power stations must be
decommissioned and new ones should be built to replace them. Thirty years is
a long time compared with the lifespan of many modern products such as
motor vehicles or electronic devices, but even so, like those other products, the
stock of power stations must be renewed regularly. When this happens, cost
comes into play.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Each time a new power station is needed, a study will be carried out to
determine what type of power plant will offer the best return. Many factors
may be taken into account including the environmental impact of different
technologies, but in most cases either the economic return that the project will
offer, or its cost, will take precedent. That means that clean technologies must
be able to produce electricity more cheaply than dirty technologies if they are
to be preferred.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The lifetime of a power plant is also a valuable metric by which to examine
aspects of its performance other than its economic ability. We can, for
example, compare the environmental cost of different power plants over the
lifetime of each, enabling us to make a comparison of the relative environmental impact of each type. Or we can examine the lifetime efficiency of a
plant in converting the energy it exploits into electricity. Lifetime analyses of
this type provide a useful means of gaining insight into other sides of performance and they can reveal unexpected benefits or deficiencies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The main factors contributing to the cost of electricity
On purely economic terms, there are two key factors which determine the cost
of a power station. The first of these is the capital cost of a power plant, which
is the cost of manufacturing or constructing the components of the station and
the cost of erecting them. Depending on the type of power plant, this will be a
sum of material costs, the manufacturing costs and the labour costs. The
capital cost of most power stations is significant and in many cases this can
only be met through some form of debt financing. The cost of this financing
will also feed into the final cost and so can be considered as a part of the
capital cost.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The other major contributing factor is the cost of the fuel used by the power
station. For combustion power plants, this will be the cost of the coal, oil, gas,
wood or waste that is used to fire the plant. For a nuclear power plant, it is the
cost of the nuclear fuel upon which this technology depends. In the case of
generating technologies that depend on renewable energy sources, the fuel e
be it water flowing along a river, the wind or the energy from the sun e is
usually free.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The fact that most conventional power stations rely on a fuel that must be
paid for while renewable energy is generally available without cost means that
the cost effectiveness of the two types of technology depends critically on the
balance between fuel cost and capital cost. For example, gas-fired power
stations can be extremely cheap to build, but the fuel, while is often cheap, can
sometimes become very expensive. This can make them periodically uneconomical to run. A hydropower station, on the other hand, will probably cost a
lot to build but will provide power at little cost once it has been paid for.
There is a third factor that also contributes to the cost of electricity, the cost
of operating and maintaining a power station. However, because all stations
require this, the relative effect on the overall cost is small.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Network factors
Electricity has some of the qualities of a commodity. It can be bought and sold
on national and international markets and the price is dependent in part at least
on demand. However, unlike a commodity, electricity has no physical presence. It is not possible to buy or sell a barrel of electricity. Electricity is
ephemeral. As soon as it is generated, it must be used. This means that the
production of electricity must be carefully balanced against demand. If these
come out of balance, problems can result.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For the majority of electricity users, power is delivered across an electricity
network. This power is generated by an array of different power stations that
are connected to the grid. At the centre of this web is a system control centre
where the amount of electricity fed into the network is balanced against the
amount being taken out. For this balancing act to be possible, the output of the
power stations on the grid must be capable of being modulated at will.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This was traditionally achieved by using different types of power station
with different characteristics, some providing constant output, the base load
for the grid, others providing a variable output to meet varying demand as
conditions changed. This traditional system has been upset in recent years by
the introduction of large quantities of renewable energy from wind and solar
power stations. The output of these plants will vary with weather conditions,
so the amount available to the grid varies, adding another source of volatility
to the grid balance equation. In addition to demand on a grid changing with
time, supply now varies too in a way that did not happen previously. Using
variable renewable energy effectively requires additional technologies to keep
the grid in balance and this can add to their cost relative to the more conventional power sources]]>
			</paragraph>
			<paragraph>
				<![CDATA[When electricity is delivered across a network, there are additional costs
added to the price of the product. The transmission and distribution of electricity across the wires of the network is not 100% efficient; it involves losses.
A part of these are the electrical losses inherent in any system in which
electrical power flows along wires. Other losses are due to poor maintenance
or to theft. In addition, the cost of managing the network has to be taken into
account. The net result is that the price of the electricity purchased by a
consumer at the end of the network will be significantly higher than the cost of
the electricity when it entered the network. In this book we will be dealing
mostly e but not entirely e with generating costs, the cost of electricity at the
point it enters the network.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Environmental and structural factors
Although there is growing awareness of the environmental impact of human
activity such as power generation, the cost associated with this is still not
widely taken into account when determining the actual cost of a kilowatt of
electrical power. It is true that there are carbon taxes or levies of various sorts
that have been introduced by different nations or regions, but these have been
piecemeal and they do not yet force power generators to pay the full cost of the
environmental damage they cause.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This imbalance normally favours combustion technologies at the expense
of renewable technologies. Righting the imbalance requires action at a government level, but in most cases this means damaging the economic activity of
the nation in question because paying the full cost of the environmental impact
of emissions such as carbon dioxide will significantly increase the cost of
electricity in many places. Thus there is a political dimension to electricity
costs and also a political risk factor for power generators. If governments
should decide to act to counter environmental damage, this might suddenly
affect the economic viability of some types of power plant. This could
potentially leave coal-fired power stations and others as ‘stranded assets’ with
outstanding debts to pay but no income from which to pay them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For the moment, that has not happened to any significant extent. But
ignoring the environmental cost of a particular generating technology is not the
only way that the true cost of generating electricity can be distorted. Another is
through selective subsidies. Again this operates at a national level, when a
government seeks to protect or encourage a particular industry by artificially
lowering its costs. There has been widespread use of subsidies to encourage
the construction of renewable energy power plants. This helps to make them
more competitive relative to other technologies. However, most analyses show
that the largest subsidies around the world are aimed at fossil fuels.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another type of subsidy that is particularly common in oil and gas producing countries is a tariff subsidy that makes the cost of electricity artificially
low. The inevitable consequence of this is that consumers use more electricity,
and where this is generated by the local oil or gas, environmental emissions
become elevated. Tariff subsidies are used across the world, often to alleviate
poverty. However, such subsidies are often poorly targeted and, again, can lead
to distortion in consumption patterns. In many cases, this type of subsidy is a
political tool too.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The historical cost of electricity is known. It can be extracted from various
data stores and laid out in tables such as those found in later chapters of this
book. These data can often be broken down to show how different factors such
as capital cost and fuel cost contribute to the final cost of energy. Costs to
different types of consumer can be shown too, as well as the distribution of
consumption between different sectors. These types of data will show us how
prices and consumption have varied over time and it can indicate trends.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The most useful cost of electricity to know, however, is the cost of a unit of
electricity at some point in the future. That is the figure that is needed when
trying to determine which type of new power station is going to provide the
most economical source once it has been built. That is the figure that planners
and proposers of different sorts want to know. Or, it is the figure that shows
how much more (or less) future electricity might cost if government policy
favours a particular technology, such as wind power or nuclear power.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some future costs can be extrapolated from historical costs. An operating
power plant that provided electricity for a certain price yesterday can be
predicted with some certainty to provide electricity for a closely related price
tomorrow. But future planning, say when deciding what type of new power
plant to build, will likely depend on knowing the cost of the electricity from
different types of power plant up to 30 years hence. Such figures can only be
determined with limited certainty. Estimating them depends on making
guesses about a number of unknowable factors and then using these guesses to
work out what the end result might be. In other words, it depends on
modelling.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Modelling for the future cost of electricity is well established and widely
used, and this book uses figures from such economic models. But there is a
high degree of uncertainty associated with the results of such modelling and
this must always be taken into account when using the figures these models
produce.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This book will be mainly concerned with numbers rather than models. Its
purpose is to provide as much guidance as is available from historical costs
and from the historical output of economic modelling of the future cost of
electricity. It will not be laying out those models or discussing them in any
detail, or at least only in as much detail as is necessary to understand where the
numbers come from. The data that form the heart of the book are taken from
internationally credited sources and will be the most up to date at the time of
publication. The aim is not to provide an economic analysis of the electricity
industry. That can be found elsewhere. The aim is, rather, to assemble as much
data as are available today to delineate the important cost trends that can be
established. The philosophical question of whether the future can be predicted
from past behaviour aside, these data can, I would suggest, provides valuable
insights.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The power generating
technologies
Modern electricity production depends on a range of technologies that convert
one form of energy into another. Before the widespread use of electrical power,
several of these technologies were used to provide mechanical power.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Historically, hydropower was the first type of generating technology to
emerge from these earlier devices. This technology uses a turbine to convert
the energy in flowing water into rotary motion. By the 19th century, water
wheels e simple turbines e had been used for millennia as sources of mechanical power, so exploiting the technology to drive a rotating dynamo and
produce electricity was an obvious step. This was swiftly followed by the
adaptation of combustion technology, already well known during the nineteenth century too from steam engines, to produce electrical power from oil or
coal. The rapid growth of electricity systems during the twentieth century then
led to the development of a diverse range of new energy conversion technologies such as solar power that had no direct precursors. Electricity production from the most important sources for the year 2017, the latest year for
which complete figures are available from the International Energy Agency
(IEA), is shown in Table 2.1.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Neither the historical development of electric power nor the technological
niceties of power generation technologies are of importance to the understanding of the cost of electricity. What is important is to understand the
different characteristics of the technologies and how these contribute to the
way they are valued as sources of electrical power. For example, combustion
power plants can usually be turned on or off at will and so they are considered
as reliable sources of electricity. However, many renewable generating technologies depend on a variable and often unpredictable energy source. This
renders them inherently less reliable. Or, some technologies are fast acting, so
they can be brought into service quickly. Others have an inherent inertia that
makes them slower to bring on line. Again, this will influence their perceived
usefulness.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal-fired power generation is both the most important and the most polluting
type of electricity generation in use today. According to the IEA, the production of electricity from coal-fired power plants exceeded 10,000 TWh for
the first time in 2018,1,2 a significant milestone. Overall output from coal-fired
plants increased by 2.6% between 2017 and 2018, and total production was
equivalent to 38% of global electricity generation of 26,300 TWh.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The use of coal for electricity production is not geographically uniform.
The continued growth in coal-fired power generation is concentrated in China,
India and Southeast Asia. The use of coal for electricity generation, meanwhile, dropped in the United States and Europe between 2017 and 2018, a
trend that is expected to continue as renewable sources become increasingly
important to these regions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal-fired power stations are relatively expensive to build, but the fuel they
burn, coal, is cheap. This makes coal the fuel of choice in many countries that
have coal deposits. However, the fuel is costly to transport over great distances, so while there is an international market for high-quality coal, this is
relatively limited with only around 21% being traded internationally in 2018
according to the World Coal Association.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The most common technology for coal-fired power generation involves
burning pulverised coal in air in a specially designed boiler where the heat
generated is used to raise steam, and the steam is used to drive a steam turbine.
The best steam turbineebased coal plants can achieve an energy conversion
efficiency of around 45%, but many older plants are much less efficient. The
efficiency of the Japanese fleet of coal-fired plants in 2016 was around 42%
and that of the US fleet was 34% according to the IEA. Meanwhile, the IEA
has put global coal plant efficiency at the end of the second decade of the 21st
century to be around 37.5%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When coal is burnt in air, the major product of the combustion process is
carbon dioxide. The more efficient a plant is, the less carbon dioxide it produces for each unit of electricity it generates, so high-efficiency plants are
considered cleaner. However, the technology for coal-fired power plants is
already highly optimised and the only way that efficiency can be increased is
by increasing the operating temperatures and pressures of the steam in the
plant. This puts a heavy load on the steam plant components, many of which
need to be made from specialised materials to be able to withstand the conditions. Advances in material development are slow and new materials are
often costly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal combustion is the most significant source of electricity sector carbon
dioxide, but it is possible to capture the gas that is produced in a coal-fired
power station before it enters the atmosphere. Various technologies capable
of achieving this have been developed. The use of these technologies reduces
the efficiency of the power plant, thereby increasing the overall cost of the
electricity it produces. The carbon capture technology also increases the
overall capital cost of a coal plant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In addition to carbon dioxide, coal combustion produces a range of other
harmful emissions including sulphur dioxide, nitrogen oxides, heavy metals
and small dust particles. All these harmful emissions must be captured before
the flue gases can be released into the atmosphere. Again, these emission
control features increase the capital cost of the power generating facility.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Modern coal-fired power stations are generally large with single units up to
1000 MW and power plants of multiple units with capacities of 5000 MW,
sometimes more. The largest plants are often built close to the mines that
supply them with fuel.
A coal-fired power station has a relatively high mechanical inertia making
it slow to start and these stations have traditionally been used to provide
base load power. More recently, coal plants in some regions have had to
provide a much more variable output, to the extent that in the United Kingdom
in 2020, coal plants have been used as peak load plants. However, the plants
are not easily or economically adapted for this type of service.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While the high inertia of a coal plant makes it less flexible than some
technologies when it comes to variable output, large inertia can be advantageous in other situations. The massive steam turbines in a coal plant have a
high rotational momentum when spinning and this enables them to be used to
help stabilise fluctuations in the grid caused by variations in supply and demand elsewhere. This ‘spinning reserve’ is a valuable resource for grid stability, particularly as more renewable plants with variable output are connected
to the grid. However, the polluting nature of coal makes it unlikely that coal
stations will be maintained simply to provide this spinning reserve.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal-fired power generation without carbon capture remains one of the
cheapest forms of power generation and this has so far enabled coal-based
generation to continue to flourish in spite of its high environmental cost.
However, the most modern renewable sources are now challenging it on cost,
if not yet on capacity. Unabated5 coal-fired generation must decline in the
coming decades if global warming is to be controlled. The additional cost of
carbon capture makes plants with this technology much less attractive
economically and it is likely only to be used as a transitional technology while
alternative technologies such as wind power and solar power are built up and
integrated into grids]]>
			</paragraph>
			<paragraph>
				<![CDATA[In spite of this, the economics of coal has made many nations reluctant to
abandon coal-fired generation. This remains one of the most contentious areas
of the global warming debate.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Gas-fired power generation
Natural gasefired power plants are the second most significant category of
fossil fuel power plants in operation today. During 2018, power plants that
exploited this fuel generated 6100 TWh of power, an increase of 4% over the
figure for 2017 according to the IEA.6 This was pushed by strong growth in the
United States where production rose by 17% and China where overall production rose by 30% but from a relative low base. In contrast, production of
power from natural gas fell by 7% in Europe where renewable generation is
advancing strongly at the expense of conventional sources. The fuel accounted
for 23% of total global power generation in 2018, well below that from coalfired plants.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unlike coal, natural gas is traded both nationally and internationally and
this has led to widespread use. While reserves of the fuel can be found in many
regions, a small number of countries hold the majority of known reserves. The
Russian Federation, Turkmenistan, Iran and Qatar held 58% of total global
reserves at the end of 2017 according to the BP Statistical Review of World
Energy.7 Other major producers include Saudi Arabia and the United States.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gas is usually distributed over land using natural gas pipelines.
These can be national systems as in the United States and in many countries in
Europe, or they can be transnational. Much of Europe’s natural gas comes
through pipelines from the Russian Federation. Further international trade in
natural gas is carried out using liquefied natural gas which can be stored in
container vessels and shipped around the world. This is then off-loaded at
natural gas terminals before being fed into local pipeline systems.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gas can be burnt in a boiler in the same way as pulverised coal to
produce steam that drives a steam turbine to produced electricity. However, the
most important means of exploiting natural gas for power generation is with a
gas turbine. These devices use natural gas combustion to generate a stream of
hot, high-pressure air that is then used to drive an air turbine to generate rotary
motion which powers a generator.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Simple gas turbine power plants can have efficiencies similar to that of the
best coal-fired power plants at around 46% energy conversion efficiency.
Higher efficiency can be achieved by using a more complex configuration
called a combined cycle plant. This uses a gas turbine as the primary turbine
generator. The exhaust gases from the gas turbine, still at high temperature, are
then used to raise steam in a waste heat boiler and this steam drives a steam
turbine. This configuration, using large gas and steam turbines, can reach an
energy conversion efficiency of perhaps 61% in the best plants today.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The combustion of natural gas produces a combination of water (as water
vapour) and carbon dioxide. There may be a small proportion of other impurities such as unburnt hydrocarbons and carbon monoxide. The combustion
process can also produce significant quantities of nitrogen oxides from air as a
result of the high temperatures reached. The nitrogen oxides and other minor
impurities are removed in gas turbine power plants, but there remains a significant amount of carbon dioxide. However, the quantity produced during
combustion is much lower than for the combustion of a similar quantity (in
energy terms) of coal. This, together with the higher efficiency, means that the
carbon intensity8 of a natural gasefired power plant is much lower than that of
a coal-fired power plant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gas turbines come in many sizes, from small units of a few
megawatts to massive turbines with generating capacities of 600 MW. The
very large gas turbines are generally designed exclusively for combined cycle
plants; a single combined cycle unit may be capable of generating up to
800 MW of power, comparable to the largest steam turbines in coal plants.
However, unlike coal plants, gas turbine combined cycle plants are relatively
flexible and can be adapted easily to grid support and peak power production.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Traditionally, gas turbines have been used for a range of grid services. The
largest combined cycle plants were intended for base load operation while
small, simple cycle gas turbines (those without attached steam turbines) were
used for producing power at times of peak demand. These small units, based
on aero engines, can be stopped and started very quickly. However, the
electricity from a small, peaking gas turbine is likely to cost much more than
from a large combined cycle gas turbine plant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Gas turbine power plants are among the cheapest power plants to erect. In
consequence, the electricity from these large plants can be extremely cheap.
However, the cost depends critically on the natural gas price which can be
highly volatile. In times of high natural gas prices, many combined cycle
power plants stand idle because they are not economical to operate.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The lower atmospheric carbon dioxide emissions from gas turbine power
plants have led to these being used in many countries and regions to reduce
emissions in order to meet global warming targets. Where these plants are
displacing coal-fired generation, this will lead to a reduction in national
emissions. However, substitution of natural gas for coal can only serve as a
temporary measure as they still produce significant quantities of carbon dioxide. As with coal plants, it would be possible to capture the carbon dioxide
from natural gas production, but this makes the plants less economical to
operate. Over the longer term, therefore, it is likely that many natural gase
fired plants will be replaced by renewable energy sources. However, natural
gas plants may have a longer role to play in grid support where their ability to
respond rapidly to demand changes makes them a good match for renewable
energy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Piston engineebased power generation
Piston engine power plants encompass a diverse range of energy conversion
devices that all exploit the movement of a piston in a barrel as the primary
means of converting heat energy in kinetic energy. They include diesel engines
and spark ignition engines e both types of internal combustion engine e as
well as external combustion engines such as the Stirling engine. The size of
units employed for electricity generation can vary from 1 kW to 65 MW.
Efficiencies vary widely too. Virtually all internal combustion engines used for
power production burn fossil fuels and so they produce carbon dioxide as well
as a range of other pollutants. External combustion engines can exploit energy
from other sources, too, such as the sun.]]>
			</paragraph>
			<paragraph>
				<![CDATA[With such a disparate array of devices falling into this category, it is
difficult to assess their global contribution to electricity production. However,
estimates suggest that perhaps 50e60 GW of generating capacity based on
these engines are installed every year.
The majority of the piston engines that are used for power generation are
derived from engines developed initially for transportation applications. The
smallest engines in use are often based on automobile engines. These engines
are cheap, inefficient and do not last for very long (in power plant terms). This
is a disadvantage where an engine is required for continuous duty but many of
these small engines are used in power backup systems where they are only
required to run under emergency conditions so that short operating life is
unimportant. These engines usually operate on petrol or diesel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Larger engines, in 10 kWe5 MW range, are variously derived from truck
or railroad engines. These engines have much longer operating lives and when
adapted for power generation service can last as long as other fossil fuel
generating technologies. They are also much more efficient than the small
engines. They comprise both spark ignition and compression ignition (diesel)
engines. The former have found a particular use in burning natural gas. Gas
engines, as these are often known, are relatively clean compared to large diesel
engines. However, the latter can be much more efficient; a large diesel engine
may be capable of an efficiency of 48%, whereas the efficiency of a gas typical
spark ignition engine is around 40%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The largest engines of all are based on marine engines. These can be as
large as 65 MW. They operate at very slow speeds compared to smaller piston
engines and they can burn very poor fuels. In addition, it is possible to
configure these large engines in a combined cycle mode by adding a small
steam turbine. With this, energy conversion efficiency can be in excess of 50%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Applications of these engines are as various as their sizes. Many are used in
emergency backup systems and medium-sized gas engines are popular
providing power to municipal facilities such as in hospitals as they are relatively clean to operate. Most piston engines are capable of load following and
their efficiency will often barely fall when output falls from 100% to 50%.
This can make them attractive for providing peak power on grid systems. The
largest engines are normally used for base or intermediate load on a grid.
Piston engine power plants have also been widely used to provide power to
remote communities that are unable to be connected to a grid.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another important use for small- and medium-sized piston engines is in
cogeneration systems. In these systems, waste heat from the energy conversion
process is captured and used to provide hot water, or in some cases to provide
heat for an industrial process. Since most engines have efficiencies of well
below 50%, more than half of the input energy is wasted. Capturing are using
the waste heat increases overall efficiency significantly. However, the application requires a local heat demand such as a hospital to make cogeneration
practical. Cogeneration can also be used domestically.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Stirling engines are a novel form of piston engine in which the heat energy
to drive the cycle is applied externally. This allows them to be used in a range
of applications, but the most significant is for solar energy conversion when
the sun is used as a heat source.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Emissions from piston engine power plants depend upon the precise fuel.
Technologies for removing nitrogen oxides and for reducing the emissions of
particles from diesel engine exhaust gases are widely deployed. However, all
these engines, when they burn a fossil fuel, will generate significant quantities
of carbon dioxide. It is unlikely ever to be economical to remove carbon dioxide from the exhaust of this type of power plant]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydropower
Hydropower is the most important renewable electricity generating technology
and the earliest renewable technology to provide a significant part of global
power generation. In 2017, hydropower plants generated 15.9% of total global
electricity production according to the IEA.9 The aggregate global installed
capacity was 1270 GW in 2016 and the figure increases, year upon year, but
the rate of increase is lower than the rate of increase in total generating capacity so that the proportion of power generated from this source is declining]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydropower potential is found in most parts of the world, excepting the
most arid, where water flows in streams and rivers to the world’s oceans. The
most developed regions and nations, such as the United States and Europe,
have exploited the best of the potential available, but elsewhere there is still
significant hydropower potential that could be exploited. Africa, in particular,
could generate significant volumes of electrical power from its resources.
Against this, the development of large hydropower schemes can be extremely
disruptive environmentally, so great care is needed when new projects are
developed.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydropower is a variable energy resource. The amount of energy available
changes by the season and depends on annual rainfall. Thus, the annual
availability can vary significantly. Global warming, which is changing weather
and rainfall patterns, can also have a major effect on availability, and over this
century it is likely that availability in some regions will fall while in others it
will rise.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydropower plants can vary in size from a few kilowatts to tens of thousands of megawatts. Sites capable of providing sufficient flowing water for the
largest projects are rare but plants that range in size from tens of megawatts
up to a thousand megawatts are relatively common. There are two primary
types of hydropower development, projects that involve building a dam and
reservoir and projects that do not have a reservoir; this latter is usually called a
run-of-river project. Hydro project reservoirs can often cover large areas of
land, displacing wildlife and people. Against this, they provide a means of
energy storage because they store water during the wettest seasons and can
make that water available for electricity generation all year around. The largest
projects often provide water for irrigation as well as other amenities, helping to
improve living standards locally. In consequence, the largest of projects are
often funded nationally or through international lending agencies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Smaller projects are more often commercial although they may still be
massive in scale. Most projects of 1 MW or more in generating capacity will
be connected to the regional grid to provide electrical power to a region or
nation. Smaller projects in the 10e1000 kW range might provide their power
locally to a small community or to an industrial facility. Smaller projects still
are built to provide power to a single dwelling, often in a remote region where
there is no grid power available.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The nature of a hydropower project, particularly one that involves the
construction of a dam, is such that the capital cost of its construction will be
high. However, the major parts of such a scheme, the dam, waterways and the
powerhouse will have a long lifetime. Provided the turbines are maintained
and repaired regularly, a large hydropower scheme can operate for a century or
more. This means that once the project has been constructed and the cost has
been met, the plant will produce extremely cheap power. Smaller plants e and
schemes without dams and reservoirs e are cheaper to build. The unit cost of a
small hydropower scheme is usually higher than that of a large scheme.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydropower is a renewable resource and this has made it important as
global warming had threatened the world. The output of a hydropower plant is
usually predictable, but it can vary significantly seasonally. In consequence,
hydropower alone cannot be relied upon as a secure source. From a grid
perspective, hydropower plants can be brought online and taken offline rapidly
and this can make them valuable for grid management. In addition, the large
turbines in some of the biggest hydropower schemes can provide spinning
reserve to aid grid stability.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One of the most important roles of hydropower for modern grid management is its ability to store energy. A hydropower scheme that includes a
reservoir has a natural store of energy in the form of the water behind the dam.
This type of plant can be used to help balance a grid, providing power when
other renewable sources such as wind and solar cannot and backing off when
the output from these plants is copious. There is a specific type of energy
storage plant that exploits this concept called a pumped-storage hydropower
plant, but any plant with reservoir storage can provide this service so long as
water remains in its reservoir. Grids with large amounts of hydropower are
therefore easier to manage than those without and the electricity they provide
is often cheaper.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While hydropower plants have many benefits, they can also lead to environmental problems. As already noted, the displacement resulting from a large
            hydropower scheme can be extremely damaging if not managed well.
Furthermore, hydropower reservoirs are capable of producing large quantities
of methane, a potent greenhouse gas, during the early years of their establishment. A dam can disrupt migratory fish movements and the movement of
silt downstream, this latter often important for downstream land fertility.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is one other type of hydropower plant that can potentially provide
significant quantities of clean energy and that is the tidal power plant. Tidal
power plants use the same types of technology as conventional hydropower
schemes, but instead of taking energy from flowing rivers, they rely on the ebb
and flow of tidal waters. Tidal power is expensive to develop but could
potentially offer a massive amount of energy. In addition, tidal power is
entirely predictable making grid management of the output much simpler than
for other hydropower schemes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power
Nuclear power is perhaps the most contentious of all the power generation
technologies in widespread use today. The technology has obvious attractions
because a nuclear power plant does not generate carbon dioxide during the
energy conversion process it exploits. Against this the potential dangers of
nuclear power, both because of its link to nuclear weapons technology and
because of the environmental damage that can be caused if a nuclear power
plant fails, mean that while some nations continue to embrace nuclear technology, others are proposing to reduce its presence or abandon it completely.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power plants accounted for 10.3% of global electricity production
in 2017 according to the IEA10 ranking the technology as the fourth most
important in terms of output after coal, natural gas and hydropower. Total
global capacity that year was 392 GW, the largest part of which, 100 GW, was
located in the United States. Other major nuclear nations include France,
Japan, China, the Russian Federation and South Korea. However, global capacity has been relatively flat since 2000 with only small capacity additions.
Furthermore, many of the power plants in operation are ageing and without
replacement or remedial action will have to be retired from service over the
next decade or two.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power exploits the ability of the atoms of certain large natural
elements to split into smaller atoms with the release of large amounts of energy. These reactions are at the heart of nuclear weapons, but the same process,
if allowed to take place under controlled conditions, can be used to make a
power plant. In this case the nuclear reactor, in which the nuclear process takes
place, acts essentially like the combustion boiler in a coal-fired power station,
producing heat that can be used to raise steam and drive a steam turbine.
Nuclear steam cycles are relatively inefficient compared to those in coal-fired
power plants. An overall heat energy to electricity conversion efficiency of
33% is typical.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The nature of the nuclear reaction makes the safety of nuclear reactors of
paramount concern. A reactor that became out of control would in essence be a
nuclear bomb. Reactors are therefore complex, high technology installations
that include multiple safety features to try to ensure that the facility can never
fail. Nuclear reactors are therefore expensive to build, probably the most
expensive type of large-scale power plant in common use. In contrast to the
high capital cost, the cost of the fuel for a nuclear power plant is relatively low
and this enables them to compete with fossil fuel power plants. However, there
are a large number of additional costs such as spent fuel reprocessing and
power plant decommissioning that can elevate the overall cost of power from
these power stations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Most of the nuclear power stations in operation today are relatively large
with single reactor sizes often in excess of 1000 MW; there can be several of
these on one site. The capital outlay for such a project can be intimidating.
There is a great deal of interest today in small nuclear reactors, units of much
more modest generating capacity which can be built in factories and then
shipped to a site. This might reduce the cost of nuclear power significantly. In
addition some of the designs being developed operate with high steam cycle
temperatures, leading to higher energy conversion efficiency.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is another type of nuclear reaction called nuclear fission that involves
the reaction between the atoms of small elements to create atoms of a larger
element, again with release of large amounts of energy. This nuclear fission
reaction has the potential to provide large amounts of electricity relatively
cheaply if it can be developed to a level to make it available commercially.
However, in spite of decades of work, a commercial fusion reactor still remains a long way off.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Conventional nuclear reactors of the type most common today were
originally designed to be operated as base load power plants, operating at full
power for very long periods without interruption. This remains the duty cycle
of many plants but as with other large power plants, nuclear plants today are
also being expected to modulate their output in order to cater for variable
amounts of renewable energy on a grid. One strategy used in the past to
manage nuclear plant output has involved building very large energy storage
plants based on hydropower to store surplus energy from the nuclear generating facility when it is not needed, then making this energy available as
demand peaks. This is a capital-intensive strategy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power plants are attractive to many nations as a means of ensuring
a secure electric power supply that is independent of the vagaries of the fossil
fuel market. Countries such as the United States, France and Japan have
invested heavily in nuclear technology for this reason. However, there are
questions in all these countries, and in others, about the environmental safety
of their nuclear facilities. Some nations categorise nuclear power as renewable
energy source but most environmentalists would challenge this. There are also
major differences of opinion about the economic effectiveness of nuclear
power.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Meanwhile the development of nuclear nuclear facilities has begun to
accelerate, slowly. According to the IEA, 11.2 GW of nuclear nuclear capacity
was brought online in 2018, the largest amount since 1989. Only a very small
number of these new plants are in developed countries that already have nuclear fleets but some of these nations are carrying out work to extend the life of
their existing plants, many of which are 30 or more years old. In the United
States, most of the existing nuclear reactors now have permits to operate for
60 years.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar power is potentially the most important renewable source of energy
available to the world and solar energy the largest natural source of energy that
we have to exploit. However its development as a generating technology has
only become significant during the current century and global capacity remains relatively small. In 2018 the total amount of power from solar cells, the
most significant means of turning sunlight into electricity, was 585 TWh according to the IEA and accounted for 2% of global electricity generation.
Meanwhile the European Photovoltaic Industry Association estimated the total
global installed capacity in 2018 to be 517 GW, rising to 634 GW at the end of
2019. In addition to generation based on solar cells there is also another
category of solar technology called solar thermal power generation; this provided roughly 300 GWh of power in 2018]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar energy is available in every part of the globe, but the absolute annual
amount will vary significantly from place to place. In general there is more
sunlight to be found near the equator and less near the poles. The amount of
energy at any point on the globe is intermittent, following a daily cycle and
there is a seasonal cycle too. Changes in weather also affect the amount of
sunlight available, making it in part unpredictable. Solar cells can harvest
sunlight in most parts of the world provided there is sunshine available.
However solar thermal power plants require a reliable source of high intensity
sunlight to operate efficiently. These plants are most suited to regions of high
insolation. Arid parts of the world and desert regions will often provide a good
resource for this type of technology.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar cells are easily scalable so that it is possible to have installations with
a few kW of generating capacity and others with hundreds of megawatts, all
based on the same technology. The wide-spread availability of the energy
source makes is practical to harvest solar energy using solar cell arrays on the
rooftops of virtually all types of buildings, from domestic dwellings to large
industrial facilities. Depending on its size and situation, this type of solar
installation might provide power directly to a single household or commercial
facility or be connected directly the local distribution network. Very large solar
cell power stations with tens to hundreds of megawatts of installed generating
capacity will normally be designed to supply power to the main grid. Solar
thermal power plants are not so easily scalable and most economic power
plants of this type are relatively large plants intended to supply power to the
grid.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar cells (often called solar photovoltaic devices) are solid state devices
constructed using technology similar to that employed for microchips. Most
are made from silicon although some other materials are also used. The
manufacture of the cells requires high technology facilities and growth in
global solar cell capacity is constrained by the quantity of solar cells that the
world’s factories can produce. In 2018, according to the IEA, 97 GW of new
capacity was installed. Solar cells can exploit both direct sunlight and diffuse
sunlight so that they can still operate when conditions are cloudy. However,
their output will be directly related to the amount of sunlight they receive so
output will fall to zero during hours of darkness. This means that grid connected solar cells require some form of support at night. On the other hand
solar cell power output usually correlates closely with daytime temperature
making it a good match for air-conditioning demand in hot climates.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar thermal power plants use mirrors to collect direct sunlight and then
focus it onto a heat collector where the concentrated heat energy is used to
generate electrical power. Some solar thermal plants use a cycle similar to a
coal-fired plant with heat producing steam to drive a steam turbine. Others
using Stirling engines to convert the heat into electrical power directly. An
important difference between solar cells and solar thermal power plants is that
some of the latter can store thermal energy which can then be used to generate
electricity when no sunlight is available. This can make them much more
reliable sources of power than solar cells and therefore much easier for grid
controllers to dispatch.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost of solar cells has fallen dramatically over the last 20 years and at
the end of the second decade of the 21st century solar cell power facilities
were capable of competing on cost with most other sources of electricity. The
nature of the manufacturing process means that costs are likely to drop further
as manufacturing volumes increase. Solar thermal power plants are relatively
more expensive but their costs are falling too. However, they are not as
competitive. Nevertheless the technology has proved attractive in some arid
regions, particularly when plants provide storage capability.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar energy is one of the important renewable energy sources, arguably
the most important. It has the potential to provide a large part of global energy
demand in the future. Solar cells require a lot of energy to manufacture but the
energy requirement is decreasing. The lifetime of solar cells is usually expected
to be around 20 years but with regular maintenance they can last much longer
without degradation. Solar thermal power plants exploit more traditional power
plant technologies and their costs and lifetimes are closely related to those of
fossil fuel power plants. Both types of solar generator are expected to play an
important role in future energy supply but solar cells will dominate. However,
neither can provide a secure supply alone and so must sit alongside other
generating or storage technologies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Wind energy is the second most significant renewable technology after hydropower in terms of electricity production. Global output from onshore wind
turbines in 2019, according to the IEA, was 1202 TWh while offshore wind
farms provided a further 66 TWh, for a total of 1268 TWh. Meanwhile figures
from the Global Wind Energy Council (GWEC)12 indicate that total installed
capacity for wind energy in 2018 was 591 GW, of which 568 GW was onshore
and 23 GW was offshore. The total capacity rose to 651 GW at the end of
2019.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Wind energy, the energy contained in a mass of moving air, is available in
most parts of the world but the size of the resource will vary from place to
place depending on the wind regime. Wind energy can be harvested on land
and at sea. The offshore resource is generally the most consistent, the most
reliable and able to supply the highest energy intensity. Onshore wind resources are more variable because the wind must travel over a land mass and it
will be affected by the contours of the land and by the ground cover. However
all wind is dependent on the prevailing weather conditions and this leads to
considerable variations in availability. Sometimes the wind blows intensely
and sometimes it does not blow at all. This means that wind power is probably
the most variable and the most unpredictable of all the renewable energy
sources. Wind output reliability can be improved by coupling wind farms that
are widely spaced geographically, in effect averaging output over a large area.
Even so it is still possible for a whole region to become becalmed at times.
Wind energy must therefore be supported by other forms of generation or by
energy storage in order for it to provide a manageable resource. Wind and
solar power can be complementary since the wind blows more strongly during
winter while solar power is most intense during the summer. The management
of wind output is one of the most challenging aspects of grid management
today.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Wind energy is captured by wind turbines. When the wind industry was
young, in the 1980s and early 1990s, there were a variety of wind turbine
designs in use but the range has gradually narrowed so that today the market is
dominated by a single type, the three blade horizontal axis wind turbine, with
the turbine and its generator sitting on top of a tall tower. Wind strength increases with height so the higher the tower, the more energy can be collected at
any given site. The sophistication and reliability of wind turbines has increased
enormously since the pioneer days and new wind turbines can be expected to
deliver power over a lifetime similar to that of other types of power generation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are two branches or families of wind turbines, onshore turbines and
offshore turbines. Today the differences between the machines used for each
are slight. Most significant is size, with offshore turbines tending to be larger
than those used onshore. This is partly a matter of practicality. Transporting
and erecting a very large turbine onshore can be very challenging in many
locations whereas the are no limits offshore, provided only that vessels are
available that can carry and install them. However, installation of turbines
offshore is much more difficult than onshore and more costly. It is therefore
more cost effective to install the largest turbine possible at an offshore site.
Typical onshore wind turbines have generating capacities of up to 4 MW.
Offshore, 6e8 MW is more typical of the capacity range, while turbines with
generating capacities of 10e12 MW are expected to enter the market for the
beginning of the third decade of the century. The main market for offshore
wind is in European waters but China has been expanding its offshore wind
capacity in recent years too.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost of wind energy has fallen dramatically over the last decade and
over the 5 years to the end of 2019, the cost of both onshore and offshore wind
had fallen on average by more than 50%, according to the GWEC. This has
made onshore wind generation easily competitive in terms of cost with fossil
fuel generating technologies and offshore wind is likely to be in the same
position in the near future. Unfortunately the unpredictability of wind power
often still leaves it at a disadvantage. One potential means of remedying this is
to combine wind power with some form of energy storage. This will increase
the overall capital cost of a facility but by increasing its reliability, makes the
energy it produces more valuable. Various schemes are being explored
including using offshore wind power to produce hydrogen which can then be
shipped ashore and used as a green energy source.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The green environmental credentials of wind power make it attractive as a
means of combatting global warming and most countries are building up wind
capacity, some faster than others. However, it is not entirely benign. Onshore
wind turbines are large additions to any landscape and they are not always
welcomed by their human neighbours. This can be problematic when
obtaining permits to construct wind farms onshore. There has also been an
issue in the past with the danger of wind turbines to birds. However, the slow
rotational speed of large modern wind turbines makes this less of a problem
today. Noise, too, can be a problem onshore so it is not usually possible to
erect wind turbines close to dwellings. Onshore construction is less of a
problem in countries such as the United States and China where are wide
expanses of uninhabited territory that can be used for wind generation.
Offshore wind experiences few problems in this respect.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biomass makes a relatively small contribution to global electricity generation,
with production of 546 TWh in 2018 according to the IEA. This figure includes production from power from waste plants, so the output from dedicated
biomass power plants is likely to be under 500 TWh. However, biomass remains a very important source of energy for many communities around the
world and accounts for around 10% of global energy consumption. Most of
this is in the form of wood burnt for cooking and heating. Power plants that
burn biomass to generate electricity are relatively uncommon. Estimates for
the total global generating capacity at the end of the first decade of the 21st
century vary between 80 and 120 GW. A large part of this capacity is in
Europe.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biomass fuel is combustion fuel that has been grown rather than mined
from the ground. It is the product of plants and trees that convert water and
carbon dioxide into organic material with the aid of sunlight. Since the latter is
the driving force for the photosynthesis process in plants, biomass energy
might be considered as another form of solar energy. In principle all plants and
trees can be used as fuel but in practice only a limited part of global biomass is
useful. The most important sources of biomass fuel are biomass wastes and
energy crops. Biomass pellets, traded internationally, are derived from energy
crops.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biomass wastes cover a range of materials the most important of which are
agricultural wastes from the harvesting of various crops such as cereals, rice,
sugar cane and maize. Wood waste from forestry management can also be
exploited but this is more labour intensive to collect and therefore tends to be
more costly. One important specialist category of waste is that produced by
sawmills and paper plants. This waste is often used at the site to produce heat
and electricity to power the industrial installation. Another specialist agricultural waste, the slurry from animal farms, is sometimes used to feed a
digester which produces methane gas.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biomass crops are specific fast-growing species that can be harvested
regularly and then converted into a form suitable for burning in a combustion
plant. The most common of these are prairie grasses and tree species such as
willow and hybrid poplar. Grasses can be harvested annually in the autumn
when they have died back and dried. The harvested material is often turned
into briquettes before being sold to power plant operators. Woods cannot be
harvested so frequently but with careful managment plantations can be rotated
to provide a regular supply of combustible material. Harvested wood usually
needs drying before use. Some power plants can burn cut wood directly but
much of it is converted into pellets that can easily be shipped to power stations.
There is a small but growing international trade in wood pellets from power
generation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The technologies used to burn biomass fuels to produce electricity are
essentially the same as those used to burn coal. Fuel is prepared, then fed into a
boiler where combustion takes place and heat is captured and used to raise
steam that drives a steam turbine. Biomass fuels have a lower energy content
that coal and burn at lower temperatures, so the efficiency of most biomass
combustion power plants is relatively low. This is compounded by the fact that
many of these plants are small, typically no larger that 50 MW. Since these are
combustion plants, emission control systems may be needed to remove pollutants from the flue gases before they are released into the atmosphere.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is another way of burning biomass fuels called co-firing that offers a
more efficient means of converting biomass into electricity. Co-firing involves
adding a proportion of biomass, often in the form of pellets, to the coal that is
used in a coal-fired power station. The boiler in a large coal plant is much more
efficient at extracting energy from fuel that that of a traditional dedicated
biomass plant, so more energy from the biomass is converted into electricity.
Today some coal-fired power stations in developed countries where coal is
being phased out as a combustion fuel are converting to 100% biomass
combustion for power generation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The combustion of biomass to generate electricity is considered to be
renewable, but only if some specific conditions are met. The argument that
biomass is a renewable energy source relies on the continuous harvesting and
regrowth of biomass. When a biomass fuel is harvested and burned, it generates carbon dioxide during the combustion process in the same way as coal
or natural gas. However, if the same amount of biofuel is regrown, it will
absorb all this carbon dioxide from the atmosphere again so the net release will
be zero. While it may appear simple to maintain this cycle when power
generation and the growth of the energy crop are closely coupled, there have
been questions raised about the sustainability of internationally traded biomass
pellets.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is an additional worry about fuel crops, that they might be grown in
place of food crops. This can be a danger if the energy crop is more valuable
than a food crop, particularly if the crop comes from an undeveloped region of
the world where food is scarce. Again it is the international trade in the energy
fuel that is likely to lead to abuse.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is potentially one extremely positive adaptation of biomass combustion. Since the combustion of biomass fuel is, in theory, carbon neutral, if
the carbon dioxide produced during combustion is captured from the exhaust
gases of a biomass plant and sequestered so that it cannot return to the atmosphere, the process will actually remove carbon dioxide from the atmosphere. This can only be cost effective in a very large combustion power plant
and since the amount of biomass that can be used globally as fuel is limited,
the actual reduction in atmospheric carbon dioxide that can be achieved in this
way is likely to be very small. However this has not stopped some companies
pursuing this goal.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost of electricity from biomass is likely to be higher than from a
conventional coal-fired power station. However if external costs such as those
associated with carbon dioxide emissions are taken into account, and if the
price for emitting carbon dioxide into the atmosphere is high enough, a
biomass power plant can prove to be cost-effective in comparison with coal or
natural gas.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Geothermal energy is energy extracted from the earth. The core of the earth is
extremely hot and this heat slowly radiates towards the surface so substrata of
our planet are warmer than the surface. The temperature gradient is relatively
low but there are areas of the earth where heat from deep within the planet has
warmed reservoirs of water close to the surface. If this hot brine13 is extracted
it can be used to raise steam and drive a steam turbine to generate power. There
are only a limited number of places around the world where underground hot
reservoirs are accessible and many of those that are within reach provide only
low temperature fluid, suitable for heating but not power generation. However
there are a few with higher temperature fluids.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is also possible to create a geothermal hot water source artificially. Since
deep underground strata can reach very high temperatures, it is possible to drill
a deep well into hot rock and pump high pressure water into the well, then
extract the heated water from a second well to provide a source of energy. This
technology, similar in concept to fracking to extract oil and gas from rock, has
been demonstrated but is costly and has not yet been exploited commercially.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The global production of electricity from geothermal power plants in 2018
was 90 TWh according to the IEA. Meanwhile the aggregate global
geothermal generating capacity in July 2019 was 14,900 MW.14 The IEA has
estimated annual capacity additions to be around 500 MW over the past
5 years. With the limited conventional resource of underground reservoirs,
geothermal power generation can only make a modest contribution to global
production.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The primary attraction of geothermal power generation is cost. The
resource e hot underground brine e does not cost anything, although drilling
down to the underground reservoir can be expensive, particularly as the
location of new reservoirs can be difficult to identify from the surface. The
technology used to generate electricity from a geothermal reservoir is conventional, based on the steam turbine cycle. A very small number of underground reservoirs will produce live steam when drilled and this can be used to
drive a steam turbine directly. More normally a very hot brine is extracted.
This hot brine can be expanded into a low-pressure chamber, producing steam
to drive a steam turbine, or it can be used to heat water or another thermodynamic fluid, the vapour from this second fluid then driving the turbine.
Whatever the process, the spent brine is an environmental hazard and will
normally be reinjected into the reservoir. Underground reservoirs are continuously heated from within the earth but the size of each reservoir is limited. If
heat is extracted more quickly than heat is added, then the temperature of the
reservoir will fall and it will become depleted. The same applies if too much
liquid is extracted and not returned.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Geothermal power is not exactly renewable because energy is being taken
from the earth and not replenished, but in practice the amount of energy
extracted is tiny and will have no effect on the temperature of the core. A
greater problem is associated with carbon dioxide that is often found alongside
hot brine in underground reservoirs and which is released into the atmosphere
when the reservoir is mined for energy. While the quantity involved is relatively small, perhaps 10% of the amount produced by a coal-fired power plant,
it is still significant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Geothermal power plants are technically relatively simple, and the capital
cost, while significant, is outweighed by the fact that the energy source has no
cost. This makes geothermal electricity generation competitive with most
alternatives.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In addition to the various technologies outlined in the preceeding sections,
there are three other types that are used for electricity generation: marine
power generation, power from waste and fuel cells. None makes a major
contribution to electricity production today, but both marine technologies and
fuel cells are seen a potentially significant technologies for the future. Power
from waste, meanwhile, has a small but potentially important part to play in
keeping the planet clean.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The term marine generating technology encompasses a diverse array of
different methods of producing electricity that have in common the exploitation of energy contained within the world’s oceans. Two of these, marine
current and wave power generation, have been developed to the precommercial stage. Ocean thermal energy technology (OTEC) has yet to show
its commercial viability.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Marine current energy production uses what is essentially an underground
wind turbine to take energy from moving water. The marine current devices
have rotors that turn in a flowing current, providing the motive force to drive a
generator. The energy density of moving water is much higher than that of
moving air and a marine current turbine will be much smaller than a wind
turbine for the same power output. An underwater turbine can be mounted on
the sea (or river) bed or it can be deployed from a floating platform. They are
generally deployed as coastal devices where they exploit the flows of water
that are generated as the tide ebbs and flows. However, floating generators
might also be used in the future to tap the great ocean currents such as the Gulf
Stream.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Wave power generators try to capture energy contained in ocean waves.
These waves are generated by the movement of winds across the world’s
oceans. The best wave regimes are usually found on a coast exposed to prevailing wind flow across a great ocean. The energy of the waves is contained in
the oscillating motion of the water column formed by the sea at any point
relative to the sea bed. A variety of devices have been designed to capture this
energy, some based on floating buoys, others on floats that flap as a wave
passes and yet others that isolate an oscillating column of water generated by
waves reaching the shore to pump air through an air turbine. Designing devices
of this type that are both robust and reliable has proved difficult.]]>
			</paragraph>
			<paragraph>
				<![CDATA[OTEC is a technology that tries to exploit the temperature gradient found
between the surface water in a tropical ocean and colder water a kilometre or
more below the surface to drive a heat engine. To do this, cold water must be
raised from the depths and used to condense water vapour that is created from
the hot surface water by expanding it into a low-pressure chamber. A flow of
low-pressure steam can be produced in this way and used to drive a turbine. An
OTEC power plant requires a temperature gradient of at least 20_C to be able
to operate effectively. Even so, efficiency is very low at around 6%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Of the three marine technologies highlighted here, marine current generators are seen as the most promising. From the perspective of the energy
resource, wave power offers the greatest potential if it can be mastered. OTEC
has been demonstrated, but rarely, and commercial operation seems distant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Power from waste is the term used to describe plants that can generate
electricity from waste material of which there are presently copious quantities
around the world. The most important of these plants are designed to burn
municipal and some industrial waste. Most of this waste is collected within the
world’s cities, and in advanced economies it is e in theory at least e sorted so
that any recyclable material is extracted. The residue can then be burnt in a
special facility designed to completely destroy the waste and turn it into ash
without releasing any noxious chemicals into the atmosphere. The heat from
combustion, meanwhile, can be used to produce electricity. Such plants are
very expensive to build and can only be operated economically if they charge a
fee to dispose of the waste material delivered to them. Power from waste plants
is therefore part of the waste disposal systems that operate in our cities and
towns and electricity is a valuable by-product.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the past e and in some regions still today e large volumes of waste have
been buried in landfill waste sites. This waste, as it is covered and compacted,
will over time start to generate methane gas as a result of the activity of natural
microbes in the soil. This methane is then released from the surface of the
landfill site. Methane is a potent greenhouse gas so the release is a significant
environmental issue. However, if the gas is captured from within the landfill, it
can be used to fire a gas engine and generate electric power. This particular
type of waste to electricity is sometimes classified as renewable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Fuel cells are electrochemical devices, closely related to batteries, that can
convert a combustible fuel into electricity without actual combustion taking
place. Most fuel cells are designed to use hydrogen as the fuel from which
electricity is derived. The gas is fed to one electrode of a fuel cell while oxygen or air is delivered to the second electrode. With special catalysts, the
reaction in which hydrogen burns in oxygen at high temperature can be carried
out at relatively low temperature and much of the heat energy that would be
released during combustion is converted, electrochemically, into electrical
power. The fuel cell process is completely different to the heat energy cycle
typically used for power generation from combustion fuel and this allows
relatively high efficiencies to be achieved.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydrogen is not widely available and so for the most part fuel cells
available today use natural gas. This is converted into hydrogen using an
energy intensive process called reforming, which reduces the overall efficiency
of electricity production.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are four principles types of fuel cell that have been developed for
power generation, the proton exchange membrane fuel cell (PEMFC), the
phosphoric acid fuel cell (PAFC), the molten carbonate fuel cell (MCFC) and
the solid oxide fuel cell (SOFC). The first two of these, the PEMFC and the
PAFC, operate at relatively low temperature and require an additional reformer
module to produce hydrogen from natural gas. The other two, the MCFC and
the SOFC, operate at elevated temperatures and the reforming of natural gas
can be carried out within the fuel cells or at their electrodes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[With the exception of the MCFC, all fuel cells are easily scalable and will
operate with similar efficiency in 1 kW modules or as 1 MW power plants.
They are relatively quiet and the only exhaust gases are water vapour and
carbon dioxide. This makes them attractive for use in urban environments and
as distributed generation modules. Some types have also been developed as
domestic combined heat and power units.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Environmentally, fuel cells are considered benign. When operating with
natural gas, they will generate carbon dioxide. It is possible to design fuel cells
in which this gas is isolated and captured. However, the most interesting
application for fuel cells is in a potential future hydrogen economy when
hydrogen gas becomes the main combustion fuel in place of natural gas and
gasoline. When burning hydrogen directly, fuel cells are extremely efficient
and clean and could provide a range of benefits. Of especial interest is their use
as transportation power units supplying electricity for an electric vehicle.
Today, they are still relatively expensive but costs are falling.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Energy storage
The ephemeral nature of electric power makes it impossible to conserve
electricity once it has been produced. This has always been a problem for the
electricity supply industry because it means that supply must be available to
meet demand and vice versa. There are ways of creating stores of electrical
energy, but this invariably involves converting it into some other form of
energy from which it can be recovered at will. Energy storage has been used to
a limited extent in the past but energy storage systems are seen as increasingly
important now that grids are having to accept large volumes of renewable
power from unpredictable or unreliable sources. Under these conditions, energy storage provides the most convenient way of managing grids by putting
excess power, from wind farms for example, away when it is not needed and
then releasing it again when demand rises and the wind drops.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydropower power stations with reservoirs can provide a form of energy
storage that can be exploited in this way but they are not always available
because the water in the reservoir can become exhausted. There is a type of
hydropower station, called a pumped storage hydropower plant, that is
designed specifically for storage. This type of facility has two reservoirs, an
upper and a lower. Water is stored in the upper reservoir until energy is needed,
when it is used to drive turbines, and then captured in the lower reservoir.
Meanwhile during periods of excess power on the grid, the surplus is used to
pump water from the lower reservoir back into the higher. Plants of this type
are generally large, up to several thousand megawatts in generating capacity,
and are costly to build but very effective.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Rechargeable batteries, which store electrical energy in chemical form, are
the most widely known form of electricity storage. They have traditionally
been used for small-scale electricity stores, but over the past two decades they
have been of increasing interest in using them on a much larger scale for grid
energy storage. A variety of battery types have been developed for large-scale
use, but the most common is the lead acid battery, common from use as starter
cells for automotive vehicles. Lithium batteries are also beginning to be
introduced. These are of particular interest for small-scale renewable energy
storage at a domestic or small commercial scale where they can store the
power from rooftop solar panels for later use.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is a subset of this group of energy storage devices called flow batteries. A conventional rechargeable battery contains within its casing all the
materials needed to store or release electrical energy so its capacity is fixed. A
flow battery has electrodes like a standard cell but the chemicals that are used
to either produce or store electricity are kept in external tanks. The amount can
therefore be increased at will and their capacity is, in principle, unlimited.
However, development of cost-effective forms of this type of cell has so far
proved elusive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Compressed air energy storage (CAES) is a storage technology that, like
pumped storage hydropower, has the potential to provide very large-scale
electricity storage. The technology is based on that of the gas (or air) turbine that uses high-pressure air to drive a turbine generator and produce
electricity. A conventional gas turbine has a compressor that provides highpressure air, a combustion chamber where fuel is burned in this air to heat
it and increase the pressure further and then an air turbine that uses this hot,
high-pressure air to drive the generator. A CAES plant divides this scheme into
two parts. When surplus energy is available, air is compressed using a
compressor and then the compressed air is stored in a tank or underground
cavern. When electricity is needed, this high-pressure air is extracted again
and used to drive an air turbine. A combustor may be added at this stage to
increase the energy available. There are a handful of plants using this technology in operation today but it has not been widely exploited.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another way, potentially, of providing large quantities of stored energy for
electricity production is via hydrogen. The gas can be produced from electricity by electrolysis of water. It must then be compressed and stored. The gas
can be moved through pipelines like natural gas and can be used to generate
electricity in most types of combustion power plant, in addition to being the
ideal fuel for use in fuel cell power plants. However the production of
hydrogen from water is relatively inefficient and to be cost-effective requires a
supply of very cheap electricity. This might be supplied in the future from lowcost renewable energy such as that produced by large offshore wind farms.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In addition to these large-scale energy storage technologies, there are a
range of systems that are intended for small-scale, often fast-acting, energy
storage. These include energy storage capacitors, often called supercapacitors,
flywheels and superconducting energy storage devices. Capacitors store
electrical energy in the form of static electric charge across the plates of an
electrical capacitor, but in a superconductor, this is supplemented electrochemically, allowing a single capacitor to carry a much higher charge. Once
charged, the device can release electrical energy very quickly, making these
devices ideal for high-speed grid backup systems.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Flywheels store energy in the rotational motion of a mass spinning at very
high speed. As with a supercapacitor, this energy can be released very quickly
and these devices can be used for high-speed grid support too. Some have also
been used for much larger scale energy supply support such as for metropolitan railway transit systems.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Superconducting magnetic energy storage (SMES) devices are novel
storage systems that rely on the ability of some special materials to reach a
state of zero electrical resistance below a certain (often very low) temperature.
As the electrical resistance falls to zero, materials of this type can carry a
circulating electrical current without any resistive losses, so the energy
contained in the electrical current can be maintained, in principle, indefinitely.
In practice, this is not quite true but losses are low. There is a cost, though,
because it is expensive to maintain the device at a very low temperature
indefinitely. Small SMES systems are extremely fast-acting and have been
used for grid support services. Larger scale SMES systems have been proposed
in the past but they do not appear to be cost-effective today.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The use of large-scale energy storage for modern grid support and stability
is extremely attractive but large systems are expensive and the structure of
modern electricity industries can make it difficult to fund their construction. If
electricity production is to be truly carbon free before the end of this century,
then such energy storage is likely to become essential.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Electricity networks
The production cost associated with the electricity-generating technologies
outlined in Chapter 2 accounts for a large part of the cost of the production of
electricity. This production cost is the cost of electric power as it leaves a
power station and enters the electricity network. At this point, a number of
other factors come into play and these may have a significant bearing on the
actual cost of this electricity within the network and for the consumer. One of
these factors is the structure of the electricity system, especially if it has been
adapted to allow market forces to operate. Where such conditions have been
put in place, the price may depend upon availability as well as production cost.
If electricity is scarce, prices within the network will rise, and if there is a
surplus, they will fall even though the end price may then have little relation to
the actual cost of production. Not all electricity supply systems function in this
way but many do. The alternative, a government owned and operated (or
privately owned - strictly regulated) vertically integrated electricity supply
system without any market element is rare now.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The structure of an electricity system does not lend itself naturally to
market-based operation, and in many cases this is a recent development, its
inception often driven ideologically. Some parts such as the electricity transmission and distribution networks are natural monopolies. Other parts such as
power generation can be more easily turned into markets, but in doing so,
governments lose the power to direct the way the industry develops and this
can lead to strategic security-of-supply weaknesses. Separation of ownership,
necessary for a market to work freely, can also lead to unexpected results.
Nevertheless, an electricity supply system structured to accommodate free
market principles is the norm today.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Historically, the electricity supply system in most nations grew in a
piecemeal fashion with small, independent, privately owned networks supplying electric power from small power stations to a limited number of customers. As the number of networks grew and individual networks became
larger, it became clear that greater efficiency could be achieved by aggregating
the individual operators and their operations into national systems. By the
middle part of the 20th century, the governments of many countries operated
national electricity systems with similar contours.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The traditional grid structure
The typical national electricity grid as it would have appeared in the 1960s or
1970s would have had a hierarchical structure1 based around large central
power stations. These power stations would have been connected to a national
transmission system, a backbone network that could carry large volumes of
electrical energy to all parts of the country. The transmission systems operated
using high-voltage alternating current to reduce transmission losses.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In exceptional cases, such as for a very large industrial facility, power
might be delivered from the transmission system directly to an end user. More
normally, the power from the transmission system was fed into local distribution systems through transformer-based substations. These distribution grids
covered a much smaller area and operated at lower voltage. Industrial and
commercial consumers would receive power directly from this distribution
system. Meanwhile smaller commercial consumers together with domestic
consumers would be supplied through a yet lower voltage distribution network
from which power cables could be taken to individual dwellings. Thus, power
was produced centrally and delivered through a hierarchical delivery system so
that power flows were always from the centre to the periphery.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At the centre of this web, there would sit a national control centre where
the supply and demand on the grid was monitored and power stations were
brought into service or taken off line as required. It was here that the various
types of power plant duty cycle were defined. Base load power plants delivered
constant output at all times into the grid: these were the cheapest to operate.
Intermediate load plants would come on line in the morning and run through
the day before backing off as load fell in the evening. And on top of these were
the peak load (peaking) units that could be brought into service quickly as
demand spiked. These fast-acting peaking units were the most expensive to
operate.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A centrally owned and operated electricity system of this type can be
developed in line with government strategy. The types of power plant that are
built, when and where each is built and how capacity and network expansion is
managed can all be decided at a national level based on a national security-of-supply considerations. This also allows costs to be controlled. The funding for
new projects would usually be government backed, and large expensive projects such as a major energy storage plant could be built relatively easily]]>
			</paragraph>
			<paragraph>
				<![CDATA[Deregulation and grid privatisation
In the middle of the 1980s, a politically driven desire to eliminate these national
electricity monopolies began to gain momentum in some countries such as the
United Kingdom and the United States2 and soon spread elsewhere. The
government-owned national systems were seen as inefficient and their practices
antiquated. They were also becoming costly for the governments to underwrite.
The solution was to break them up and sell the parts to private sector companies.
To achieve this, the national monopolies were first split into separate transmission, distribution and generating companies. The transmission system was a
natural monopoly necessitating a single, regulated company to be formed to
own and manage it. The array of distribution networks were similarly divided
into regulated regional monopolies that were sold off individually. Finally, the
numerous power plants that together make the national generating company
were sold, usually individually, to private operators.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In order to make this into a functioning marketplace, it was necessary to
add a further layer, an electricity market into which power plants offered
power for sale and consumers e these being either large industrial and commercial organisations or the regional distribution companies e that would
purchase power. This market would support various long-term contracts between producers and consumers as well as a spot market for the instantaneous
purchase of electrical power.
This ‘deregulation’ of electricity supply markets led to some notorious
episodes including market manipulation and massive spot price spikes, but it
was eventually tamed into the electricity supply system found in many parts of
the world today.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One of the key features of a market of this type is that the players, now
private companies seeking to make a profit, are usually driven by strict economic considerations when making strategic decisions such as the type of
power plant to build. So, for example, there is little incentive for any company,
be it a generating company or a distribution company to build an expensive
energy storage plant because it will not provide the return that the private
company investors seek. From a strategic national perspective, these decisions
can often appear shortsighted, but a government can only try to encourage
companies to behave strategically by introducing additional incentives.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Whether this way of operating electricity supply systems is the most
effective method available is a matter for political debate and there are voices
that question its utility. For the moment, however, it is the standard for most
democracies and many less democratic nations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Electricity prices, electricity supply contracts and the
spot market
The electricity market in a deregulated electricity system is linked closely to
the activities of the system operator which is charged with balancing the
system so that supply and demand match. In a market system, the cost of the
commodity in question will be determined by demand. The higher the demand
at a particular time, the higher the price is likely to be. In the electricity
marketplace, as in other similar energy markets, the varying hourly and daily
cost of electricity is called the spot price.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The system market operator which manages the functioning of the market
will police that sale and purchase of electricity with the spot price signalling
the value of electricity at any given time. A rising spot price is an indication
that demand is high and increasing and this will encourage generators to bring
into service as much capacity as they can. Conversely, when the spot price is
falling, generators will back out of the market with their most expensive
generating units. On the other side, consumers such as distribution companies
will have to pay the spot price to be able to maintain supply to their clients.
However, this can leave both sides vulnerable to wild swings in costs. For
example, while the average spot price in different regions on the Australian
National Electricity Market in the year to June 2018 was between Aus$73/
MWh and Aus$98/MWh, the price could theoretically rise to $14,500/MWh
before it is capped]]>
			</paragraph>
			<paragraph>
				<![CDATA[To combat this danger, consumers can hedge their costs by striking contracts for electricity with generators. These contracts ensure that even if the
spot price for electricity should rise dramatically, the consumer will only pay
to price agreed by the contract for the amount of electricity covered by that
contract. Of course if the contract does not cover all the needs of the consumer,
then it will still have to go to the spot market for the remainder and pay the
spot price. The contracts, which are always for future supply when made, will
be at a price that reflects the expected spot price for power when the electricity
must be supplied. Contract of this type allows both generators and consumers
to make economic plans with the security of knowing that these contract prices
will be stable. If a power plant is contracted to supply power to a distribution
company at a particular fixed price and it does not do so, then it will be
penalised at a level relating to the spot price at the time when it reneged.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is another type of power supply contract called a Power Purchase
Agreement (PPA) that can be struck between a power generating company and
another party e usually a distribution company, but in some instances an industrial facility e in which the buyer contracts to buy all (usually) of the
output from the power generating facility over the term of the contract, which
is usually for a long period and at a fixed price. This type of contract is often
used by renewable generating companies seeking to provide the financial security to be able to develop the project. It will offer long-term security but with
the caveat that the cost agreed when the contract is struck may be wildly
different from the actual market price of electricity over 10, 20 or 30 years.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While distribution companies purchase power using contracts and the spot market,
they offer this power for sale to their customers at a fixed price, the tariff, which is
determined by the contract between the two parties. The tariff price may vary with
the time of day, and from time to time, the prices will change as costs to the distribution company change, but consumers will normally be able to predict the cost
of their energy at least over a horizon of a few months.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This consumer tariff will be significantly higher than the wholesale price
that the distribution companies pay for their electricity. For example, in
Australia, the wholesale price paid by the distribution company typically accounts for only 30%e40% of the consumer price.4 Another 40%e50% will be
accounted for by the transmission network costs (including profits for the
operator). There is a 5%e15% addition to cover the environmental costs
resulting from government schemes and then a further 5%e15% that covers
the residual costs of the distribution company and provides the profit that this
company expects to make on its sales.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The difference between the wholesale and the retail cost will vary from
country to country, but a more than doubling of the end cost to the consumer is
probably typical. These end costs will also vary from consumer to consumer.
For example, in the New England region of the US grid in the middle of 2018,
residential customers paid just over $200/MWh, commercial customers paid
around $160/MWh while industrial customers paid perhaps $120/MWh.]]>
			</paragraph>
			<paragraph>
				<![CDATA[These price variations can have important implications for some types of
power generation. A household that is considering installing solar cells onto
the roof in order to generate electric power will break even if it can undercut
the retail cost of electricity from the grid. This, in turn, means that manufacturers of rooftop solar installations for domestic use do not need to be able
to provide technology that can undercut the wholesale price when the local
retail price is much higher. This can make even relatively expensive renewable
installations competitive in the right situation.
The later chapters of this book will be primarily concerned with the actual
cost of production of electricity from different types of generator. This will be
most closely related to the wholesale cost of production of electrical power
before it actually enters the network.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Distributed generation
The introduction of an electricity market into an electricity supply system does
not by itself affect the overall structure of an electricity system which remains
hierarchical after these changes have been introduced. However, the introduction of market forces coupled with the changes in the way electricity is
being generated has begun to break down this strict hierarchical structure. For
example, consumers large and small may seek to generate their own electricity
either for local consumption or to sell back to the market. At the same time, a
variety of small-scale generation technologies have grown up, technologies
that make local generation more easily achievable. This is the purview of what
is known as distributed generation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Distributed generation refers to the production of electric power at the
periphery of the grid rather than at the centre, so that the source of electricity is
much closer to the consumer. Rooftop solar panels are a perfect example of
this type of generation. When a domestic dwelling installs solar cells on the
roof, these generating devices provide power directly to the household, not
across the grid, so any grid losses are eliminated. In many cases, the electricity
from these units is used exclusively by the household, perhaps with local
domestic storage batteries to manage any excess, unused power. This power
production and consumption all takes place on the consumer’s side of the
electricity meter.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are increasing instances, however, where electricity generation at the
household level is not restricted to that household circuitry alone. Often there
are arrangements with the local distribution company which allow surplus
electricity to be exported across the domestic meter and sold to the distribution
company, thereby entering the distribution grid. And there may even be laws
that require that distribution companies purchase this electricity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In addition to this type of household production, small power stations are
being designed to supply power to the distribution grid rather than to the
transmission system. The advantage of this is, again, that the electricity is
produced much closer to the consumers and that means that energy losses from
the transmission and distribution of the electricity are much lower. So while
the cost of electricity from a small power station might be higher than from a
large central power station, the proximity to the consumer makes the price
lower than it would be if the same power was supplied from the central power
station.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A further advantage of this type of generation is that it allows generating
capacity to be increased in much smaller tranches as demand rises. Instead of
needing to fund a massive central power station, a generating company may
choose to build 5 or 10 small stations as required.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Once this type of generation is embraced, there are any number of different
configurations that can be envisaged. A local community might decide to build
its own power station using wind turbines and solar cells, selling the power
to the members of the community and delivering it across the local distribution
network. The distribution company will charge for the service, but it
may still allow the community power to undercut the grid supply. Or, a
company that has installed backup generators in case of power supply failure
might chose to run these units at times of peak demand and sell the power they
produce to the grid.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Distributed generation has many attractions both in terms of cost and energy efficiency. But, the introduction of distributed generation means that the
distribution network is no longer a passive network that takes power from the
transmission system and delivers it to the consumer. Now there are power
flows into the distribution network from both the transmission grid and the
generators that are connected directly to the distribution grid. In extreme cases,
there may be power flows from the distribution network back into the transmission network. The distribution network has thus become an active rather
than a passive network, and it needs its own network control centre to manage
the energy that is running through it. Life for distribution companies has
suddenly become much more complicated.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The smart grid
For distribution network operators, the relatively simple process of buying
power from the national market and then selling it to a consumer has become
more of a matter of balancing the amount of power on the distribution
network, from both the traditional suppliers and the new distributed generators, against demand. Meanwhile at the transmission grid level, there are new
complications resulting from both the potential flow of power from distribution networks into the transmission network and the variable supply from wind
and solar power plants, all of which must be balanced against demand. These
are some of the factors that have led to the development of what has become to
be known as the smart grid.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The smart grid is, in essence, the addition of a computer communications
network that can run alongside the power distribution network, mimicking it.
Once this network is in place, intelligent devices e computers e can be
installed at key points in the network and these devices can communicate
with one another and so help manage the electricity network itself. This may
be as simple as an intelligent meter (smart meter) in a household that can
signal back to the network control centre the amount of power that household
is using and what machines are using it. In times of high demand, the control
centre operator may then be able to ask some of the machines in the
household to shut down until demand drops. This is known as demand
management.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another smart grid technology is congestion control. Power networks are
physical structures, much like roads, and if too much power is trying to pass
down the same power line, congestion occurs. Sensors placed at key points
across the network can monitor these flows, just as traffic can be monitored
and controlled using close circuit TV cameras, and the information from these
sensors can be used to control how much power passes down each line. At the
limit, automated devices can act autonomously to maintain the stability of the
grid. This type of technology is a key to managing a distribution network that
must now be capable of operating pro-actively.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Smart grid techniques are equally important for the transmission system.
One of the most important areas in which these technologies can help with grid
management is with the integration of renewable energy into the supply. Some
of these energy sources, but particularly wind and solar power, are both variable and unpredictable. Balancing the grid with these uncertain sources can be
tricky, particularly as they often provide the cheapest power when they are
generating and so this must be dispatched first. One of the smart grid tools to
help manage renewable power is weather forecasting. If accurate day-ahead or,
better, hour-ahead forecasting is available for features like wind speed over
different parts of the region and cloud cover in the case of solar generators, this
can automatically be fed into the grid management control centre and changes
in renewable output can be predicted and countered ahead of time.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another valuable way in which smart technology of this type can help with
renewable energy is via the concept of a virtual power station. Wind power
provides a good example of how this concept can be advantageous. Wind
speed varies from hour to hour and day to day, and at any single wind farm site
this will lead to a wildly varying output. However, the variation in wind intensity over a wide geographical region can be much lower. If the wind is not
blowing in one place, it will probably be blowing in another. If several wind
farms, separated geographically, are combined into a single, virtual power
station, the output from the power station will be much less variable and
therefore much more reliable than from any one of them alone. In effect the
wind is being averaged, geographically. This makes the management of the
output from the wind plants easier and the electricity they produce more
valuable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This concept can be expanded to include other generating technologies, a
technique called power pooling. Wind, solar power and even some fossil fuel
generation might be included in a single virtual power plant if the result was
advantageous from an economic and reliability perspective.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Microgrids
One of the new concepts that emerged from the combination of distributed
generation and the smart grid is the microgrid. A microgrid is a small, self-sufficient grid that includes a defined set of consumers together with power
generation facilities that are capable of supplying them with all the power
they need. While these technologies might include small fossil fuel generation units such as gas engines, many will be designed to use renewable
power from wind and solar sources. And because these cannot provide a
reliable supply alone, a microgrid of this type will also need some form of
energy storage.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A microgrid is not isolated; it is connected to a part of a larger distribution
network and it can both take power from that network and deliver power back
into it. However, it is managed locally as a single unit, and power delivery
across the microgrid is managed using local smart grid technologies. Most
importantly, the microgrid is capable of operating as an isolated grid if the
main distribution grid should fail. This resilience is a key part of the concept.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It has been suggested that microgrids might form the building blocks of
future national grid systems. Taken to this extreme, a national grid would
simply be an array of interconnected microgrids that use smart technologies to
cooperate as a single large grid but in the event of failures can fall back on
themselves. This idea presupposes that all electricity generation takes place at
the microgrid level, something that seems extremely unlikely given the economic advantages of larger power plants. Even so the concept could be
important as an element of future hybrid grid structures.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Electric vehicles
One of the most important sources of greenhouse gases, aside from fossil fuel
power plants, is that generated by transportation, by vehicles that use petrol
and diesel fuel as their energy source. One of the solutions to that problem
today is the electric vehicle that uses an electric motor to provide motive
power. That motor is in turn powered by a rechargeable battery, and in order
for this technological solution to be effective, vehicle batteries must be
recharged regularly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Electric vehicles present the electricity industry with a major problem. A
wholesale shift to electric vehicle power will lead to an enormous increase in
the load on the grid. This new load must be managed and smart grid technologies are likely to be essential to enable this to be carried out smoothly.
At the same time, electric vehicles potentially offer a solution to one of the
great problems of the modern grid, a lack of energy storage. A massive
population of electric vehicles can also be seen as a massive collection of
energy storage devices, the vehicles’ batteries. If the aggregate capacity of
these batteries could be harnessed, then it could help manage the integration of
renewable energy into the grid and reduce overall grid operating costs, without
the need for large energy storage plants.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Achieving this in practice will be difficult. It is only when an electric
vehicle is connected into the grid that its storage capacity can be harnessed for
grid support purposes. While it is likely that a large number of vehicles will be
connected and charging at any one time, there needs to be a way of managing
their number: a sudden rush to the seaside by half the population on a hot day
might bring the grid down, otherwise. Nevertheless, the concept of distributed
storage capacity based on vehicle batteries provides an interesting and
potentially extremely valuable extension of smart grid technologies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Supergrids
At the other end of the scale from the microgrid in terms of size and ambition
is the supergrid. At its simplest, a supergrid is a supranational (or supraregional) grid that acts to combine the grids of different nations so that they can
share their electricity generation resources. A scheme of this sort has been
proposed to integrate Europe’s national grids and allow power to be moved
from country to country more easily than is possible now. There are also some
special supergrids that have been proposed to solve particular energy harvesting and supply issues.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Supergrids are based on high-voltage direct current (HVDC) power lines
that can move power over long distances with lower losses than would be
found with the equivalent alternating current power line. Elements of supergrids already exist within national certain grids. In the United States, for
example, there are some HVDC interties that connect regional grid systems
together, while in China, HVDC power lines are used to move power from
regions with large generating capacities to centres of demand. However,
neither of these applications can be viewed as a coherent supergrid.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Some ambitious supergrid schemes have been proposed in the last decade
or two, though none has yet been constructed. One such is an offshore
supergrid in the seas of western Europe. Such a grid might run from Scandinavia all the way to Portugal. The purpose of such a grid would be to bring
offshore wind power from these waters onshore and to deliver it to the nations
along the western coast of Europe. However it would also provide a way of
interconnecting grids of the various seaboard nations so that they can exchange
power too. A subset of this is a proposal to build a North Sea supergrid to serve
wind farms in the North Sea and at the same time to interconnect the nations
with North Sea coasts.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A second supergrid scheme that has been proposed would link grids in
southern Europe to those in North Africa. The latter has massive amounts of
solar energy available, which could be captured in large solar power plants
built in the desert regions of the countries of North Africa. This power could
then be exported to the demand centres in Europe through the proposed
supergrid.
Much of the work on supergrids is speculative. However, the ideas being
developed for this, and other smart grid concepts, are likely to find their way
into the electricity systems of the future in one form or another.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Fuel costs
The cost of electricity from a power station depends on a number of different
factors, but the two most important are the capital cost of building the facility
and the cost of the energy that it uses to produce electricity. For many types of
power station, the source of energy that is used to make electricity is a
combustion fuel such as coal, natural gas or oil. Others burn biomass-derived
combustion fuels, and in the future, some stations will burn hydrogen. All
these fuels are more or less transportable and the fuel will be delivered to the
power station which will pay for each unit of fuel it receives.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power stations also require fuel, in this case a specialised fuel that
is normally based on uranium. This fuel, which is fabricated into special fuel
rods, is also delivered to the power station where it is utilised and again there
will be a cost attached to each unit.
There are other types of power station, renewable energy plants that utilise
an energy source that is generally considered free to use. While it is possible in
some cases to claim ownership of the resource that is being exploited, for
example, ownership of the land upon which a hydropower plant is built, this is
unusual. The energy from the sun, the wind and the energy in flowing water
are normally available to turn into electricity without any cost attached to the
consumption of the energy.]]>
			</paragraph>
			<paragraph>
				<![CDATA[While the energy is free, the use of renewable energy will often impose
other constraints on power station construction. Hydropower plants can only
be built at river sites where the water flow is suitable for exploitation. This may
mean building a power plant far from the main centres of consumption. Wind
power also relies on a suitable resource being available. Some of the best wind
sites are offshore, but these are also much more difficult to exploit than
onshore wind sites. Solar power is somewhat more equitable in this regard, but
the economics of utility scale solar power plants will restrict the available
sites, particularly where solar thermal power generation is concerned.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The effect of these constraints will be to increase the capital cost of construction of the renewable power plant. If the facility is located far from the
main grid, then additional cost will be attached to the construction of a power
line to carry the power to the grid. The location may also make the job of
construction more difficult. In contrast, a combustion plant can be built anywhere, and major coal and natural gas-fired power plants are usually located
close to the transmission grid.]]>
			</paragraph>
			<paragraph>
				<![CDATA[These factors may affect the capital cost of construction of a power plant,
but there is, nevertheless, a fundamental difference between power plants that
require a combustion fuel and those that use a renewable energy source. For
the former, there will always be a fuel cost associated with the production of
electricity. For a renewable energy plant, in contrast, the cost of production
will be very small once the capital cost of the power plant has been met.
Hydropower plants, once their financial costs have been paid down, provide
some of the cheapest power available. As these power plants can have
extremely long operational lives, this situation is not uncommon.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Combustion fuels
The main combustion fuels for power generation are coal and natural gas. In
the past, oil was an important source of electricity too, but most of the oil that
is pumped is used elsewhere today. However, there is still limited use of oil in
small piston engine power plants, and some large combustion power stations in
oil-producing countries continue to burn the fuel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Oil and its byproducts are valued because they are liquid fuels, they can be
easily transported and a small quantity contains a large amount of energy.
Crude oil typically contains 46 MJ/kg.1 The high energy density and the
portability are reflected in the cost of the fuel which is the most expensive of
the fossil fuels. Natural gas can be delivered by pipeline and it can be
distributed in liquefied form, but it is less adaptable than traditional liquid
fuels. It has a higher energy density than oil, 54 MJ/kg, but its volume energy
density is much lower. Coal, the most popular fuel for power generation, is the
least easily transported of the major fossil fuels. Its energy density, typically
24 MJ/kg, is lower than either oil or natural gas and transportation costs can be
high. In consequence, coal is often used close to the source, the mine.
Some high-quality coal is transported over large distances but most is
consumed locally.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 4.1 shows the variation in the cost of these fuels on a unit of energy
basis when they are used for power generation in the United States. While
local factors will affect the actual figures, the relative costs are generally
similar in most parts of the world, but with a caveat concerning the cost of
natural gas, noted below.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The most expensive of the fuels is high-quality distillate fuel, which, in the
United States, costs $15.16/million BTU in 2019. Residual fuel oil, the lowest
quality fuel oil, costs $12.72/million BTU. Natural gas for power generation
had a typical cost of $2.88/million BTU, 19% of the cost of a similar amount
of distillate fuel, in energy terms. Meanwhile, coal, the cheapest fuel in
Table 4.1, had a cost in 2019 of $2.02/million BTU or 13% of the cost of
distillate fuel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost difference between natural gas and coal in the table is relatively
small. This, at least in part, reflects the low cost of natural gas in the United
States in consequence of the expansion of shale gas production there. The
difference in cost between coal and natural gas is likely to be much larger in
other parts of the world. Given the prices differences shown in the table, it is
no surprise that oil is rarely used for large-scale electricity production.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 4.2 compares the annual consumption of these fuels in the United
States for power generation between 2010 and 2019. The most notable feature
in the table is the decline in the use of coal for electricity generation over this
period. Consumption of coal in 2019 at 7,919,000 billion BTU was only 56%
of the level in 2010. Over the same period, the annual consumption of natural
gas grew, from 3,396,000 billion BTU to 5,436,000 billion BTU, an increase
of 60%. These figures are indicative of a switch from coal to natural gas over
this period in the United States. Similar trends will be found in many advanced
nations as a result of environmental concerns. Elsewhere the picture is not
quite so clear with coal consumption for power generation still increasing in
China and India. The other observation to make from the table is that petroleum liquids have only limited use for power generation in the United States in
the 21st century and that usage is declining. This reflects the fact that petroleum liquids are generally too expensive to use in power plants.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Oil
Crude oil is a liquid hydrocarbon formed from organic material buried and
compressed within rock in the earth’s surface. The composition of crude oil
varies but mostly contains between 82% and 87% carbon and 12%-15%
hydrogen by weight. The most important constituents of crude oil are paraffins, naphthalenes and aromatics. Paraffins are the main constituents of gasoline, used for transportation, while aromatics and naphthalenes are often used
by the chemical industry as precursors for other products.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Global oil reserves are not distributed evenly across the globe. The largest
regional reserves are found in the Middle East with 834 thousand million
barrels of proven reserves at the end of 2019.2 Largest reserves were in Saudi
Arabia, Iran, Iraq and Kuwait. South America also has significant reserves,
primarily as a result of those in Venezuela, which total 304 thousand million
barrels. Other countries with notable reserves include Canada with 170
thousand million barrels, the Russian Federation with 107 thousand million
barrels and the United States with 69 thousand million barrels. In Africa, only
Libya, Nigeria and Algeria have significant reserves and the proven reserves
are limited in the Asia Pacific region too.]]>
			</paragraph>
			<paragraph>
				<![CDATA[When crude oil is extracted from the ground, it is usually sent to a refinery
where it is separated into different constituents that can be used for a variety of
purposes. The processing leaves a residual oil that is often used for power
generation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The standard measure of crude oil is the barrel, a unit defined by the US oil
industry. A barrel of oil is equivalent to 42 US gallons of oil, or 159 L.
Elsewhere in the world, weight is often used to define crude oil quantities. The
relationship between volume and weight depends on the quality of the crude
oil, but a barrel of light oil might weigh around 140 kg.3 A tonne of a similar
oil would contain just over seven barrels. Oil markets usually price oil per
barrel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 4.3 shows figures from the BP Statistical Review of World Energy
2020 for the average annual price of a barrel of Brent Crude between 1989 and
2019, adjusted to $2019 to take account of inflation. As the table shows, the
cost of a barrel of oil can be extremely volatile. The price often depends on
global economic conditions, with a high price typical during periods of rapid
economic growth and a low price when there is a slump. Geopolitical factors
also play an important role. When oil supply is threatened in a particular part
of the world, prices are likely to rise.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The average cost in 1989 was $37.58/barrel and the average prices (not
shown) were relatively stable during the next decade, dropping to a low of
$19.94/barrel in 1998. The price began to rise during the next decade, drifting
upwards so that near the end of the decade, in 2008, they peaked at $115.48/
barrel before falling back sharply the next year, to $73.49/barrel, as a result of
the global financial crisis. This dip was short-lived, and by 2011, the average
price was $126.45/barrel. This proved to be a peak. The cost dropped abruptly
in 2015 to $56.51/barrel and remained in this region until 2019 when the
average price was $64.21/barrel. However, the average price is likely to show a
sharp fall again in 2020 as a result of the coronavirus pandemic which pushed
down consumption during the early part of the year. This, coupled with a
policy of oversupply from OPEC and Russia, led to the weekly price of Brent
Crude dipping to below $25/barrel at one point in April 2020.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There has, historically, been a strong correlation between the price of oil
and the price of natural gas with a rise in the price of oil pulling up the price of
natural gas and a fall in the oil prices depressing natural gas prices. This
linkage appeared to have broken between 2008 and 2019, at least in the United
States, and since 2008 the prices of the two commodities have not tracked one
another so closely. This has been put down to the rise in production of shale
gas in the United States, which has depressed natural gas prices. However the
rupture of the link may be less strong in other regions where there is no e or
limited e domestic natural gas production. Production of oil and natural gas
are linked because in most cases they are taken from the same wells and so
some connection remains inevitable. It is conceivable, too, that a stronger
correlation between the two may return.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gas
Natural gas, like crude oil, is found by drilling into underground rock strata.
Like oil, it is the product organic material that has been buried within the
surface of the earth over aeons. When it emerges from the ground, natural gas
is a mixture of combustible hydrocarbons. The main component is methane,
which normally accounts for 70%e90% of the total. Other hydrocarbons such
as ethane, propane and butane can account for up to 20% and there may be up
to 8% carbon dioxide, small amounts of oxygen and nitrogen and up to 5%
hydrogen sulphide. The gas is normally cleaned after it has been pumped from
the ground to remove impurities such as hydrogen sulphide (this can be processed into pure sulphur), carbon dioxide and water. The higher hydrocarbons
such as propane and butane may also be removed for industrial use leaving the
cleaned gas, now referred to as dry natural gas. This is the gas that is typically
pumped into pipelines.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gas is found in many parts of the world, but most countries have
only small reserves. Regionally, the Middle East has the largest total proven
recoverable reserves, 75.6 trillion cubic metres or 38% of the global total at the
end of 2019.5 Iran alone commands 16% of global reserves and Qatar a further
12%. Meanwhile the Russian Federation holds the largest single national
reserve, 19% of the global total. Turkmenistan has a further 10% and the
United States 7%. These five countries together command close to two-thirds
of the world’s proven natural gas reserves.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Unlike oil, there is no global benchmark for the price of natural gas and the
cost varies from region to region. Table 4.4 contains figures for average annual
costs in $/million BTU in the United States (Henry hub), the import price in
Germany which is typical of the price in western Europe and the cost of
liquefied natural gas (LNG) imported into Japan where there are no indigenous
reserves that are exploited. The final column compares the cost of crude oil for
the same quantity of energy. Looking across the table during the early years for
which figures are presented suggests that the prices are broadly similar across
the globe and that the gas price is not dissimilar to that of crude oil. For
example, in 1999, the average cost of a unit of gas at Henry Hub was $2.27/
million BTU while the cost of a unit of crude oil was $2.98/million BTU,
while 2 years later the average prices of these two commodities were $4.07/
million BTU and $4.08/million BTU. The cost of LNG in Japan was slightly
higher in both years, a difference to be expected when the fuel must be shipped
from gas-producing nations to Japan. Meanwhile, the cost of natural gas in
Germany which is delivered by pipeline from the Russian Federation, from
Norway or from North Africa was slightly lower than at Henry Hub in the
United States.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This pattern of prices broadly held until 2007, although by this date the
price of a unit of crude oil had begun to rise above that of natural gas in most
of markets shown in the table. The regional natural gas prices in the table were
still close to one another, although the cost of imported gas in Germany has by
2007 risen above both Japanese LNG and the price at Henry Hub in the United
States.]]>
			</paragraph>
			<paragraph>
				<![CDATA[By 2008, there were signs of a change in the relative prices across the table
and this was amplified in the succeeding years. The most immediately
noticeable feature is that the price of a unit of gas at Henry Hub in the United
States had fallen dramatically compared to both the cost of gas in Germany
and Japan and the price of a unit of crude oil. In 2012, for example, the cost of
a unit of gas in the United States was $2.76/million BTU while the cost in
Germany was $10.93/million BTU and in Japan the average LNG price was
$16.75/million BTU. The crude oil price was $18.82/million BTU.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cause of this change was the sudden availability of large quantities of
shale gas in the United States, which served to depress the price there
compared to other parts of the world. It also severed to linkage between natural
gas and oil in the United States because the rise and fall in natural gas prices
no longer tracked that of oil.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In contrast, the prices for natural gas in Germany and for LNG in Japan
continued to show some connection to the oil price. There was quite close
correlation between Japanese LNG and global oil prices and both continued to
track the global economic situation with a slump after 2008 as a result of the
global financial crisis followed by a rapid recovery between 2010 and 2014.
Prices of both then dipped during 2015e17 before climbing again in 2018.
The correlation between oil price and the cost of imported gas in Germany was
less clear after 2008 with the price differential increasing markedly after that
year. Natural gas prices in Germany during this period reflected the changing
market for natural gas in Europe. So while there was a clear global linkage
between oil and natural gas prices at the beginning of the century, by the end
of the second decade of the 21st century, natural gas prices showed a strong
local component.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gas is a much cleaner fuel than coal, so switching generation from
coal to natural gas can reduce greenhouse gas emissions significantly. In
addition, natural gasefired power plants are relatively cheap to build. In the
United States, a decade of relatively low natural gas prices has encouraged
utilities to burn natural gas in preference to coal, as was shown in Table 4.2.
The experience has not been quite so simple in other parts of the world. Some
utilities in Europe also chose to build natural gasefired power plants while
gas prices were low, but the volatility that still exists in the natural gas price
in most parts of the world has led to these plants becoming uneconomical to
operate when prices rise too far. The situation has been aggravated in
Western Europe by a glut of cheap coal from the United States as a result of
coal being displaced there by natural gas. This has made generation from
coal relatively more economical during the middle of the second decade of
the 21st century.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is important for power plant developers to be aware of the risk that can be
associated with fuel price volatility, particularly with natural gas. It is never
safe to assume that prices will remain low. There is an additional problem too.
In the past if natural gas prices rose, utilities could increase the price of their
electricity to compensate. Today the large generating capacity based on
renewable resources available in many countries sets a backstop or hedge
price. The renewable resources may be unreliable over the short term, but over
the long term, they provide a stable supply at a stable price. Natural gasefired
plants must be able to undercut this price if they are to remain competitive.
What is more, the stable, long-term cost of renewable energy is falling.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal
The term coal embraces a range of materials. Within this range, there are a
number of distinct types of coal, each with different physical properties. These
properties affect the suitability of the coal for power generation.
The hardest coal is anthracite. This coal contains the highest percentage of
carbon (up to 92%) and very little volatile matter or moisture. When burnt, it
produces little ash and relatively low levels of pollution other than carbon
dioxide. Anthracite is typically slow-burning and often difficult to fire in a
power station boiler and it has traditionally been used for heating rather than
industrial use. However, it is becoming more common as a power plant fuel in
countries with large reserves such as Russia, which have switched to anthracite
for national power generation in order to free natural gas for export.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The most abundant of the coals are the bituminous coals. These coals
contain significant amounts of volatile matter. When they are heated, they
form a sticky mass, from which their name is derived. Bituminous coals
normally contain between 76% and 86% carbon (dry content). Moisture
content when mined is between 8% and 18%. They burn easily, especially
when grounded or pulverised. This makes them ideal fuels for power stations.
Bituminous coals are further characterised, depending on the amount of volatile matter they contain, as high, medium or low volatility bituminous coals.
Some bituminous coals contain high levels of sulphur which can be a handicap
for power generation purposes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A third category called sub-bituminous coals or soft coals are black or
black-brown. These coals contain between 70% and 76% carbon (dry content)
and 18%e38% water as mined, even though they appear dry. They burn well,
making them suitable power plant fuels, and sulphur content is low.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The last group of coals that are widely used in power stations is lignite.
These are brown rather than black and have a carbon content of 65%e70%
after they have been dried but the moisture content is 53%e63% when taken
from the ground. Lignites are formed from plants which were rich in resins and
contain a significant amount of volatile material. The amount of water in
mined lignite, and its consequent low carbon content, makes the fuel uneconomic to transport over any great distance. Lignite-fired power stations are
generally found adjacent to the source of fuel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A type of unconsolidated lignite, usually found close to the surface of the
earth where it can be strip-mined, is sometimes called brown coal. (This name
is common in Germany.) Brown coal has a moisture content of around 45%.
Peat, which has a dry carbon content of less than 60%, is also burned in power
plants, though rarely.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal is found in most parts of the world. The largest regional reserves are
in the Asia Pacific region where China, India, Australia and Indonesia all have
significant amounts of all types. The United States also has very large reserves
of coal. In Europe, Ukraine and Poland have large amounts of anthracite and
bituminous coals, while Germany and Turkey have sub-bituminous coals and
lignite in abundance. There are limited reserves in South and Central America,
the Middle East and in Africa, where only South Africa uses coal extensively.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The value, and therefore the cost, of these different coals vary enormously.
In the United States, in 2018, the cost of a tonne of anthracite was $110, for
bituminous coal the cost was $66/tonne, the average price paid for subbituminous coal was $15/tonne while for lignite it was $22/tonne.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The average cost of coal in different regions of the world between 1999 and
2019 is shown in Table 4.5, based on figures from the BP Statistical Review of
World Energy 2020. Traditionally, coal prices have not been as volatile as
those of oil or natural gas, but as the figures in the table show, there was a
significant increase in the cost of a tonne of coal in all regions in 2008 as
global growth surged, just before the global financial crisis.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As coal is mostly consumed where it is mined, coal prices should reflect as
strong local dimension. Nevertheless, the variations between different regions
listed in the table are not large.
In the United States, the average cost of a tonne in 1999 was $31.29. The
price moved both up and down during the next 8 years before peaking at
$117.42/tonne in 2008. Prices quickly fell back again to $60.73/tonne in 2009,
after which the price remained in a band between $84.75/tonne and $51.45/
tonne.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost of coal in Northwest Europe was broadly in similar territory over
the 20 years shown in the table but with somewhat greater volatility than in the
United States and with a peak of $147.67/tonne in 2008 and another of
$121.48/tonne in 2011. Japan imports all its coal. Even so the prices in Table 4.5 show the cost of coal in Japan to be competitive with that in Europe and
the United States, at least until 2008. However, the spike in prices in Japan in
2008 was not tempered until 2014, with the price reaching a peak of $136.21/
tonne in 2011. Prices began to fall back slightly after 2013 but rose again from
2017.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The spot price for coal in China is shown in the final column of Table 4.5.
Here prices were relatively stable during the first years of the 21st century,
rising slowly towards the end of the first decade when they peaked at $104.97/
tonne in 2008. There was a further peak in 2011, after which prices stabilised,
but at a much higher price than in the first decade of the century. In 2019, the
average price was $85.89/tonne, higher than in either the United States or
Northwest Europe.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biomass
The term biomass fuel encompasses a wide range of materials that can be used
for power generation, as noted in Chapter 2. Waste materials, particularly
agricultural wastes, are one of the important local sources and where they are
available they can be used as combustion fuel in small combustion power
plants that are typical in the biomass sector. For larger scale power plants, a
dedicated supply of biomass fuel is needed. This can be derived from biomass
crops, usually fast-growing trees or grasses. Alternatively, some biomass power plant operators rely on biomass pellets, often made from forest wood,
which are traded both nationally and internationally.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biomass wastes are the cheapest of these fuels. Prices will vary but should
usually be low, making generation from these fuels economical even in
relatively inefficient power plants. Table 4.6 shows figures for biomass waste
(residuals) in the United States in early 2020 derived from data published by
Statista. Waste from wood manufacturing plants was the most expensive at
$41.4/tonne, followed by sawmill residuals at $36.3/tonne. Roundwood and
pulpwood (wood suitable for papermaking) cost $33.7/tonne and other residuals cost $32.1/tonne. These prices would suggest that waste or residual
fuels of this type are relatively competitive with coal.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Annual average monthly prices for biomass pellets in the United States,
collated by the US Energy Information Administration, are collected in
Table 4.7 for 2016e19. Biomass pellets are manufactured from felled wood
and are considered a high-quality, tradable commodity. Large volumes of
biomass pellets are exported from the United States and Canada to Europe.
The average prices shown in the table are notably higher than for the biomass
residuals in Table 4.6. For example, in 2016, the average price was $178/tonne.
Average annual prices were not available for succeeding years so monthly
average prices for December each year are shown. In 2017, this was $168/
tonne; in 2018, it was $180/tonne and in 2019, it was $192/tonne. This
suggests that biomass pellets are significantly more expensive than coal.
Similar prices can be found in Europe where bulk pellet prices were between
V170/tonne and V323/tonne in December 2018 according to Bioenergy
Europe.6 Power plants burning biomass can both avoid carbon taxes and
benefit from renewable subsidies in some European countries; this can help
offset the high price for the fuel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Locally grown chipped biomass wood fuel is likely to be more competitive
than pellets for combustion in a biomass power plant. This type of fuel will
typically be supplied on a long-term contract between the company supplying
the fuel and the power company burning it. Prices for such contracts are not
generally available.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydrogen is beginning to appear as a commercial power plant fuel. It is
attractive because, of itself, it is a clean fuel with little environmental impact.
The gas can be produced from natural gas, but this process leaves carbon
dioxide as a waste byproduct and damages the environmental credentials of the
fuel. Industrial production of this type is taking place in Brunei to produce
hydrogen from natural gas, the product being exported to Japan for use in a
power plant. The Japanese market for hydrogen is expected to expand rapidly
during the third decade of the 21st century.]]>
			</paragraph>
			<paragraph>
				<![CDATA[More important from an environmental perspective is the production of
hydrogen by the electrolysis of water. If this can be achieved using low-cost
renewable electricity, then the resulting fuel gas is truly clean because the
only product of its combustion in air is water vapour. The cost of hydrogen
generated by electrolysis will depend on the production cost of the electricity
used in the process. It has been suggested by the International Renewable
Energy Agency (IRENA) that by 2018 it was possible to generate hydrogen for
$5e6/kg by connecting a hydrolyser to the Danish electricity grid. Meanwhile,
the US Department of Energy has set a target of $5/kg for hydrogen dispensed
from pumps, and in Japan, the target is to reduce the pump price of $10/kg in
2018 to $3/kg by 2030.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear fuel
The cost of a unit of nuclear fuel cannot be directly related to the cost of
electricity in the same way as can the cost of a unit of any of the other fuels
discussed above. In contrast to the situation for fossil fuel, biomass or
hydrogen, it is not particularly useful to discuss the energy input in terms of a
kilogram or a tonne of nuclear fuel.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One kilogram of radioactive U235 could, in theory, release 24,000,000 kWh
of heat energy while a kilogram of coal produces around 8 kWh. A typical
nuclear reactor will contain about 100 tonnes of uranium but only about 2% of
this will be U235. A typical 1000 MW nuclear power plant would probably
consume around 3 kg of this radioactive uranium each day.
The fuel for a nuclear power plant is contained in technically complex fuel
rods, and the nuclear material in each fuel rod is itself the product of a long
manufacturing process. It is possible to put a market price on the important
constituent of most fuel rods, pure uranium, but that does not provide much
guidance when examining the economics of nuclear power.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In spite of these caveats, the nuclear industry does advance figures suggesting that it is possible to compare nuclear power plant fuel costs with the
fuel costs for fossil fuel plants. For example, the US Nuclear Energy Institute
has estimated that the cost of fuel for a nuclear plant is around 14% of total
costs, although this increases to 34% when factors such as waste management
and additional front-end costs are included.8 The fuel cost for a coal-fired plant
is, on the same basis, estimated to be 78% of total costs and for a gas-fired
power plant it is 87%. Against this, the cost of construction of a nuclear power plant is significantly higher than the cost of construction of a fossil fuel
power plant. And the comparison with renewable energy cannot easily be
made because in general the fuel cost for a renewable plant is zero.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The capital cost of a power
plant
The construction of a power station is a major civil and electrical engineering
project, and the total cost of even the smallest generating facility will be an
important factor in determining its economic viability. The capital cost will
therefore have a major bearing on the choice of power station technology.
Capital costs are not static and the relative cost of different types of generating
technology will vary over time. This will change the economic balance between them. And while the ultimate determinant of economic viability should
be the cost of the electricity the power station produces over its lifetime, it is
quite possible that at the time of construction the owner of the plant will
choose the least expensive to build]]>
			</paragraph>
			<paragraph>
				<![CDATA[The capital cost is itself determined by several factors including the sophistication of the generating technology, by the materials involved in its
construction and by the work required to build the power station. Some very
small power plants, a small piston engine for backup power for example, can
be purchased from a factory and delivered ready to plug in and switch on.
More usually there will be a significant amount of civil engineering involved
too, preparing the site for the power plant and installing the components.
Some, such as large hydropower plants, will involve massive civil construction
projects. In other cases, such as onshore wind turbines, the site preparation is
less complex but installation is a highly skilled process.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The type of power station chosen will have a major effect on the balance of
these costs. There are several power plant types that use components that are
mass-produced and then delivered to the site. Gas turbines, piston engines and
even wind turbines are fabricated in factories and delivered ready to use or
ready to assemble. For these factory-produced power plants, the unit cost often
depends on global competition between different manufacturers. For other
technologies, much of the fabrication takes place at the power station site.
Many of the components of a large coal-fired power station will be built at the
site, as will a great deal of a nuclear power plant. For these plants, labour costs
will be high and the local cost of labour will impinge significantly on the
overall capital cost. This will vary from country to country.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Project finance
Then, on top of these other considerations, there will be the cost of financing
the project. It is rare today for a power plant to be built without the need for a
loan to finance its construction. This loan must eventually be paid down,
usually with revenues from the power station. Financing will add another
significant component to the overall cost.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Debt financing brings into focus another factor, the length of time over
which the loan must be paid down. Most power plants are expected to have
operating lives of between 20 and 30 years and the loan repayment will
normally be stretched over as much of this lifetime as possible to keep the
payments low. It is rare for loan repayments to be longer than 30 years e often
they are shorter e but there are types of power plants which have much longer
lifetimes. Hydropower plants can often operate for 100 years or more with
adequate maintenance. Many nuclear power plants have also had their lives
extended to 50 or 60 years, although this may involve additional expense.
Well-maintained solar cell power plants may also be able to operate for longer
than their nominal lifetimes.]]>
			</paragraph>
			<paragraph>
				<![CDATA[If the operating life of a power plant is much longer than the period over
which a financing loan is paid down, this will front-load the cost of the plant
making it relatively more expensive than a scheme where the loan repayment
matches the plant lifetime. On the other hand, once the debt has been repaid,
the cost of electricity from the long-lived power station will drop significantly.
There are hydropower plants and nuclear plants operating today, free of debt,
that produce some of the cheapest power available.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Project financing, while widespread, can sometimes lead to problems in a
world where economic or environmental directions are changing. Of particular
concern in the third decade of the 21st century is the prospect that new fossil
fuel power stations will become redundant well before the end of their operating life because they are supplanted by renewable energy sources. If a plant
is taken out of service before its debt is repaid, it becomes what is known as a
‘stranded asset’. Stranded assets become a financial liability for their owners
who will try to recoup their losses, perhaps by charging more for electricity
from other power stations they own. It is a contentious issue but one that needs
to be acknowledged as global warming rises up the international agenda.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Power plant capacity factors
When comparing the cost of different power generating technologies today, it
is not enough simply to look at the cost per unit of generating capacity of each.
That would be reasonable if each type of generating technology could run for
the same amount of time each year, but such is not the case. Fossil fuel and
nuclear power plants should be capable of running for most of the time without
interruption, if required, but many renewable power plants can only operate
intermittently. This difference must be taken into account when comparing one
with another.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The parameter that is used to measure the availability of a power plant is
called its capacity factor. The capacity factor is simply the number of hours in
a year that the plant is able to operate divided by the total number of hours in a
year. A power station with a capacity factor of 100% would be capable of
running endlessly, while one with a capacity factor of 20% would only be able
to operate for 1 h in every five. This latter plant would only be able to produce
one-fifth of the electricity of the former in every year. Or, five of these plants
would be needed to replace one plant of similar size with a 100% capacity
factor.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is a further level of complexity to consider, the theoretical and the
actual capacity factor. The theoretical capacity factor of a power plant is the
capacity factor it would achieve if it was able to operate for as long as it was
capable. For a conventional fossil fuel or nuclear power plant, this would be
limited only by the amount of time it must be taken out of service each year for
maintenance (or in the case of a nuclear power plant for refuelling) and the
average amount of time lost due to failure of components. A coal-fired power
station would be expected to have a theoretical capacity factor of around 85%,
and for a natural gas-fired combined cycle power plant, it would be 87%. A
nuclear power station might typically achieve 90% capacity factor, similar to
that of a geothermal power plant while a biomass-fired combustion plant could
be expected to reach a capacity factor of 83%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Generating plants based on renewable sources of energy will generally
have lower theoretical capacity factors. A hydropower plant could in theory
have a capacity factor similar to that of a fossil fuel plant, but in practice it is
usually significantly much lower and many vary from year to year as a result of
variations in annual rainfall. The theoretical capacity factor of wind and solar
plants depends both on the technology and on the resource. Solar capacity
factors are often below 30%, and for wind it may be 40% or less, but there are
exceptions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Actual capacity factors are another matter. For many fossil fuel power plants
today, this will depend on the duty cycle required of the plant. With the advance
of renewable energy, gas turbine, some coal and even nuclear plants may be
used by grid operators for support rather than base load generation. This will
reduce the actual capacity factor compared to that theoretically achievable.
The reverse is often true of renewable resources. Wind and solar power
plants are generally dispatched first and so they will often achieve close to the
theoretical limit. Of these renewable sources, only hydropower plants are
likely to be held in reserve.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.1 shows figures for the annual capacity factors of conventional
power plants in the United States between 2010 and 2019 based on figures
from the US Energy Information Administration (US EIA). These figures
illustrate many of the points discussed above. For example, the figures for US
nuclear power plants show that they have typical annual capacity factors of
over 90% and reached 94% in 2019. This is a reflection of the fact that the US
nuclear plants are running flat out providing base load generation. Many of
these US plants are over 30 years old and have paid down their construction
debt, making the power they produce relatively cheap.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The capacity factors for coal-fired power plants show an (almost) steady
decline between 2010 and 2019. While there are times where the capacity
factor rose, year upon year, the fall from 67% in 2010 to 48% in 2019 is
consistent with a gradual phasing out of coal-fired power in the United States,
partly in consequence of environmental considerations but heavily influenced
by the availability of cheap natural gas during this period. Supporting this, the
annual capacity factor for natural gasefired combined cycle plants increased
from 44% in 2010 to 57% in 2019. While the rate of increase was not
monotonic, the trend is consistent with the increased use of cheap natural gas
for power generation. This is a uniquely US factor, not mirrored across the
globe, although there has been a similar shift from coal to natural gas in many
developed nations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The other two columns in Table 5.1 show the annual capacity factor for
open cycle natural gas turbines, typically used for peak power production and
gas engines, also often used for peak production and grid support. Consistent
with this, both show annual capacity factors which are under 12% in most
years.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.2 presents similar sets of figures to Table 5.1 for US renewable
energy plants. These figures are for the most part a reflection of the maximum
capacity factor of which these technologies are currently capable. For solar
photovoltaic (solar PV) power plants, the typical annual capacity factor of
around 20% in 2010 had risen to 25% by 2013 and remained in that region for
the rest of the decade until 2019. Solar thermal power plants have shown a
more variable annual capacity factor ranging from 25% in 2010 to 17% in
2013. This technology depends more critically on the quality of sunlight
available and will be more dependent on meteorological conditions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The output from the US wind farms shows a valuable increase in capacity
factor during the decade in the table. From 29% in 2010, the average capacity
factor had risen to a consistent 35% by 2019. This is a reflection of the
improvement in wind technology. Wood biomass power plants, meanwhile,
were consistently operating at around 60% during the whole of the decade.
This would suggest that the theoretical capacity factor attributed to these
biomass plants of 83% from the US EIA quoted above is rather optimistic.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first column in Table 5.2 shows annual capacity factors for US hydropower plants. These range from 36% in 2015 to 46% in 2011. Part of this
variability will be a result of variations in annual rainfall in the United States.
However, another consideration is the increasing use of hydropower for grid
support rather than base load generation. The US geothermal power plants, not
shown in the table, had average annual capacity factors of between 68% and
76% in the decade to 2019.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.3 presents a set of complementary figures for global average capacity factors for renewable technologies assembled by the International
Renewable Energy Agency (IRENA). The first column shows global average
figures for solar PV. These are notably lower than the US figures, ranging from
14% in 2010 to 18% in 2019. However, the steady improvement over this
period once again reflects improvements in solar PV technology. Similarly the
average capacity factor for solar thermal plants rose from 30% in 2010 to 45%
in 2019. In addition to improvements in the technology, this may be a result of
more solar thermal plants around the world being equipped with energy
storage.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Globally, hydropower plants showed a larger average capacity factor
than the US plants in the previous table. However, the maximum was still
only 51%. Bioenergy plants had average capacity factors of between 67% and
86%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Perhaps the most notable figures in Table 5.3 are for wind energy. The third
column shows average annual capacity factors for onshore wind. This rose
steadily during the decade shown on the table, from 27% in 2010 to 37% in
2019. Again, this is a reflection of the improvement in the wind technology
being deployed over the period of the table. Column four shows similar figures
for offshore wind. These are more variable, with a minimum of 30% in 2014
and a maximum of 45% in 2013 and 2017. These figures are consistently
higher than similar figures for onshore wind and show to extent to which the
offshore resource is superior to that onshore.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Figures published by Statistica for UK offshore wind farms, one of the
largest national fleets, showed an average annual capacity factor in 2019 of
40.6%,2 in line with the figure in Table 5.3. Notable, however, was the performance of one Scottish offshore wind farm e as reported by Energy
Numbers e which achieved a record average annual availability of 53.3%.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Capital costs
The cost of building a new power station will vary from project to project.
Even seemingly identical projects may have different costs and the prospective cost will only emerge when detailed planning is carried out. There
are a range of important situations, however, when some idea of cost is
needed before detailed planning begins. To meet this need, a variety of national and international organisations regularly calculate the expected capital
cost of power plant construction for a range of different types of technology.
Important series of this type are published by the International Energy
Agency (IEA), by the US EIA, by Lazard, by the IRENA, by trade bodies for
specific technologies such as solar power or wind power and by government
agencies in many parts of the world. The figures from these studies can be
used by project planners to help select a specific technology suitable to a
particular situation. An examination of the year-on-year changes in these
cost figures can also reveal important trends that can influence policy and
decision-making.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In order to be able to compare the cost of a coal-fired power plant with that
of a wind farm, or the cost of a hydropower plant with that cost of a nuclear
power plant, the costs for each must be reduced to a standard unit of measure. In
this case, the most common approach is to present the cost for each kilowatt of
generating capacity. This figure will allow direct comparisons to be made between technologies, and by scaling up to the size of a proposed project, a rough
estimate of actual total cost can be obtained. The figures are normally calculated
before any cost for financing a project and are taken into consideration. This
figure is sometimes called the overnight cost. All the figures in the tables below
are overnight costs for plant construction.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.4 presents figures produced by Lazard for the capital cost of all the
main generating technologies in the United States in 2019. These figures
provide an annual snapshot of the expected cost in the United States but in
many cases will be broadly transferrable to other regions and nations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The figures in the table offer cost ranges and the most expensive of these is
nuclear power which was estimated to cost between $6900/kW and $12,200/
kW. Of all the established technologies such as coal, natural gas and nuclear
power plants that have historically been used for base load generation, new
nuclear power plants have the highest price, by a significant margin.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are three fossil fuelebased generating technologies included in the
table. The most expensive of these is coal-fired technology which had an
estimated capital cost in the range $3000/kW to $6250/kW in 2019. A natural
gas-fired combined cycle plant, the most common technology for large-scale
utility gas-fired power generation, costs between $700/kW and $/1300/kW
or roughly four to five times less than a similarly sized coal plant. The third
fossil fuel technology, an open cycle gas turbine, had an estimated cost of
$700/kW to $900/kW. These latter units are usually used for providing power
during periods of peak demand.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Six renewable technologies and configurations are also included in
Table 5.4. The most expensive of these is solar thermal power generation with
energy storage with a cost range of $6000/kW to $9100/kW. Geothermal
generating capacity had an expected cost in 2019 of $3950/kW to $6600/kW.
Onshore wind power, which is now one of the most important renewable
generating technologies, globally, costs between $1100/kW and $1500/kW in
the United States, while offshore wind capacity could be built for $2925/kW.
The cheapest renewable technology in terms of capital cost was utility-scale
solar generation, which had a price of $900/kW to $1100/kW. Rooftop solar
deployment is more expensive with a price range of $1750/kW to $2950/kW,
but this is deployed at the consumer or distribution system level where the cost
of electricity is also higher.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Capital cost trends for all the most important technologies will be discussed in the sections below, but Table 5.5 shows figures from IRENA for
the average global annual total installed cost for the main renewable technologies. These figures are based on actual power plants. The table contains
figures for both hydropower and bioenergy, neither of which is included in
Table 5.4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The global 2019 capital costs in Table 5.5 show broad agreement with the
US figures in Table 5.4 for most of the technologies that appear in both. The
only exception is offshore wind, which has an average global capital cost in
2019 of $3800/kW, based on IRENA’s database of projects, compared with
$2925/kW for the US EIA estimate. Some of the trends in Table 5.5 appear
erratic, but for the new renewable technologies, wind and solar, the cost in
2019 is lower than it was in 2010, and in the case of solar PV, the price drop is
dramatic.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The two established renewable technologies, hydropower and bioenergy,
show no obvious trends in Table 5.5. Prices for each rise fall sharply again
and then rise once more. For hydropower, the average installed cost rose from
$1254/kW in 2010 to $1704/kW in 2019, a price rise that is probably
consistent with inflationary increases over that period. The bioenergy capital
costs are much more erratic, ranging from $1300/kW in 2011 to $2028/kW in
2013. This variability is probably a result of the small number of such projects built each year. More consistent figures from the United States are
presented below and these put the cost of a new plant in 2019 at around
$4000/kW.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal-fired power plants are normally large, complex facilities. New generating
stations will generally employ the most efficient technology available based on
what are known as supercritical or ultra-supercritical boilers. These boilers
burn pulverised coal to generate high-temperature, high-pressure steam. The
temperatures and pressures involved necessitate the use of advanced materials
which are more expensive than conventional steels. Moreover, a coal-fired
power station will also require a range of emission control facilities to clean
the exhaust gases from the boiler before they can be released into the atmosphere. Electricity from such a plant is generated using a large steam turbine
generator. Typical capacities for new plants range from 700 to 1500 MW and
efficiencies from 41% to 51%.4 The largest plants can have generating capacities in excess of 4000 MW]]>
			</paragraph>
			<paragraph>
				<![CDATA[Many of the components of a coal-fired station will be fabricated and
erected at the site. This will incur high labour costs so the cost of labour in the
region where the plant is built will be an important consideration. Further, the
use of large quantities of materials such as concrete and steel will make costs
vulnerable to swings in global commodity prices. Construction times are long,
typically 3e4 years. No major coal-fired power stations have yet been built
with carbon capture and storage (CCS) but that may become a requirement in
future years. If so, it will increase the capital cost further as well as reducing
plant efficiency.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.6 shows the estimated overnight capital cost of coal-fired power
generation in the United States between 2000 and 2019 based on figures from
the US EIA. The estimated cost of advanced coal technology was $1021 in
2000 and then rose slowly during the first 5 years of the decade, reaching
$1167/kW in 2005. From that point, prices rose sharply over the next 5 years
so that in 2010 they were estimated to be $2625/kW, an increase of 125%
compared to 5 years earlier. Much of this rise can be attributed to a massive
increase in global commodity prices in the middle and late part of the first
decade of the century, a trend that was brought to an abrupt halt by the global
financial crisis of 2008. In consequence, the estimated cost of new coal-fired
capacity stabilised again, rising to $2726/kW in 2014 e a rise of only 4%
in 4 years.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The US EIA capital cost series for unabated coal plants stopped between
2014 and 2018, only resuming in 2019 when the estimated cost for a new plant
was put at $3361/kW e a 23% increase over the price in 2014. In 2012, the US
EIA also began to produce estimates for a coal plant with CCS.5 That year, the
cost was put at $4662/kW, a 73% premium over the cost of a similar plant
without CCS. By 2019, the cost estimate for a coal plant with CCS was $5851/
kW, or 60% higher than the unabated plant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The IEA, Nuclear Energy Agency (NEA) and Organisation for Economic
Co-operation and Development (OECD) have together produced a series of
reports charting the projected cost of generating electricity across the globe.
The most recent of these five yearly reports, Projected Costs of Generating
Electricity, 2015 Edition (IEA report) was published in 2015. The capital costs
recorded in this report are all for advanced coal-fired stations. For OECD
countries, the cost for a new plant ranged from $1218/kW in South Korea to
$3067/kW in Portugal. Of the two non-OECD nations reporting costs, the
overnight cost in China was put at $813/kW, while in South Africa, it was
$2222/kW. Most of the OECD costs quoted are in line with those in the United
States between 2010 and 2015 in Table 5.6. However, both South Korea and
especially China are outliers. It is possible that cost in both nations is a
reflection of government subsidies supporting local industries and thereby
depressing commodity and labour prices.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gasefired power stations come in a variety of configurations, but the
most common types likely to be built in the third decade of the 21st century are
based around gas turbines. The most efficient and the most effective of these
stations are combined cycle gas turbine plants which can be used for base load
generation or, increasingly, for grid support services. Simpler open cycle gas
turbines are also widely used, in this case usually for peak power generation at
times of high demand.]]>
			</paragraph>
			<paragraph>
				<![CDATA[All gas turbines are technically advanced prime movers and they are
manufactured by a limited number of companies globally. There is often
intense competition between these companies and that has led to exceptionally
low prices for some turbines. Combined cycle gas turbines are manufactured in
factories and then shipped to the power plant site, essentially complete.
Installation, while still complex, is therefore much simpler than for a coal plant
and on-site preparation will be simpler too. The time from order to completion
is around 3 years. Simple cycle gas turbines have much smaller generating
capacities than combined cycle units and easier to install. Lead times for plants
based on these are typically 2 years.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gasefired power plants are cleaner than coal-fired plants and the
emission control systems they require are simpler. No large gas turbine plant
has been built with CCS, but this will likely become a requirement during the
third decade of the century.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.7 contains figures for the estimated overnight capital cost of three
types of gas turbine power plant in the United States between 2000 and 2019.
The combined cycle power plant in the table is an advanced, high-efficiency
flexible unit. For this type of station, the cost in 2000 was estimated to be
$533/kW. The cost both rose and fell between 2000 and 2005 when it was
$532/kW, virtually identical to the price 5 years earlier. The global boom in the
last years of the first decade then pushed up the cost to $917/kW by 2010, a
rise of 72% in 5 years. Prices subsequently stabilised again so that although
there were small rises in the middle of the next decade, the estimated cost in
2019 was $954/kW, only 4% higher than in 2010.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Open cycle gas turbines are generally somewhat cheaper than the combined cycle variants and this can be seen in Table 5.7. Here the estimated cost
of an open cycle turbine in 2000 was $440/kW, but by 2005, this had fallen to
$367/kW. Prices jumped somewhat in the last part of that decade, reaching
$626/kW in 2010, and then stabilised again. In 2019, the estimated cost for an
advanced open cycle gas turbine power plant was $710/kW, 13% higher than a
decade earlier.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In 2003, the US EIA started to estimate the annual capital cost for an
advanced combined cycle power plant with CCS. That year it put the cost at
$969/kW, 70% higher than for the plant without CCS. The estimated cost of
this configuration rose towards the end of the decade and then stabilised during
the succeeding decade. In 2019, the estimated cost was $2470/kW, nearly
160% more costly than the plant without CCS.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Figures for the capital cost of combined cycle power plants from the 2015
IEA report put the price for plants built within OECD nations at between $845/
kW in South Korea and $1289/kW in New Zealand with many plants being
built at a cost similar to those in Table 5.7 for the period between 2010 and
2015.6 The efficiencies of the cited plants ranged from 45% to 60%. However,
the cost in China, the only non-OECD nation included, was $627/kW. As with
coal-fired plants, South Korea and China are again outliers. For open cycle gas
turbine power stations, the cost variation within the OECD was from $500/kW
in the United Kingdom to $933/kW in Belgium.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power stations are probably the most technically complex power plants
used to generate electricity commercially in the 21st century. These plants
have a radioactive core where the nuclear reactions that generate heat occur.
This core is surrounded by a containment vessel that will retain any radioactive
material in case of a plant failure. The heat generated in the core is used to make
steam to drive a large steam generator. The use of radioactive materials requires
that nuclear power plants have extensive safety features to prevent failure or in
the case of failure to shut the plant down without harm. These safety features are
expensive to implement.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Construction of a nuclear power station will involve massive civil engineering work at the site and large volumes of concrete and steels will be
consumed. Some of the components will be manufactured off-site, but much of
the fabrication will take place at the site of the station. These plants have the
longest lead times of any power project, typically 6 years; some advanced
stations have taken much longer to complete.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power plants are usually large. Minimum unit capacity is rarely
below 1000 MW and a unit size of 1600 MW is not uncommon. When two or
more units are constructed at a single site, capacity can easily exceed
3000 MW. This size, combined with the technical complexity required to
maintain safety, makes the construction of a nuclear power station the most
capital intensive of any type of power project.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.8 contains figures for the estimated annual overnight capital cost of
a new advanced nuclear power station in the United States. In 2000, the
estimated cost was $1729/kW. Between 2000 and 2005, estimated costs moved
both up and down so that in 2005 the cost was put at $1744/kW, a less than 1%
increase in 5 years. Prices rose steeply between 2005 and 2010, again
reflecting increasing commodity prices, so that by 2010 the estimated cost was
$4567/kW, an increase of 162%. Costs stabilised during the succeeding 5 years
before starting to increase again in the middle of the second decade, and by
2019, the estimated cost was $6016/kW, a rise of 32% in 9 years. This makes
nuclear power the most expensive of all the established generating
technologies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Global costs are generally similar to those observed in the United States,
with some notable exceptions. The IEA report found the cost of nuclear
plants in OECD nations built or started between 2010 and 2015 to be between
$2021/kW in South Korea and $6216/kW in Hungary. The cost of a nuclear
plant in China was between $1807/kW and $2615/kW. Again the two Asian
nations are outliers. Cost variations will often depend on the differences in
the regulatory regimes covering nuclear power in the different nations or
regions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Fuel cells are technically advanced devices, similar in concept to a battery, that
have the ability to generate electricity from gaseous hydrogen. There are
several different types of fuel cell. Most are available in modular form, with
sizes ranging from a few kW for domestic use to hundreds of kilowatts for
commercial use. Large power plants are constructed by installing multiple
units.]]>
			</paragraph>
			<paragraph>
				<![CDATA[All fuel cells are fabricated in factories and then shipped to the site where
they are to be used. This makes installation relatively simple, particularly for
smaller units. They have low environmental impact and this allows them to be
used in urban settings if necessary. Some of the smaller units are particularly
suited to combined heat and power applications. Efficiency is high when a fuel
cell uses hydrogen as fuel, but this fuel has limited availability so many use
natural gas instead. This is converted into hydrogen before use. Many fuel cell
types require catalysts such as palladium metal to operate and this makes them
expensive. While economies of scale should help reduce prices over the long
term, production volumes are not sufficiently high for this to have much effect
yet. In consequence, fuel cells are still relatively expensive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.9 shows the estimated US EIA annual overnight capital cost for
fuel cells in the United States between 2000 and 2019. At the start of this
series, the cost of a fuel cell power plant was $1767/kW. However, by 2005,
this had risen to $3787/kW, and in 2010, it reached $5846/kW, 231% higher
than 10 years earlier. Much of this steep rise in costs can be put down to
commodity prices, particularly for the rare metals needed as catalysts. Prices
stabilised somewhat after 2010, and by 2019, the prices was $6671/kW, an
increase of only 14% in 9 years.
The capital cost of a fuel cell is high compared to many competing technologies. However, the devices are often deployed at the distribution system
level where wholesale electricity prices are higher.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydropower is the oldest and best established renewable technology. The
technology is usually subdivided into two types, small hydropower and large
hydropower. Large hydropower plants, often with dams and reservoirs, are
massive civil engineering projects with concomitant price tags. These schemes
can provide water for irrigation and for drinking as well as electric power and
may often be subsidised by governments or local authorities. Small hydro-power schemes are less disruptive and cheaper to build, but their unit capital
cost is often higher than for a large project. Costs are very site-specific too, so
schemes of identical generating capacity may have markedly different capital
costs.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A major hydropower scheme will often have a generating capacity of
several hundred megawatts, with the largest reaching over 1000 MW. Smaller
projects can range in size from 1 or 2 megawatts to 20 e30 MW. Much smaller
schemes are possible too, and generators with capacities of 1 kW or less are
common.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The major electromechanical component for a hydropower plant is the
turbine generator. These devices are fabricated in factories and then shipped to
the project site. However, most of the construction work involved in a hydropower scheme must take place at the site. In consequence, the capital cost
will depend critically on local labour costs. Where a large dam is constructed,
there may also be sensitivity to the price of concrete.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.10 shows figures from the United States for the estimated overnight
capital cost of a medium- or large-scale hydropower scheme from 2006 to
2019. The US EIA did not produce estimates for this technology between 2000
and 2005. The capital cost of a hydropower plant was put at $1364/kW in
2006. By 2010, this had risen to $2019/kW. There was a gradual increase in the
estimated cost during the succeeding 9 years, and by 2019, the capital cost was
estimated to be $2752/kW.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The 2015 IEA report contains overnight costs for hydropower plants across
the globe. For large hydropower schemes, the costs reported varied between
$2933/kW in Portugal for a 144 MW plant and $1567/kW for an 800 MW
project in Brazil. Smaller projects were relatively more expensive. A 12 MW
scheme in Japan costs $8687/kW, while a 10 MW project in Switzerland had a
capital cost of $6848/kW. In China, meanwhile, the cost of a 13,050 MW
hydropower scheme was $598/kW.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biomass power plants are combustion plants that use similar technology to that
in a coal-fired power station. Most biomass plants are much smaller than coal-fired plants and the technology is often rather more primitive. A biomass plant
will typically have a capacity of 30 MW or less, although larger plants have
been constructed. This makes them relatively less efficient that most coal
plants do. The combustion process will generate some nitrogen oxides and
carbon dioxide and the former may need control. It is unlikely that carbon
capture will be added to a conventional small biomass combustion plant but it
has been mooted for one or two large coal-fired power stations that have been
converted to biomass.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first column of Table 5.11 shows US EIA figures for the annual
overnight capital cost of a biomass combustion plant in the United States
between 2000 and 2019. The capital cost in 2000 was estimated to be $1464/
kW, and like capital costs for other technologies in this chapter, the price rose
slowly during the first half of the succeeding decade before accelerating so that
by 2010 it had reached $3395/kW, a rise of 132% in 10 years. Costs stabilised
during the next decade, before eventually rising slowly so that in 2019 the
estimated cost was $4080/kW.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Figures from the IEA report for biomass combustion plants between 2010
and 2015 indicate that a 100 MW plant in the United States costs $4587/kW.
Meanwhile, the addition of co-firing of wood pellets to a coal-fired power
plant in the Netherlands cost $587/kW, while in the United Kingdom, it cost
$719/kW.
In addition to biomass combustion technology, Table 5.11 also contains
figures for the cost of biomass electricity production from landfill gas. This is
based on gas engines, reciprocating engines that burn the biogas generated
from landfill sites. This represents a small part of the biomass energy sector,
but one that is easy to exploit.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The US EIA figures for landfill gas-based generation in the second column
of Table 5.11 contain unexplained anomalies. The cost of the technology in
2000 was estimated to be $1308/kW and this rose slowly until 2006 and then
more rapidly so that in 2009 the estimated cost was $2377/kW. However, in
2010, this rose, without explanation, to $7698/kW, or three times the cost
1 year earlier. Prices remained in this region until 2018 when the cost was
estimated to be $8313/kW, but in 2019, the price fell, again without explanation, to $1557/kW.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Other overnight costs from the IEA report can add some perspective to
these US EIA figures. In Italy, the cost of a plant burning biogas in a gas
engine was $8667/kW, close to the EIA figures between 2010 and 2015.
However, the cost for the same technology in Spain was between $1852/kW
and $3773/kW. The limited sample of projects makes it difficult to draw and
clear conclusions for this technology.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Geothermal power plants use heat extracted from underground reservoirs of
hot brine to drive a heat engine and generate electric power. Normally, the
brine is pumped to the surface and then used to produce low-pressure steam
that will drive a steam turbine. The low pressure and relatively low temperature of the steam produced results in a geothermal energy conversion process
that has low efficiency. Even so this can be economically viable because the
energy source is free.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.12 shows estimates for the overnight capital cost of geothermal
power technology in the United States between 1999 and 2019. For the first
decade of the century, these figures are rather erratic, starting at $1626/kW in
1999 and rising to $2960/kW before falling back to $1057/kW in 2007.
However, from 2010 to 2019, the series shows more stable prices, from $2364/
kW in 2010 to $2680/kW in 2019.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The IEA report contains overnight capital costs for geothermal technology
in other parts of the world between 2010 and 2015 which show a wide spread.
For example in Turkey a 24 MW plant cost $1493/kW, while in New Zealand
the cost for a 250 MW power plant was $3331/kW. In the United States, during
this period, an 80 MW plant cost $6291/kW and a 90 MW plant cost $5992/
kW. The capital cost of geothermal technology is extremely site-specific and
this will account for the wide variation in the cost of actual plants reported by
the IEA.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Wind power is one of the important new renewable technologies. Total global
installed capacity has grown rapidly during the 21st century and the
technology has become more robust and reliable. Modern wind turbines are
manufactured by a limited number of key companies around the world which
sell their products internationally. This has created global competition which
has helped force prices down. However, there are also regional factors
which affect prices.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are two principal branches of the wind turbine market, onshore wind
turbines and offshore wind turbines. The use of onshore wind turbines is
widespread, but the main offshore market is in Europe, and particularly in the
United Kingdom. The size of turbines for offshore use continues to increase,
helping reduce the unit capital cost, but installing very large turbines onshore
is often limited by the ability to transport the units to the site. Most wind
turbines are installed as part of a wind farm comprising an array of turbines
operating as a single power station. All modern wind turbines are manufactured in a factory and then assembled on site. The main components are
usually made from steel or special composite materials, so manufacturing
costs will depend at least partly on commodity prices. However, standardisation and the use of multiple units in wind farms helps keep the cost of
installation to a minimum.]]>
			</paragraph>
			<paragraph>
				<![CDATA[nstallation to a minimum.
Table 5.13 shows US EIA figures for the annual overnight capital cost of
wind power in the United States. The estimated cost for onshore wind turbines
in 2000 (column one of the table) was $919/kW, but by 2010, this had risen to
$2251/kW, an increase of 145% in 10 years. This mirrors the rise in cost for
other technologies during the first decade of the century. Prices stabilised
around 2010 and then the effect of global competition began to take hold so
that during the second decade of the century prices began to drop. By 2019, the
estimated cost for onshore wind power in the United States was $1319/kW, a
fall of just over 40% in 9 years. This fall in capital cost has made onshore wind
power one of the most competitive sources of electric power in the United
States.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The second column of Table 5.13 contains a similar series of costs for
offshore wind power in the United States between 2007 and 2019. Offshore
wind is more expensive than onshore wind because of the much higher cost of
installation. In 2007, the cost of offshore wind was estimated to be $2547/kW
or 90% higher than for an onshore wind installation. Prices rose to a peak of
$4648/kW in 2016, after which the estimates become somewhat erratic. The
estimated price in 2019 was $4356/kW.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Figures from the IEA report for the early part of the second decade of the
21st century suggest that the price of onshore wind in nations of the OECD up
to 2015 was between $1571/kW in the United States and $2999/kW in Japan.
The equivalent capital cost in China was between $1200/kW and $1400/kW.
Offshore wind capital costs varied between $3703/kW in the United Kingdom
and $5413 in France. However, given the trend shown in Table 5.13, it is likely
that the cost for both onshore and offshore wind would be significantly lower
in 2019.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar power is the second important new renewable generating technology
alongside wind power, and like wind power, the installed capacity based on
solar power has soared during the 21st century. The principal type of
solar power generation is based on solar (PV) cells, solid state devices that
convert sunlight directly into electricity. The solar converters are manufactured in high volumes using solid state fabrication techniques, and this has
allowed enormous economies of scale to be achieved. While the manufacture
of these devices is limited to a number of high-technology companies, the
market for solar cells has become global and this has also resulted in
extremely competitive pricing, particularly during the second decade of the
century.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 5.14 contains, in the second column, UA EIA figures for the estimated annual overnight cost for utility-scale solar PV power plants in the
United States between 2000 and 2019. The cost in 2000 was $3681/kW. This
climbed to $5879/kW in 2009 before economies of scale and the effect on
global markets of cheaper solar cells manufactured in China started to
depress prices everywhere. From 2010 to 2019, the cost of solar cells has
fallen year upon year, and in 2019, the estimated capital cost in the United
States was $1331/kW. This dramatic fall e the cost in 2010 was over three
times higher than in 2019 e has had an equally dramatic effect on the market
for solar cells.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The table does not show the cost of rooftop solar cells which are generally
more expensive than the large arrays for utility-scale generation. However,
Table 5.4 shows both and indicates a cost that is two to three times higher than
the utility array cost. Meanwhile, global overnight costs published in the IEA
report indicate that costs in other parts of the world were much lower than in
the United States. In fact, the figures for solar cells in Table 5.14 look overly
pessimistic.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is a second type of solar power plant called a solar thermal plant.
These use the sun as a heat source to drive a heat engine and so share similarities with more conventional power plants. Solar thermal plants rely on
arrays of mirrors which are costly to manufacture and to install. The energy
conversion technology can be novel and expensive too, though some use
conventional steam cycles. The global capacity of this type of generation is
relatively small and costs remain high compared with many other
technologies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The first column of Table 5.14 shows estimated annual capital cost for solar
thermal technology. The cost in 2000 was estimated to be $2394/kW, but this
had risen to $4333/kW by 2010. Costs stabilised somewhat during the succeeding decade, peaking at $4715/kW in 2013 before falling back slightly, but
in 2019, the US EIA revised its estimate, pushing the cost up sharply, to
$7191/kW.]]>
			</paragraph>
			<paragraph>
				<![CDATA[These figures may be compared with figures from the IEA report for
overnight costs from the early part of the second decade of the century,
between 2010 and 2015. For two US solar thermal plants, the installation cost
was $3561/kW and $4901/kW, while a plant constructed in Spain during this
period had an installed cost of $8142/kW. Solar thermal technology should
benefit from economies of scale, but it is unlikely that it will ever compete
directly on cost with solar PV technology.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The capital cost of a power station, discussed in the previous chapter, provides
a metric against which to compare different types of power generation technology. Of itself it provides a limited indication of the economic value of a
particular power plant, but it is one of the key inputs into another metric called
the levelized cost of electricity (LCOE). The LCOE is the output from an
economic model that attempts to assess the present, and future, cost of electricity from a power station that has not yet been constructed. The model is an
example of what is known as a lifecycle analysis, in this case of the financial
inputs and outputs from a power station from its construction and through its
life until final decommissioning.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The LCOE model adds together the cost of building a power plant, the cost
of financing any loans required to facilitate its construction, the cost of
operating the power plant over its full lifetime and the cost of any fuel it uses
during that period, the cost of maintenance and repair and finally the cost of
decommissioning and dismantling the power plant once its usefulness is
exhausted. This total lifetime cost of the power station is then compared with
the total amount of electricity that the power plant will actually produce over
the same lifetime to arrive at a cost for each unit of power the plant generates.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This type of calculation could be carried out once the power plant has been
decommissioned but that would be of limited use. What is required is a cost
today for a power plant that has not yet been built. To achieve this, the model
makes a variety of assumptions about future conditions in order to arrive at a
meaningful future cost of electricity. These assumptions may lead to inaccuracy and bias, but the model does provide a metric for comparing the cost of
electricity from different technologies. The LCOE for different technologies
will be explored more fully in the final chapter of the book, but the concept is
examined here because it is an important example of lifecycle analysis as
applied to power plants.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The LCOE model is an economic model dealing specifically with financial
costs. There are a range of other lifecycle analyses that can be applied to power
station technologies in order to illuminate other aspects of their performance.
For example, by using a similar approach, lifecycle analysis can show how
much carbon dioxide each type of power station will emit over its lifetime for
each unit of electricity it generates while similar analyses can be applied to
other pollutant emissions such as sulphur dioxide, nitrogen oxides or particulates. A lifecycle emission analysis will add together the amount of pollutant
produced during the manufacture of the materials needed to build the power
plant, emissions during its construction, further emissions that take place while
it is operating and emissions that are consequent on its decommissioning a
dismantling. These emissions are then added together and divided by the total
number of units of electricity the plant produces over its lifetime to provide an
emission rate per unit of electricity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another group of lifecycle analyses considers the energy performance of
power plants. Energy conversion efficiency is a simple metric of this type;
another type of energy analysis examines the time it takes for a power plant to
generate the amount of energy used in its construction. This is closely related
to the energy payback ratio for a power plant, how much energy a plant gives
back compared with the energy invested in it. Similar to this is the energy
return on investment (EROI), which can take a broader view. EROI has
recently been used as part of a wider analysis of the value of energy to society
and how its value influences the ability of societies to support various
activities.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Many of these metrics change over time. For example, the EROI will
change for fossil fuel power plants as it becomes more difficult, and more
expensive in energy terms, to recover the fossil fuel from the earth. Or, the
greenhouse gas emissions per unit of power from a solar cell will fall when the
energy used to make the actual solar cell material is produced from renewable
sources rather than fossil fuels.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Lifecycle analyses can provide valuable insights into the relative performance
of different technologies, particularly in areas other than economic, but their
results depend critically on how the limits or boundary conditions of each
analysis is defined. Boundary conditions are necessary because a power station
(or any other object of lifecycle analysis) is part of an interacting system we
call the world. So, if we want to calculate how much energy a power station
uses over its lifetime, we will include any energy consumed transporting fuel
to the plant. But should we consider it appropriate to include the energy each
power station worker consumes when preparing breakfast each morning?]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are clear arguments against extending the boundaries of a study this
far. Breakfast is a part of everyday life whether one is working or not. In other
cases, the choice of boundary might be more arbitrary. For example, when
calculating the energy inputs for an analysis of nuclear power, the boundary
conditions may include the cost of refining uranium from uranium ore but omit
the energy cost of mining and transporting the ore, and it almost certainly will
not include societal cost associated with the labour required to mine and
transport the ore. Or in the case of wind power the boundary conditions might
include the energy cost of building the foundations for the plant, but the
analysis may choose to exclude the energy required to manufacture the concrete that was used in the foundation construction.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One of the most contentious boundary discussions centres around externalities such as the cost to society of pollutant emissions from power plants.
Putting a cost on these factors is difficult, and fossil fuel power plants have,
traditionally, not taken them into account when lifecycle economic analyses
are carried out. The LCOE lifecycle analysis discussed above rarely includes
the environmental cost of different generating technologies. However carbon
emission costs may be included. Taxing carbon emissions is a way of
attempting to include some of these external factors associated with global
warning but even this is unlikely to account for the full cost. This is discussed
more fully in Chapter 8.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As will be clear by now, by careful choice of boundary conditions it is
possible to skew the results of a lifecycle analysis to present one technology in
a better light than others. But even when the drawing of the boundaries is
attempted objectively, there can still be a systematic bias. It is important,
therefore, to ensure that boundary conditions are clearly and transparently
defined.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One of the most important areas in which lifecycle analysis can be applied to
power generation is energy balance e how much energy comes out of a power
station compared with the amount that goes in. Perhaps the simplest of this
type of calculation is the energy conversion efficiency of the power station.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Energy conversion efficiency is an important thermal power plant parameter because it provides a measure of the efficiency of the energy conversion
process, how much of the input energy e the fuel e is actually converted into
useful electricity. So, for example, an older coal-fired power station with a
subcritical boiler might achieve an energy conversion efficiency of 38% while
a modern supercritical plant can push this to 45%. More advanced ultra-supercritical boilers can raise this to 48% and may in future achieve 50% or
higher. Clearly the higher the efficiency, the more electricity is produced from
each unit of coal, and this will improve both the economic and the environmental performance of the plant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The boundaries of the analysis that produces this type of figure are very
tightly drawn, and they are normally restricted to the actual power station
itself. The main aim is to show how much electricity is generated for each unit
of energy that enters the plant. To achieve this, the amount of fuel or energy
entering the plant is compared with the amount of electrical energy that leaves.
Table 6.1 shows the energy conversion efficiencies for all the most common generating technologies in use today. Looking at thermal technologi
first, the most efficient is the natural gasefired combined cycle power plant
with an efficiency of 60%. Diesel engines are relatively efficient too at 50%,
while modern coal-fired power plants typically achieve 45%. Nuclear power is
a thermal technology too. Large nuclear stations operate at around 33% energy
conversion efficiency.]]>
			</paragraph>
			<paragraph>
				<![CDATA[For renewable technologies, the comparison is generally less favourable. A
modern wind farm might be able to reach 40% efficiency in converting wind
energy into electricity while a solar photovoltaic plant will probably only
reach 20%. Solar thermal technology is even less efficient; 15% efficiency is a
typical figure for a plant of this type.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is a snag here, however. When a wind turbine captures energy from
the wind and converts it into electricity, the energy it does not capture continues on its way, as wind. However, when a coal-fired power plant burns a
tonne of coal, converting 45% of the energy it contains into electricity, the
55% of the energy not converted into electricity has still been consumed and
will emerge from the plant as waste heat. This heat is released into the
environment and may be considered an additional emission. So to compare the
energy conversion efficiency of a combined cycle plant to that of a wind
turbine is not to compare like with like. And while the energy conversion
efficiency figures for the different technologies are useful from an electro-mechanical point of view, comparing them in this way across technologies is
not particularly illuminating in most contexts.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A more useful metric can be obtained if the amount of energy contained within
the fuel or energy source is removed from the analysis. The cost in energy
terms of mining and transporting a fuel and the cost of cleaning up any
]]>
			</paragraph>
			<paragraph>
				<![CDATA[emissions are included, but the quantity of energy that enters the power station,
be that contained in sunlight, wind or natural gas, is not included. What is of
interest here is the amount of energy emerging from the power plant as
electricity compared to the amount of energy spent building and operating the
plant (including any fuel mining and transportation energy costs).]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this case, the ‘birth to death’ analysis of the energy performance of a
power plant tries to show how much energy is actually provided to society by
the operation of the power station, distilling the performance into a single
parameter. This may be expressed as an energy payback time, the amount of
time it takes for the power plant to generate energy equivalent to that required
to built and operate it or by factoring in the lifetime of the power plant it can
be expressed as an energy payback ratio showing the amount of energy the
power plant delivers over the course of its lifetime for each unit of energy
spent.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 6.2 shows a set of energy payback ratios published by the Canadian
utility Hydro Quebec in 2004. Bearing in mind that Hydro Quebec operates a
large fleet of hydropower stations, it may not be surprising to find that hydropower is by some margin the best performing technology in the table, with
an energy payback ratio of 170e180. These figures may be somewhat optimistic, but hydropower plants do have extremely long lifetimes if well
designed and most analyses place them among the best performing technologies on this metric. The next highest performing technology in the table is
wind power with a payback ratio of up to 34 for onshore wind but as low
as 18 for offshore wind. More recent analysis shows offshore wind in a much
more favourable light. However, the payback ratio for solar photovoltaic
generation is only 3e6. This is a reflection of the high energy cost for the
manufacture of the single crystal silicon needed for high-efficiency solar cells.
Nuclear power has an energy payback ratio of 14e16 based on this analysis.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Combustion technologies fare less well. Biomass combustion has an energy payback ratio of 3e27, but this higher figure is probably for the combustion of waste biomass fuel. A pulverised coal-fired power plant and a
natural gasefired power plant both show similar ratios (2.5e5.1 and 2.5e5.0,
respectively) while a pulverised coal plant with the addition of carbon capture
and storage pushes this down to between 1.6 and 3.3. Bearing in mind that a
ratio of less than one indicates that a power plant consumes more energy over
its lifetime than it actually generates, the latter figures are not encouraging for
this technology.
These figures present renewable technologies in a favourable light
compared with fossil fuel technologies. Other analyses, discussed below, can
present then in a different light.]]>
			</paragraph>
			<paragraph>
				<![CDATA[EROI has, in the last decade, become a more favoured way of presenting this
type of lifecycle energy analysis. While the result is essentially the same as the
energy payback ratio discussed above, recent studies have looked in much
more detail at the boundary conditions used to calculate the EROI, at the way
in which values vary from country to country and at the way in which EROIs
vary over time. In addition to this, a broader analysis of EROI and energy costs
has been applied to societies in general to provide insight into how the cost of
energy affects the economic wealth and well-being of a society or nation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One way of looking at the economic implications of energy is to calculate
the annual monetary cost of energy to a nation e the amount it costs to buy all
the energy needed in a year e and then compare this with the annual national
gross domestic product.3 According to published observations, when the value
of this ratio is around 5%, which is considered low, then an economy can grow
strongly and it can afford to invest in such areas as scientific research and can
support artistic endeavours. However, a rise to between 10% and 14% is
usually linked to an economic recession. Such rises are often found during
periods of energy price shock such as sudden rises in the cost of oil.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 6.3 presents figures for the EROI of a variety of common fossil fuels
and power generation technologies from a paper published in Energy Policy.4
These figures are based on a meta-analysis of figures published in a large
number of other studies. The figures for the fuels in the table do not necessarily
relate to electricity production. They simply compare the energy spent making
the energy source available to society with the energy that the source actually
provides e the heat energy released when natural gas is burned, for example.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The mean EROI for the production of oil and gas in Table 6.3 is 20:1. A
broader examination of the EROI for these fuels over time suggests that the
value is gradually falling.5 For example, the value in 1995 was estimated to be
around 30:1, but by 2006, it had fallen to 18:1. This appears to be related to the
increased cost of both prospecting for oil and gas and recovering it. Alternative
sources of oil and gas such as tar sands and oil shale tend to show much lower
EROI values, with a typical value of 4:1 as shown in Table 6.3.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The EROI for the other major fossil fuel, coal, is relatively high at 46:1. Its
value appears to remain relatively high over time, although there are changes
observable. For example, one study found that the EROI for coal in the United
States was around 80:1 in the 1950s but fell to 30:1 in the mid-1980s and then
rose again to around 80:1 by the 1980s.7 The variation in the EROI is likely to
be related to the ease of recovery of coal, including extended use of surface
mining which is less costly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A figure for the EROI for ethanol is also included in Table 6.3. The mean
figure quoted is 5:1, but many of the studies from which this mean was
calculated were lower than this, with one or two high outliers pushing the
mean up, the authors note. This may suggest the true EROI for the biomass
fuel is actually lower than 5:1.
The EROI figures for combustion fuels in Table 6.3 cannot all be compared
directly with those in Table 6.2 for combustion technologies because the latter
refers exclusively to use of fuels for electricity generation while Table 6.3
figures include other uses such as for transportation fuel. However, for other
types of generation, the figures are comparable because the product in each
case is electricity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Most nuclear power plants produce electricity alone. For nuclear power, the
mean EROI from the studies examined was 14:1. This matches very closely to
the figure quoted in Table 6.2. However, other studies (see below) arrive at
much higher figures for nuclear technology.
The technology with the highest EROI, by some margin in Table 6.3, is
hydropower. The figure quoted in the table is 84:1, which, while significantly
lower than the figure in Table 6.2, is close to twice the value for the nearest
rival. However, the EROI for hydropower varies from site to site and values
might be much lower.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Of the other renewables, wind power has an EROI of 18:1 to 20:1. For solar
photovoltaic, the EROI quoted in Table 6.3 is 10:1. This latter ratio is much
higher than in Table 6.2. However the higher figure is reliant on a weighting
for the high-quality power generated by solar cells. Unweighted estimates are
often closer to the 3:1 ratio from the earlier table. Finally, geothermal energy
has an EROI of 9:1.
Table 6.4 provides figures from another study, published in Energy,8 that
looked exclusively at the energy return for power generation. In this case, the
figure for natural gas generation is 28.0, higher than the estimate for oil and
gas as a fuel, while that for coal is 20.0, much lower than in the previous table.
However, brown coal is given a much higher figure, 31.0, probably consistent
with the fact that brown coal is usually surface mined and the power plant is
often adjacent to the mine, both of which lower the energy associated with
recovery and transportation. Other figures in the table are broadly consistent
with those in the earlier table with the exception of nuclear power which is
assigned an EROI of 75.0 here. The exceptional figure appears to be a result of
a variety of changes to the assumptions for nuclear power including increases
in the plant lifetime and in the number of hours of operation each year. Such
variations from study to study again emphasise that it is important to examine
the assumptions and boundary conditions when looking at lifecycle analyses of
this sort.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The study from which the figures in Table 6.4 were taken also attempted to
calculate a value for the EROI of the main renewable technologies when account is taken of their variability and the need, therefore, for some form of
backup to support them. This served to reduce the overall EROI of all these
technologies. For example, the value for solar thermal technology from the
table, 21.0, was reduced to 9.6 using this assumption. For wind energy, the
reduced EROI was only 4.0 while for hydropower it was reduced from 50.0 to
35.0 and for solar photovoltaic generation it was reduced to 2.3.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 6.4 also contains estimates for the energy payback time for different
generating technologies. These vary from 9 days for a natural gas-fired
combined cycle plant to 6 years for a solar cell. As with all the figures quoted
in Tables 6.2, 6.3 and 6.4, the values depend critically on the assumptions and
boundary conditions. Change one or both of these and the results will change.
So while there is some consistency between the three tables, there is also a
great deal of variability.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another important type of lifecycle analysis shows how much of a pollutant
gas is emitted by a power plant over its lifetime for each unit of electricity the
plant produces. The boundary conditions for this type of study tend to be more
clearly defined than for EROI studies and the results are generally more
consistent. However, the values will change, particularly for combustion
technologies when efficiencies improve.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This type of study is important because it will provide a direct comparison
of the environmental impact of power generation technologies on the environment. There are several important pollutants that can be studied in the way.
They include carbon dioxide, sulphur dioxide, nitrogen oxides and
particulates.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 6.5 presents figures for the lifecycle greenhouse gas emissions from
the main generation technologies. The figures in the table are presented as
grams of carbon dioxide equivalent for each kilowatt hour of electricity
generated (gCO2 equivalent/kWh). This unit is used because there are a range
of gases that can cause greenhouse warming of the atmosphere in addition to
carbon dioxide. The most important of these is methane, which is released in
smaller quantities, but is more potent than carbon dioxide. Methane dissipates
from the atmosphere more quickly than carbon dioxide too, but over a 20 year
time frame it is 84 times more potent. This reduces to 28 times over 100 years.
Other potent greenhouse gases include nitrous oxide, which is around 260
times more potent over both time frames.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As might be expected, the largest greenhouse gas emissions are from coal-fired power plants. Based on studies by the US National Renewable Energy
Laboratory (NREL) shown in the table, coal plants emit 980 gCO2 equivalent/
kWh. Coal is primarily composed of carbon so its main combustion product is
carbon dioxide.
An open cycle gas turbine burning natural gas typically emits 670 gCO2
equivalent/kWh while a natural gasefired combined cycle plant emits 450
gCO2 equivalent/kWh. The lower figure for the combined cycle plant is due to
its much higher energy conversion efficiency. Natural gas plants produce less
carbon dioxide than coal plants, but the production of natural gas leads to
significant releases of methane into the atmosphere, which affect the overall
environmental performance.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Biomass power plants produce typically 45 gCO2 equivalent/kWh when
burning a wood fuel that is grown specially for the purpose. A plant of this
type will produce large quantities of carbon dioxide during combustion, but
when the wood crop is regrown, it absorbs some of that carbon dioxide again,
leading to the low emission performance.
Nuclear power, which is often considered a low emission technology, emits
17 gCO2 equivalent/kWh based on NREL analysis. Geothermal power, also
based on thermal technology, emits between 11 gCO2 equivalent/kWh and 47
gCO2 equivalent/kWh depending on the type of underground reservoir. Some
of the latter may release carbon dioxide or methane during geothermal
operation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Of the principal renewable technologies, the best performing is hydropower with 4 gCO2 equivalent/kWh. Some hydro schemes can produce
methane from the organic material that is submerged when a reservoir is
created, but if this happens, it will normally subside over time. Wind power
typically produces 11 gCO2 equivalent/kWh and ocean power around 16 gCO2
equivalent/kWh. Solar thermal power plants generate slightly more, 23 gCO2
equivalent/kWh, but the emission for solar photovoltaic plants is higher at 45
gCO2 equivalent/kWh. This is a result of the electrical energy needed to
manufacture the pure silicon which often comes from fossil fuel power
stations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The other pollutants that are released into the atmosphere during electricity
production are also the result of fossil fuel combustion. The emission of
sulphur dioxide is linked to coal combustion and depends on the amount of
sulphur in the coal. Today, this is normally removed from flue gases before
they are released into the atmosphere. However, emissions can be as high as
1360 kg/GWh, even with flue gas desulphurisation.12 Plants that burn heavy
fuel oil may also release large amounts of sulphur dioxide.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal, natural gas-fired and diesel power plants will all produce nitrogen
oxides. There are various technologies that can be used to control these
emissions and more advanced plants tend to produce less of each. For
example, a coal plant in the United Kingdom without technology to remove
nitrogen oxides released 2200 kg/GWh, while a plant with removal technology
released 700 kg/GWh according to the World Energy Council figures.13 All
these plants also produce carbon-based particulates. These are usually
removed from coal plants together with dust in the flue gases, but emissions
can be up to 9800 kg/GWh. Plants burning other fuels generally produce much
lower levels of particulates, but emissions from small diesel engine plants
without control can be high.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The equivalent lifecycle emissions of these pollutants from nuclear and
renewable technologies generally depend on how various materials used in
their construction were made. With the exception of biomass combustion,
these technologies do not produce any emissions during electricity generation.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Structural issues
One of the biggest issues facing the electricity sector today is to be able to
increase the use of renewable energy while maintaining electricity system
stability. All the main renewable technologies, hydropower, wind power and
solar power, are to differing degrees, intermittent and unpredictable. Both of
these characteristics lead to uncertainty regarding the amount of power
available. However a power system must always have sufficient reliable power
available to meet demand.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are a number of ways that power system stability can be managed in
this situation. The simplest, but potentially the most expensive is to maintain
sufficient fast-acting standby capacity that can cut in when demand exceeds
supply. The only way this can be achieved today at the scale required is with
fossil fuel power plants, generally based on gas turbines burning natural gas.
This is the traditional method of maintaining supply and demand in balance, as
was discussed in Chapter 3.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another way of overcoming the problem is to install more renewable
generating capacity than is required to meet demand and then invest heavily in
energy storage. By this means, surplus power from renewable generators can
be stored so that it will be available to use if the renewable generation falls
below demand. Most energy storage technologies are relatively fast-acting.
The weakness of this approach becomes apparent when renewable generation
fails over a long period of time and the stored energy is all used. In addition,
energy storage systems are relatively expensive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another useful tool for managing supply and demand is demand management. If some grid demand can be shut down when supply levels are
marginal, then the need for additional generation can be avoided. However
demand management must be capable of being controlled at the grid system
operator level with some electricity users agreeing to reduce their demand on
request. There are a range of smart grid technologies that can facilitate this
type of functionality including the use of smart meters that are in direct
communication with the system operator and can receive instructions to
control various loads as demand levels vary.]]>
			</paragraph>
			<paragraph>
				<![CDATA[System stability is not only a matter of the quantity of power available to
meet demand but also of the quality of that power. There are certain types of
grid event that can cause fluctuations in the grid frequency and grid voltage. In
traditional grid systems, the size and speed of these fluctuations will be
flattened by the inertia of the generating units connected to the system. Large
steam turbines and hydro turbines carry massive rotational inertia and this
helps smooth fluctuations. However, much of the new renewable generation is
based on either wind or solar power. Wind turbines have relatively small
inertia, and solar cells are essentially isolated from the grid because they
generate DC power which is converted electronically to alternating current,
and these electronic systems do not have any physical momentum or inertia.]]>
			</paragraph>
			<paragraph>
				<![CDATA[All of these means of managing system stability have financial implications. The cost of supplying them is often invisible, but it is nevertheless
important. Some power generating technologies rely on these services to a
larger extent than others, and this has an impact on the cost of the electricity
they produce. This chapter outlines some of the technologies available and the
services they can provide.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Peak power
The level of demand on a power system fluctuates continually and this fluctuation must be balanced on the supply side. Small changes in demand can be
met relatively easily by allowing the output of base load stations to fluctuate,
or by modulating the output from renewable plants. There are also much larger
daily fluctuations that cannot be managed in this way, such as the daytime
peak in demand typical on most power systems.]]>
			</paragraph>
			<paragraph>
				<![CDATA[These larger fluctuations must be met by having additional capacity
available that can be brought on line quickly as demand rises. The traditional
approach to this has been to provide the grid with quick responding open cycle
gas turbine units that can ramp up and down rapidly as required. These units
are relatively inefficient, and they are costly to operate but provide the required
additional capacity to maintain grid stability. More recently, larger combined
cycle gas turbine plants have also been adapted so that they can respond to
demand changes. These plants are less agile than open cycle gas turbines but
can provide a good degree of flexibility. Coal-fired and nuclear power plants
are also being adapted for flexible operation but they are much less agile than
combined cycle plants.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another approach to managing peak power is to use energy storage plants.
When nuclear power was introduced in the 1950 and 1960s, these plants were
designed to operate continuously at full output, night and day. However, as
nuclear units became larger, this base load operation could lead to surplus
power on the grid when demand was low, particularly at night. To manage this,
many systems with large nuclear plants also invested in pumped storage hydropower plants that could absorb and store the excess output from these large
generation units. This power was then available to help manage the daytime
peak in demand. In addition, these hydropower storage plants were extremely
fast-acting, so they could increase grid stability. However, storage plants of
this type are costly to build and today they are often difficult to finance.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are many other types of energy storage technology, and in the last
two decades, several of these technologies have been introduced at different
grid levels to help manage demand. For example, flywheel storage systems are
being used to provide extremely fast-acting backup for companies with
mission-critical computer systems in case of failure of the grid supply. But, as
with pumped storage hydropower, cost is an issue. Tariffs that pay operators of
plants that can offer such grid support can help make energy storage
economically viable.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Intermittency
The converse of demand fluctuations is fluctuations in the supply side of the
system. In traditional grid systems, this was usually the result of the failure of
a generating unit e an outage e or a fault in the grid. However, in modern
systems that absorb large volumes of renewable power, a variation in supply is
a constant feature that is made more significant by the need to dispatch
renewable energy first when it is available.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Managing fluctuations in the supply side of the grid relies on the same
types of technology that are used to manage peaks in demand, but the
unpredictability of these supply side fluctuations makes it much more difficult
to manage them. Energy storage systems offer one of the best solutions since
they can cut in quickly when needed. Another valuable asset for managing
renewable unpredictability is traditional hydropower capacity based on dam
and reservoir plants. Like storage plants, these power stations can be brought
on line and throttled back rapidly and they offer a cheap way of managing
fluctuating output from other plants. However, this capacity is only available
when there is water in the reservoir of the hydropower plant, so the capacity
must be managed carefully.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The alternative is to use conventional fossil fuel plants, typically open
cycle and combined cycle gas turbines. Modern examples of the latter are
often designed to be maintained in a parked state so that they can be started
rapidly and many have been modified to provide rapid ramping of output, both
up and down. One drawback of this mode of operation is a fall in efficiency
and an increase is emissions of all types. And for the future, zero emissions
can only be achieved by capturing carbon dioxide emitted from such plants
and sequestering it so that it cannot reach the atmosphere.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Smart grid technologies bring the functionality of computers and communication systems to bear on the power supply network. There are a number of
ways in which smart grid technologies can be used to help maintain system
stability. One of the most important is through automated demand management. By setting up rapid communication systems that link the grid operator to
the consumers, signals can be passed from one to the other to control the loads
that are connected at different times.]]>
			</paragraph>
			<paragraph>
				<![CDATA[At an industrial level, certain large consumers will agree to a reduced tariff
with the condition that if demand begins to outstrip supply they will temporarily shut down some or all of their operations in order to reduce the system
load. This type of demand management can be fully automated with shutdown
activated at different trigger points. However, in many cases, the companies
involved will require notice of an impending cutoff in order to shut down in a
controlled manner.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Similar control can, in principle, be introduced at the domestic level too,
allowing a considerable level of control over demand. One way that this can be
implemented is through smart domestic meters which communicate directly
with the system controller. These meters, in turn, have control of certain types
of domestic appliance such as washing machines or air conditioning systems.
With two-way communication, the system control centre can ask these appliances to shut down temporarily when demand is high. Equally importantly
though, some devices may also be asked to switch on when there is a surplus
of supply over demand, as for example when there is excess wind power on a
system.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Smart technologies can also help with the supply side of the grid. One
simple application is the use of advanced weather forecasting to predict the
output of wind and solar plants attached to the grid. With these forecasts in
place, the system controller can schedule additional capacity to come online
when output from these renewable plants is expected to be low and plan to take
these additional units off line when output rises again. By providing a longer
time frame for this type of scheduling to take place, forecasting can reduce the
cost attached to adding or removing capacity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another useful tool is the virtual power plant. With sufficient communication capacity, it is possible, for example, to aggregate wind plants from
different geographical locations and operate them as if they were a single
power plant. While wind is unpredictable, the level of unpredictability is much
smaller when averaged over a large geographic area. A virtual wind farm of
this type will therefore provide a much more reliable output than a single wind
farm at a specific location. Moreover, different types of power plant can be
aggregated: wind and solar plants are often complementary over a long time
scale, for example, with wind output greater in the winter while solar output is
higher in summer. The more reliable the output from these virtual plants, the
more valuable the power and the higher the price that can be charged for
delivery of the power.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Spinning reserve and system stability
The frequency and voltage of a modern grid must be controlled within narrow
bands in order for the system to be stable and for consumers to be able to
operate their own loads reliably. In a traditional grid system, a major
component of the stability was contributed by the large turbine generators
connected to the system with their large rotational inertias. These massive
devices help smoothen short-term fluctuations.]]>
			</paragraph>
			<paragraph>
				<![CDATA[With the growth of renewable capacity based on wind and solar power, a
significant part of this grid inertia has been lost because these renewable units
do not present the grid with the same level of rotational inertia. It is possible to
compensate for some of this loss with fast-acting energy storage systems,
particularly superconducting magnetic energy storage. However, this is
insufficient to replace all that has been lost.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The alternative is to pay power plants with large turbine generators to stay
on line with their turbines spinning so that they can continue to provide system
inertia as needed. This ‘spinning reserve’ is usually maintained in order to
provide the grid operator with fast access to additional power but it can also be
used simply to provide grid stability.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The largest source of this type of spinning reserve is from nuclear and
fossil fuel plants, particularly coal-fired stations and combined cycle plants. It
can also be provided by large hydropower turbines, either in conventional
hydropower plants or in pumped storage hydropower stations. Spinning
reserve is one of a range of ancillary services that power plant operators can
provide to the grid and they offer new ways of gaining revenue from plants that
might otherwise be made redundant by the advance of renewable power.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydrogen is a fuel that could potentially provide a solution to several of the
problems of grid stability outlined above. The reason for this is that hydrogen
can be produced directly from electricity. In particular, in the context of a
world that is struggling with global warming, it can be made using surplus
renewable electricity from wind and solar power plants. With sufficient
renewable capacity to operate in this way, any excess power above that
demanded by the grid can be used to make hydrogen which is then stored.
When demand rises, the power directed to hydrogen production is reduced,
helping keep the grid in balance.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Once it has been made and stored, the hydrogen can be used to generate
more power. It can be burned in a gas turbine or in a conventional boiler and it
can be used as fuel in a fuel cell. Because the hydrogen produced in this way is
‘clean’ it can be produced and then burnt without adding to the greenhouse gas
load in the atmosphere. Produced in large enough quantities, it can potentially
be transported in pipelines to other locations and it can also be used as vehicle
fuel.
There are a number of barriers to achieve this type of hydrogen economy,
but the technologies needed to achieve it are beginning to be put in place.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are a number of factors that can distort the economics of electricity
production. These factors come in various forms. The last chapter discussed
the issue of grid stability and the need for support for intermittent renewable
technologies. This support comes at a cost to the system. The additional cost
must be added to the nominal cost of production from the renewable generators to provide a true cost of production from these sources. Otherwise the
cost of renewable electricity will appear artificially low in comparison to that
from other sources.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another issue that many observers would consider the source of major
distortion is that of externalities. In economics, an externality is an effect that
an action by one party has on a second party, the cost of which to the second
party is not priced into the cost of the activity to the first. Externalities can be
both positive and negative. Within the power sector, the most significant set of
externalities relate to the harm caused by the combustion of fossil fuels
through damage to the environment, through damage to human health and
through the effects of global warming. For the most part, the cost of this
damage is not priced into the cost of electricity generation from coal, gas or
oil. In other words, the polluter causes the damage but the cost of reparation
falls elsewhere.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One way of attempting to price in the environmental effect of different
types of power generation is by imposing a penalty on those activities that can
cause such damage. Measures include the cap-and-trade systems that have
been introduced to control carbon dioxide emissions in the European Union
(EU) and in other parts of the world, and in the application elsewhere of
carbon taxes which tax each unit of carbon dioxide released.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There is a third, very straightforward, means of distorting the cost of energy, through subsidies which directly affect the price consumers pay for their
energy. Subsidies take a variety of forms, from preferential grants or discounts
for certain types of fuel to direct government subsidies that reduce the cost of
electricity to particular consumers. The largest part of global energy subsidies
is directed towards fossil fuels today, but renewable generation technologies
also benefit from support in many countries too. Subsidies lead to an artificial
price for energy that is usually below the actual cost of production.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Subsidies are widespread within the energy industry, and subsidising the cost
of fuel or energy is often used as a political tool. In some developing nations
with large fossil fuel reserves, the cost of gasoline and electricity to consumers
may be subsidised in order to encourage and maintain support for the regime
in power. In others, there may be social tariff subsidies that are targetted to
help the poorer sections of the population. In developed countries, this type of
tariff support is less usual, but subsidies to support particular industries such as
coal, nuclear power or renewable energy are not uncommon, and these may,
again, have a political incentive. Globally, the larger part of these subsidies
relates to fossil fuels.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In the past two decades, as the threat of global warming has become more
and more acute, the issue of subsidies for fossil fuels has been highlighted by
many of the world’s international agencies such as the International Energy
Agency (IEA), the World Bank (WB) and the International Monetary Fund
(IMF).
Subsidies which reduce the cost of fuel to a consumer lead to increased
consumption of the fuel, and in the case of fossil fuels, this leads to larger
carbon dioxide emissions as well as more emissions of a range of other
harmful pollutants. International agencies such as the IEA, the WB and the
IMF have therefore campaigned to encourage countries to reduce these subsidies. This can be politically difficult. If a nation’s consumers have become
used to low energy prices, then raising the cost will often lead to political
unrest. The result is that, often, subsidies are reintroduced. However, the
historically low energy prices in 2019 followed by the global pandemic of
2020 and the resulting depressed energy costs may offer a unique opportunity
to start to eliminate them.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 8.1 presents figures for cumulative global fossil fuel subsidies in
2019, as collated by the IEA. These figures are estimated by using what is
known as the price-gap methodology, which compares the average price paid
by consumers in each nation to a reference price for the full cost of supply. The
difference between the two is then the subsidy level.]]>
			</paragraph>
			<paragraph>
				<![CDATA[According to the IEA analysis, the largest fossil fuel subsidies were
directed towards oil with global subsidies of US$150bn in 2019. This was
followed by electricity where the subsidy level was US$115bn, natural gas
with subsidies of US$50bn and coal with US$2.5bn. Taken together, these
figures show total fossil fuel subsidies in 2019 of US$317.5bn. According to
the IEA, this represented a fall in overall annual subsidies from 2018 of
US$120bn, putting the total annual subsidy in 2018 at US$437.5bn. The fall
noted by the IEA from 2018 to 2019 is mostly accounted for by the large drop
in oil subsidies as a result of the lower cost of oil products during 2019.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Meanwhile, the International Renewable Energy Agency (IRENA), which
takes the IEA figures as a starting point but applies a broader approach,
calculated that total fossil fuel subsidies in 2017 were US$447bn. Again the
subsidies for oil-based products were the highest at US$220bn, followed by
electricity at US$128bn.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 8.2 presents a breakdown of subsidies by nation for the 25 countries
with the largest level of subsidies, based again on IEA analysis. By far the
highest level of subsidies was found in Iran which underwrote national fossil
fuel purchases with US$86bn in 2019. Of these subsidies, the largest part,
US$51.7bn, was accounted for by electricity subsidies with the remainder
relatively evenly split between gas and oil. The nation with the next highest
subsidy level was China with US$30.5bn in subsidies, US$18.1bn for oil and
US$12.4bn for electricity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[fossil fueleproducing nations. For example, Saudi Arabia had the third highest
level of subsidies in 2019 at US$28.7bn, mostly supporting the use of oil,
while the fourth nation, Russia, provided subsidies of US$24.1bn equally
divided between electricity and natural gas. India (US$21.9bn), Indonesia
(US$19.2bn), Egypt (US$16.4bn), Venezuela (US$12.7bn) and Iraq
(US$7.4bn) made up the rest of the top 10 by subsidy level. Of all the nations
in the table, only one, Kazakhstan, provides a significant level of subsidy for
coal. However some of the countries listed, particularly China, will be
providing a subsidy for coal combustion through their subsidising of the cost
of electricity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The IEA has also estimated the total level of subsidies for each nation as a
proportion of its gross domestic product (GDP). On this basis the outliers are
Iran where subsidies are 18.8% of GDP, Venezuela with 16.7% of GDP
devoted to fossil fuel subsidies and Libya, also with 16.7%. Egypt, Algeria,
Uzbekistan and Turkmenistan all spend over 5% of GDP on fuel subsidies.
The level in China is 0.2% of GDP.]]>
			</paragraph>
			<paragraph>
				<![CDATA[hose most widely used in support of fossil fuel consumption. There has been
an effort in many parts of the developed world to encourage the use of
renewable electricity generation by the application of incentives of different
sorts. For example, the US government has supported wind and solar power
through a system of tax credits. Elsewhere, there are feed-in tariffs that allow
renewable generators to sell power to the grid at a predetermined price and
contracts for difference which make up the payment to a renewable generator
so that it achieves a fixed tariff level.]]>
			</paragraph>
			<paragraph>
				<![CDATA[IRENA has recently attempted to estimate in a systematic way the total
level of subsidies for renewable generation1 and how these might evolve over
the next 30 years based on a scenario in which the world remains on track to
meet the United Nations Framework Convention on Climate Change
(UNFCCC) Paris Agreement climate change target of keeping global warming
to 2_C or less.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 8.3 shows the results of the IRENA analysis, with figures for
renewable power generation subsidies for 2017 for several nations and regions
as well as an estimate of the global total. The region with the largest level of
support for renewable generation is the EU which has set ambitious targets for
renewable generation and emissions reduction. IRENA estimates that the EU
subsidised renewable generation with around US$78.4bn through feed-in
tariffs, green certificates, investment grants and some other tariff support
schemes. Within the EU, Germany offered the highest level of support, followed by Italy, the United Kingdom and Spain.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Japan provided US$18.8bn in renewable support in 2017 as the nation
seeks to reduce its reliance on imported of fossil fuels; the country has no
significant indigenous fossil fuel resources to call upon and imports all its
fossil fuels. Japanese support primarily takes the form of feed-in tariffs
designed to encourage solar photovoltaic deployment. China also provided
around US$15.2bn in 2017 through feed-in tariffs for wind and solar power
aimed at accelerating deployment. Meanwhile, the United States subsidised
renewable generation with about US$8.9bn through tax credits and investment
tax breaks. IRENA found support of around US$2.9bn in India, while for the
rest of the world, the cumulative total was US$3.8bn. Based on these figures,
the organisation estimated that the global total subsidy for renewable power
generation in 2017 was US$128bn.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Broken down by technology, solar photovoltaic received the largest share
of subsidies in 2017 around 48% of the total or US$60.8bn. Onshore wind
received US$31.6bn or 25% of the total, biomass US$21.9bn (17%) and
offshore wind US$6.6bn (5%).
Table 8.4 shows how IRENA has predicted that subsidy regimes will
evolve over the coming 30 years, with estimates for subsidy levels in 2030 and
2050 to complement the figures for 2017. The predicted trend is for fossil fuel
subsidies to fall sharply between 2017 and 2030 and then continue to tail off
towards 2050, while renewable subsidies rise slowly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Fossil fuel subsidies of US$447bn in 2017 are, on this basis, predicted to
fall to US$165bn by 2030, a fall of 63%, and then to US$139bn in 2050 or
69% lower than in 2017. Over the same period, renewable subsidies (unlike
Table 8.3, the figures in this table include transportation biofuels as well as
subsidies for power generation) rise from US$166bn in 2017 to US$192bn in
2030 and US$209bn in 2050. This last figure is still less than half the level of
fossil fuel subsidies in 2017.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 8.4 also includes estimates of the level of nuclear power generation
subsidies. The organisation suggests that these are much more obscure than the
subsidies for fossil fuels and renewables and therefore more difficult to pin
down accurately. Subsidies include government support for nuclear waste
management and in the case of a new nuclear plant at Hinkley Point in the
United Kingdom, significant tariff support of perhaps as much as US$1.4bn/
year.2 However, the global total for nuclear power is relatively small compared
to either fossil fuels or renewables. IRENA estimates that the total was
US$21bn in 2017. This is predicted to rise to US$27bn in 2030 but then fall
back to US$21bn in 2050.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The analyses above indicate the level of direct subsidies for both fossil fuels
and renewable generating technologies. However this does not account for all
the subsidies because it fails to cost negative externalities, particularly those
associated with fossil fuels. These subsidies are unpriced and therefore
invisible to consumers except in so far as they have an impact on their lives.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The consumption of fossil fuels leads to an environmental impact that can
be both serious and wide ranging. The most significant today is global
warming caused by the release of carbon dioxide into the atmosphere when
fossil fuels are consumed.3 There are also much more localised effects on
health and hence mortality caused by the emission of harmful pollutants such
as particulates from diesel engines, nitrogen oxides from engines and from
power plants and sulphur dioxide from coal-fired power stations. These latter
can also release heavy metals into the atmosphere.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Estimating the cost of these negative effects on the environment is
extremely difficult. The IMF has recently published a study in which is used
the concept of an economically efficient fossil fuel price to estimate the level
of hidden subsidy associated with fossil fuel consumption.4 Broadly, an
economically efficient price is a price at which the cost of production plus the
cost of mitigating any negative effects of the use of the fuel is balanced by the
cost the consumer pays for the fuel. The gap between the economically
efficient price and the actual price paid by consumers (in this case significantly
lower than the economic cost) is the external, unaccounted cost.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Using this approach, the IMF concluded that global energy subsidies were
US$4.7 trillion in 2015 and rose to US$5.2 trillion in 2017. Other sources have
arrived at different figures. For example, IRENA put the cost of unpriced
externalities for fossil fuels at US$3.1 trillion in 2017, lower by US$2.1 trillion
but still a staggeringly large figure. The IMF figure indicates that total fossil
fuel subsidies are 30 times those received by renewable generating technologies, while the IRENA figure puts the multiple at 19]]>
			</paragraph>
			<paragraph>
				<![CDATA[This underpricing of the cost of fossil fuel combustion can be broken down
into components. According to the IMF, the largest component is underpricing
for local air pollution, which accounted for 48% of the estimate in 2015.
Global warming accounted for a further 24% and underpricing of the environmental cost of road fuels accounted for a further 15%. This insight suggests
that while global warming cannot be controlled by one country alone, local
taxes or incentives to reduce pollution levels can have a significant effect
locally on air quality and hence human health.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As far as the individual fuels are concerned, coal carries the largest
unpriced subsidy, 44% of the total, followed by petroleum with 41% and
natural gas with 10% while direct electricity output carries a further 4%.
However, given that most of the world’s coal is burnt to generate electricity
and a large quantity of natural gas is used for power generation too, the
subsidy contribution to power generation based on fossil fuels will be more
significant than this.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The IMF analysis also breaks down the total subsidy including externalities
by country. The top 10 nations by subsidy level are shown in Table 8.5 for
2015. Head of the league, by a large amount, is China. The country, which
generates around two-thirds of its electricity from coal, provided an estimated
overall subsidy including external costs of US$1432bn. The level of subsidy
was more than twice that of the next nation, the United States, with US$649bn.
As with China, the United States has relied heavily on coal plants for electricity generation. However, the amount of coal in the US power generation
mix has been falling since the beginning of the second decade of the century
and this decline continues as natural gas and renewable sources become more
important.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Other nations with large total subsidy levels in Table 8.5 include Russia
with subsidies of US$551bn, India with US$209bn, Japan with US$177bn and
Saudi Arabia with US$117bn. The top 10 nations are rounded off with Iran
(US4111bn), Indonesia (US$97bn), Germany (US$72bn) and Turkey
(US$64bn).]]>
			</paragraph>
			<paragraph>
				<![CDATA[Taxes
One way that the imbalance in pricing resulting from externalities can be
corrected is by the use of direct taxes or other financial tools. A number of
such tools have been developed. These can be applied to correct any of the
types of imbalance discussed above but today they are most often used to
tackle the issue of global warming and the release of carbon dioxide and other
greenhouse gases into the atmosphere as a result of fossil fuel combustion.
These measures are variously known as carbon taxes or carbon pricing
mechanisms.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The WB has identified five types of initiatives that attempt to put a price on
greenhouse gas emissions.5 The first and simplest is a carbon tax. This sets a
fixed price that must be paid for the release of 1 tonne of carbon dioxide
equivalent (tCO2equivalent) into the atmosphere. The tax might be framed as
an excise duty or a levy but it is essentially a carbon tax.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The second financial instrument is an emissions trading system (ETS).
These come in two forms. A cap-and-trade system sets an annual cap on the
total quantity of greenhouse gas that can be released into the atmosphere
within a particular political region and then issues a set number of emissions
certificates equivalent to this amount each year, each one permitting to the
release of 1 tCO2equivalent of these gases. These certificates may be allotted
to particular emitters or they may be auctioned. Any facility emitting green-house gases must then submit a certificate for each tCO2equivalent that it
releases. However organisations with certificates can also sell them on the ETS
market where other organisations might choose to buy them in order to increase the quantity they can emit. An alternative system, called baseline-and-credit, sets a baseline level of emissions that each regulated emitter can
release. If the emitter does not reach this baseline, it can be issued with certificates for the difference which it can sell on the ETS market. Emitters which
seek to exceed their baseline must buy and surrender certificates for all their
excess emissions.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A third type of financial instrument is called a carbon crediting mechanism.
This allows a jurisdiction to issue certificates for projects that actively reduce
emissions beyond any regulated level. This might involve financing the
planting of trees or supporting renewable development in another country. Any
certificates issued in this way can then be traded for financial gain. Finally,
result-based climate finance is a system whereby targets are set, and upon
reaching the target, a recipient will receive funds from the finance provider.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The most important global carbon emission schemes in operation are based
either on carbon taxes or on ETS systems. The WB analysis revealed that there
were 61 carbon pricing initiatives in place or scheduled for implementation at
the end of 2019. These included 31 ETS systems and 30 carbon taxes. It
estimated that these covered around 22% of global greenhouse gas emissions.
Together they allowed governments to raise over US$45bn in 2019.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One of the largest schemes is the European Union’s ETS scheme, which
was launched in 2005. The scheme is based on the European Union Allowance
or EUA. One EUA allows the holder the right to emit 1 tonne of carbon dioxide or the equivalent for N2O and perfluorocarbons. The market price of a
trading certificate has varied markedly since the scheme was launched. The
prices of units traded in the early years, when units were allocated, were
relatively high but they stabilised towards the end of the first decade of the
century. Table 8.6 shows the price over the 10 years between January 2010 and
January 2020. In 2010, the cost was 12.79 euro, and in 2011, it had risen to
14.28 euro. The cost fell back after that, to 6.35 euro in 2012 and as low as 4.59 euro in
2014. The cost remained relatively low until 2018 when prices started to rise
sharply so that at the beginning of 2019 the market price was 22.24 euro and in
2020 it reached 24.26 euro.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In this, and other similar schemes, the unit price is intended to act as a
market signal that will influence emitters. If the cost to emit greenhouse gases
is too high, then consumers will switch to alternative energy sources, but if it is
too low, then it may be cheaper to pay the price and continue to emit. The IEA
believes that a carbon price of US$75e100/tCO2equivalent is needed to
maintain a trajectory that would keep the world in line with the commitments
in the Paris Agreement on climate change. On the other hand, the IMF has
suggested that some countries can meet their Paris Agreement targets with a
price in 2030 of US$35.6 Others would need at least double of that to achieve
the same end.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In addition to this and other ETS schemes, there are a number of countries
that have introduced carbon taxes. The highest carbon tax is found in Sweden
where the unit cost is US$119/tCO2equivalent. Switzerland and Liechtenstein
have taxes of US$99/tCO2equivalent. However almost half of the emissions
that are subject to pricing have a cost of less than US$10/tCO2equivalent. This
will not provide a strong enough signal to encourage the change of behaviour
needed to combat global warming.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Similar incentives and systems have been used to control other harmful
emissions from power plants. For example, the United States introduced a cap-and-trade system in the 1990s to control the emission of sulphur dioxide from
coal-fired power plants. A scheme to limit nitrogen oxide emissions was
introduced in 2003.
As already noted, rather than distorting the cost of electricity, all these
schemes attempt to price in the external cost of the targetted emissions.
However, their imposition is generally a political decision.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost of electricity is important at every level of global electricity systems.
Consumers will normally seek the lowest cost supplier, commensurate with
their needs. In a deregulated electricity, market retail and wholesale suppliers
will seek to buy their electricity from the lowest cost generator. Meanwhile,
generators will seek the lowest cost source for the power they intend to
generate. And while a range of factors will come to bear of the final cost of a
unit of electricity, the overriding factor will generally be the cost of electricity
from a power station.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The electricity industry is a conservative industry based on production
units e power plants e that are expected to last for years if not decades. Even
so, there is constant change. Demand rises as societies advance. Those societies begin to demand cleaner power. And power plants get old and must
eventually be retired. Each of these factors can lead to the need for new
generating capacity to be built.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The typical lifetime of many types of generating plant is around 30 years,
so even without considering other factors, capacity must be replaced or old
plants rehabilitated over this timescale. And each time a new power plant is
required, a decision must be taken about the type of power plant to be built.
Sometimes this decision will be based on local factors such as a particular
resource that can usefully be exploited, but in virtually all cases, one of the key
considerations will be the type of power plant that can provide the cheapest
electricity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost of electricity from a power plant of any type depends on a range
of factors. First, there is the cost of building the power station and buying all
the components needed for its construction. In addition, most large power
projects today are financed using loans, so there will be a cost associated with
paying back the loan, with interest. Then there is the cost of operating and
maintaining the plant over its lifetime, including fuel costs. Finally, the overall
equation should include the cost of decommissioning the power station once it
is removed from service.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It would be possible to add up all these cost elements to provide a total cost
of building and running the power station over its lifetime, including the cost
of decommissioning, and then divide this total by the total number of units of
electricity that the power station produced over its lifetime. The result would
be the real lifetime cost of electricity from the plant. Unfortunately such
calculation could only be completed once the power station was no longer in
service. From a practical point of view, this would not be of much use. The
point in time at which the cost-of-electricity calculation of this type is most
needed is before the power station is built. This is when a decision is made to
build a particular type of power plant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Levelized Cost of energy model
In order to get around this problem, economists have devised a model that
provides an estimate of the lifetime cost of electricity before the station is
built. Of course, because the plant does not yet exist, the model requires a large
number of assumptions to be made. In order to make this model as useful as
possible, all future costs are converted to the equivalent cost today by using a
parameter known as the discount rate. The discount rate is almost the same as
the interest rate and relates to the way in which the value of one unit of
currency falls (most usually, but it could rise) in the future. This allows, for
example, the maintenance cost of a steam turbine 20 years into the future to be
converted into an equivalent cost today. The discount rate can be applied to the
cost of electricity from each type of plant in 20 years’ time too.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The economic model is called the levelized cost of electricity (LCOE)
model, a type of lifecycle analysis that was discussed briefly in Chapter 6. The
model contains a lot of assumptions and flaws, but it is the most commonly used
method available for estimating the cost of electricity from a new power plant.
The LCOE model treats all types of power station equally. However, there
are significant differences between technologies that must also be taken into
account. A major division is between the main combustion technologies and
nuclear power, on the one hand e technologies where a fuel is required to
maintain output e and, on the other hand, the primary renewable technologies,
wind, solar and hydropower, which exploit a free renewable resource.]]>
			</paragraph>
			<paragraph>
				<![CDATA[One difference between the groups relates specifically to the energy source.
Power stations based on combustion technology such as coal-fired and gas-fired power plants, as well as nuclear plants, all require a fuel to be supplied continuously in order to operate and this fuel comes at a regular cost.
This type of plant can often be relatively cheap to build, so the cost of the fuel
will play a large role in determining the cost of electricity from each station.
Plants based on the main renewable technologies may be more expensive to
build, but there are no fuel charges and so the cost of electricity from these
plants is very closely related to the relative cost of building each type of plant.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Another important difference between the two types of technology is what
is known as dispatchability. Power plants based on combustion and nuclear
technologies can be controlled to deliver power as needed; they are considered
dispatchable. The renewable technologies are intermittent and unpredictable
and they cannot be relied upon to provide power when required; these are
usually considered non-dispatchable and this affects the market value of the
electricity they produce.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In order to take account of this difference, the US Energy Information
Administration (US EIA), which assembles LCOE tables for each type of
technology for its Annual Energy Outlook each year, has in recent years begun
to add a new type of economic modelling called the levelized avoided cost of
electricity (LACE). This tries to take account of the different levels of grid
service each type of technology provides. Combining the LCOE and the LACE
can provide a fuller picture of the benefits of each technology.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The LCOE estimates the future cost of electricity from a particular technology. Looked at another way, this figure may also be considered to be the
cost that must be charged for electricity from the plant if it is to cover the cost
of its construction and operation. The LCAE, in contrast, is an estimate of the
revenue that the new plant would be able to expect from its electricity in the
prevailing market if it were constructed. This will depend on the competing
plants that are available to supply power at the same time. In principle,
therefore, if the LCOE is higher than the LACE, then the plant will be
operating at a loss while if the LACE is higher than the LCOE, the plant would
be economically attractive to build.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The LCOE remains the simplest and most used metric for comparing the
generating costs of different technologies and that is the model that is used for
the most part in what follows. When it is used, its limitations should be kept in
mind.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Electricity generating costs
The LCOE from different types of generating plant will vary from place to
place. Many of the figures quoted below are based on the US market but even
here there can be wide variations depending upon location. The main factors
that will lead to differences are the variable cost of fuels in different places and
to a smaller extent the variation in the cost of labour required to build and
operate power plants. The figures quoted below are mostly what are known as
overnight costs which do not include any financing costs. The latter must be
added to provide the real cost in any specific situation but for the purposes of
comparison the overnight cost is a more valuable metric.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The financial advisory and asset management company Lazard has been
publishing an annual levelized cost analysis of the power sector in the
United States for over a decade. Figures from the 2019 report are shown in
Table 9.1. The analysis includes calculation of the LCOE for new power
stations from a range of conventional and renewable technologies, in most
cases providing a range of final costs, the variation reflecting the difference
found in the cost of electricity from the same power source in different parts
of the United States. This may be considered a proxy for the variability that
is likely to be found in other parts of the world e although this assumption
should not be applied too loosely. The United States is, after all, still the
richest nation on earth.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Based on the Lazard analysis, the new fossil fuel fire plant with the lowest
cost electricity in 2019 was a natural gas-fired combined cycle plant with an
LCOE of US$44e68/MWh. The natural gas plant is not fitted with carbon
capture and storage (CCS). For a coal-fired power station with CCS, the
estimated LCOE is US$66e152/MWh, while the LCOE for a natural gas-fired
open cycle gas turbine was US$150e199/MWh. Power from the latter units is
expensive, but they are generally only used to provide power to the grid during
periods of peak demand. Meanwhile, the cost of power from a new nuclear
power plant was estimated to be US$118e192/MWh. On this basis, nuclear
power looks like an uneconomical choice for new technology compared to
either coal or natural gas.]]>
			</paragraph>
			<paragraph>
				<![CDATA[It is notable, however, based on the figures in Table 9.1, that the cheapest
new source of electricity in the United States is from none of these plants
because they are all undercut by the best renewable technologies. The most
competitive is onshore wind power which has an LCOE of US$28e54/MWh,
followed by utility-scale solar photovoltaic (solar PV) with a levelized cost of
US$36e44/MWh. This calculation does not take account of the cost of grid
support needed for renewable generation, but it does provide a useful guide to
the effectiveness of these technologies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Offshore wind is more expensive than onshore wind, with an estimated
LCOE of US$89/MWh. Two other solar PV costs are also included in the
table. The calculated LCOE for commercial and industrial rooftop solar PV
was US$75e154/MWh, significantly higher than for utility-scale solar PV.
And domestic rooftop solar PV with an LCOE of US$151e252/MWh was
more expensive still. However it is important to remember that rooftop solar
PV is a distributed technology that feeds power either directly to the consumer
or into a distribution grid. At this point in the electricity system, power is much
more expensive than at the transmission system level. So, even power as
expensive as this can still be competitive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[A figure for solar thermal generation, US$125e156/MWh, is also included
in Table 9.1. This refers to a plant with energy storage which is considerably
more dispatchable than solar PV. The cost of new geothermal power is also
included, with an estimated LCOE of US$69e112/MWh. There is limited
geothermal capacity available, anywhere in the world.
The US EIA produces an annual estimate of the LCOE from a range of
technologies as part of its Annual Energy Outlook. Figures from the most
recent report are shown in Table 9.2. The EIA takes a slightly different
approach to LCOE analysis by providing an estimate for the cost of electricity
from different technologies all entering service at the same future date. As
some plants, a nuclear power plant for example, may take 5 years from initial
order to entering service, the estimates in the first column of Table 9.2 are for
plants entering service in 2025.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As with the data in Table 9.1, the fossil fuel station that will provide the
lowest cost power in 2025 is a natural gas-fired combined cycle plant with an
LCOE of US$38/MWh. For an ultra-supercritical coal-fired power plant, this
time without CCS, the estimated cost of power is US$76/MWh. Both are
broadly in line with the earlier table. However, the cost of power from an open
cycle gas turbine, US$67/MWh, is notably lower than in the previous table as
is the cost of electricity from an advanced nuclear power plant at US$82/
MWh. These differences reflect differences in the assumptions made about
these two technologies in the two studies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Again the US EIA analysis reveals that the main renewable technologies
are extremely competitive based on the LCOE analysis. Solar PV is the
cheapest source in Table 9.2 with an estimated LCOE of US$36/MWh while
onshore wind is close behind at US$40/MWh. The electricity from a
geothermal power plant was estimated by the US EIA to be much cheaper than
in the previous table, at US$37/MWh. Offshore wind remains expensive with
an LCOE for plants entering service in 2025 of US$122/MWh. Table 9.2 also
contains an estimated LCOE for a hydropower plant entering service in 2025.
At US$53/MWh, this is relatively competitive. However, a new biomass power
plant, with an estimated LCOE of US$95/MWh in 2025, looks relatively
expensive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[In addition to the LCOE for power plants entering service in 2025, the US
EIA has also calculated the LCOE for the same type of plant entering service
in 2040. These figures are shown in the second column of Table 9.2. Most of
the changes from one column to the next are small. For a coal-fired power
plant, the LCOE drops from US$76/MWh in 2025 to US$72/MWh in 2040,
while for a natural gas-fired combined cycle plant, it rises from US$38/MWh
to US$43/MWh. The cost of power from an advanced nuclear plant falls from
US$82/MWh to US$74/MWh in 2040. The cost of biomass power stays the
same while that for hydropower increases very slightly.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost of electricity from the main renewable sources apart from hydropower falls between 2025 and 2040. For solar PV, the LCOE in 2025 of
US$36/MWh drops to US$30/MWh in 2040. The LCOE for onshore wind
falls from US$40/MWh to US$36/MWh and for offshore wind the cost falls
from US$112/MWh to US$86/MWh. These figures imply a further improvement in the competitiveness of renewable power compared to fossil fuel and
nuclear as the century progresses.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 9.3 compares the LCOE for the main generating technologies with
the LACE for plants entering service in 2025. Again these figures are from the
US EIA. As noted earlier, a higher LACE should imply that a plant based on
the technology will be economically viable while a higher LCOE suggests that
the full costs of operating the plant will not be met. However, none of the
estimates for the LACE in Table 9.3 are higher than the corresponding LCOE.
It is also notable that the LACE figures in the table occupy a narrow band of
costs, from US$32/MWh to US$42/MWh.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are, nevertheless some significant differences between the sets of
figures in the two columns of Table 9.3. For a natural gas-fired combined cycle
plant, the LCOE and LACE are US$38/MWh and US$37, respectively, while
for solar PV, the two figures are US$36/MWh and US$34/MWh; in both cases,
the difference is probably too small to be significant. For onshore wind, the
LCOE is US$40/MWh and the LACE US$32/MWh, putting new wind power
at a slightly greater disadvantage than either solar PV or a combined cycle
plant. However for coal-fired technology the LACE is US$36/MWh while that
of LCOE is US$76/MWh and for an advanced nuclear plant the LACE is
US$36/MWh while the LCOE is US$82/MWh. These figures suggest both
would operate at a significant loss based on this simple comparison. A similar
conclusion applies to biomass and offshore wind while hydropower with an
LOCE of US$53/MWh in 2025 and an LACE of US$35/MWh falls in the
middle ground between best and worst.]]>
			</paragraph>
			<paragraph>
				<![CDATA[There are no comprehensive sets of global estimates of the cost of electricity from the complete range of different generating technologies to set
against these US figures, but the International Renewable Energy Agency
(IRENA) has published costs for the main renewable generating technologies
for the decade from 2010 to 2019. These are shown in Table 9.4.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The figures in the last row of Table 9.4 can be compared with those in
Table 9.1 in order to gauge in some measure where the US market fits into the
global market. The global average LCOE for solar PV in 2019 from Table 9.4
is US$68/MWh, well above the US$36e44/MWh in the United States. The
average global cost of a new solar thermal power plant in 2019, US$182/
MWh, is also higher than the US estimate of US$126e156/MWh. On the other
hand, onshore wind with a global average LCOE of US$53/MWh does fall
within the range found in the United States of US$28e54/MWh. However, the
LCOE for offshore wind power, US$115/MWh, is higher than the estimate for
the United States of US$89/MWh.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Hydropower does not appear in the Lazard analysis and in the figures in
Table 9.1. Table 9.4 has a global average LCOE for this technology, US$47/
MWh. This is the cheapest source of power in the table, outperforming both
onshore wind and solar PV. The United States and the developed nations of
Europe have exploited their best hydropower sites so there is little scope for
large expansion here but there remain good resources to exploit in other parts
of the world.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The figures in Table 9.4 also show the cost trends for these technologies.
The trends for the individual technologies will be examined in more detail
below, but the salient feature of this table is the fall in the cost of electricity
from solar PV plants over the decade noted in the table. The LCOE in 2010
was over five times higher than in 2019. There is a fall in the LCOE for
onshore wind too, but it is much smaller. The following sections will look in
more detail at trends in the LCOE for the technologies in Table 9.1.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 9.5 shows figures for the LCOE of electricity from a new coal-fired
power plant with CCS based on analysis from Lazard between 2009 and
2019. In this case the table contains a single figure representing the average
price across the United States each year rather than a range as in Table 9.1 for
a single year, 2019.
The LCOE figures in the table, with two exceptions, show little variation.
In 2009, Lazard estimated an LCOE of US$123/MWh, and in 2011, it estimated a cost of US$95/MWh. Otherwise the figures all fall between US$102/
MWh and US$109/MWh. Costs rose slightly during the middle of the decade
shown and then fell back before rising again at the end of the decade.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal-fired power generation is becoming increasingly unpopular, particularly across the developed world, as a result of the high greenhouse gas
emissions from coal combustion. Its use in the United States has been
declining since the beginning of the second decade. However coal continues to
be popular in some developing nations, particularly in India and China. The
figures in Table 9.5 are for a plant with CCS and this puts the technology at a
disadvantage compared to a combined cycle plant without CCS or compared to
onshore wind and solar PV. It should be noted, however, that new coal-fired
power plants are not being built with CCS and so their costs are likely to be
lower.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The International Energy Agency (IEA), Nuclear Energy Agency (NEA)
and Organisation for Economic Co-operation and Development (OECD) have
produced a series of five yearly reports called Projected Cost of Generating
Electricity (IEA report). The eighth of these reports was published in 2015.
The report uses a different methodology to that for the reports so far cited so
the results are not directly comparable. In particular, the LCOE figures include
financing costs at a range of discount rates. The LCOE estimate also includes a
carbon cost of US$30/t carbon dioxide.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The IEA report estimated the LCOE for an advanced coal-fired plant in the
United States without CCS operating at a capacity factor of 85% as between
US$83/MWh (discount rate 3%) and US$104/MWh (discount rate 10%). In
China, the range was US$74/MWheUS$82/MWh while in Japan the LCOE
range was US$95/MWheUS$119/MWh.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Coal-fired power plants are generally designed for base load operation at
their maximum capacity factor. If these plants are operated at a lower capacity
factor efficiency is likely to fall and emissions may rise. In consequence the
LCOE rises for a capacity factor of 50% instead of 85%. In the United States,
for example, with a 50% capacity factor, the range of LCOEs was US$101/
MWheUS$137/MWh. The increase in the use of renewable generation means
that many former base load plants such as coal-fired power plants are being
required to operate at a lower capacity factor than in the past so the sensitivity
of the LCOE to capacity factor should be taken into account today.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Natural gasefired combined cycle plants
The LCOE for a US natural gas-fired combined cycle power plant between
2009 and 2019 is shown in Table 9.6. Unlike the figures for coal-fired plants,
above, there is (excepting the first figure1
) a monotonic decrease in the cost
over the period. In 2010, the estimated LCOE was US$96/MWh but by 2015
this had fallen to US$64/MWh and in 2019 the LCOE was US$56/MWh, a fall
of 42% over 9 years. The cost of electricity from a natural gas combined cycle
power plant is very sensitive to the cost of natural gas and in the United States,
during the decade from 2010, the cost of natural gas has been falling as a
consequence of the development of shale gas deposits in the country. A similar
LCOE price trend may not, therefore, be found in other parts of the world.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Figures from the IEA for the cost of electricity2 in the United States in its
2015 report provide an LCOE of US$61/MWh at a discount rate of 3% rising
to US$71/MWh at a discount rate of 10%. This is broadly in line with
Table 9.6 when the additional carbon cost used by the IEA is taken into
account. Elsewhere, the LCOE varies markedly. In the United Kingdom, for
example, the LCOE range was US$213/MWh (3%) to US$263/MWh (10%)
for the most expensive plant cited in the report. In China, the cost varied
between US$90/MWh and US$96/MWh.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Capacity factor also has an effect on the cost of power from a combined
cycle power plant. The US cost from a plant at a 3% discount rate and 85%
capacity factor of US$61/MWh rose to US$68/MWh at 50% capacity factor.
Modern combined cycle plants are relatively capable when it comes to
modulating their output but efficiency falls and emissions can rise as the capacity factor falls.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The open cycle gas turbine is an agile, fast-acting power unit that can be
brought into service rapidly and removed again swiftly. This has made it one
of the main sources of peak power on grids across the world. The units tend to
be relatively expensive and less efficient than the base load plants such as the
combined cycle plant, and this is reflected in the LCOE of power from these
power units.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 9.7 shows the LCOE trend for open cycle gas turbines according to
the annual Lazard analysis. From an LCOE of US$275/MWh in 2009 in the
United States the cost fell to US$192/MWh in 2015 and US$175/MWh in
2019. As with the natural gas-fired combined cycle plant discussed above, the
fall in the cost of electricity from these plants is mostly attributable to the fall
in the cost of natural gas in the United States. The IEA does not consider open
cycle gas turbines in its five yearly reports, but the LCOE from these units in
other parts of the world will likely be higher than in the United States.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power is a controversial technology that is being promoted as a carbon
free source of power in some constituencies but at the same time is being
phased out elsewhere. While old nuclear power plants that have had their
construction costs paid down can be a cheap source of power, the economic
argument revolves around the cost of new nuclear power. Table 9.8 presents
Lazard figures for the levelized cost of new nuclear power in the United States
between 2009 and 2019. In 2009, the estimated LCOE was US$123/MWh.
This fell sharply in 2010 to US$96/MWh but by 2015, it had risen to US$117/
MWh and in 2019 it was US$155/MWh. This puts nuclear power at a significant disadvantage compared to both coal-fired technology and natural
gasefired combined cycle technology. The rise in LCOE reflects a steep rise in
the capital cost of nuclear power in the United States.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The IEA in its report of 2015 estimated the LCOE of nuclear power in the
United States to be US$54/MWh at a 3% discount rate and US$102/MWh at a
10% discount rate. Costs for new plants in Europe were comparable. However
for two Asian nations the costs were estimated to be much lower. In South
Korea, the estimated LCOE in 2015 was US$27/MWh (3%) to US$51/MWh
(10%), while in China the estimated cost was US$26/MWh to US$49/MWh
for the cheapest plant cited. The stark variation in cost between the United
States and Europe on the one hand and South Korea and China on the other is
the result of the difference in capital costs. These are substantially lower in
both China and South Korea.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Nuclear power plant costs are also sensitive to capacity factor. Nuclear
plants have traditionally been considered as base load generators and they do
not usually operate comfortably at low capacity factors. According to IEA estimates, the LCOE for power from a US nuclear power plant operating in 2015
rose from US$54/MWh at an 85% capacity factor to US$77/MWh at a capacity
factor of 50%. This increase is typical of European nuclear plants too, but the
Asian plants (China and South Korea) were estimated to be less sensitive.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Onshore wind power plants
The LCOE for onshore wind power plants in the United States between 2009
and 2019 is shown in Table 9.9. The figures in this table show a sharp
discontinuity between 2010 and 2011. In 2009, the estimated LCOE was
US$135/MWh, and in 2010, it was US$124/MWh but in 2011 it had fallen to
US$71/MWh. After that the LCOE falls, a trend consistent with the gradual
improvement in wind power capital cost and performance over the period. In
2015 the estimated LCOE was US$55/MWh and by 2019 it had fallen to
US$41/MWh, a fall of 42% in 8 years.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The IEA 5-year report shows a wide variability in the cost of wind power in
the United States in 2015. The LCOE from the lowest cost plant cited was
estimated to be US$33/MWh (3% discount rate), while the most expensive
was US$116/MWh. (As a comparison, the Lazard analysis from 2015 showed
an overnight LCOE range of US$32e77/MWh.3
) At a discount rate of 10%,
the IEA range was US$52e188/MWh. Similar variability was reported by the
IEA elsewhere. In South Korea, for example, the LCOE for wind power was as
high as US$214/MWh (3%). Meanwhile, in China, the lowest LCOE was
US$46/MWh (3%), the next lowest cost after the USA figure.]]>
			</paragraph>
			<paragraph>
				<![CDATA[As the cost of wind power scales closely to the capital cost of building
wind power plants, this variation between and within nations reflects a similar
variation in capital costs.
The cost of electricity from wind turbines does not vary with their capacity
factor. However wind power is both intermittent and unpredictable and this
means that while it will usually be dispatched when available, there must
always be a source to replace it when the wind fails. In consequence, while the
cost of electricity from wind power plants can be extremely competitive, it is
considered less valuable to the grid.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Offshore wind farms are more expensive to develop than similar facilities
onshore. Against this, they usually offer a better wind regime and plants of
large aggregate capacity can be developed. As with onshore wind, the LCOE
of electricity from these plants will scale with the capital cost of their
development. For example, Bloomberg New Energy Finance estimated the
average global LCOE for offshore wind in 2019 to be US$78/MWh.4 Most
offshore wind capacity is located in European waters.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar photovoltaic power plants
The cost of electricity from solar PV power plants has shown the most dramatic change over the 10 years from 2009 to 2019 of any generating technology. This was highlighted in Table 9.4 and is emphasised again with the
figures from Lazard in Table 9.10. In 2009, the estimated average LCOE from
US utility solar PV plants was US$359/MWh, the highest of all the main
power generating technologies. This fell sharply in the succeeding years, to
US$248/MWh in 2010 and US$157/MWh in 2011. By 2015, the estimated
average LCOE for US solar PV was US$65/MWh, and in 2019, it had fallen to
US$40/MWh, making it the least cost source of all the primary technologies.]]>
			</paragraph>
			<paragraph>
				<![CDATA[This dramatic fall in the cost of solar electricity is a result of a massive fall
in the cost of solar cells. During the decade from 2009, these devices became
global commodities with cells manufactured in China particularly competitive.
While the rate at which costs are falling has slowed, there may still be room
for a further decrease.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The LCOE figures in Table 9.10 are for utility-scale solar PV plants.
A large proportion of solar PV installations are on rooftops. These tend to be of
smaller capacity than the utility plants and the costs are significantly higher.
For example, the LCOE estimates from the 2019 Lazard analysis puts the cost
of a commercial and industrial rooftop facility at around twice the cost of a
utility-scale installation. A domestic rooftop PV installation is around twice as
much again. At the same time, the cost of electricity to these consumers is
much higher than the wholesale price of power. For example, in the United
States, in June 2019, the average retail cost of electricity to a US consumer
was US$134/MW while for a commercial customer it was US$109/MWh.5
With consumer prices at these levels, rooftop solar PV installations can provide competitively prices electricity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The IEA 5-year report provides LCOE estimates from 2015 for a range of
solar PV installations across the globe.6 For a large US utility-scale plant, the
LCOE was US$54/MWh at 3% discount rate and US$103/MWh at a 10%
discount rate. The Chinese LCOE was similar at US$55/MWh (3%) and
US$87/MWh (10%). In comparison, the LCOE for a French plant of US$104/
MWh was high, as it was in South Korea at US$102/MWh, both at 3% discount rate]]>
			</paragraph>
			<paragraph>
				<![CDATA[For domestic rooftop installations, the LCOE in the United States was
US$106/MWh (3%) while in France it was US$214/MWh and in South Korea
it was US$156/MWh. The wide variation in prices reflects the differing local
markets for solar cells. It should also be remembered that the steep fall in the
cost of solar cells means that prices for all these categories at the beginning of
the third decade of the 21st century are likely to be much lower.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Solar thermal power plants are hybrid power generators that utilise the heat
from the sun to drive a thermodynamic engine, usually a steam turbine, to
provide electricity. As such they are much more complex than solar PV plants
and capital costs are significantly higher.
Table 9.11 presents figures from Lazard for the LCOE for a US solar
thermal power plant between 2009 and 2019. The figures in the table show no
consistent trend. The estimated cost of electricity in 2009 was US$168/MWh.
This had fallen to US$124/MWh by 2014, but the cost rose in succeeding
years and then fell again, so that in 2019 the LCOE was US$141/MWh.]]>
			</paragraph>
			<paragraph>
				<![CDATA[These figures suggest that solar thermal electricity is relatively expensive.
However, a solar thermal plant can include energy storage, allowing the plant
to supply electricity at night as well as during the day. This makes the power
from these plants more readily dispatchable and therefore more valuable.
The IEA 5-year report from 2015 contains some LCOE estimates for solar
thermal power plants. In the United States, the LCOE for a solar thermal plant
with 6h of energy storage was US$79/MWh (3% discount rate) while for a
plant with 12h or energy storage it was US$66/MWh. For a plant with storage
in South Africa (storage capacity not quoted), the LCOE was US$139/MWh,
while in Spain for a plant without storage capacity it was US$263/MWh. This
is a developing technology and prices may fall much lower if the global
installation volume was to rise. However, these plants are currently only
chosen in exceptional circumstances.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Geothermal power plants are relatively cheap to build, but the cost of prospecting for underground geothermal reservoirs suitable to power a generating
plant usually increases the capital cost of development and this affects the
LCOE from such plants. On the other hand if a resource, once found, is
managed well then once capital costs are paid down these plants become a
cheap source of electricity.]]>
			</paragraph>
			<paragraph>
				<![CDATA[Table 9.12 contains LCOE estimates for new geothermal power plants in
the United States between 2009 and 2019. The first figure in the table appears
anomalous. The LCOE for a US geothermal plant in 2010 was estimated to be
US$107/MWh. This rose to US$116/MWh in 2012 but started to fall in 2015
and by 2019 the LCOE was estimated to be US$91/MWh. These estimates
make geothermal power relatively expensive compared with other renewable
technologies. Nevertheless, where geothermal reserves suitable for power
generation exist, it has often proved economical to exploit.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The 2015 IEA report on the Projected Costs of Generating Electricity
contains a small number of LCOE estimates for geothermal electricity in
countries that have useful geothermal resources. In the United States, the
LCOE was US$55/MWh at 3% discount rate and US$99/MWh at a discount
rate of 10%. Turkey is another country that has exploited geothermal energy.
The LCOE range there from the 2015 report was US$109/MWh (3%) to
US$123/MWh (10%). In Italy, the range was US$60/MWheUS$100/MWh.]]>
			</paragraph>
			<paragraph>
				<![CDATA[The cost of electricity from hydropower plants depends on the capital cost,
which is usually relatively high. Against that, many hydropower plants have
extremely long lifetimes and once their capital outlay is paid off the power
they generate is at a low cost. The 2015 IEA report contains some LCOE
estimates for hydropower. These show a wide variation. In Portugal, for
example, the LCOE for power from a 144 MW dam and reservoir plant was
estimated to be US$90/MWh at a 3% discount rate, rising to US$284/MWh at
a discount rate of 10%. In Turkey, the LCOE for power from a 20 MW hydropower plant was US$30/MWh at 3% discount rate and US$54/MWh at
10%. For a 1000 MW pumped storage project in Switzerland, the equivalent
costs were US$36/MWh (3%) and US$107/MWh (10%). These figures can be
compared with the figures in Table 9.4, which show the global average LCOE
for hydropower in 2015 to be US$39/MWh (but in 2016, it was US$55/MWh).]]>
			</paragraph>
			<paragraph>
				<![CDATA[The LCOE for a 100 MW biomass plant in the United States, from the IEA
report of 2015,7 was US$99/MWh at 3% discount rate and US$138/MWh at
10%. For a 10 MW plant in Spain, the LCOE was US$152/MWh (3%) and
US$190/MWh (10%). Biomass is a combustion technology but the plants are
small compared with typical fossil fuel plants and efficiencies are much lower.
This leads to higher capital costs, reflected in these figures.
The wide variation in the cost of electricity from some of the technologies
discussed above makes it important to take into account important regional
factors such as variations in the cost of loans, different local legislation and the
different cost of commodities and labour in different countries and regions.
The LCOE provides a convenient metric for comparing the future cost of
electricity but it must be used wisely.]]>
			</paragraph>
		</content>
	</book>
</books>